:::MLL 1560880567.058113927 submission_org: {"value": "Intel_Corp", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.060469736 submission_platform: {"value": "32xCLX-8260L_CPUs", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.062170931 submission_division: {"value": "closed", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.063802382 submission_status: {"value": "onprem", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.065492505 submission_benchmark: {"value": "minigo", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.067207105 submission_poc_name: {"value": "Guokai Ma, Letian Kang, Christine Cheng, Mingxiao Huang", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.068825659 submission_poc_email: {"value": "guokai.ma@intel.com, letian.kang@intel.com, christine.cheng@intel.com, mingxiao.huang@intel.com", "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880567.070488091 submission_entry: {"value": {"framework": "TensorFlow 1.13.1", "power": "none", "notes": "none", "interconnect": "OPA", "os": "Oracle Linux Server 7.6", "libraries": "MKLDNN (v0.18), MKL (v2019.0.3.20190220), IntelMPI (2018.1.163)", "compilers": "GCC6.3", "nodes": [{"num_nodes": 32, "cpu": "Intel(R) Xeon(R) Platinum 8260L CPU @ 2.40GHz", "num_cores": 48, "num_vcpus": "NA", "accelerator": "NA", "num_accelerators": 0, "sys_mem_size": "192G", "sys_storage_type": "SSD", "sys_storage_size": "800G", "cpu_accel_interconnect": "100Gb OPA", "network_card": "100Gb OPA", "num_network_cards": 1, "notes": "NA"}]}, "metadata": {"lineno": 0, "file": "manual"}}
:::MLL 1560880572.043318000 cache_clear: {value: true, metadata: {lineno: 0, file: manual}}
~/submission/benchmarks/minigo/implementations/tensorflow ~/submission/benchmarks/minigo/clx-8260l-2s-x32
Physical cores = 48
Virtual cores = 96
NUMA cores = 24
KMP_HW_SUBSET = 2T
Output to /lfs/lfs12/gma_akey
./run_mn.sh: line 20: ulimit: max user processes: cannot modify limit: Operation not permitted
Wiping dir /lfs/lfs12/gma_akey/results/epb002
:::MLL 1560880579.252790 init_start: {"value": null, "metadata": {'lineno': 742, 'file': 'ml_perf/reference_implementation.py'}}
Making dir /lfs/lfs12/gma_akey/results/epb002/models
Making dir /lfs/lfs12/gma_akey/results/epb002/data/selfplay
Making dir /lfs/lfs12/gma_akey/results/epb002/data/holdout
Making dir /lfs/lfs12/gma_akey/results/epb002/sgf/eval
Making dir /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks
Making dir /lfs/lfs12/gma_akey/results/epb002/work_dir
Making dir /lfs/lfs12/gma_akey/results/epb002/mpi
[2019-06-18 11:56:23] Selfplay nodes = ['epb002', 'epb001', 'epb054', 'epb053', 'epb052', 'epb051', 'epb050', 'epb049', 'epb080', 'epb081', 'epb082', 'epb084', 'epb085', 'epb086', 'epb087', 'epb088', 'epb089', 'epb030', 'epb031', 'epb032', 'epb033', 'epb034', 'epb035', 'epb036', 'epb037', 'epb038']
[2019-06-18 11:56:23] Train nodes = ['epb039', 'epb047', 'epb046', 'epb045', 'epb044', 'epb043']
[2019-06-18 11:56:23] Eval nodes = ['epb002', 'epb001', 'epb054', 'epb053', 'epb052', 'epb051', 'epb050', 'epb049', 'epb080', 'epb081', 'epb082', 'epb084', 'epb085', 'epb086', 'epb087', 'epb088', 'epb089', 'epb030', 'epb031', 'epb032', 'epb033', 'epb034', 'epb035', 'epb036', 'epb037', 'epb038']
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.38s/it]
[2019-06-18 11:59:18] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/tools/strip_unused_lib.py:86: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
[2019-06-18 11:59:18] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.remove_training_nodes`
2019-06-18 11:59:18.074389: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-06-18 11:59:18.088505: I tensorflow/core/common_runtime/process_util.cc:113] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
[2019-06-18 11:59:18] From ./quantize_graph.py:351: quantize_v2 (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.
Instructions for updating:
`tf.quantize_v2` is deprecated, please use `tf.quantization.quantize` instead.
2019-06-18 11:59:18.429539: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
[2019-06-18 11:59:18] From ./dual_net.py:679: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99']
Reading tf_records from 1 inputs
[2019-06-18 11:59:22] minmax time: 3.826 seconds
2019-06-18 11:59:22.266073: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 11:59:22.271487: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 11:59:22.276091: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880762.360468 init_stop: {"value": null, "metadata": {'lineno': 614, 'file': 'ml_perf/reference_implementation.py'}}
:::MLL 1560880762.360843 run_start: {"value": null, "metadata": {'lineno': 615, 'file': 'ml_perf/reference_implementation.py'}}
:::MLL 1560880762.361250 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 0}}
[2019-06-18 11:59:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=2 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir 
[2019-06-18 11:59:22] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-2-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=2 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=1023779833 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=2047559664 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=3071339495 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=4095119326 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=5118899157 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=6142678988 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=7166458819 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=8190238650 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=9214018481 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=10237798312 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=11261578143 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=12285357974 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=13309137805 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=14332917636 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=15356697467 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=16380477298 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=17404257129 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=18428036960 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000001-000000 --seed=19451816791 : \
-host epb0
[2019-06-18 11:59:56] selfplay finished: 34.204 seconds
[2019-06-18 11:59:56] selfplay mn: 34.227 seconds
[2019-06-18 11:59:56] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-2-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779833 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559664 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339495 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119326 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899157 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678988 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458819 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238650 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018481 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798312 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578143 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357974 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137805 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917636 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697467 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477298 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257129 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036960 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816791 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596622 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376453 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156284 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000001-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_
[2019-06-18 12:00:17] divide_golden_chunk finished: 21.283 seconds
[2019-06-18 12:00:17] generate golden chunk: 21.299 seconds
[2019-06-18 12:00:23] train finished: 60.854 seconds
:::MLL 1560880782.384573 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.385010 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.385401 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.908252 47548204790656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.388101 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.388503 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.388827 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.908263 47544977605504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.323831 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.324564 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.325260 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.908302 47989174203264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.326258 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.327040 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.327728 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.908300 47066358723456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:42.909307 47548204790656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkwtkz1gy
W0618 11:59:42.909333 47544977605504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0dfvm2uq
W0618 11:59:42.909377 47066358723456 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpm1bey6pf
W0618 11:59:42.909412 47989174203264 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxyfm2bx3
I0618 11:59:42.910393 47548204790656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkwtkz1gy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3ef6718e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.910408 47544977605504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0dfvm2uq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3e36168e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.910444 47066358723456 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpm1bey6pf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acec62e0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.910501 47989174203264 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxyfm2bx3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba5a244ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.910833 47548204790656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.910833 47544977605504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.910880 47066358723456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.910937 47989174203264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:42.923882 47548204790656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.923867 47544977605504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.923904 47989174203264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.923887 47066358723456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.946140 47066358723456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:42.946173 47548204790656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:42.946274 47544977605504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:42.946450 47989174203264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880782.416005 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.416425 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.416803 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.971574 48006971331456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.418032 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.418383 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.418711 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.971596 47587518894976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.374455 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.375419 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.376254 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.971751 47563639690112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.382596 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.383362 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.384162 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.971789 47590728532864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:42.972263 48006971331456 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpv7k5xy4a
W0618 11:59:42.972288 47587518894976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpt5eyj46f
I0618 11:59:42.972979 47587518894976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpt5eyj46f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b481dbf0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.972980 48006971331456 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpv7k5xy4a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba9c70f6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.973292 47587518894976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.973299 48006971331456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:42.972816 47563639690112 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmptqdgep8o
W0618 11:59:42.972841 47590728532864 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppsu97ary
I0618 11:59:42.973852 47563639690112 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmptqdgep8o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b428e6f6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.973855 47590728532864 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppsu97ary', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b48dd0e5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.974278 47563639690112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.974279 47590728532864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880782.483176 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.483616 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.484002 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.975071 47960788190080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.479119 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.479629 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.480126 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.975098 47282953782144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.398664 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.399393 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.400111 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.975400 47167644414848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.384108 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.385049 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.385908 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.975441 47446510216064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.430332 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.430710 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.431030 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.976976 47618855736192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.388251 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.388969 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.389663 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.977004 47564358783872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.428958 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.429344 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.429672 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.976994 47360029700992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.390516 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.391257 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.391918 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.977018 47116900627328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:42.976166 47282953782144 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpe4ykqf1h
I0618 11:59:42.976197 47960788190080 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9f06549cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.977232 47282953782144 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpe4ykqf1h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0134405e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.977406 47960788190080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:42.976692 47167644414848 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0btxrmvq
W0618 11:59:42.976742 47446510216064 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxim0luou
I0618 11:59:42.977669 47282953782144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.977851 47167644414848 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0btxrmvq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae65b46fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.977876 47446510216064 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxim0luou', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2748f97e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.978348 47167644414848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.978364 47446510216064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:42.978044 47618855736192 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpa0u22csh
W0618 11:59:42.978082 47564358783872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpoaledbvi
W0618 11:59:42.978112 47116900627328 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpawdr8jgd
W0618 11:59:42.978142 47360029700992 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpu9jhnhpz
I0618 11:59:42.979119 47618855736192 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpa0u22csh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4f69916e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.979154 47564358783872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpoaledbvi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b42b94bde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.979166 47116900627328 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpawdr8jgd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada8ab63e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.979228 47360029700992 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpu9jhnhpz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1326558e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.979566 47618855736192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.979608 47116900627328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.979602 47564358783872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.979678 47360029700992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:42.986472 47446510216064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.986532 47960788190080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.986520 47282953782144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.986620 47167644414848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.988663 47116900627328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.988662 47564358783872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.988679 47618855736192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.988748 47360029700992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.989079 47563639690112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.989072 47590728532864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880782.495179 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.495614 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.495986 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.989314 47829444375424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.409028 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.409923 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.410808 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.989331 47520367764352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.417553 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.418339 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.419045 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.989326 46962441782144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.491794 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.492308 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.492714 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.989408 47669770679168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:42.990466 47587518894976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:42.990423 47829444375424 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp38lf0309
W0618 11:59:42.990464 47520367764352 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpb9w7o526
W0618 11:59:42.990490 46962441782144 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpdbgiibff
W0618 11:59:42.990515 47669770679168 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7icyg4bj
I0618 11:59:42.991526 47829444375424 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp38lf0309', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8071a0fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.991547 47520367764352 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpb9w7o526', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b387b3a2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.991566 46962441782144 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpdbgiibff', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab6943f4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.991579 47669770679168 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7icyg4bj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5b4455ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 11:59:42.991567 48006971331456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 11:59:42.991976 47829444375424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.991979 47520367764352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.992003 46962441782144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.992022 47669770679168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880782.405146 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.406078 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.406940 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.995415 46943571153792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.484534 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.484979 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.485370 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.995416 47561576436608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.412332 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.413106 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.413772 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.995448 47393388770176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
:::MLL 1560880782.484550 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880782.484988 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880782.485375 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 11:59:42.995459 47783559025536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_99 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz_0
W0618 11:59:42.996518 47393388770176 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp14hum28p
W0618 11:59:42.996544 46943571153792 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpoj_l6jho
W0618 11:59:42.996605 47783559025536 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmp8b9453
W0618 11:59:42.996579 47561576436608 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_jdb0f7q
I0618 11:59:42.997611 47393388770176 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp14hum28p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1aeab05e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.997624 46943571153792 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpoj_l6jho', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab22f786e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.997654 47561576436608 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_jdb0f7q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4213748e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.997681 47783559025536 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmp8b9453', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b75c2a61e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 11:59:42.998052 47393388770176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.998060 46943571153792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.998092 47561576436608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 11:59:42.998128 47783559025536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 11:59:43.001132 47669770679168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:43.001129 46962441782144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:43.001141 47520367764352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:43.001160 47829444375424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:43.007594 46943571153792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:43.007605 47393388770176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:43.007691 47446510216064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.007693 47561576436608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:43.007715 47783559025536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 11:59:43.007727 47066358723456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:43.007753 47544977605504 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:43.008072 47167644414848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.008856 47282953782144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.009070 47960788190080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.010227 47563639690112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.010229 47590728532864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.010675 47618855736192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.010943 47116900627328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.010954 47564358783872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.010546 47989174203264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:43.011019 47360029700992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.012303 47587518894976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.012606 47066358723456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:43.012679 47544977605504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:43.012747 47548204790656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 11:59:43.013413 48006971331456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.015515 47989174203264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 11:59:43.017644 47548204790656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 11:59:43.018353 47066358723456 estimator.py:1111] Calling model_fn.
W0618 11:59:43.018473 47066358723456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 11:59:43.018459 47544977605504 estimator.py:1111] Calling model_fn.
W0618 11:59:43.018579 47544977605504 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:43.020048 47066358723456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:43.020186 47544977605504 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 11:59:43.021330 47989174203264 estimator.py:1111] Calling model_fn.
W0618 11:59:43.021457 47989174203264 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 11:59:43.023370 47520367764352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.023411 46962441782144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.023443 47669770679168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 11:59:43.023051 47989174203264 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 11:59:43.023634 47829444375424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 11:59:43.023414 47548204790656 estim[2019-06-18 12:00:23] iteration time 0: 60.881 seconds
2019-06-18 12:00:23.605805: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880823.242528 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 1}}
[2019-06-18 12:00:26] minmax time: 3.224 seconds
2019-06-18 12:00:26.839351: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:00:26.844718: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:00:26.849397: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880826.860649 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 0}}
[2019-06-18 12:00:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000002-000001 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=3 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir 
[2019-06-18 12:00:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-2-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=2 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=1023779833 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=2047559664 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=3071339495 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=4095119326 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=5118899157 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=6142678988 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=7166458819 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=8190238650 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=9214018481 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=10237798312 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=11261578143 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=12285357974 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=13309137805 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=14332917636 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=15356697467 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=16380477298 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=17404257129 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=18428036960 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=19451816791 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/checkpoint.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000001-000001 --seed=20475596622 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_
[2019-06-18 12:00:39] eval finished: 13.002 seconds
[2019-06-18 12:00:39] Win rate 000001-000001 vs checkpoint: 0.620
:::MLL 1560880839.928559 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 0}}
[2019-06-18 12:00:39] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-3-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=3 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=1023779834 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=2047559665 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=3071339496 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=4095119327 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=5118899158 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=6142678989 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=7166458820 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=8190238651 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=9214018482 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=10237798313 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=11261578144 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=12285357975 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=13309137806 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=14332917637 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=15356697468 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=16380477299 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=17404257130 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000002-000000 --seed=18428036961 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/
[2019-06-18 12:01:09] selfplay finished: 29.944 seconds
[2019-06-18 12:01:09] selfplay mn: 29.962 seconds
[2019-06-18 12:01:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-3-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779834 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559665 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339496 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119327 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899158 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678989 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458820 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238651 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018482 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798313 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578144 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357975 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137806 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917637 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697468 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477299 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257130 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036961 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816792 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596623 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376454 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156285 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000002-000000/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_
[2019-06-18 12:01:10] train finished: 44.056 seconds
:::MLL 1560880832.142414 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.143310 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.144187 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.211497 47638872216448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.153569 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.154367 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.155121 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.211524 47312692007808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.212625 47638872216448 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4y3nldr1
W0618 12:00:32.212651 47312692007808 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp368o_zyx
I0618 12:00:32.213722 47638872216448 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4y3nldr1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5412a4ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.213741 47312692007808 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp368o_zyx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0820c9ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.214168 47638872216448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.214183 47312692007808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.219750 47638872216448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.219767 47312692007808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.241879 47638872216448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.242220 47312692007808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880832.181460 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.182370 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.183211 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.252164 47736613389184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.190102 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.190874 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.191623 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.252209 47461579920256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.253180 47736613389184 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpanvrje0z
W0618 12:00:32.253204 47461579920256 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpg1e1aohv
I0618 12:00:32.254255 47461579920256 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpg1e1aohv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2acb32ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.254256 47736613389184 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpanvrje0z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ad4789e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.254698 47461579920256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.254703 47736613389184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880832.225771 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.226151 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.226478 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.257136 47178800984960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.222956 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.223405 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.223733 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.257150 47521861579648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.258173 47521861579648 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3punlia0
W0618 12:00:32.258142 47178800984960 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpswiod01v
I0618 12:00:32.259100 47178800984960 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpswiod01v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae8f442ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.259144 47521861579648 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3punlia0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b38d4440e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.259540 47178800984960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.259576 47521861579648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.260021 47461579920256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.260021 47736613389184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.264217 47178800984960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.264257 47521861579648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880832.249842 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.250285 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.250685 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.279024 47242747655040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.210810 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.211735 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.212614 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.279676 47055308850048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.231361 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.232145 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.232915 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.279778 47708979692416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.253125 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.253561 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.253947 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.279989 47754483417984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.280019 47242747655040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqfs1s80_
I0618 12:00:32.280982 47242747655040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqfs1s80_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af7d7c78e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:32.280663 47055308850048 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp64yj4k4y
I0618 12:00:32.281367 47242747655040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.280792 47708979692416 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp774j5_a_
I0618 12:00:32.281714 47055308850048 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp64yj4k4y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acc338e5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:00:32.280934 47754483417984 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp176mkqv7
I0618 12:00:32.281823 47708979692416 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp774j5_a_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b64655fce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.281903 47754483417984 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp176mkqv7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6efd9b9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.282128 47055308850048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.282247 47708979692416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.282294 47754483417984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.282567 47461579920256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.282682 47736613389184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.283986 47178800984960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.284088 47521861579648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.286121 47242747655040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.286893 47754483417984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.287403 47055308850048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.287527 47708979692416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880832.220396 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.221343 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.222216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.290693 47036334764928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.235657 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.236472 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.237246 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.290739 47254298461056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.291759 47254298461056 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpndk9fc5t
W0618 12:00:32.291729 47036334764928 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphze8s2ux
I0618 12:00:32.292806 47036334764928 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphze8s2ux', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac7c89cce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.292826 47254298461056 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpndk9fc5t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afa8842de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.293232 47036334764928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.293248 47254298461056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.294158 47638872216448 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.294476 47312692007808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.298460 47036334764928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.298528 47254298461056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.298802 47638872216448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:32.299132 47312692007808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:00:32.304245 47638872216448 estimator.py:1111] Calling model_fn.
W0618 12:00:32.304360 47638872216448 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:32.304613 47312692007808 estimator.py:1111] Calling model_fn.
W0618 12:00:32.304728 47312692007808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:32.305987 47242747655040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.305813 47638872216448 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:32.306553 47754483417984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.306190 47312692007808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:32.308643 47055308850048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.308773 47708979692416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880832.243825 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.244725 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.245598 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.311351 47662034019200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.249329 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.250096 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.250775 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.311350 47241785922432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.312332 47662034019200 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppxjv6a6w
W0618 12:00:32.312355 47241785922432 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9km4amje
I0618 12:00:32.313339 47241785922432 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9km4amje', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af79e748dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.313338 47662034019200 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppxjv6a6w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b597731bdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.313735 47241785922432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.313759 47662034019200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880832.292625 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.293084 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.293488 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.315391 47907416793984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.316365 47907416793984 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8nfjwr_h
I0618 12:00:32.317322 47907416793984 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8nfjwr_h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b929925ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.317744 47907416793984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.318785 47036334764928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.318831 47241785922432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.318878 47662034019200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.319140 47254298461056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880832.278623 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.279379 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.280126 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.320956 46926249513856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.256787 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.257711 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.258535 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.321026 47898453238656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.322381 47907416793984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880832.300820 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.301262 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.301663 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.322227 47354072740736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.321942 46926249513856 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpts800adh
W0618 12:00:32.321989 47898453238656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpdqne3x0n
I0618 12:00:32.322939 46926249513856 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpts800adh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae27051e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.322991 47898453238656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpdqne3x0n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9082e0be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.323335 46926249513856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.323379 47898453238656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.323189 47354072740736 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkngbbfb3
I0618 12:00:32.324147 47354072740736 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkngbbfb3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b11c3458e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.324543 47354072740736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.328425 46926249513856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.328445 47898453238656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.329051 47354072740736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.331433 47178800984960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.332415 47521861579648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.333694 47736613389184 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.333866 47461579920256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880832.306114 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.306527 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.306897 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.335263 47201337832320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
:::MLL 1560880832.306038 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880832.306454 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880832.306829 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:00:32.335413 47280865108864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz_0
W0618 12:00:32.335739 47178800984960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:32.336236 47201337832320 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphetvgo2c
W0618 12:00:32.336793 47521861579648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:32.336364 47280865108864 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpw479o_um
I0618 12:00:32.337204 47201337832320 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphetvgo2c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aee338fce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.337328 47280865108864 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpw479o_um', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b00b7c1be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:00:32.337594 47201337832320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:00:32.337715 47280865108864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:00:32.338310 47736613389184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:32.338513 47461579920256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:32.338721 47241785922432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.338988 47662034019200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:00:32.340775 47178800984960 estimator.py:1111] Calling model_fn.
W0618 12:00:32.340880 47178800984960 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:32.342103 47907416793984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:00:32.341944 47521861579648 estimator.py:1111] Calling model_fn.
W0618 12:00:32.342058 47521861579648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:32.342276 47280865108864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.342216 47201337832320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:00:32.342226 47178800984960 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:00:32.343740 47736613389184 estimator.py:1111] Calling model_fn.
W0618 12:00:32.343429 47521861579648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:32.343855 47736613389184 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:00:32.343991 47461579920256 estimator.py:1111] Calling model_fn.
W0618 12:00:32.344105 47461579920256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:00:32.345296 47736613389184 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:32.345557 47461579920256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:00:32.348252 46926249513856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.348409 47898453238656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.348731 47354072740736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:00:32.354122 47242747655040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.354194 47754483417984 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.357383 47055308850048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.357439 47708979692416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:00:32.358522 47754483417984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:00:32.358493 47242747655040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/es[2019-06-18 12:01:13] divide_golden_chunk finished: 3.301 seconds
[2019-06-18 12:01:13] generate golden chunk: 3.315 seconds
[2019-06-18 12:01:13] moving /lfs/lfs12/gma_akey/results/epb002/models/000002-000001.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000002-000002.meta
[2019-06-18 12:01:13] moving /lfs/lfs12/gma_akey/results/epb002/models/000002-000001.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000002-000002.data-00000-of-00001
[2019-06-18 12:01:13] moving /lfs/lfs12/gma_akey/results/epb002/models/000002-000001.index --> /lfs/lfs12/gma_akey/results/epb002/models/000002-000002.index
[2019-06-18 12:01:13] moving /lfs/lfs12/gma_akey/results/epb002/models/000002-000001.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb
[2019-06-18 12:01:13] iteration time 1: 50.007 seconds
2019-06-18 12:01:13.647910: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880873.249602 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 2}}
[2019-06-18 12:01:16] minmax time: 3.230 seconds
2019-06-18 12:01:16.887715: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:01:16.893399: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:01:16.898024: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880876.907802 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 1}}
[2019-06-18 12:01:16] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000003-000002 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=4 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir 
[2019-06-18 12:01:16] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-3-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=3 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=1023779834 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=2047559665 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=3071339496 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=4095119327 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=5118899158 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=6142678989 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=7166458820 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=8190238651 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=9214018482 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=10237798313 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=11261578144 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=12285357975 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=13309137806 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=14332917637 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=15356697468 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=16380477299 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=17404257130 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=18428036961 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=19451816792 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000002-000002 --seed=20475596623 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:01:27] eval finished: 10.981 seconds
[2019-06-18 12:01:27] Win rate 000002-000002 vs 000001-000001: 0.590
:::MLL 1560880887.952260 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 1}}
[2019-06-18 12:01:27] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-4-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=4 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=1023779835 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=2047559666 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=3071339497 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=4095119328 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=5118899159 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=6142678990 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=7166458821 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=8190238652 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=9214018483 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=10237798314 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=11261578145 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=12285357976 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=13309137807 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=14332917638 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=15356697469 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=16380477300 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=17404257131 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000003-000001 --seed=18428036962 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/
[2019-06-18 12:01:58] selfplay finished: 30.926 seconds
[2019-06-18 12:01:58] selfplay mn: 30.943 seconds
[2019-06-18 12:01:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-4-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779835 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559666 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339497 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119328 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899159 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678990 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458821 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238652 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018483 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798314 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578145 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357976 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137807 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917638 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697469 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477300 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257131 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036962 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816793 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596624 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376455 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156286 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000003-000001/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_
[2019-06-18 12:02:01] train finished: 44.805 seconds
:::MLL 1560880882.131882 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.132613 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.133297 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.197983 47796436554624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880882.128140 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.129062 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.129735 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.198131 47805784593280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.199060 47796436554624 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp52ftj1fi
W0618 12:01:22.199185 47805784593280 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpk0hue5_e
I0618 12:01:22.200171 47796436554624 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp52ftj1fi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b78c235ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.200284 47805784593280 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpk0hue5_e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7aef656e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.200636 47796436554624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:22.200722 47805784593280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:22.206110 47796436554624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.206125 47805784593280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.228881 47805784593280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.228935 47796436554624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880882.205331 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.205778 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.206108 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.239714 47227693863808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.240787 47227693863808 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpov6o2p3p
I0618 12:01:22.241862 47227693863808 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpov6o2p3p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af45680ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.242300 47227693863808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880882.208347 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.208789 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.209169 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.245051 47552004117376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.246089 47552004117376 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmplyu3j4we
I0618 12:01:22.247147 47552004117376 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmplyu3j4we', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3fd8e6ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:22.247456 47227693863808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:01:22.247556 47552004117376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:22.252373 47552004117376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880882.189299 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.190232 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.191133 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.263039 47075378152320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880882.203642 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.204444 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.205166 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.263055 46940046533504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.264178 46940046533504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6t9lc2xe
W0618 12:01:22.264206 47075378152320 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7o9zb5a4
I0618 12:01:22.265284 46940046533504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6t9lc2xe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab15d62ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.265328 47075378152320 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7o9zb5a4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad0dfc79e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.265734 46940046533504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:22.265811 47075378152320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:22.268426 47227693863808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.271079 46940046533504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.271201 47075378152320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.273063 47552004117376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.282720 47805784593280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:22.282990 47796436554624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880882.213294 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.214055 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.214751 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.286708 47570655024000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880882.207429 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.208308 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.209169 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.286714 47520256701312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.287632 47805784593280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:22.287869 47796436554624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:22.287839 47570655024000 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5yir4b5r
W0618 12:01:22.287876 47520256701312 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpyowyd1cf
I0618 12:01:22.288924 47570655024000 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5yir4b5r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b443094ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.288993 47520256701312 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpyowyd1cf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b38749b8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.289375 47570655024000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:22.289442 47520256701312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:22.293678 46940046533504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.293857 47075378152320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:01:22.293432 47805784593280 estimator.py:1111] Calling model_fn.
W0618 12:01:22.293552 47805784593280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:22.293600 47796436554624 estimator.py:1111] Calling model_fn.
W0618 12:01:22.293720 47796436554624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:22.294716 47520256701312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.294766 47570655024000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880882.261133 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.261577 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.261965 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.294733 47027580347264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.295068 47805784593280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:22.295254 47796436554624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:22.295806 47027580347264 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpg6arrv5q
I0618 12:01:22.296860 47027580347264 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpg6arrv5q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac5becefe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560880882.265356 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.265806 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.266202 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.297089 47739392045952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
I0618 12:01:22.297278 47027580347264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:22.298104 47739392045952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmph3k2i0d7
I0618 12:01:22.299182 47739392045952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmph3k2i0d7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6b7a177e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.299614 47739392045952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880882.225921 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.226829 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.227513 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.301603 47726065472384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880882.229642 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.230349 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.231024 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.301759 47613983773568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.302259 47027580347264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.302709 47726065472384 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpd2lag3pw
W0618 12:01:22.302808 47613983773568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9hnxy0gc
I0618 12:01:22.303820 47726065472384 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpd2lag3pw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b685fc42e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.303920 47613983773568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9hnxy0gc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4e472d2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:22.304419 47739392045952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:01:22.304261 47726065472384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:22.304367 47613983773568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880882.257308 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.257719 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.258065 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.305301 47079872029568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880882.256619 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.257054 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.257411 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.305404 47130817876864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.306384 47079872029568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppgtga85k
W0618 12:01:22.306463 47130817876864 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpheast_az
I0618 12:01:22.307426 47079872029568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppgtga85k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad1eba2be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.307492 47130817876864 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpheast_az', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2addc83eae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.307849 47079872029568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:22.307909 47130817876864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:22.309756 47726065472384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.309792 47613983773568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.312787 47079872029568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.312834 47130817876864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.316019 47520256701312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.316203 47570655024000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.316008 47227693863808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:22.320254 47552004117376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:22.320297 47227693863808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:22.322441 47027580347264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.324268 47739392045952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.324556 47552004117376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:01:22.325369 47227693863808 estimator.py:1111] Calling model_fn.
W0618 12:01:22.325479 47227693863808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:22.326825 47227693863808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:01:22.329632 47552004117376 estimator.py:1111] Calling model_fn.
W0618 12:01:22.329742 47552004117376 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:22.331001 47726065472384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.331291 47613983773568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.331095 47552004117376 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:22.333247 47130817876864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.333247 47079872029568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880882.279813 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.280620 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.281390 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.338165 47094035780480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880882.265797 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.266751 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.267649 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.338199 47639310844800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.339303 47094035780480 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6y3jxs3k
W0618 12:01:22.339331 47639310844800 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpm3wjn69d
I0618 12:01:22.340407 47094035780480 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6y3jxs3k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad537dc6dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.340445 47639310844800 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpm3wjn69d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b542cc99dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.340856 47094035780480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:22.340875 47639310844800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880882.310390 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.310765 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.311088 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.344514 47336125969280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.345349 47075378152320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:22.345494 46940046533504 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880882.312809 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.313252 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.313650 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.346153 47070230258560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.346242 47094035780480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.346315 47639310844800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.345551 47336125969280 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1rtx20jk
I0618 12:01:22.346585 47336125969280 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1rtx20jk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0d958f8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.347007 47336125969280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:22.347165 47070230258560 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acfacf0fd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.348342 47070230258560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:22.349987 47075378152320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:22.350179 46940046533504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:22.352035 47336125969280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:01:22.353261 47070230258560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:01:22.355443 47075378152320 estimator.py:1111] Calling model_fn.
W0618 12:01:22.355561 47075378152320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:22.355681 46940046533504 estimator.py:1111] Calling model_fn.
W0618 12:01:22.355794 46940046533504 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:22.357019 47075378152320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:22.357262 46940046533504 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:01:22.363952 47520256701312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:22.364384 47570655024000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:22.368277 47520256701312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:22.368743 47570655024000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:01:22.369315 47639310844800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.369353 47094035780480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:01:22.369858 47027580347264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880882.298933 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.299714 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.300390 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.369917 47689846182784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
:::MLL 1560880882.302045 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880882.302881 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880882.303622 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:01:22.370371 47750916789120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz_0
W0618 12:01:22.371180 47739392045952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:01:22.370934 47689846182784 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp77a6a205
I0618 12:01:22.372000 47689846182784 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp77a6a205', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5ff0ed9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:01:22.371398 47750916789120 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpe602g99_
I0618 12:01:22.372440 47689846182784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:01:22.372526 47750916789120 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpe602g99_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6e29052e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:01:22.372999 47750916789120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:01:22.372878 47336125969280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:01:22.373389 47520256701312 estimator.py:1111] Calling model_fn.
W0618 12:01:22.373495 47520256701312 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:01:22.373877 47570655024000 estimator.py:1111] Calling model_fn.
W0618 12:01:22.373983 47570655024000 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:01:22.374167 47027580347264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.[2019-06-18 12:02:02] divide_golden_chunk finished: 3.414 seconds
[2019-06-18 12:02:02] generate golden chunk: 3.427 seconds
[2019-06-18 12:02:02] moving /lfs/lfs12/gma_akey/results/epb002/models/000003-000002.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000003-000003.data-00000-of-00001
[2019-06-18 12:02:02] moving /lfs/lfs12/gma_akey/results/epb002/models/000003-000002.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb
[2019-06-18 12:02:02] moving /lfs/lfs12/gma_akey/results/epb002/models/000003-000002.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000003-000003.meta
[2019-06-18 12:02:02] moving /lfs/lfs12/gma_akey/results/epb002/models/000003-000002.index --> /lfs/lfs12/gma_akey/results/epb002/models/000003-000003.index
[2019-06-18 12:02:02] iteration time 2: 49.119 seconds
2019-06-18 12:02:02.807677: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880922.368555 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 3}}
[2019-06-18 12:02:06] minmax time: 3.211 seconds
2019-06-18 12:02:06.029186: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:02:06.035001: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:02:06.039887: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880926.050337 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 2}}
[2019-06-18 12:02:06] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=5 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir 
[2019-06-18 12:02:06] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-4-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=4 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=1023779835 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=2047559666 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=3071339497 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=4095119328 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=5118899159 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=6142678990 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=7166458821 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=8190238652 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=9214018483 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=10237798314 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=11261578145 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=12285357976 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=13309137807 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=14332917638 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=15356697469 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=16380477300 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=17404257131 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=18428036962 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=19451816793 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000003-000003 --seed=20475596624 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:02:17] eval finished: 11.316 seconds
[2019-06-18 12:02:17] Win rate 000003-000003 vs 000002-000002: 0.460
:::MLL 1560880937.427944 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 2}}
[2019-06-18 12:02:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-5-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=5 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=1023779836 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=2047559667 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=3071339498 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=4095119329 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=5118899160 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=6142678991 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=7166458822 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=8190238653 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=9214018484 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=10237798315 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=11261578146 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=12285357977 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=13309137808 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=14332917639 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=15356697470 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=16380477301 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=17404257132 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000004-000002 --seed=18428036963 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/
[2019-06-18 12:02:48] selfplay finished: 30.990 seconds
[2019-06-18 12:02:48] selfplay mn: 31.010 seconds
[2019-06-18 12:02:48] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-5-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779836 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559667 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339498 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119329 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899160 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678991 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458822 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238653 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018484 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798315 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578146 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357977 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137808 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917639 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697470 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477301 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257132 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036963 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816794 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596625 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376456 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156287 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000004-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_
[2019-06-18 12:02:50] train finished: 44.417 seconds
:::MLL 1560880931.283839 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.284653 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.285414 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.330971 47720917689216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.264155 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.265038 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.265855 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.331008 46949858440064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.331970 47720917689216 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzlo75ope
W0618 12:02:11.332002 46949858440064 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpph7aalzw
I0618 12:02:11.333035 47720917689216 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzlo75ope', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b672cef2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.333072 46949858440064 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpph7aalzw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab3a638ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.333454 47720917689216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.333495 46949858440064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.338636 46949858440064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.338658 47720917689216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.359451 46949858440064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.359683 47720917689216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880931.350100 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.350478 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.350795 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.386978 47621702947712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.352059 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.352517 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.352889 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.388597 47300274664320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.388089 47621702947712 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpvsj6l3k1
I0618 12:02:11.389114 47621702947712 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpvsj6l3k1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5013465e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.389566 47621702947712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.389657 47300274664320 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmph2po24pq
I0618 12:02:11.390726 47300274664320 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmph2po24pq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b053ca7fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.391144 47300274664320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.394563 47621702947712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.396035 47300274664320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.412251 46949858440064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:11.412791 47720917689216 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:11.415540 47621702947712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.416698 46949858440064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880931.352374 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.353109 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.353878 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.416949 47240207807360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.354280 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.355015 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.355674 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.416980 47185209340800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.417113 47300274664320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.417307 47720917689216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:11.417949 47185209340800 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp03bo3qay
W0618 12:02:11.417981 47240207807360 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpuhu_udgf
I0618 12:02:11.418949 47185209340800 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp03bo3qay', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aea723a8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.418974 47240207807360 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpuhu_udgf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af740647e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.419358 47185209340800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.419376 47240207807360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.421926 46949858440064 estimator.py:1111] Calling model_fn.
W0618 12:02:11.422039 46949858440064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:02:11.422580 47720917689216 estimator.py:1111] Calling model_fn.
W0618 12:02:11.422699 47720917689216 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:11.423496 46949858440064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:11.424124 47720917689216 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:11.424439 47185209340800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.424451 47240207807360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.444271 47185209340800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.444419 47240207807360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880931.382660 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.383550 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.384248 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.452613 47278834422656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.381483 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.382380 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.383218 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.452692 47734274024320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.378767 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.379715 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.380601 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.452635 47645720056704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.394087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.394930 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.395736 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.452785 47299025548160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.453697 47278834422656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2et9_ezc
W0618 12:02:11.453788 47734274024320 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0zrgtr3d
W0618 12:02:11.453711 47645720056704 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpttxqdgny
I0618 12:02:11.454776 47278834422656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2et9_ezc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b003eb7ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:11.453901 47299025548160 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphozxk2z5
I0618 12:02:11.454786 47645720056704 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpttxqdgny', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55aace6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.454896 47734274024320 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0zrgtr3d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6a4908be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.455016 47299025548160 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphozxk2z5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04f233fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.455222 47278834422656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.455241 47645720056704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.455334 47734274024320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.455477 47299025548160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.460482 47278834422656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.460456 47645720056704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.460596 47734274024320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.460794 47299025548160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.463056 47621702947712 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:11.464235 47300274664320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880931.430864 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.431362 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.431771 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.466194 48010857595776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.467344 47621702947712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:11.467265 48010857595776 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppm0l_j5c
I0618 12:02:11.468328 48010857595776 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppm0l_j5c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2baaaeb31e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.468765 48010857595776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.468531 47300274664320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880931.437323 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.437768 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.438154 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.472013 47683996799872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
I0618 12:02:11.472393 47621702947712 estimator.py:1111] Calling model_fn.
W0618 12:02:11.472500 47621702947712 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:11.473650 48010857595776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:02:11.472993 47683996799872 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5e94472d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.473581 47300274664320 estimator.py:1111] Calling model_fn.
W0618 12:02:11.473691 47300274664320 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:11.473846 47621702947712 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:02:11.474101 47683996799872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.475048 47300274664320 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560880931.440744 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.441135 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.441473 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.477787 47475034514304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.442260 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.442690 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.443053 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.477823 47222867325824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.440637 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.441061 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.441408 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.478076 47468394718080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.441945 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.442328 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.442684 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.478195 47533147042688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.478667 47683996799872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.478794 47475034514304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp68lhrm4t
W0618 12:02:11.478822 47222867325824 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpus1j2j32
I0618 12:02:11.479751 47475034514304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp68lhrm4t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ded27be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.479774 47222867325824 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpus1j2j32', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af336d1ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.480134 47475034514304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.480161 47222867325824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.479129 47468394718080 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpw3i3j1hl
W0618 12:02:11.479191 47533147042688 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4u658ir7
I0618 12:02:11.480208 47468394718080 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpw3i3j1hl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c61647e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.480259 47533147042688 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4u658ir7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3b74ee8da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.480601 47468394718080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.480644 47533147042688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.482655 47278834422656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.482727 47734274024320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.483007 47645720056704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.483335 47299025548160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.484823 47475034514304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.484807 47222867325824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.485343 47533147042688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.485377 47468394718080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880931.422901 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.423860 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.424736 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.491247 47180788855680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.429669 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.430419 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.431122 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.491333 47560460788608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.492897 47185209340800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:11.493001 47240207807360 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:02:11.492400 47180788855680 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppnm5hfeh
W0618 12:02:11.492494 47560460788608 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpe1pm8pwv
W0618 12:02:11.493276 48010857595776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:02:11.493507 47180788855680 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppnm5hfeh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae96abf3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.493596 47560460788608 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpe1pm8pwv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b41d0f50e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.493953 47180788855680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.494049 47560460788608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:02:11.497220 47185209340800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:02:11.497327 47240207807360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880931.431210 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.432112 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.433010 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.497736 47024067990400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
:::MLL 1560880931.436944 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.437693 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.438351 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.497847 47291093635968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.498327 47683996799872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.498767 47024067990400 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpwr1ifhk6
W0618 12:02:11.499259 47180788855680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.498841 47291093635968 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxmb9ipe4
W0618 12:02:11.499264 47560460788608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:02:11.499827 47024067990400 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpwr1ifhk6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac4ed748e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.499882 47291093635968 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxmb9ipe4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b03196cadd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.500252 47024067990400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.500304 47291093635968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:11.502281 47185209340800 estimator.py:1111] Calling model_fn.
W0618 12:02:11.502389 47185209340800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:02:11.502393 47240207807360 estimator.py:1111] Calling model_fn.
W0618 12:02:11.502502 47240207807360 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:02:11.503746 47185209340800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:11.503864 47240207807360 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:02:11.504484 47222867325824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.504676 47475034514304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.505067 47533147042688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.505160 47468394718080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.505584 47291093635968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.505588 47024067990400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:02:11.521846 47180788855680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.522273 47560460788608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880931.487496 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.487877 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.488205 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.524646 46989240337280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.525300 47024067990400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:02:11.525301 47291093635968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880931.488843 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880931.489215 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880931.489543 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:02:11.525044 47478724301696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz_0
W0618 12:02:11.525649 46989240337280 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4qqlzpk2
I0618 12:02:11.526617 46989240337280 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4qqlzpk2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abcd190de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:02:11.526032 47478724301696 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxmpw_yum
I0618 12:02:11.526992 47478724301696 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxmpw_yum', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ec9156e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:02:11.527012 46989240337280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:02:1[2019-06-18 12:02:51] divide_golden_chunk finished: 3.346 seconds
[2019-06-18 12:02:51] generate golden chunk: 3.360 seconds
[2019-06-18 12:02:51] iteration time 3: 49.431 seconds
2019-06-18 12:02:52.282955: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560880971.799799 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 4}}
[2019-06-18 12:02:55] minmax time: 3.221 seconds
2019-06-18 12:02:55.514482: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:02:55.519982: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:02:55.524379: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560880975.536118 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 3}}
[2019-06-18 12:02:55] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000005-000003 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=6 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir 
[2019-06-18 12:02:55] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-5-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=5 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=1023779836 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=2047559667 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=3071339498 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=4095119329 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=5118899160 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=6142678991 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=7166458822 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=8190238653 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=9214018484 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=10237798315 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=11261578146 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=12285357977 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=13309137808 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=14332917639 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=15356697470 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=16380477301 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=17404257132 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=18428036963 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=19451816794 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000004-000003 --seed=20475596625 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:03:07] eval finished: 12.179 seconds
[2019-06-18 12:03:07] Win rate 000004-000003 vs 000002-000002: 0.850
:::MLL 1560880987.778689 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 3}}
[2019-06-18 12:03:07] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-6-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=6 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=1023779837 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=2047559668 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=3071339499 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=4095119330 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=5118899161 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=6142678992 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=7166458823 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=8190238654 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=9214018485 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=10237798316 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=11261578147 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=12285357978 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=13309137809 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=14332917640 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=15356697471 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=16380477302 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=17404257133 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000005-000002 --seed=18428036964 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/
[2019-06-18 12:03:37] selfplay finished: 29.775 seconds
[2019-06-18 12:03:37] selfplay mn: 29.793 seconds
[2019-06-18 12:03:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-6-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779837 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559668 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339499 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119330 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899161 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678992 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458823 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238654 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018485 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798316 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578147 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357978 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137809 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917640 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697471 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477302 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257133 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036964 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816795 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596626 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376457 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156288 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000005-000002/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_
[2019-06-18 12:03:40] train finished: 44.523 seconds
:::MLL 1560880980.785444 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.786342 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.787202 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.858230 47681686791040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.793370 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.794099 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.794791 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.858307 47418323239808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.859234 47681686791040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp51prvdi3
W0618 12:03:00.859270 47418323239808 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgywb_gts
I0618 12:03:00.860232 47681686791040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp51prvdi3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5e0a973e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.860274 47418323239808 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgywb_gts', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b20b8e64e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.860636 47681686791040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.860671 47418323239808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.865938 47681686791040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.865985 47418323239808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880980.802423 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.803329 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.804193 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.877882 46952306656128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.811207 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.811954 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.812587 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.877950 47675843064704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.879037 46952306656128 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppv_8lkjf
W0618 12:03:00.879063 47675843064704 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpaexo2hug
I0618 12:03:00.880133 46952306656128 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppv_8lkjf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab438259e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.880153 47675843064704 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpaexo2hug', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5cae46fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.880603 46952306656128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.880602 47675843064704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.885964 47675843064704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.886022 46952306656128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.888326 47681686791040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.888678 47418323239808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880980.860530 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.860978 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.861410 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.897915 47298865800064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.898972 47298865800064 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpvpv7iiwg
I0618 12:03:00.900062 47298865800064 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpvpv7iiwg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04e8ae7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.900497 47298865800064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560880980.864960 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.865452 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.865882 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.905149 47149608522624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.905608 47298865800064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.906147 47149608522624 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmprn3oiftn
I0618 12:03:00.907177 47149608522624 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmprn3oiftn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae228411e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.907587 47149608522624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.908405 47675843064704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.908812 46952306656128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880980.872083 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.872503 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.872873 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.911127 47854268220288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.872316 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.872732 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.873075 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.911139 47113953018752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.838124 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.838937 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.839758 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.911335 47469289837440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.839283 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.840128 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.840786 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.911451 47968379052928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.912264 47149608522624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.912168 47854268220288 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqfxmnd5y
W0618 12:03:00.912195 47113953018752 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1k0oyex3
I0618 12:03:00.913165 47854268220288 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqfxmnd5y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b86393ebda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:00.912453 47469289837440 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpm147488x
I0618 12:03:00.913168 47113953018752 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1k0oyex3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad9db055e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:00.912553 47968379052928 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9dq_x0rb
I0618 12:03:00.913586 47469289837440 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpm147488x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c96beee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.913679 47968379052928 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9dq_x0rb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba0cac7ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.913553 47113953018752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.913560 47854268220288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.914039 47469289837440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.914138 47968379052928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.918405 47854268220288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.918412 47113953018752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.919404 47968379052928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.919538 47469289837440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.925544 47298865800064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.931845 47149608522624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560880980.874767 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.875592 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.876336 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.936501 47645967905664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.862255 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.863198 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.864068 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.936519 47638827111296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.895210 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.895610 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.895962 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.937800 47773978719104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.938210 47113953018752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.938216 47854268220288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.937646 47638827111296 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_zfjasw7
:::MLL 1560880980.892721 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.893126 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.893460 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.938576 47217639768960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.937684 47645967905664 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqmtcb2i1
I0618 12:03:00.938742 47638827111296 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_zfjasw7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b540ff46dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.938808 47645967905664 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqmtcb2i1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55b9944e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.939188 47638827111296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.939256 47645967905664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.939016 47681686791040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:00.939102 47418323239808 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:00.938866 47773978719104 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzd0npa5g
I0618 12:03:00.939972 47773978719104 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzd0npa5g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b73879e4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.940403 47773978719104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.939623 47217639768960 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpq6c5pn9t
I0618 12:03:00.940697 47217639768960 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpq6c5pn9t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af1ff3b9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.941130 47217639768960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.941625 47968379052928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.942119 47469289837440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.943359 47681686791040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:00.943454 47418323239808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:00.944559 47638827111296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.944572 47645967905664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.945160 47773978719104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.945687 47217639768960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:03:00.948641 47681686791040 estimator.py:1111] Calling model_fn.
W0618 12:03:00.948758 47681686791040 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:00.948784 47418323239808 estimator.py:1111] Calling model_fn.
W0618 12:03:00.948900 47418323239808 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:00.950219 47681686791040 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:00.950376 47418323239808 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:00.957570 47675843064704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560880980.888141 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.888947 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.889818 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.957648 47094247621504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.888074 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.888886 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.889726 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.957829 47437982421888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.958194 46952306656128 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:00.958633 47094247621504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpec2d3fl6
W0618 12:03:00.958830 47437982421888 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpnlfxbbeq
I0618 12:03:00.959635 47094247621504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpec2d3fl6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad5447cde48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.959831 47437982421888 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpnlfxbbeq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b254cadae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.960038 47094247621504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.960231 47437982421888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.961906 47675843064704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:00.962543 46952306656128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560880980.892038 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.892967 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.893881 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.964277 47038732501888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.903966 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.904765 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.905468 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.964353 47535295705984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.964894 47773978719104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.965448 47217639768960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.965182 47094247621504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.965236 47437982421888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560880980.926483 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.926855 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.927207 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.965220 47182530368384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
:::MLL 1560880980.927471 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560880980.927845 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560880980.928165 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:00.965416 47728709931904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz_0
W0618 12:03:00.965341 47038732501888 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmbxnw0an
W0618 12:03:00.965373 47535295705984 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpl768r6th
I0618 12:03:00.966391 47038732501888 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmbxnw0an', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac857875e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.966402 47535295705984 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpl768r6th', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3bf5008e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.966794 47535295705984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.966796 47038732501888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:00.966989 47675843064704 estimator.py:1111] Calling model_fn.
W0618 12:03:00.966874 47638827111296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.966190 47182530368384 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfc6zf8lq
W0618 12:03:00.967097 47675843064704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:00.966882 47645967905664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.966403 47728709931904 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp89bspfft
I0618 12:03:00.967161 47182530368384 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfc6zf8lq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae9d28cae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.967368 47728709931904 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp89bspfft', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b68fd635e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:00.967659 46952306656128 estimator.py:1111] Calling model_fn.
I0618 12:03:00.967553 47182530368384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.967769 46952306656128 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:00.967752 47728709931904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:00.968465 47675843064704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:00.969138 46952306656128 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:00.971585 47038732501888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.971565 47535295705984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.972254 47182530368384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.972330 47728709931904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:00.973743 47298865800064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:00.978121 47298865800064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:00.979007 47149608522624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:03:00.983238 47298865800064 estimator.py:1111] Calling model_fn.
W0618 12:03:00.983311 47149608522624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:00.983347 47298865800064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:00.984701 47298865800064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:00.985205 47437982421888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.985194 47094247621504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.985816 47854268220288 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:00.986082 47113953018752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:03:00.988324 47149608522624 estimator.py:1111] Calling model_fn.
W0618 12:03:00.988434 47149608522624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:00.989771 47149608522624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:00.990101 47854268220288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:00.990397 47113953018752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:00.991292 47535295705984 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.991614 47038732501888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.991832 47182530368384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.991829 47728709931904 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:00.992995 47968379052928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors[2019-06-18 12:03:40] divide_golden_chunk finished: 3.407 seconds
[2019-06-18 12:03:40] generate golden chunk: 3.422 seconds
[2019-06-18 12:03:40] moving /lfs/lfs12/gma_akey/results/epb002/models/000005-000003.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000005-000004.meta
[2019-06-18 12:03:41] moving /lfs/lfs12/gma_akey/results/epb002/models/000005-000003.index --> /lfs/lfs12/gma_akey/results/epb002/models/000005-000004.index
[2019-06-18 12:03:41] moving /lfs/lfs12/gma_akey/results/epb002/models/000005-000003.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000005-000004.data-00000-of-00001
[2019-06-18 12:03:41] moving /lfs/lfs12/gma_akey/results/epb002/models/000005-000003.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb
[2019-06-18 12:03:41] iteration time 4: 49.237 seconds
2019-06-18 12:03:41.658949: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881021.037049 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 5}}
[2019-06-18 12:03:44] minmax time: 3.193 seconds
2019-06-18 12:03:44.862528: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:03:44.868243: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:03:44.872727: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881024.883167 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 4}}
[2019-06-18 12:03:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=7 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir 
[2019-06-18 12:03:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-6-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=6 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=1023779837 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=2047559668 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=3071339499 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=4095119330 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=5118899161 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=6142678992 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=7166458823 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=8190238654 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=9214018485 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=10237798316 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=11261578147 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=12285357978 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=13309137809 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=14332917640 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=15356697471 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=16380477302 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=17404257133 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=18428036964 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=19451816795 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000005-000004 --seed=20475596626 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:03:57] eval finished: 12.310 seconds
[2019-06-18 12:03:57] Win rate 000005-000004 vs 000004-000003: 0.440
:::MLL 1560881037.254309 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 4}}
[2019-06-18 12:03:57] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-7-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=7 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=1023779838 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=2047559669 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=3071339500 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=4095119331 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=5118899162 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=6142678993 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=7166458824 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=8190238655 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=9214018486 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=10237798317 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=11261578148 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=12285357979 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=13309137810 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=14332917641 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=15356697472 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=16380477303 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=17404257134 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000006-000003 --seed=18428036965 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/
[2019-06-18 12:04:28] selfplay finished: 30.754 seconds
[2019-06-18 12:04:28] selfplay mn: 30.773 seconds
[2019-06-18 12:04:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-7-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779838 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559669 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339500 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119331 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899162 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678993 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458824 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238655 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018486 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798317 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578148 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357979 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137810 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917641 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697472 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477303 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257134 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036965 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816796 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596627 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376458 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156289 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000006-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_
[2019-06-18 12:04:29] train finished: 44.345 seconds
:::MLL 1560881030.144634 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.145515 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.146365 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.216817 47732920517504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881030.149658 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.150358 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.151028 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.216881 47281956356992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.217882 47732920517504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpuhabiju3
W0618 12:03:50.217916 47281956356992 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpb5_9w8qe
I0618 12:03:50.218960 47732920517504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpuhabiju3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b69f85bde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.218974 47281956356992 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpb5_9w8qe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b00f8cccda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.219389 47732920517504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:50.219418 47281956356992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.224835 47732920517504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.224944 47281956356992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.247492 47732920517504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.248978 47281956356992 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881030.218281 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.218710 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.219148 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.259697 47435172217728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881030.193425 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.194202 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.194912 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.261649 47368718058368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881030.186925 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.187859 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.188713 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.261664 46935833912192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.260743 47435172217728 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpn1iursjh
I0618 12:03:50.261816 47435172217728 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpn1iursjh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b24a52d4da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.262231 47435172217728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.262746 46935833912192 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4hpqp4dg
W0618 12:03:50.262778 47368718058368 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmplxad75_c
I0618 12:03:50.263818 46935833912192 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4hpqp4dg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab0624b7da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.263861 47368718058368 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmplxad75_c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b152c335e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.264260 46935833912192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:50.264322 47368718058368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881030.224986 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.225448 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.225835 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.264698 47325391405952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.265740 47325391405952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpypky966v
I0618 12:03:50.266707 47325391405952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpypky966v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b15bb0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.267096 47325391405952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.267151 47435172217728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.269603 46935833912192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.269664 47368718058368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.271751 47325391405952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.286995 47435172217728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881030.247712 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.248090 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.248437 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.291261 47807603614592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.291353 47325391405952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.292066 46935833912192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.292333 47368718058368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.292256 47807603614592 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpx4l0lc5s
:::MLL 1560881030.246642 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.247038 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.247398 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.292868 47525708206976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
I0618 12:03:50.293291 47807603614592 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpx4l0lc5s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7b5bd17e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.293713 47807603614592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.293855 47525708206976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmprdp7nnd7
I0618 12:03:50.294889 47525708206976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmprdp7nnd7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b39b98aee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.295308 47525708206976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.298638 47807603614592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.299978 47525708206976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.300176 47732920517504 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:50.301085 47281956356992 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:50.304857 47732920517504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:50.305731 47281956356992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:03:50.310351 47732920517504 estimator.py:1111] Calling model_fn.
W0618 12:03:50.310467 47732920517504 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:50.311216 47281956356992 estimator.py:1111] Calling model_fn.
W0618 12:03:50.311328 47281956356992 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:50.311923 47732920517504 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:50.312793 47281956356992 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:50.318465 47807603614592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881030.246437 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.247359 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.248098 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.319387 47925920461696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881030.250397 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.251162 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.251856 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.319457 47426670629760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.319622 47525708206976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.320467 47925920461696 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2v7uzq6g
W0618 12:03:50.320525 47426670629760 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjm9a20xe
I0618 12:03:50.321577 47925920461696 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2v7uzq6g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b96e80d4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.321669 47426670629760 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjm9a20xe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b22aa714e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.322009 47925920461696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:50.322105 47426670629760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.327286 47925920461696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.327321 47426670629760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881030.265302 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.266113 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.266867 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.328425 47314445390720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881030.256708 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.257645 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.258485 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.328453 47818512999296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.329588 47314445390720 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpdqt770b3
W0618 12:03:50.329614 47818512999296 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpy2dimc2s
I0618 12:03:50.330702 47314445390720 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpdqt770b3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b08894c2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.330718 47818512999296 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpy2dimc2s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7de6115e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.331147 47314445390720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:50.331163 47818512999296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.334242 47435172217728 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:50.336515 47314445390720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.336525 47818512999296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.338506 47435172217728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:50.338773 47325391405952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:50.340620 46935833912192 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:50.341020 47368718058368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:50.343091 47325391405952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:03:50.343539 47435172217728 estimator.py:1111] Calling model_fn.
W0618 12:03:50.343646 47435172217728 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:50.344918 46935833912192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:50.345348 47368718058368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:50.344995 47435172217728 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881030.305614 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.306129 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.306509 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.347404 47461951456128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881030.305103 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.305508 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.306019 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.347402 47388367750016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
I0618 12:03:50.348156 47325391405952 estimator.py:1111] Calling model_fn.
W0618 12:03:50.348262 47325391405952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:50.348432 47461951456128 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp__bo21ur
W0618 12:03:50.348463 47388367750016 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpl375u4g4
I0618 12:03:50.349469 47461951456128 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp__bo21ur', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2ae1581e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.349509 47388367750016 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpl375u4g4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b19bf69cda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:03:50.349557 47925920461696 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:03:50.349889 47461951456128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:50.349923 47388367750016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.349924 47426670629760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:03:50.349993 46935833912192 estimator.py:1111] Calling model_fn.
W0618 12:03:50.349618 47325391405952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:50.350103 46935833912192 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:50.350445 47368718058368 estimator.py:1111] Calling model_fn.
W0618 12:03:50.350558 47368718058368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881030.282474 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.283190 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.283885 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.350863 46958362604416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881030.275485 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.276416 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.277327 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.350991 47162272420736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.351468 46935833912192 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:50.351918 47368718058368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:50.351969 46958362604416 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpre4sj10_
W0618 12:03:50.352092 47162272420736 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5nu6q620
I0618 12:03:50.353077 46958362604416 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpre4sj10_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5a11bfe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.353194 47162272420736 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5nu6q620', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae51b14ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.353528 46958362604416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:50.353653 47162272420736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.354741 47461951456128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.354727 47388367750016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.356698 47314445390720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.356809 47818512999296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.358988 46958362604416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.359159 47162272420736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881030.290164 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.291085 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.291953 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.362679 47936279585664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
:::MLL 1560881030.296296 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.297042 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.297751 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.362782 47499262997376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.363739 47936279585664 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1ielw_pg
W0618 12:03:50.363788 47499262997376 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpvyr2dsdw
I0618 12:03:50.364735 47936279585664 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1ielw_pg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b995180fdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.364768 47499262997376 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpvyr2dsdw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b339148fdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.365135 47936279585664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881030.324121 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.324555 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.324932 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.365133 47641088181120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
I0618 12:03:50.365170 47499262997376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881030.323098 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.323533 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.323963 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.365195 47059889783680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz_0
W0618 12:03:50.366118 47807603614592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:50.366573 47525708206976 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:03:50.366166 47641088181120 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6tlq6i42
W0618 12:03:50.366243 47059889783680 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkwl4yofu
I0618 12:03:50.367130 47641088181120 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6tlq6i42', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5496b99dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.367223 47059889783680 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkwl4yofu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acd4499de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:03:50.367527 47641088181120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:03:50.367621 47059889783680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:03:50.370439 47807603614592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:50.370353 47936279585664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.370360 47499262997376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.370854 47525708206976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:03:50.372356 47641088181120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.372384 47059889783680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:03:50.374385 47388367750016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.374521 47461951456128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:03:50.375527 47807603614592 estimator.py:1111] Calling model_fn.
W0618 12:03:50.375642 47807603614592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:03:50.375894 47525708206976 estimator.py:1111] Calling model_fn.
W0618 12:03:50.376003 47525708206976 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:03:50.377004 47807603614592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:50.377348 47525708206976 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:03:50.381482 46958362604416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:03:50.381953 47162272420736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881030.350967 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881030.351443 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881030.351850 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:03:50.388018 47506823881600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/[2019-06-18 12:04:31] divide_golden_chunk finished: 3.296 seconds
[2019-06-18 12:04:31] generate golden chunk: 3.311 seconds
[2019-06-18 12:04:31] iteration time 5: 50.303 seconds
2019-06-18 12:04:31.891459: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881071.340358 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 6}}
[2019-06-18 12:04:35] minmax time: 3.206 seconds
2019-06-18 12:04:35.107505: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:04:35.113007: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:04:35.117541: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881075.130001 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 5}}
[2019-06-18 12:04:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=8 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir 
[2019-06-18 12:04:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-7-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=7 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=1023779838 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=2047559669 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=3071339500 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=4095119331 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=5118899162 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=6142678993 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=7166458824 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=8190238655 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=9214018486 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=10237798317 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=11261578148 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=12285357979 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=13309137810 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=14332917641 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=15356697472 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=16380477303 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=17404257134 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=18428036965 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=19451816796 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000006-000004 --seed=20475596627 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:04:46] eval finished: 11.018 seconds
[2019-06-18 12:04:46] Win rate 000006-000004 vs 000004-000003: 0.460
:::MLL 1560881086.212265 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 5}}
[2019-06-18 12:04:46] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-8-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=8 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=1023779839 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=2047559670 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=3071339501 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=4095119332 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=5118899163 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=6142678994 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=7166458825 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=8190238656 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=9214018487 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=10237798318 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=11261578149 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=12285357980 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=13309137811 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=14332917642 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=15356697473 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=16380477304 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=17404257135 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000007-000003 --seed=18428036966 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/
[2019-06-18 12:05:17] selfplay finished: 31.216 seconds
[2019-06-18 12:05:17] selfplay mn: 31.233 seconds
[2019-06-18 12:05:17] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-8-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779839 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559670 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339501 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119332 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899163 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678994 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458825 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238656 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018487 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798318 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578149 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357980 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137811 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917642 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697473 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477304 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257135 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036966 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816797 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596628 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376459 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156290 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000007-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_
[2019-06-18 12:05:19] train finished: 44.138 seconds
:::MLL 1560881080.387341 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.388099 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.388768 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.451796 47817108255616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881080.377919 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.378805 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.379630 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.451855 47278465713024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.452839 47817108255616 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgjfes50n
W0618 12:04:40.452869 47278465713024 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_6i3sdbk
I0618 12:04:40.453827 47817108255616 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgjfes50n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d9256ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.453828 47278465713024 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_6i3sdbk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0028bdde10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.454252 47817108255616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:40.454255 47278465713024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.459477 47817108255616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.459524 47278465713024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.482090 47817108255616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.482262 47278465713024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881080.453873 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.454338 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.454717 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.497925 47758415029120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.498972 47758415029120 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzpvb_zhr
I0618 12:04:40.500087 47758415029120 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzpvb_zhr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6fe7f33e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.500553 47758415029120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881080.429277 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.430048 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.430760 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.501053 47866723550080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881080.427459 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.428215 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.429014 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.501314 47031189767040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.502186 47866723550080 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4r577nbt
W0618 12:04:40.502403 47031189767040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpg47niw3a
I0618 12:04:40.503279 47866723550080 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4r577nbt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b891fa41e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881080.455501 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.455875 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.456224 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.502742 47656427922304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
I0618 12:04:40.503502 47031189767040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpg47niw3a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac695f25e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.503721 47866723550080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:40.503984 47031189767040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.503758 47656427922304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7l0lbhcg
I0618 12:04:40.504768 47656427922304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7l0lbhcg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b58290b7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.505159 47656427922304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.505342 47758415029120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.509022 47866723550080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.509490 47031189767040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.509737 47656427922304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.525090 47758415029120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881080.454605 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.455364 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.456100 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.525670 47399176385408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881080.457285 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.458030 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.458728 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.525915 47893785060224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.526699 47399176385408 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2v9pbkoi
W0618 12:04:40.526889 47893785060224 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpl1i0ey77
I0618 12:04:40.527701 47399176385408 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2v9pbkoi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1c43a88e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.527859 47893785060224 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpl1i0ey77', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8f6ca1fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.528107 47399176385408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:40.528250 47893785060224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.529340 47656427922304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.531617 47866723550080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.531836 47817108255616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:40.532709 47031189767040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.532718 47278465713024 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:40.533118 47399176385408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.533159 47893785060224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881080.490510 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.490926 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.491279 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.534284 47065017512832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881080.490390 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.490799 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.491157 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.534284 47758642951040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.535359 47758642951040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpwnqs7tdr
W0618 12:04:40.535390 47065017512832 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp82_xwu1m
I0618 12:04:40.536389 47758642951040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpwnqs7tdr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6ff5890e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881080.464864 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.465767 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.466574 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.536169 47573110862720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881080.471567 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.472289 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.472961 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.536176 47813282571136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
I0618 12:04:40.536421 47065017512832 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp82_xwu1m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ace763cce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:40.536094 47817108255616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:04:40.536824 47758642951040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:40.536852 47065017512832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.537074 47278465713024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:40.537282 47813282571136 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpt6q19gul
W0618 12:04:40.537310 47573110862720 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkplwpcgt
I0618 12:04:40.538376 47813282571136 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpt6q19gul', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7cae4f7da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.538410 47573110862720 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkplwpcgt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b44c2f60e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.538830 47813282571136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:40.538861 47573110862720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.541572 47758642951040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:40.541119 47817108255616 estimator.py:1111] Calling model_fn.
W0618 12:04:40.541231 47817108255616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:40.541606 47065017512832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:40.542194 47278465713024 estimator.py:1111] Calling model_fn.
W0618 12:04:40.542303 47278465713024 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:40.542586 47817108255616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:40.543659 47278465713024 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:40.544126 47813282571136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.544205 47573110862720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.553107 47893785060224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.553703 47399176385408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.561131 47758642951040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.561374 47065017512832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.566685 47813282571136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.566910 47573110862720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881080.530053 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.530440 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.530767 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.572826 47782742778752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.572963 47758415029120 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:40.573856 47782742778752 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzjs4qy8c
I0618 12:04:40.574895 47782742778752 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzjs4qy8c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7591ff3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881080.532737 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.533171 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.533556 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.574858 47164202648448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
I0618 12:04:40.575315 47782742778752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.575882 47164202648448 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppvpqebs8
I0618 12:04:40.576968 47164202648448 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppvpqebs8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae58e21de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:40.576732 47656427922304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:04:40.577404 47164202648448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881080.534061 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.534529 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.534930 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.577287 47988540035968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.577300 47758415029120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:40.578384 47988540035968 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_uc694yo
I0618 12:04:40.579457 47988540035968 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_uc694yo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba57c782e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.579903 47988540035968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.580238 47782742778752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881080.538144 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.538599 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.538976 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.580847 47040956625792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.581058 47656427922304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:40.582031 47164202648448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:40.581882 47040956625792 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac8dc18ad30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:40.582724 47866723550080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:04:40.582426 47758415029120 estimator.py:1111] Calling model_fn.
W0618 12:04:40.582535 47758415029120 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:40.583065 47040956625792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:04:40.583829 47031189767040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:40.583914 47758415029120 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:40.584773 47988540035968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:04:40.586134 47656427922304 estimator.py:1111] Calling model_fn.
W0618 12:04:40.586242 47656427922304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:40.587212 47866723550080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:40.587678 47040956625792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.587597 47656427922304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:40.588358 47031189767040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881080.526164 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.526947 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.527693 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.588685 47898977006464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
:::MLL 1560881080.519409 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.520341 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.521222 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.588736 47587518940032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.589784 47898977006464 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpp2mq165f
W0618 12:04:40.589806 47587518940032 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3h1y24j7
I0618 12:04:40.590870 47898977006464 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpp2mq165f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b90a218be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.590887 47587518940032 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3h1y24j7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b481dbfce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:04:40.591301 47898977006464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:40.591324 47587518940032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:40.592517 47866723550080 estimator.py:1111] Calling model_fn.
W0618 12:04:40.592627 47866723550080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:04:40.593733 47031189767040 estimator.py:1111] Calling model_fn.
W0618 12:04:40.593850 47031189767040 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:40.594085 47866723550080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:40.595318 47031189767040 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:40.596107 47587518940032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.596130 47898977006464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:04:40.600102 47782742778752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.601297 47893785060224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:40.601839 47399176385408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:40.602121 47164202648448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.604362 47988540035968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.605598 47893785060224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:40.606133 47399176385408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:04:40.607289 47040956625792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:04:40.609113 47758642951040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:04:40.609536 47065017512832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:04:40.610639 47893785060224 estimator.py:1111] Calling model_fn.
W0618 12:04:40.610748 47893785060224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881080.540957 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.541781 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.542471 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.610570 47994914591616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
I0618 12:04:40.611169 47399176385408 estimator.py:1111] Calling model_fn.
:::MLL 1560881080.539102 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881080.539878 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881080.540762 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:04:40.611049 47728786318208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz_0
W0618 12:04:40.611277 47399176385408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:04:40.612101 47893785060224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:04:40.611673 47994914591616 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8vq_mz1y
W0618 12:04:40.612643 47399176385408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:04:40.612778 47994914591616 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8vq_mz1y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6f86c2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:04:40.612136 47728786318208 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmvrost1d
I0618 12:04:40.613219 47994914591616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:04:40.613283 47728786318208 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmvrost1d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6901f0fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replic[2019-06-18 12:05:20] divide_golden_chunk finished: 3.302 seconds
[2019-06-18 12:05:20] generate golden chunk: 3.317 seconds
[2019-06-18 12:05:20] iteration time 6: 49.424 seconds
2019-06-18 12:05:21.364260: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881120.764647 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 7}}
[2019-06-18 12:05:24] minmax time: 3.215 seconds
2019-06-18 12:05:24.589647: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:05:24.595066: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:05:24.599662: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881124.611486 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 6}}
[2019-06-18 12:05:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000008-000004 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=9 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir 
[2019-06-18 12:05:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-8-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=8 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=1023779839 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=2047559670 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=3071339501 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=4095119332 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=5118899163 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=6142678994 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=7166458825 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=8190238656 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=9214018487 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=10237798318 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=11261578149 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=12285357980 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=13309137811 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=14332917642 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=15356697473 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=16380477304 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=17404257135 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=18428036966 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=19451816797 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000007-000004 --seed=20475596628 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:05:35] eval finished: 10.722 seconds
[2019-06-18 12:05:35] Win rate 000007-000004 vs 000004-000003: 0.510
:::MLL 1560881135.393932 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 6}}
[2019-06-18 12:05:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-9-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=9 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=1023779840 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=2047559671 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=3071339502 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=4095119333 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=5118899164 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=6142678995 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=7166458826 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=8190238657 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=9214018488 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=10237798319 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=11261578150 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=12285357981 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=13309137812 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=14332917643 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=15356697474 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=16380477305 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=17404257136 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000008-000003 --seed=18428036967 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/
[2019-06-18 12:06:07] selfplay finished: 32.591 seconds
[2019-06-18 12:06:08] selfplay mn: 32.611 seconds
[2019-06-18 12:06:08] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-9-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779840 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559671 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339502 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119333 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899164 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678995 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458826 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238657 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018488 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798319 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578150 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357981 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137812 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917643 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697474 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477305 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257136 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036967 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816798 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596629 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376460 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156291 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000008-000003/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_
[2019-06-18 12:06:09] train finished: 44.403 seconds
:::MLL 1560881129.894652 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.895403 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.896069 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:29.965708 47210225374080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881129.889574 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.890474 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.891317 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:29.965718 47863179453312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:29.966738 47210225374080 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0gheefp6
W0618 12:05:29.966765 47863179453312 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpscaxt7yq
I0618 12:05:29.967741 47210225374080 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0gheefp6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af0454cdda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:29.967742 47863179453312 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpscaxt7yq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b884c657e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:29.968142 47210225374080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:29.968146 47863179453312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:29.973385 47863179453312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:29.973421 47210225374080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:29.996093 47863179453312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:29.996881 47210225374080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881129.924673 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.925558 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.926438 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.004747 47608660472704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881129.930856 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.931589 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.932292 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.004819 48006917047168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.005841 47608660472704 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8qvtaa0v
W0618 12:05:30.005903 48006917047168 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5not3iz5
I0618 12:05:30.006922 47608660472704 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8qvtaa0v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d09e1fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.006994 48006917047168 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5not3iz5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba9c3d31e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.007362 47608660472704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:30.007439 48006917047168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.012703 48006917047168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.012712 47608660472704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881129.936957 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.937859 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.938694 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.012807 47488450020224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881129.950553 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.951418 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.952283 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.012827 47424803787648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.013866 47424803787648 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpoejpuyyp
W0618 12:05:30.013898 47488450020224 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7tvy7t2b
I0618 12:05:30.014904 47424803787648 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpoejpuyyp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b223b2bae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.014908 47488450020224 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7tvy7t2b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b310cc81e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.015307 47424803787648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:30.015313 47488450020224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881129.968565 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.969008 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.969409 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.014947 46951034528640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.015904 46951034528640 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpeth02tvz
:::MLL 1560881129.972898 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.973367 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.973767 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.016646 47796935021440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
I0618 12:05:30.016880 46951034528640 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpeth02tvz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab3ec527e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.017273 46951034528640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.017592 47796935021440 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3_j6f0r3
I0618 12:05:30.018561 47796935021440 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3_j6f0r3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b78dfebae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.018951 47796935021440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.020407 47424803787648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.020384 47488450020224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.021910 46951034528640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.023468 47796935021440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.035094 47608660472704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.035057 48006917047168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881129.984377 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.984927 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.985372 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.038119 47945462432640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881129.992116 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.992491 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.992818 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.038467 47362745283456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.039109 47945462432640 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpyxn7ggcw
I0618 12:05:30.040118 47945462432640 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpyxn7ggcw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9b74d81e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:30.039399 47362745283456 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0mbi1jlx
I0618 12:05:30.040420 47362745283456 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0mbi1jlx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b13c831fda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.040535 47945462432640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:30.040840 47362745283456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.041569 46951034528640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.042705 47488450020224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.043271 47424803787648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.042870 47796935021440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.045180 47945462432640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.045419 47362745283456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.046110 47863179453312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:30.046570 47210225374080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:30.050468 47863179453312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:30.050896 47210225374080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881129.979038 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.979775 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.980491 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.053038 47698212447104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881129.983643 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.984413 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.985105 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.053900 47123619402624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881129.979508 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.980390 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.981133 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.053938 47432292320128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881129.981678 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881129.982419 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881129.983075 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.054652 47560016036736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.054069 47698212447104 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp76h7ioml
I0618 12:05:30.055052 47698212447104 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp76h7ioml', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b61e398be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:30.054917 47123619402624 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjr0fo6yc
I0618 12:05:30.055499 47698212447104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.054945 47432292320128 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpuotd3cpx
I0618 12:05:30.055896 47123619402624 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjr0fo6yc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adc1b2eae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.055921 47432292320128 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpuotd3cpx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b23f9859e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.055554 47863179453312 estimator.py:1111] Calling model_fn.
W0618 12:05:30.055664 47863179453312 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:30.056297 47123619402624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:30.056314 47432292320128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:05:30.056000 47210225374080 estimator.py:1111] Calling model_fn.
W0618 12:05:30.056113 47210225374080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:30.055695 47560016036736 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpz155h2a8
I0618 12:05:30.056871 47560016036736 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpz155h2a8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b41b672de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:30.057010 47863179453312 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:05:30.057332 47560016036736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.057476 47210225374080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:30.060519 47698212447104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.061265 47123619402624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.061267 47432292320128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881130.018532 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881130.019014 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881130.019422 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.061194 47369375429504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.062591 47560016036736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.062282 47369375429504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmph2myside
I0618 12:05:30.063354 47369375429504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmph2myside', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b155361fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.063797 47369375429504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.064617 47945462432640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.064925 47362745283456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.068818 47369375429504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881130.024943 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881130.025401 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881130.025811 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.072449 47998648374144 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.073415 47998648374144 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpasbnjh75
I0618 12:05:30.074388 47998648374144 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpasbnjh75', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7d6f8fda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.074789 47998648374144 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.079450 47998648374144 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.080298 47698212447104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.080880 47123619402624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.080982 47432292320128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.083516 48006917047168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:30.083737 47608660472704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:30.085574 47560016036736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.087844 48006917047168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:30.088054 47608660472704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:30.088706 47369375429504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:05:30.089628 46951034528640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:30.089968 47796935021440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881130.043364 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881130.043803 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881130.044174 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.090882 47288356782976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881130.045901 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881130.046331 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881130.046710 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.091467 47989416579968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.091857 47288356782976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmvu1tybl
I0618 12:05:30.092832 47288356782976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmvu1tybl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b02764b9dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.092926 48006917047168 estimator.py:1111] Calling model_fn.
W0618 12:05:30.093037 48006917047168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:30.092434 47989416579968 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqzw40531
I0618 12:05:30.093148 47608660472704 estimator.py:1111] Calling model_fn.
I0618 12:05:30.093227 47288356782976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.093257 47608660472704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:30.093397 47989416579968 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqzw40531', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba5b0b72dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.093787 47989416579968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.094395 48006917047168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:30.093969 46951034528640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:30.094612 47608660472704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:30.094262 47796935021440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:30.095228 47488450020224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:30.095984 47424803787648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:05:30.098002 47288356782976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881130.021492 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881130.022467 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881130.023414 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.097707 47628834767744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
W0618 12:05:30.098516 47989416579968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.099016 47998648374144 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:05:30.099072 46951034528640 estimator.py:1111] Calling model_fn.
W0618 12:05:30.099180 46951034528640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:30.099280 47796935021440 estimator.py:1111] Calling model_fn.
W0618 12:05:30.099391 47796935021440 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:30.098809 47628834767744 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmplydizlv7
W0618 12:05:30.099917 47488450020224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:05:30.099935 47628834767744 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmplydizlv7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b51bc5d4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:05:30.100393 47628834767744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:05:30.100636 47424803787648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:05:30.100556 46951034528640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:30.100750 47796935021440 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:05:30.105444 47488450020224 estimator.py:1111] Calling model_fn.
W0618 12:05:30.105580 47488450020224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:05:30.106199 47424803787648 estimator.py:1111] Calling model_fn.
W0618 12:05:30.106029 47628834767744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:05:30.106320 47424803787648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:05:30.107105 47488450020224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:05:30.107814 47424803787648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881130.061865 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881130.062248 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881130.062586 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.109116 47646276731776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
:::MLL 1560881130.062791 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881130.063246 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881130.063603 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:05:30.109197 46965672596352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz_0
I0618 12:05:30.110126 47646276731776 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55cbfc9d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:05:30.110149 46965672596352 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqyii8s8p
I0618 12:05:30.111111 46965672596352 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqyii8s8p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': Non[2019-06-18 12:06:11] divide_golden_chunk finished: 3.446 seconds
[2019-06-18 12:06:11] generate golden chunk: 3.461 seconds
[2019-06-18 12:06:11] moving /lfs/lfs12/gma_akey/results/epb002/models/000008-000004.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000008-000005.meta
[2019-06-18 12:06:11] moving /lfs/lfs12/gma_akey/results/epb002/models/000008-000004.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000008-000005.data-00000-of-00001
[2019-06-18 12:06:11] moving /lfs/lfs12/gma_akey/results/epb002/models/000008-000004.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb
[2019-06-18 12:06:11] moving /lfs/lfs12/gma_akey/results/epb002/models/000008-000004.index --> /lfs/lfs12/gma_akey/results/epb002/models/000008-000005.index
[2019-06-18 12:06:11] iteration time 7: 50.743 seconds
2019-06-18 12:06:12.137050: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881171.507612 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 8}}
[2019-06-18 12:06:15] minmax time: 3.231 seconds
2019-06-18 12:06:15.377524: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:06:15.383035: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:06:15.387657: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881175.398381 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 7}}
[2019-06-18 12:06:15] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000009-000005 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=10 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:06:15] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-9-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=9 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=1023779840 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=2047559671 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=3071339502 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=4095119333 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=5118899164 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=6142678995 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=7166458826 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=8190238657 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=9214018488 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=10237798319 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=11261578150 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=12285357981 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=13309137812 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=14332917643 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=15356697474 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=16380477305 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=17404257136 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=18428036967 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=19451816798 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000008-000005 --seed=20475596629 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/result
[2019-06-18 12:06:27] eval finished: 12.055 seconds
[2019-06-18 12:06:27] Win rate 000008-000005 vs 000007-000004: 0.590
:::MLL 1560881187.516416 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 7}}
[2019-06-18 12:06:27] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-10-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=10 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=1023779841 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=2047559672 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=3071339503 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=4095119334 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=5118899165 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=6142678996 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=7166458827 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=8190238658 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=9214018489 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=10237798320 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=11261578151 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=12285357982 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=13309137813 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=14332917644 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=15356697475 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=16380477306 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=17404257137 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000009-000004 --seed=18428036968 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:06:57] selfplay finished: 30.001 seconds
[2019-06-18 12:06:57] selfplay mn: 30.021 seconds
[2019-06-18 12:06:57] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-10-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779841 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559672 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339503 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119334 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899165 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678996 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458827 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238658 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018489 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798320 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578151 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357982 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137813 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917644 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697475 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477306 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257137 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036968 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816799 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596630 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376461 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156292 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000009-000004/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:06:59] train finished: 43.983 seconds
:::MLL 1560881180.685544 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.686375 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.687251 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.768519 47076635136896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881180.686326 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.687255 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.688010 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.768598 47073352094592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.769648 47076635136896 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpab26hg65
W0618 12:06:20.769688 47073352094592 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjkjug42y
I0618 12:06:20.770764 47076635136896 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpab26hg65', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad12ab3ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.770766 47073352094592 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjkjug42y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad067046e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.771206 47076635136896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.771211 47073352094592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.776620 47076635136896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.776642 47073352094592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881180.690560 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.691312 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.691937 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.777774 47217147560832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881180.689185 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.689917 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.690691 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.777835 47402364556160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.778898 47217147560832 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpplhibstn
W0618 12:06:20.778933 47402364556160 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjgnezlxo
I0618 12:06:20.780006 47217147560832 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpplhibstn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af1e1e51e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.780045 47402364556160 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjgnezlxo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1d01b02e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.780472 47217147560832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.780494 47402364556160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.785800 47402364556160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.785831 47217147560832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.798891 47076635136896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.798963 47073352094592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881180.750745 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.751302 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.751695 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.803216 47409326064512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881180.753842 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.754248 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.754603 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.803169 47252208931712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.804228 47409326064512 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9h_0d_6a
W0618 12:06:20.804199 47252208931712 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgjzfu2ym
I0618 12:06:20.805197 47252208931712 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgjzfu2ym', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afa0bb6fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.805210 47409326064512 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9h_0d_6a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1ea0a05e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.805593 47409326064512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.805594 47252208931712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.808155 47402364556160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.808322 47217147560832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.810485 47409326064512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.810496 47252208931712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881180.755733 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.756224 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.756708 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.811284 47597859218304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.812315 47597859218304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp69s57hfl
I0618 12:06:20.813299 47597859218304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp69s57hfl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4a8613fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881180.756788 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.757267 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.757632 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.813200 47393513829248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
I0618 12:06:20.813684 47597859218304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.814175 47393513829248 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpiqad0dgc
I0618 12:06:20.815143 47393513829248 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpiqad0dgc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1af224ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.815530 47393513829248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.818293 47597859218304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.820075 47393513829248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.829968 47252208931712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.830043 47409326064512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881180.757739 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.758562 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.759273 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.836857 47810377241472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881180.760423 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.761122 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.761870 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.836906 47944979936128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.837887 47597859218304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.837964 47810377241472 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzb3gcuo7
W0618 12:06:20.838006 47944979936128 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpuduj_tck
I0618 12:06:20.839071 47810377241472 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzb3gcuo7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7c01239e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.839097 47944979936128 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpuduj_tck', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9b5815be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.839511 47810377241472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.839546 47944979936128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.839812 47393513829248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.844832 47810377241472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.844883 47944979936128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881180.771979 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.772833 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.773652 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.848335 46989670675328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881180.771623 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.772495 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.773334 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.848407 47056367305600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.849387 47056367305600 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6rq4o3fa
W0618 12:06:20.849354 46989670675328 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpe0kadywt
I0618 12:06:20.850367 46989670675328 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpe0kadywt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abceb374e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.850386 47056367305600 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6rq4o3fa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acc72a51e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:06:20.850516 47076635136896 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:20.850548 47073352094592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:06:20.850779 47056367305600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.850777 46989670675328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.855167 47076635136896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:20.855197 47073352094592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:20.855770 47056367305600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.855798 46989670675328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.856960 47402364556160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:20.857591 47217147560832 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:06:20.860624 47076635136896 estimator.py:1111] Calling model_fn.
I0618 12:06:20.860651 47073352094592 estimator.py:1111] Calling model_fn.
W0618 12:06:20.860743 47076635136896 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:20.860764 47073352094592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:20.861272 47402364556160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:20.861934 47217147560832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:20.862214 47076635136896 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:20.862225 47073352094592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881180.783732 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.784652 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.785540 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.863755 47156077589376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881180.791845 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.792581 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.793246 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.863786 47841047692160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.864895 47841047692160 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmkoc10tp
W0618 12:06:20.864938 47156077589376 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpu9pujbfa
I0618 12:06:20.866000 47841047692160 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmkoc10tp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b83253d9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.866061 47156077589376 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpu9pujbfa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae3a9d74e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.866459 47841047692160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.866506 47156077589376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.866310 47402364556160 estimator.py:1111] Calling model_fn.
W0618 12:06:20.866418 47402364556160 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:20.867090 47810377241472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:06:20.867055 47217147560832 estimator.py:1111] Calling model_fn.
W0618 12:06:20.867162 47217147560832 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:20.867620 47944979936128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.867757 47402364556160 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:20.868527 47217147560832 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881180.824453 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.824837 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.825177 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.871300 47539836461952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881180.825554 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.825924 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.826245 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.871242 46958407865216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.871830 47841047692160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.871842 47156077589376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.872258 46958407865216 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8ed738pv
W0618 12:06:20.872290 47539836461952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpnjuhhyeh
I0618 12:06:20.873253 46958407865216 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8ed738pv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5a3ceae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.873268 47539836461952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpnjuhhyeh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3d03a6fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.873649 46958407865216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.873671 47539836461952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.875655 47056367305600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.876085 46989670675328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.877597 47409326064512 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:20.877860 47252208931712 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881180.811112 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.811872 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.812608 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.877757 47718104241024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
:::MLL 1560881180.800991 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.801955 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.802823 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.877732 47490710836096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.878308 47539836461952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.878308 46958407865216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.878818 47718104241024 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpow58_6tr
W0618 12:06:20.878846 47490710836096 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp65pckvu3
I0618 12:06:20.879894 47718104241024 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpow58_6tr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b66853d5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.879929 47490710836096 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp65pckvu3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3193896e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.880289 47718104241024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.880327 47490710836096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.881890 47409326064512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:20.882214 47252208931712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:20.884979 47718104241024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.884994 47490710836096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.885349 47597859218304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:06:20.886945 47409326064512 estimator.py:1111] Calling model_fn.
W0618 12:06:20.887055 47409326064512 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:06:20.887302 47252208931712 estimator.py:1111] Calling model_fn.
W0618 12:06:20.887414 47252208931712 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:20.887244 47393513829248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:06:20.888412 47409326064512 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:20.888783 47252208931712 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:20.889678 47597859218304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:20.891582 47393513829248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:06:20.894147 47841047692160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.894191 47156077589376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:06:20.894733 47597859218304 estimator.py:1111] Calling model_fn.
W0618 12:06:20.894846 47597859218304 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:06:20.896198 47597859218304 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:06:20.896647 47393513829248 estimator.py:1111] Calling model_fn.
W0618 12:06:20.896754 47393513829248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881180.848027 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.848464 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.848833 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.897275 47881657762688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.897819 46958407865216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.897895 47539836461952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881180.850689 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881180.851164 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881180.851586 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:06:20.898141 47934199587712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz_0
W0618 12:06:20.898112 47393513829248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:06:20.898348 47881657762688 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzz5k0zgy
I0618 12:06:20.899324 47881657762688 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzz5k0zgy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c99ca1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.899722 47881657762688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:06:20.899169 47934199587712 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b98d586bd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:06:20.900265 47934199587712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:06:20.904361 47881657762688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.904920 47718104241024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:06:20.904812 47934199587712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:06:20.905050 47490710836096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions [2019-06-18 12:07:00] divide_golden_chunk finished: 3.413 seconds
[2019-06-18 12:07:00] generate golden chunk: 3.428 seconds
[2019-06-18 12:07:00] moving /lfs/lfs12/gma_akey/results/epb002/models/000009-000005.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb
[2019-06-18 12:07:00] moving /lfs/lfs12/gma_akey/results/epb002/models/000009-000005.index --> /lfs/lfs12/gma_akey/results/epb002/models/000009-000006.index
[2019-06-18 12:07:00] moving /lfs/lfs12/gma_akey/results/epb002/models/000009-000005.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000009-000006.meta
[2019-06-18 12:07:00] moving /lfs/lfs12/gma_akey/results/epb002/models/000009-000005.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000009-000006.data-00000-of-00001
[2019-06-18 12:07:01] iteration time 8: 49.499 seconds
2019-06-18 12:07:01.672277: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881221.006345 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 9}}
[2019-06-18 12:07:04] minmax time: 3.229 seconds
2019-06-18 12:07:04.911871: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:07:04.917313: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:07:04.921825: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881224.932989 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 8}}
[2019-06-18 12:07:04] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000010-000006 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=11 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:07:04] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-10-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=10 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=1023779841 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=2047559672 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=3071339503 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=4095119334 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=5118899165 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=6142678996 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=7166458827 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=8190238658 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=9214018489 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=10237798320 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=11261578151 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=12285357982 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=13309137813 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=14332917644 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=15356697475 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=16380477306 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=17404257137 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=18428036968 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=19451816799 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000009-000006 --seed=20475596630 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:07:15] eval finished: 10.795 seconds
[2019-06-18 12:07:15] Win rate 000009-000006 vs 000008-000005: 0.500
:::MLL 1560881235.791720 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 8}}
[2019-06-18 12:07:15] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-11-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=11 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=1023779842 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=2047559673 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=3071339504 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=4095119335 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=5118899166 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=6142678997 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=7166458828 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=8190238659 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=9214018490 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=10237798321 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=11261578152 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=12285357983 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=13309137814 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=14332917645 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=15356697476 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=16380477307 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=17404257138 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000010-000005 --seed=18428036969 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:07:44] selfplay finished: 29.150 seconds
[2019-06-18 12:07:44] selfplay mn: 29.167 seconds
[2019-06-18 12:07:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-11-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779842 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559673 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339504 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119335 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899166 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678997 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458828 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238659 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018490 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798321 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578152 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357983 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137814 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917645 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697476 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477307 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257138 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036969 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816800 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596631 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376462 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156293 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000010-000005/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:07:48] divide_golden_chunk finished: 3.301 seconds
[2019-06-18 12:07:48] generate golden chunk: 3.315 seconds
[2019-06-18 12:07:49] train finished: 44.831 seconds
:::MLL 1560881230.231031 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.231761 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.232468 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.310500 47687976055680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.233839 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.234571 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.235175 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.310549 47010428679040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.311642 47687976055680 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpcfa_vaqr
W0618 12:07:10.311673 47010428679040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpm8sbhae2
I0618 12:07:10.312726 47687976055680 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpcfa_vaqr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5f8175ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.312758 47010428679040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpm8sbhae2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac1c07d4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.313159 47687976055680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.313178 47010428679040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.318197 47687976055680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.318279 47010428679040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.341284 47687976055680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.341883 47010428679040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881230.261192 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.262078 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.262985 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.355793 47555917165440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.267087 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.267785 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.268509 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.355996 47348895126400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.356920 47555917165440 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8poiagtt
W0618 12:07:10.357082 47348895126400 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8pweuaa_
I0618 12:07:10.358021 47555917165440 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8poiagtt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b40c2230e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.358168 47348895126400 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8pweuaa_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b108ea95e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.358471 47555917165440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.358600 47348895126400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.363759 47555917165440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.363879 47348895126400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881230.305372 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.305794 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.306118 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.365438 47335475729280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.308115 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.308489 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.308808 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.365519 47159883187072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.366420 47335475729280 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp28qpivk0
W0618 12:07:10.366476 47159883187072 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmv8zba3i
I0618 12:07:10.367416 47335475729280 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp28qpivk0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0d6ecdae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.367446 47159883187072 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmv8zba3i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae48cac1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.367809 47335475729280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.367839 47159883187072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881230.309903 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.310694 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.311459 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.370129 47875515597696 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.288683 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.289626 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.290459 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.370149 47037183005568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.371266 47875515597696 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmplp4px2bs
W0618 12:07:10.371234 47037183005568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6n781m_5
I0618 12:07:10.372261 47037183005568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6n781m_5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac7fb2bee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.372289 47875515597696 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmplp4px2bs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8b2bb00e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.372668 47037183005568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.372693 47875515597696 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.372432 47159883187072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.372453 47335475729280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.377379 47037183005568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.377468 47875515597696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881230.303359 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.304258 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.305158 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.383139 47263626466176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.313372 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.314130 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.314801 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.383272 47817164198784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.384195 47263626466176 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmwv4855y
W0618 12:07:10.384282 47817164198784 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpn4i9sn_l
I0618 12:07:10.385264 47263626466176 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmwv4855y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afcb440edd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.385338 47817164198784 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpn4i9sn_l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d95ac5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.385697 47263626466176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.385764 47817164198784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.386031 47348895126400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881230.310280 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.310663 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.311002 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.385848 47906245161856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.386072 47555917165440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881230.305028 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.305495 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.305913 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.385937 47927659053952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.386827 47906245161856 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpko1_z3u7
W0618 12:07:10.386883 47927659053952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpug25w8rb
I0618 12:07:10.387812 47906245161856 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpko1_z3u7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b92534fee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.387850 47927659053952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpug25w8rb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b974fae0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.388205 47906245161856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.388234 47927659053952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.390695 47263626466176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.390726 47817164198784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.391840 47159883187072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.391860 47335475729280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.392019 47687976055680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.392368 47010428679040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.392923 47927659053952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.392922 47906245161856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.396315 47687976055680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:10.396678 47010428679040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:10.399698 47037183005568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.400211 47875515597696 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:07:10.401365 47687976055680 estimator.py:1111] Calling model_fn.
W0618 12:07:10.401476 47687976055680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:10.401733 47010428679040 estimator.py:1111] Calling model_fn.
W0618 12:07:10.401842 47010428679040 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:10.402911 47687976055680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:10.403307 47010428679040 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:10.410809 47263626466176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.411004 47817164198784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.412362 47927659053952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.412359 47906245161856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881230.372946 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.373373 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.373700 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.422831 47326054679424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.371177 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.371559 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.371883 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.422973 47587602801536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.423833 47326054679424 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpbp9r_668
W0618 12:07:10.423955 47587602801536 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpl07jp8qt
I0618 12:07:10.424810 47326054679424 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpbp9r_668', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b3d43de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.424929 47587602801536 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpl07jp8qt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4822bf7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.425199 47326054679424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.425319 47587602801536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881230.346088 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.346956 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.347776 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.426104 47564429742976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.346923 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.347801 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.348552 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.426124 46928800523136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.427101 46928800523136 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpd4nsc37q
W0618 12:07:10.427129 47564429742976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpw6xqbuol
I0618 12:07:10.428094 46928800523136 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpd4nsc37q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aaebf127e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.428113 47564429742976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpw6xqbuol', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b42bd869e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.428491 46928800523136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.428502 47564429742976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.429801 47326054679424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.429936 47587602801536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.433471 46928800523136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.433486 47564429742976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.435792 47555917165440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.435756 47348895126400 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881230.388534 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.388977 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.389371 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.435969 47841375138688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.437053 47841375138688 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqp1ulex7
I0618 12:07:10.438193 47841375138688 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqp1ulex7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8338c20e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.438634 47841375138688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.439351 47159883187072 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.439467 47335475729280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.440219 47555917165440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:10.440194 47348895126400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881230.396079 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.396545 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.396930 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.442457 47246772106112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
W0618 12:07:10.443670 47841375138688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.443642 47159883187072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:10.443753 47335475729280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881230.364161 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.365054 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.365940 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.443896 47636984656768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
:::MLL 1560881230.371757 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881230.372491 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881230.373167 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:10.443949 47441430717312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz_0
I0618 12:07:10.443506 47246772106112 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af8c7a7bd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.444677 47246772106112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.445394 47555917165440 estimator.py:1111] Calling model_fn.
I0618 12:07:10.445386 47348895126400 estimator.py:1111] Calling model_fn.
W0618 12:07:10.445503 47555917165440 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:10.445506 47348895126400 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:10.445014 47636984656768 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpugpaekt6
W0618 12:07:10.445056 47441430717312 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppiea3r33
I0618 12:07:10.446140 47636984656768 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpugpaekt6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b53a222ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.446183 47441430717312 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppiea3r33', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b261a367e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:10.446581 47636984656768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:10.446633 47441430717312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:10.446977 47555917165440 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:10.446979 47348895126400 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:07:10.448699 47159883187072 estimator.py:1111] Calling model_fn.
W0618 12:07:10.448809 47159883187072 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:10.449231 47326054679424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:07:10.448813 47335475729280 estimator.py:1111] Calling model_fn.
W0618 12:07:10.448924 47335475729280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:10.449284 47587602801536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.449653 47246772106112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.450176 47159883187072 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:10.450261 47335475729280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:10.451815 47037183005568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.451874 47441430717312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.451958 47636984656768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:10.452772 47875515597696 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.453080 46928800523136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.453278 47564429742976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:10.456360 47037183005568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:10.457403 47875515597696 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:10.459763 47927659053952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.459694 47263626466176 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:10.459815 47817164198784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable us[2019-06-18 12:07:49] moving /lfs/lfs12/gma_akey/results/epb002/models/000010-000006.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000010-000007.meta
[2019-06-18 12:07:49] moving /lfs/lfs12/gma_akey/results/epb002/models/000010-000006.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000010-000007.data-00000-of-00001
[2019-06-18 12:07:49] moving /lfs/lfs12/gma_akey/results/epb002/models/000010-000006.index --> /lfs/lfs12/gma_akey/results/epb002/models/000010-000007.index
[2019-06-18 12:07:49] moving /lfs/lfs12/gma_akey/results/epb002/models/000010-000006.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb
[2019-06-18 12:07:49] iteration time 9: 48.817 seconds
2019-06-18 12:07:50.549205: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881269.823456 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 10}}
[2019-06-18 12:07:53] minmax time: 3.219 seconds
2019-06-18 12:07:53.778533: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:07:53.783964: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:07:53.788595: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881273.799785 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 9}}
[2019-06-18 12:07:53] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000011-000007 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=12 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:07:53] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-11-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=11 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=1023779842 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=2047559673 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=3071339504 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=4095119335 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=5118899166 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=6142678997 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=7166458828 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=8190238659 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=9214018490 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=10237798321 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=11261578152 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=12285357983 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=13309137814 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=14332917645 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=15356697476 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=16380477307 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=17404257138 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=18428036969 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=19451816800 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000010-000007 --seed=20475596631 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:08:05] eval finished: 11.538 seconds
[2019-06-18 12:08:05] Win rate 000010-000007 vs 000009-000006: 0.700
:::MLL 1560881285.398875 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 9}}
[2019-06-18 12:08:05] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-12-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=12 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=1023779843 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=2047559674 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=3071339505 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=4095119336 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=5118899167 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=6142678998 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=7166458829 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=8190238660 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=9214018491 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=10237798322 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=11261578153 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=12285357984 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=13309137815 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=14332917646 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=15356697477 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=16380477308 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=17404257139 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000011-000006 --seed=18428036970 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:08:37] selfplay finished: 31.837 seconds
[2019-06-18 12:08:37] selfplay mn: 31.855 seconds
[2019-06-18 12:08:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-12-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779843 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559674 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339505 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119336 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899167 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678998 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458829 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238660 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018491 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798322 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578153 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357984 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137815 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917646 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697477 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477308 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257139 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036970 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816801 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596632 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376463 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156294 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000011-000006/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:08:38] train finished: 44.730 seconds
:::MLL 1560881279.116367 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.117185 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.117900 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.188992 46959469138816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.095816 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.096737 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.097587 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.189013 47426756043648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.190111 46959469138816 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmplfy9w_vm
W0618 12:07:59.190144 47426756043648 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfcxk7y0a
:::MLL 1560881279.103282 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.104192 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.105073 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.191237 47252825592704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.111990 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.112711 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.113394 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.191231 47559327314816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 12:07:59.191202 46959469138816 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmplfy9w_vm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5e3105e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.191249 47426756043648 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfcxk7y0a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b22af889e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.191650 46959469138816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.191701 47426756043648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.192329 47559327314816 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpv9zdkob0
W0618 12:07:59.192361 47252825592704 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpylvj2jog
I0618 12:07:59.193414 47559327314816 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpv9zdkob0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b418d65be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.193442 47252825592704 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpylvj2jog', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afa3078ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.193855 47559327314816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.193876 47252825592704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.196938 46959469138816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.196949 47426756043648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.199107 47252825592704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.199111 47559327314816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881279.127412 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.128357 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.129244 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.209398 47761640735616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.210436 47761640735616 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpp3068bti
I0618 12:07:59.211479 47761640735616 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpp3068bti', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b70a8379dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.211905 47761640735616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.217118 47761640735616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.219278 47426756043648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.219297 46959469138816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881279.161236 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.162012 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.162725 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.219663 47642431341440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.221321 47559327314816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.221450 47252825592704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.220651 47642431341440 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxgrro0ra
I0618 12:07:59.221650 47642431341440 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxgrro0ra', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b54e6c88e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.222054 47642431341440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881279.172648 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.173057 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.173399 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.225718 47915343090560 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.169629 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.170118 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.170450 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.225805 47650269541248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.226799 47642431341440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881279.166402 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.166801 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.167128 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.227171 47372358620032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.168366 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.168803 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.169172 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.227204 47635367912320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.226721 47915343090560 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpof40u65s
W0618 12:07:59.226781 47650269541248 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjnw3jqaa
I0618 12:07:59.227695 47915343090560 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpof40u65s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9471976e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.227758 47650269541248 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjnw3jqaa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b56b9fa0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.228091 47915343090560 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.228150 47650269541248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.228147 47372358620032 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpy5hpretn
W0618 12:07:59.228175 47635367912320 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp512d_0ln
I0618 12:07:59.229132 47372358620032 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpy5hpretn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b160531ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.229146 47635367912320 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp512d_0ln', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5341c53e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.229525 47372358620032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.229527 47635367912320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.232823 47915343090560 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.232879 47650269541248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.234250 47372358620032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.234255 47635367912320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.236614 47761640735616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881279.158398 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.159317 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.160228 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.240537 47855228904320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.164324 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.165073 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.165775 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.240607 47872844202880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.241647 47855228904320 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpy7rhadcy
:::MLL 1560881279.157471 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.158421 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.159354 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.242101 47645734466432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.241732 47872844202880 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3olzzi97
:::MLL 1560881279.168211 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.168957 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.169680 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.242254 47780718760832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 12:07:59.242743 47855228904320 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpy7rhadcy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b867281be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.242860 47872844202880 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3olzzi97', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8a8c75de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.243180 47855228904320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.243315 47872844202880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.243105 47645734466432 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3m89r5u0
W0618 12:07:59.243253 47780718760832 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmps9yf9qn8
I0618 12:07:59.244078 47645734466432 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3m89r5u0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55abaa3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.244253 47780718760832 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmps9yf9qn8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b75195b1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.244511 47645734466432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.244675 47780718760832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.246163 47642431341440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.248408 47855228904320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.248630 47872844202880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.249447 47645734466432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.249636 47780718760832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.252279 47915343090560 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.252432 47650269541248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.253522 47372358620032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.253776 47635367912320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.269917 47252825592704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.269969 47559327314816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.269774 47426756043648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.270132 46959469138816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.270663 47855228904320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.271748 47872844202880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.271776 47645734466432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.272362 47780718760832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:07:59.274220 47252825592704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.274279 47559327314816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.274205 47426756043648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.274555 46959469138816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:07:59.279286 47252825592704 estimator.py:1111] Calling model_fn.
I0618 12:07:59.279361 47559327314816 estimator.py:1111] Calling model_fn.
W0618 12:07:59.279395 47252825592704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.279469 47559327314816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:59.279385 47426756043648 estimator.py:1111] Calling model_fn.
W0618 12:07:59.279510 47426756043648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:07:59.279772 46959469138816 estimator.py:1111] Calling model_fn.
W0618 12:07:59.279888 46959469138816 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.280748 47252825592704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.280832 47559327314816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881279.227539 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.227913 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.228240 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.280950 47308294755200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.280952 47426756043648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.281336 46959469138816 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881279.202230 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.202992 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.203753 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.281585 47250151551872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.204371 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.205114 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.205827 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.281752 47037074916224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.229692 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.230074 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.230451 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.281732 47558778049408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.281927 47308294755200 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpc7x8727w
:::MLL 1560881279.232877 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.233259 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.233617 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.282840 47983776031616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
I0618 12:07:59.283003 47308294755200 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpc7x8727w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b071ab0ddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:59.282594 47250151551872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmaf_7jn2
:::MLL 1560881279.233725 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.234106 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.234429 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.283258 47253118894976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.282732 47037074916224 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp11j7qi0d
I0618 12:07:59.283430 47308294755200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.283597 47250151551872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmaf_7jn2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af991160e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.282808 47558778049408 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b416ca89d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.283723 47037074916224 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp11j7qi0d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac7f4ba9dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.284042 47250151551872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.284141 47037074916224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.283975 47558778049408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.283896 47983776031616 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpi08gxic7
I0618 12:07:59.284871 47983776031616 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpi08gxic7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba460832e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:59.284636 47761640735616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.284256 47253118894976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqv1_1ruu
I0618 12:07:59.285235 47253118894976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqv1_1ruu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afa41f40da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.285266 47983776031616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.285641 47253118894976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.288075 47308294755200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.288529 47558778049408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.289061 47250151551872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.288921 47761640735616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:07:59.289102 47037074916224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.290114 47983776031616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.290316 47253118894976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:07:59.293409 47642431341440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:07:59.293974 47761640735616 estimator.py:1111] Calling model_fn.
W0618 12:07:59.294086 47761640735616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:07:59.295428 47761640735616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:07:59.297716 47642431341440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881279.245894 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.246289 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.246632 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.297837 47481209766784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
:::MLL 1560881279.245128 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881279.245598 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881279.245947 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:07:59.297858 47560380298112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000001-000000.tfrecord.zz_0_0
W0618 12:07:59.298835 47481209766784 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpvmtn4swd
W0618 12:07:59.298867 47560380298112 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjnqaa5zg
I0618 12:07:59.299821 47481209766784 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpvmtn4swd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2f5d3a8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:07:59.299843 47560380298112 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjnqaa5zg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b41cc28de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:07:59.299741 47915343090560 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:07:59.300240 47560380298112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:07:59.300254 47481209766784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:07:59.300173 47650269541248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.300991 47372358620032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:07:59.301608 47635367912320 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:07:59.302[2019-06-18 12:08:40] divide_golden_chunk finished: 3.289 seconds
[2019-06-18 12:08:40] generate golden chunk: 3.304 seconds
[2019-06-18 12:08:40] moving /lfs/lfs12/gma_akey/results/epb002/models/000011-000007.index --> /lfs/lfs12/gma_akey/results/epb002/models/000011-000008.index
[2019-06-18 12:08:40] moving /lfs/lfs12/gma_akey/results/epb002/models/000011-000007.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000011-000008.meta
[2019-06-18 12:08:40] moving /lfs/lfs12/gma_akey/results/epb002/models/000011-000007.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb
[2019-06-18 12:08:40] moving /lfs/lfs12/gma_akey/results/epb002/models/000011-000007.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000011-000008.data-00000-of-00001
[2019-06-18 12:08:40] iteration time 10: 50.778 seconds
2019-06-18 12:08:41.399310: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881320.601482 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 11}}
[2019-06-18 12:08:44] minmax time: 3.232 seconds
2019-06-18 12:08:44.641605: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:08:44.646912: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:08:44.651576: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881324.662880 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 10}}
[2019-06-18 12:08:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=13 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:08:44] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-12-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=12 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=1023779843 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=2047559674 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=3071339505 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=4095119336 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=5118899167 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=6142678998 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=7166458829 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=8190238660 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=9214018491 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=10237798322 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=11261578153 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=12285357984 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=13309137815 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=14332917646 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=15356697477 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=16380477308 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=17404257139 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=18428036970 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=19451816801 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000011-000008 --seed=20475596632 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:08:57] eval finished: 12.618 seconds
[2019-06-18 12:08:57] Win rate 000011-000008 vs 000010-000007: 0.440
:::MLL 1560881337.344694 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 10}}
[2019-06-18 12:08:57] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-13-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=13 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=1023779844 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=2047559675 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=3071339506 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=4095119337 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=5118899168 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=6142678999 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=7166458830 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=8190238661 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=9214018492 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=10237798323 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=11261578154 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=12285357985 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=13309137816 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=14332917647 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=15356697478 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=16380477309 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=17404257140 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000012-000007 --seed=18428036971 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:09:29] selfplay finished: 31.941 seconds
[2019-06-18 12:09:29] selfplay mn: 31.959 seconds
[2019-06-18 12:09:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-13-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779844 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559675 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339506 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119337 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899168 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142678999 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458830 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238661 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018492 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798323 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578154 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357985 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137816 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917647 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697478 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477309 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257140 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036971 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816802 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596633 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376464 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156295 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000012-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:09:29] train finished: 44.703 seconds
:::MLL 1560881329.952199 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.952960 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.953630 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.027391 47816595547008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.945254 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.946127 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.946936 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.027457 47586404963200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.028411 47816595547008 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpngzbmw08
W0618 12:08:50.028442 47586404963200 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmhjmcfqm
I0618 12:08:50.029397 47816595547008 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpngzbmw08', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7d73c76e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.029422 47586404963200 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmhjmcfqm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b47db59ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.029785 47816595547008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.029822 47586404963200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.034629 47586404963200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.034588 47816595547008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.055905 47816595547008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.056807 47586404963200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881329.981334 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.982255 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.983068 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.063998 47233856226176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.991472 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.992188 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.992896 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.064015 47284485968768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.065103 47233856226176 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1xpb2d_u
W0618 12:08:50.065165 47284485968768 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpx8gk8opc
I0618 12:08:50.066247 47233856226176 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1xpb2d_u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af5c5cf1da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.066282 47284485968768 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpx8gk8opc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b018f93ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.066705 47233856226176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.066724 47284485968768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.072052 47233856226176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.072080 47284485968768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881329.990799 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.991547 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.992268 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.078499 47909958099840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881329.993035 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881329.993788 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881329.994412 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.078544 47307246183296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.079606 47307246183296 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpsuv199u2
W0618 12:08:50.079643 47909958099840 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpbu4fbjgb
I0618 12:08:50.080702 47307246183296 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpsuv199u2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b06dc30ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.080735 47909958099840 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpbu4fbjgb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b93309efe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.081147 47307246183296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.081189 47909958099840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.086717 47307246183296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.086745 47909958099840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881330.029650 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.030035 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.030413 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.091395 47818431808384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881330.028522 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.028893 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.029237 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.091656 47952220156800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.092384 47818431808384 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp53io1b0r
W0618 12:08:50.092615 47952220156800 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpiryju2_l
I0618 12:08:50.093364 47818431808384 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp53io1b0r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7de13a8da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.093588 47952220156800 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpiryju2_l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9d07a2ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.093756 47818431808384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.093969 47952220156800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.094283 47233856226176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.094381 47284485968768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.098367 47818431808384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.098535 47952220156800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881330.025628 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.026444 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.027133 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.104902 47146251879296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881330.024340 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.025168 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.025952 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.104917 47951727735680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.105889 47146251879296 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqs27dt0o
W0618 12:08:50.105924 47951727735680 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpp5aj0nd8
:::MLL 1560881330.023926 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.024817 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.025707 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.106534 47360325956480 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881330.032810 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.033484 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.034165 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.106656 47016063603584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
I0618 12:08:50.106884 47146251879296 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqs27dt0o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae1602ede48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.106905 47951727735680 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpp5aj0nd8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9cea490e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.107323 47146251879296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.107344 47951727735680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.107286 47816595547008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:50.107626 47360325956480 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpvpqedk8u
W0618 12:08:50.107702 47016063603584 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpt0cg9r20
I0618 12:08:50.108601 47360325956480 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpvpqedk8u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1337fdfdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:50.108285 47586404963200 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:08:50.108703 47016063603584 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpt0cg9r20', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac3105b5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:50.108832 47307246183296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:50.108987 47360325956480 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.108949 47909958099840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:08:50.109100 47016063603584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.111566 47816595547008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:50.112209 47146251879296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.112218 47951727735680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.112566 47586404963200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:50.113811 47360325956480 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.113880 47016063603584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881330.060954 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.061329 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.061651 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.115041 47506407420800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881330.063060 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.063460 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.063803 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.115214 47544019297152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881330.052072 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.052526 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.052898 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.115579 47636042933120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881330.051429 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.051950 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.052345 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.115600 47116819702656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.116055 47506407420800 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzaahzp03
W0618 12:08:50.116180 47544019297152 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpk0jnaglj
I0618 12:08:50.116633 47816595547008 estimator.py:1111] Calling model_fn.
I0618 12:08:50.117078 47506407420800 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzaahzp03', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b353b204e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:50.116741 47816595547008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:50.117276 47544019297152 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpk0jnaglj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3dfcf7ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:50.116641 47636042933120 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6t3jlo3u
I0618 12:08:50.117485 47506407420800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.116670 47116819702656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjp3j9673
I0618 12:08:50.117619 47636042933120 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6t3jlo3u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b536a011e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.117664 47116819702656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjp3j9673', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ada85e37e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.117698 47544019297152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.118021 47636042933120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.117689 47586404963200 estimator.py:1111] Calling model_fn.
I0618 12:08:50.118056 47116819702656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.117793 47586404963200 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:50.117811 47818431808384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.118099 47816595547008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:50.118162 47952220156800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.119175 47586404963200 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881330.037587 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.038393 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.039125 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.120633 47842304775040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881330.040887 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.041689 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.042388 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.120747 47165679936384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.122141 47506407420800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.122266 47544019297152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.122656 47636042933120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.122693 47116819702656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.121714 47842304775040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpb6owb1rg
W0618 12:08:50.121799 47165679936384 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9ya37bnq
I0618 12:08:50.122842 47842304775040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpb6owb1rg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b83702b2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.122922 47165679936384 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9ya37bnq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae5e62f7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.123330 47842304775040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.123387 47165679936384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.128767 47165679936384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.128816 47842304775040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.131632 47146251879296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.131667 47951727735680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.133609 47360325956480 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.133751 47016063603584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.141493 47506407420800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.141529 47544019297152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.141953 47636042933120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.142076 47116819702656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.144047 47233856226176 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:50.144178 47284485968768 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:50.148356 47233856226176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:50.148495 47284485968768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:50.150846 47165679936384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:08:50.151291 47842304775040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881330.095400 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.095857 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.096246 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.151774 47998985343872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
I0618 12:08:50.153541 47233856226176 estimator.py:1111] Calling model_fn.
W0618 12:08:50.152810 47998985343872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6048fj9b
I0618 12:08:50.153592 47284485968768 estimator.py:1111] Calling model_fn.
W0618 12:08:50.153651 47233856226176 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:08:50.153697 47284485968768 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:08:50.153791 47998985343872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6048fj9b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7eb0eee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:08:50.154214 47998985343872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881330.099783 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.100227 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.100608 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.154244 47388146451328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.155013 47233856226176 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:08:50.155046 47284485968768 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:08:50.155319 47388146451328 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b19b2390d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:50.156397 47909958099840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:08:50.156432 47388146451328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.156579 47307246183296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881330.103291 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.103700 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.104050 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.158720 47737754878848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
:::MLL 1560881330.102524 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881330.102982 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881330.103349 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:08:50.158786 47221959299968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000002-000000.tfrecord.zz_0_0
W0618 12:08:50.158930 47998985343872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:08:50.159756 47737754878848 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0ttdyzg1
W0618 12:08:50.159785 47221959299968 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkt0jn2gw
I0618 12:08:50.160784 47737754878848 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0ttdyzg1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6b18824e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:50.160712 47909958099840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:08:50.160813 47221959299968 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkt0jn2gw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af300b25e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:08:50.160909 47307246183296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:08:50.161019 47388146451328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:08:50.161195 47737754878848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:08:50.161215 47221959299968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:08:50.165147 47818431808384 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:50.165584 47952220156800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:08:50.165900 47221959299968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:08:50.165815 47909958099840 estimator.py:1111] Calling model_fn.
W0618 12:08:50.165898 47737754878848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (fro[2019-06-18 12:09:32] divide_golden_chunk finished: 3.301 seconds
[2019-06-18 12:09:32] generate golden chunk: 3.315 seconds
[2019-06-18 12:09:32] iteration time 11: 52.020 seconds
2019-06-18 12:09:33.425366: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881372.621248 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 12}}
[2019-06-18 12:09:36] minmax time: 3.251 seconds
2019-06-18 12:09:36.686390: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:09:36.691744: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:09:36.696092: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881376.708474 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 11}}
[2019-06-18 12:09:36] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000013-000008 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=14 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:09:36] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-13-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=13 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=1023779844 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=2047559675 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=3071339506 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=4095119337 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=5118899168 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=6142678999 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=7166458830 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=8190238661 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=9214018492 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=10237798323 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=11261578154 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=12285357985 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=13309137816 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=14332917647 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=15356697478 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=16380477309 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=17404257140 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=18428036971 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=19451816802 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000012-000008 --seed=20475596633 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:09:47] eval finished: 10.847 seconds
[2019-06-18 12:09:47] Win rate 000012-000008 vs 000010-000007: 0.550
:::MLL 1560881387.616700 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 11}}
[2019-06-18 12:09:47] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-14-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=14 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=1023779845 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=2047559676 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=3071339507 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=4095119338 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=5118899169 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=6142679000 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=7166458831 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=8190238662 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=9214018493 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=10237798324 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=11261578155 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=12285357986 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=13309137817 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=14332917648 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=15356697479 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=16380477310 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=17404257141 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000013-000007 --seed=18428036972 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:10:18] selfplay finished: 30.418 seconds
[2019-06-18 12:10:18] selfplay mn: 30.436 seconds
[2019-06-18 12:10:18] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-14-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779845 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559676 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339507 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119338 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899169 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679000 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458831 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238662 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018493 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798324 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578155 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357986 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137817 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917648 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697479 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477310 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257141 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036972 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816803 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596634 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376465 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156296 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000013-000007/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:10:21] divide_golden_chunk finished: 3.291 seconds
[2019-06-18 12:10:21] generate golden chunk: 3.306 seconds
[2019-06-18 12:10:21] train finished: 44.641 seconds
:::MLL 1560881381.944303 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881381.945217 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881381.945921 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.026772 47323500499840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881381.948251 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881381.948995 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881381.949672 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.026802 47710974497664 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.027799 47323500499840 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpvitfxp5u
W0618 12:09:42.027832 47710974497664 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7b9tsskn
I0618 12:09:42.028773 47323500499840 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpvitfxp5u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0aa5062e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.028812 47710974497664 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7b9tsskn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b64dc462e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.029175 47323500499840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.029216 47710974497664 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.033878 47323500499840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.033879 47710974497664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.056201 47710974497664 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.056712 47323500499840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881382.007692 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.008414 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.009095 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.085417 47878657135488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.001965 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.002896 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.003745 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.085678 47864378798976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.086426 47878657135488 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp36c9j2sa
W0618 12:09:42.086665 47864378798976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpcvkhz7oh
I0618 12:09:42.087435 47878657135488 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp36c9j2sa', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8be6f01e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.087666 47864378798976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpcvkhz7oh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8893e20e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.087827 47878657135488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.088065 47864378798976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881382.023864 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.024324 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.024707 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.088776 47104677245824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.089877 47104677245824 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkbfkmwl9
I0618 12:09:42.090968 47104677245824 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkbfkmwl9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad7b2243e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.091424 47104677245824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.092897 47878657135488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.093076 47864378798976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.096290 47104677245824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881382.023820 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.024278 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.024664 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.096254 47987459199872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.097212 47987459199872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp978hfcpt
I0618 12:09:42.098228 47987459199872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp978hfcpt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba53c0bee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.098641 47987459199872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.103409 47987459199872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.109446 47710974497664 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:42.109920 47323500499840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:42.112722 47864378798976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.112816 47878657135488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.113795 47710974497664 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:42.114256 47323500499840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:42.116096 47104677245824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881382.030257 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.030998 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.031658 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.118545 47821688484736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.027589 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.028357 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.029057 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.118567 47226248070016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
I0618 12:09:42.118919 47710974497664 estimator.py:1111] Calling model_fn.
W0618 12:09:42.119047 47710974497664 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:42.119318 47323500499840 estimator.py:1111] Calling model_fn.
W0618 12:09:42.119428 47323500499840 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:42.119670 47821688484736 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpcg3hsfyo
W0618 12:09:42.119635 47226248070016 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpo8clkn8a
I0618 12:09:42.120689 47226248070016 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpo8clkn8a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af40053ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.120696 47821688484736 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpcg3hsfyo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ea3577e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:42.120417 47710974497664 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:42.120782 47323500499840 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:09:42.121148 47821688484736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.121148 47226248070016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881382.032877 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.033658 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.034321 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.121506 47570330846080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.022754 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.023670 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.024546 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.121488 47109572965248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.122747 47987459199872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.122670 47109572965248 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpcagaitpt
W0618 12:09:42.122701 47570330846080 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpieo3xb8f
I0618 12:09:42.123749 47109572965248 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpcagaitpt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad8d5f30e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.123796 47570330846080 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpieo3xb8f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b441d422e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.124187 47109572965248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.124225 47570330846080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.126736 47821688484736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.126720 47226248070016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.129575 47570330846080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.129579 47109572965248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881382.021942 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.022894 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.023865 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.134604 47684832449408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.030909 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.031689 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.032389 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.134658 47557001012096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.135704 47684832449408 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpce_535x7
W0618 12:09:42.135763 47557001012096 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpuke__zr5
I0618 12:09:42.136816 47684832449408 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpce_535x7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5ec6162da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.136861 47557001012096 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpuke__zr5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4102bd2da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.137259 47684832449408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.137303 47557001012096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.142537 47684832449408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.142557 47557001012096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.148753 47226248070016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.148882 47821688484736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.152147 47109572965248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.152311 47570330846080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881382.094389 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.094819 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.095181 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.157953 47524175967104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.091445 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.091989 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.092402 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.157988 46990265664384 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.091812 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.092237 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.092556 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.158238 47901443597184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.087999 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.088560 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.088970 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.158342 46956551680896 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.159026 47524175967104 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp479b1v50
W0618 12:09:42.158999 46990265664384 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp87xehg9p
I0618 12:09:42.159972 46990265664384 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp87xehg9p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abd0eae1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.159993 47524175967104 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp479b1v50', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b395e36ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.159269 47901443597184 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b91351dfd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:42.159310 46956551680896 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmprm1dfnt2
I0618 12:09:42.160362 46990265664384 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.160389 47524175967104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.160263 46956551680896 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmprm1dfnt2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5352b8dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.160364 47901443597184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.160657 46956551680896 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.161359 47864378798976 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881382.095820 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.096329 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.096777 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.161405 47535638950784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.100385 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.100798 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.101161 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.161617 47492774241152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.065447 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.065914 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.066307 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.161859 47875474531200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
:::MLL 1560881382.065955 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.066397 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.066765 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.161917 47689073038208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.162148 47878657135488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:09:42.162456 47535638950784 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7szc_hn0
W0618 12:09:42.162632 47492774241152 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpv77trz92
I0618 12:09:42.163511 47535638950784 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7szc_hn0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c09760e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:09:42.162951 47689073038208 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8n_kas61
W0618 12:09:42.162911 47875474531200 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpo439u0o7
I0618 12:09:42.163647 47492774241152 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpv77trz92', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b320e866dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.163911 47875474531200 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpo439u0o7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8b293d7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.163940 47689073038208 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8n_kas61', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5fc2d85e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.163910 47535638950784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.164039 47492774241152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.163832 47104677245824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:09:42.164304 47875474531200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:09:42.164327 47689073038208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.164339 47684832449408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.164446 47557001012096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.165126 46990265664384 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.165011 47901443597184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.165150 47524175967104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.165294 46956551680896 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.165746 47864378798976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:42.166476 47878657135488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:42.168157 47104677245824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:09:42.168680 47535638950784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.168741 47492774241152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.169055 47689073038208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.169058 47875474531200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.170067 47987459199872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:09:42.170811 47864378798976 estimator.py:1111] Calling model_fn.
W0618 12:09:42.170919 47864378798976 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:09:42.171559 47878657135488 estimator.py:1111] Calling model_fn.
W0618 12:09:42.171668 47878657135488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881382.100262 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.101053 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.101774 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.172226 47073907360640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.172289 47864378798976 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881382.087837 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881382.088799 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881382.089680 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:09:42.172343 47250991747968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000003-000001.tfrecord.zz_0_0
W0618 12:09:42.173025 47878657135488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:09:42.173213 47104677245824 estimator.py:1111] Calling model_fn.
W0618 12:09:42.173323 47104677245824 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:42.173260 47073907360640 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgyiqm_rd
W0618 12:09:42.173314 47250991747968 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpt25ugpw8
I0618 12:09:42.174276 47073907360640 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgyiqm_rd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad0881d0dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.174293 47250991747968 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpt25ugpw8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af9c32a6dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:09:42.174681 47073907360640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.174357 47987459199872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:09:42.174700 47250991747968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:09:42.174684 47104677245824 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:42.179570 47250991747968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:09:42.179572 47073907360640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:09:42.179427 47987459199872 estimator.py:1111] Calling model_fn.
W0618 12:09:42.179536 47987459199872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:09:42.180891 47987459199872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:09:42.184377 47901443597184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.184451 47524175967104 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.184420 46990265664384 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.184760 46956551680896 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.188028 47535638950784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:09:42.188069 47492774241152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.[2019-06-18 12:10:21] moving /lfs/lfs12/gma_akey/results/epb002/models/000013-000008.index --> /lfs/lfs12/gma_akey/results/epb002/models/000013-000009.index
[2019-06-18 12:10:21] moving /lfs/lfs12/gma_akey/results/epb002/models/000013-000008.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000013-000009.data-00000-of-00001
[2019-06-18 12:10:21] moving /lfs/lfs12/gma_akey/results/epb002/models/000013-000008.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb
[2019-06-18 12:10:21] moving /lfs/lfs12/gma_akey/results/epb002/models/000013-000008.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000013-000009.meta
[2019-06-18 12:10:21] iteration time 12: 48.789 seconds
2019-06-18 12:10:22.267112: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881421.410449 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 13}}
[2019-06-18 12:10:25] minmax time: 3.221 seconds
2019-06-18 12:10:25.497710: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:10:25.503214: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:10:25.507818: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881425.519275 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 12}}
[2019-06-18 12:10:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000014-000009 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=15 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:10:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-14-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=14 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=1023779845 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=2047559676 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=3071339507 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=4095119338 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=5118899169 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=6142679000 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=7166458831 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=8190238662 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=9214018493 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=10237798324 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=11261578155 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=12285357986 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=13309137817 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=14332917648 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=15356697479 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=16380477310 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=17404257141 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=18428036972 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=19451816803 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000013-000009 --seed=20475596634 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:10:37] eval finished: 11.703 seconds
[2019-06-18 12:10:37] Win rate 000013-000009 vs 000012-000008: 0.620
:::MLL 1560881437.287108 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 12}}
[2019-06-18 12:10:37] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-15-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=15 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=1023779846 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=2047559677 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=3071339508 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=4095119339 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=5118899170 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=6142679001 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=7166458832 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=8190238663 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=9214018494 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=10237798325 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=11261578156 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=12285357987 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=13309137818 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=14332917649 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=15356697480 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=16380477311 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=17404257142 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000014-000008 --seed=18428036973 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:11:06] selfplay finished: 28.851 seconds
[2019-06-18 12:11:06] selfplay mn: 28.872 seconds
[2019-06-18 12:11:06] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-15-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779846 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559677 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339508 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119339 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899170 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679001 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458832 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238663 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018494 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798325 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578156 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357987 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137818 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917649 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697480 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477311 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257142 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036973 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816804 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596635 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376466 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156297 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000014-000008/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:11:09] divide_golden_chunk finished: 3.248 seconds
[2019-06-18 12:11:09] generate golden chunk: 3.262 seconds
[2019-06-18 12:11:10] train finished: 44.475 seconds
:::MLL 1560881430.801998 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.802730 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.803392 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.888870 47687488955264 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.798733 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.799531 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.800253 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.889250 47255214961536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.889976 47687488955264 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4ejmftxd
I0618 12:10:30.891098 47687488955264 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4ejmftxd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5f646d3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:30.890369 47255214961536 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpo8v2y2gn
I0618 12:10:30.891503 47255214961536 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpo8v2y2gn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afabee38e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.891546 47687488955264 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:30.891972 47255214961536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:30.896831 47687488955264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.897612 47255214961536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.919159 47687488955264 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.920247 47255214961536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881430.835203 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.836108 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.836973 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.926471 47759705539456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.835805 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.836737 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.837562 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.927128 47804998484864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.927608 47759705539456 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpikmsx4x6
I0618 12:10:30.928696 47759705539456 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpikmsx4x6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7034dece10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:30.928210 47804998484864 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp50uldcsi
I0618 12:10:30.929144 47759705539456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:30.929298 47804998484864 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp50uldcsi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7ac08a4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.929762 47804998484864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881430.868936 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.869435 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.869857 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.929653 47416650883968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.873395 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.873816 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.874216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.930739 47110441788288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.930647 47416650883968 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphr5p5kfd
I0618 12:10:30.931622 47416650883968 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphr5p5kfd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2055383e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.932011 47416650883968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:30.931704 47110441788288 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmptu99abwc
I0618 12:10:30.932685 47110441788288 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmptu99abwc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad909bc3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.933075 47110441788288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:30.934405 47759705539456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.935034 47804998484864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.936743 47416650883968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.937691 47110441788288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881430.858244 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.858954 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.859641 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.941250 47725264974720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.856273 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.857040 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.857843 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.941384 47262765990784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.942338 47725264974720 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpp514w2hw
W0618 12:10:30.942501 47262765990784 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmprtqmvf3z
I0618 12:10:30.943449 47725264974720 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpp514w2hw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b68300d8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.943572 47262765990784 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmprtqmvf3z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afc80f70e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.943867 47725264974720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:30.943982 47262765990784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:30.948752 47725264974720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.948781 47262765990784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881430.806857 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.807713 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.808538 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.948863 47597062407040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.808643 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.809448 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.810186 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.949222 47896262636416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.949982 47597062407040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0mofvsmm
W0618 12:10:30.950274 47896262636416 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpy65uadoe
I0618 12:10:30.951095 47597062407040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0mofvsmm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4a56959e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.951357 47896262636416 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpy65uadoe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b90004ebe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.951541 47597062407040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:30.951814 47896262636416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:30.956074 47416650883968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.956609 47759705539456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.956817 47597062407040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.956977 47896262636416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.956914 47110441788288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.957562 47804998484864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881430.895420 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.895918 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.896333 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.961857 47462838813568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.962901 47462838813568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpi99rdv4n
I0618 12:10:30.963989 47462838813568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpi99rdv4n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2b163c1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.964434 47462838813568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:30.967980 47687488955264 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:30.968407 47725264974720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.968449 47262765990784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881430.844558 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.845012 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.845334 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.968572 47776212964224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.968587 47255214961536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:30.969512 47462838813568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.969591 47776212964224 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp88qu1tt3
I0618 12:10:30.970648 47776212964224 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp88qu1tt3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b740cca2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.971072 47776212964224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881430.844143 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.844599 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.845000 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.971277 47230474105728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.900880 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.901292 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.901652 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.971719 47449983214464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.972288 47687488955264 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:30.972239 47230474105728 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpidot2y2z
I0618 12:10:30.973206 47230474105728 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpidot2y2z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af4fc37ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:10:30.972912 47255214961536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:30.972767 47449983214464 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpiu0lug9k
I0618 12:10:30.973599 47230474105728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:30.973732 47449983214464 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpiu0lug9k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2817fb3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.974122 47449983214464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:30.975768 47776212964224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:10:30.977330 47687488955264 estimator.py:1111] Calling model_fn.
W0618 12:10:30.977439 47687488955264 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:30.977916 47896262636416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.977874 47597062407040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.978126 47230474105728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:10:30.977951 47255214961536 estimator.py:1111] Calling model_fn.
W0618 12:10:30.978064 47255214961536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:30.978673 47449983214464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.978798 47687488955264 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:30.979408 47255214961536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881430.890797 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.891732 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.892604 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.983797 47897474888576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.901669 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.902423 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.903129 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.983805 47050812076928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.984899 47897474888576 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmprq3dze2f
W0618 12:10:30.984930 47050812076928 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0zjcx6_4
I0618 12:10:30.986014 47897474888576 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmprq3dze2f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9048904dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.986031 47050812076928 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0zjcx6_4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acb27870e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.986469 47897474888576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:30.986477 47050812076928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:30.989032 47462838813568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.991838 47897474888576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.991862 47050812076928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:30.995230 47776212964224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881430.920568 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.921375 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.922115 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.996291 47696392643456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.907681 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.908580 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.909494 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:30.996390 47926524818304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:30.997300 47230474105728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.997872 47449983214464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:30.997323 47696392643456 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjmmr_qlm
W0618 12:10:30.997378 47926524818304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4y4znnn7
I0618 12:10:30.998320 47696392643456 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjmmr_qlm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b617720ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.998347 47926524818304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4y4znnn7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b970c130e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:30.998729 47696392643456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:10:30.998740 47926524818304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881430.939793 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.940175 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.940493 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:31.000744 47520479314816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
:::MLL 1560881430.942002 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881430.942426 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881430.942822 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:10:31.002016 47919720559488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000004-000002.tfrecord.zz_0_0
W0618 12:10:31.001804 47520479314816 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp49wdxlw2
I0618 12:10:31.002822 47520479314816 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp49wdxlw2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3881e05e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:31.003228 47520479314816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:31.003206 47416650883968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:31.003668 47696392643456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:31.003668 47926524818304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:10:31.003052 47919720559488 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9576824d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:10:31.004218 47919720559488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:10:31.004069 47110441788288 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:31.007186 47759705539456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:31.007626 47804998484864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:31.007505 47416650883968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:31.008090 47520479314816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:31.008377 47110441788288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:31.008960 47919720559488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:10:31.011548 47759705539456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:31.011923 47804998484864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:10:31.012541 47416650883968 estimator.py:1111] Calling model_fn.
W0618 12:10:31.012653 47416650883968 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:10:31.013423 47110441788288 estimator.py:1111] Calling model_fn.
W0618 12:10:31.013533 47110441788288 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:31.013966 47897474888576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:31.014084 47050812076928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:31.013997 47416650883968 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:31.014884 47110441788288 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:10:31.016617 47759705539456 estimator.py:1111] Calling model_fn.
W0618 12:10:31.016725 47759705539456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:31.016704 47725264974720 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:10:31.016988 47804998484864 estimator.py:1111] Calling model_fn.
W0618 12:10:31.017100 47804998484864 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:10:31.016931 47262765990784 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:31.018069 47759705539456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:31.018461 47804998484864 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:10:31.021026 47725264974720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:31.021259 47262765990784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:10:31.023379 47696392643456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:31.023381 47926524818304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:10:31.025727 47896262636416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:10:31.025982 47597062407040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
[2019-06-18 12:11:10] moving /lfs/lfs12/gma_akey/results/epb002/models/000014-000009.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000014-000010.data-00000-of-00001
[2019-06-18 12:11:10] moving /lfs/lfs12/gma_akey/results/epb002/models/000014-000009.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb
[2019-06-18 12:11:10] moving /lfs/lfs12/gma_akey/results/epb002/models/000014-000009.index --> /lfs/lfs12/gma_akey/results/epb002/models/000014-000010.index
[2019-06-18 12:11:10] moving /lfs/lfs12/gma_akey/results/epb002/models/000014-000009.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000014-000010.meta
[2019-06-18 12:11:10] iteration time 13: 48.644 seconds
2019-06-18 12:11:10.900254: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881470.054278 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 14}}
[2019-06-18 12:11:14] minmax time: 3.225 seconds
2019-06-18 12:11:14.135452: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:11:14.140956: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:11:14.145499: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881474.156812 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 13}}
[2019-06-18 12:11:14] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000015-000010 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=16 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:11:14] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-15-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=15 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=1023779846 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=2047559677 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=3071339508 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=4095119339 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=5118899170 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=6142679001 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=7166458832 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=8190238663 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=9214018494 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=10237798325 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=11261578156 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=12285357987 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=13309137818 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=14332917649 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=15356697480 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=16380477311 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=17404257142 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=18428036973 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=19451816804 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000014-000010 --seed=20475596635 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:11:25] eval finished: 11.710 seconds
[2019-06-18 12:11:25] Win rate 000014-000010 vs 000013-000009: 0.570
:::MLL 1560881485.928857 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 13}}
[2019-06-18 12:11:25] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-16-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=16 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=1023779847 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=2047559678 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=3071339509 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=4095119340 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=5118899171 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=6142679002 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=7166458833 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=8190238664 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=9214018495 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=10237798326 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=11261578157 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=12285357988 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=13309137819 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=14332917650 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=15356697481 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=16380477312 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=17404257143 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000015-000009 --seed=18428036974 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:11:55] selfplay finished: 30.009 seconds
[2019-06-18 12:11:55] selfplay mn: 30.030 seconds
[2019-06-18 12:11:55] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-16-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779847 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559678 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339509 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119340 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899171 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679002 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458833 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238664 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018495 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798326 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578157 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357988 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137819 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917650 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697481 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477312 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257143 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036974 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816805 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596636 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376467 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156298 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000015-000009/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:11:58] train finished: 44.270 seconds
:::MLL 1560881479.442801 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.443691 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.444532 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.534170 47846890722176 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.450627 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.451347 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.452033 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.534173 47057518474112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:19.535335 47846890722176 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjx7snrib
W0618 12:11:19.535388 47057518474112 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpq7lneuqj
I0618 12:11:19.536426 47846890722176 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjx7snrib', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8481832da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.536541 47057518474112 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpq7lneuqj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2accb7428da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.536871 47846890722176 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.537004 47057518474112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.542085 47846890722176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.542231 47057518474112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881479.472941 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.473797 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.474579 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.562773 47705116033920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.472730 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.473510 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.474319 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.562856 47016656860032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:19.563879 47846890722176 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.564555 47057518474112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.563926 47705116033920 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp24v0u8xt
W0618 12:11:19.563956 47016656860032 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpk6kl_3qv
I0618 12:11:19.565047 47705116033920 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp24v0u8xt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b637f151e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.565057 47016656860032 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpk6kl_3qv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac333b7ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.565499 47016656860032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.565503 47705116033920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.570834 47016656860032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.570880 47705116033920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881479.515991 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.516391 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.516713 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.579607 47559795549056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.518118 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.518492 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.518819 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.579706 47721001407360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:19.580632 47559795549056 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpg9vxwxf2
W0618 12:11:19.580748 47721001407360 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpco3od2op
I0618 12:11:19.581644 47559795549056 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpg9vxwxf2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b41a94e7e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.581750 47721001407360 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpco3od2op', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6731ec9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.582048 47559795549056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.582142 47721001407360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.586659 47559795549056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.586709 47721001407360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.593004 47016656860032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.593482 47705116033920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.605816 47559795549056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.605969 47721001407360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881479.545322 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.545704 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.546040 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.612009 47168343352192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.522425 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.523326 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.524117 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.612444 47986109318016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.542234 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.542737 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.543144 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.612232 47591459574656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.521952 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.522758 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.523590 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.612585 46924795282304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:19.612983 47168343352192 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpymnxqdqj
W0618 12:11:19.613206 47591459574656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpizmpg_1b
I0618 12:11:19.613957 47168343352192 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpymnxqdqj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae684efee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:19.614217 47846890722176 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:19.613551 47986109318016 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7amyeejl
W0618 12:11:19.613668 46924795282304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpcfkmrr54
I0618 12:11:19.614177 47591459574656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpizmpg_1b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4908a12e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.614654 47986109318016 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7amyeejl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba4eb965e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.614371 47168343352192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.614759 46924795282304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpcfkmrr54', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aadd0575e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.614572 47591459574656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.615108 47986109318016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.615206 46924795282304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.615314 47057518474112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:19.618596 47846890722176 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:19.618989 47168343352192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.619171 47591459574656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.619736 47057518474112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:19.620436 46924795282304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.620440 47986109318016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:11:19.623639 47846890722176 estimator.py:1111] Calling model_fn.
W0618 12:11:19.623750 47846890722176 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:11:19.624805 47057518474112 estimator.py:1111] Calling model_fn.
W0618 12:11:19.624914 47057518474112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:19.625100 47846890722176 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:19.626265 47057518474112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881479.538374 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.539283 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.540161 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.626662 47474608149376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.538365 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.539280 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.540167 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.626671 47154954867584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:19.627756 47474608149376 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6u0win8c
W0618 12:11:19.627727 47154954867584 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpaq1ahji9
I0618 12:11:19.628721 47154954867584 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpaq1ahji9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae366ebee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.628743 47474608149376 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6u0win8c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2dd3bdbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.629128 47154954867584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.629157 47474608149376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.634097 47474608149376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.634098 47154954867584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.638334 47168343352192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.638386 47591459574656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.642650 46924795282304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.642678 47986109318016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.642680 47705116033920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:19.642675 47016656860032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:19.646959 47705116033920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:19.646977 47016656860032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:19.651988 47705116033920 estimator.py:1111] Calling model_fn.
I0618 12:11:19.652018 47016656860032 estimator.py:1111] Calling model_fn.
W0618 12:11:19.652102 47705116033920 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:19.652131 47016656860032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:19.653046 47559795549056 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:19.653657 47721001407360 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:19.653456 47705116033920 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:19.653489 47016656860032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:19.653604 47154954867584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.653871 47474608149376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.657325 47559795549056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:19.657970 47721001407360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:19.662360 47559795549056 estimator.py:1111] Calling model_fn.
W0618 12:11:19.662466 47559795549056 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:11:19.663043 47721001407360 estimator.py:1111] Calling model_fn.
W0618 12:11:19.663155 47721001407360 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:19.663790 47559795549056 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:11:19.664511 47721001407360 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881479.604608 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.605028 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.605348 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.664851 47277390001024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.606397 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.606781 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.607105 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.665101 47680999228288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:19.665875 47277390001024 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpg2yhed81
W0618 12:11:19.666064 47680999228288 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpen6zvk65
I0618 12:11:19.666845 47277390001024 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpg2yhed81', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2affe89fce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881479.604645 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.605238 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.605697 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.666589 47347485553536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
I0618 12:11:19.667016 47680999228288 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpen6zvk65', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5de19bcda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.667237 47277390001024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.667401 47680999228288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.667674 47347485553536 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9reb0p7e
I0618 12:11:19.668747 47347485553536 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9reb0p7e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b103aa50e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.669203 47347485553536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.671830 47277390001024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.671952 47680999228288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.674228 47347485553536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881479.618660 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.619121 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.619503 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.675912 47522869252992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
I0618 12:11:19.676921 47522869252992 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b391053dd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.678160 47522869252992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881479.580736 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.581631 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.582488 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.682053 47103668740992 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.598604 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.599370 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.600070 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.682032 48009622897536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:19.682855 47522869252992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.683205 47103668740992 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphjehj9xo
W0618 12:11:19.683239 48009622897536 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpdq9686k1
I0618 12:11:19.684291 47103668740992 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphjehj9xo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad77607be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.684328 48009622897536 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpdq9686k1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2baa651b1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.684752 47103668740992 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:11:19.684770 48009622897536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.685850 47168343352192 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:19.686015 47591459574656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881479.580338 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.581270 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.582123 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.688549 46980590203776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
:::MLL 1560881479.580458 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881479.581383 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881479.582205 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:11:19.688850 47020370277248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000005-000002.tfrecord.zz_0_0
W0618 12:11:19.689573 46980590203776 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5lgnb3sz
W0618 12:11:19.690125 48009622897536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.690148 47103668740992 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.689838 47020370277248 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpwu2n6_n4
I0618 12:11:19.690564 46980590203776 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5lgnb3sz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abacdfa3dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:11:19.690165 47168343352192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:11:19.690299 47591459574656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:11:19.690841 47020370277248 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpwu2n6_n4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac4110dfe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:11:19.690967 46980590203776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.690956 47277390001024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.691167 47680999228288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:11:19.691248 47020370277248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:11:19.693969 47347485553536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:11:19.694664 46924795282304 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:11:19.695034 47986109318016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:11:19.695201 47168343352192 estimator.py:1111] Calling model_fn.
W0618 12:11:19.695307 47168343352192 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:11:19.695327 47591459574656 estimator.py:1111] Calling model_fn.
W0618 12:11:19.695435 47591459574656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:11:19.695786 46980590203776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.696089 47020370277248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:11:19.696657 47168343352192 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops)[2019-06-18 12:11:59] divide_golden_chunk finished: 3.335 seconds
[2019-06-18 12:11:59] generate golden chunk: 3.349 seconds
[2019-06-18 12:11:59] moving /lfs/lfs12/gma_akey/results/epb002/models/000015-000010.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000015-000011.data-00000-of-00001
[2019-06-18 12:11:59] moving /lfs/lfs12/gma_akey/results/epb002/models/000015-000010.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000015-000011.meta
[2019-06-18 12:11:59] moving /lfs/lfs12/gma_akey/results/epb002/models/000015-000010.index --> /lfs/lfs12/gma_akey/results/epb002/models/000015-000011.index
[2019-06-18 12:11:59] moving /lfs/lfs12/gma_akey/results/epb002/models/000015-000010.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb
[2019-06-18 12:11:59] iteration time 14: 49.301 seconds
2019-06-18 12:12:00.274468: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881519.355826 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 15}}
[2019-06-18 12:12:03] minmax time: 3.242 seconds
2019-06-18 12:12:03.526878: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:12:03.532284: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:12:03.536842: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881523.548296 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 14}}
[2019-06-18 12:12:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=17 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:12:03] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-16-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=16 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=1023779847 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=2047559678 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=3071339509 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=4095119340 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=5118899171 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=6142679002 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=7166458833 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=8190238664 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=9214018495 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=10237798326 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=11261578157 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=12285357988 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=13309137819 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=14332917650 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=15356697481 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=16380477312 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=17404257143 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=18428036974 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=19451816805 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000015-000011 --seed=20475596636 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:12:14] eval finished: 11.127 seconds
[2019-06-18 12:12:14] Win rate 000015-000011 vs 000014-000010: 0.470
:::MLL 1560881534.740347 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 14}}
[2019-06-18 12:12:14] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-17-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=17 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=1023779848 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=2047559679 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=3071339510 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=4095119341 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=5118899172 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=6142679003 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=7166458834 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=8190238665 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=9214018496 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=10237798327 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=11261578158 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=12285357989 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=13309137820 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=14332917651 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=15356697482 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=16380477313 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=17404257144 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000016-000010 --seed=18428036975 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:12:45] selfplay finished: 30.588 seconds
[2019-06-18 12:12:45] selfplay mn: 30.606 seconds
[2019-06-18 12:12:45] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-17-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779848 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559679 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339510 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119341 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899172 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679003 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458834 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238665 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018496 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798327 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578158 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357989 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137820 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917651 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697482 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477313 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257144 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036975 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816806 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596637 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376468 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156299 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000016-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:12:48] train finished: 44.504 seconds
:::MLL 1560881528.832215 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.832967 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.833596 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.911104 46982294557568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.819647 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.820513 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.821304 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.911140 47189802836864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:08.912249 47189802836864 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpwjlb_97p
W0618 12:12:08.912204 46982294557568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqc9d7wy5
I0618 12:12:08.913372 47189802836864 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpwjlb_97p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeb8405be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.913380 46982294557568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqc9d7wy5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb3390ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.913833 46982294557568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:08.913842 47189802836864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:08.919165 47189802836864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.919203 46982294557568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.941302 47189802836864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:08.941387 46982294557568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881528.857493 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.858400 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.859245 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.951323 47682866148224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:08.952411 47682866148224 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgpul0hn2
I0618 12:12:08.953470 47682866148224 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgpul0hn2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5e50e2ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.953902 47682866148224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881528.851446 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.852190 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.852869 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.955792 47027993117568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.848286 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.849116 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.849817 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.956194 47375075767168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:08.956915 47027993117568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp88j34b7r
I0618 12:12:08.958021 47027993117568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp88j34b7r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac5d7694e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:08.957268 47375075767168 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpq_krcqzz
I0618 12:12:08.958365 47375075767168 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpq_krcqzz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b16a7264da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.958461 47027993117568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:08.958815 47375075767168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:08.958874 47682866148224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881528.877409 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.878221 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.878942 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.958931 47535813768064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:08.959919 47535813768064 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpduwkbapx
I0618 12:12:08.960927 47535813768064 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpduwkbapx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c13e15e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.961363 47535813768064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:08.963644 47027993117568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.964020 47375075767168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881528.872803 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.873725 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.874566 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.965216 47987916338048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.882805 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.883563 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.884240 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.965369 47539619054464 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:08.966180 47535813768064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.966280 47987916338048 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp27vbocu8
W0618 12:12:08.966384 47539619054464 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2rtx7pff
I0618 12:12:08.967416 47987916338048 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp27vbocu8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba5574b4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.967492 47539619054464 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2rtx7pff', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3cf6b19e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.967844 47987916338048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:08.967915 47539619054464 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881528.903262 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.903644 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.903962 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.968596 47135500350336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.904605 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.904977 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.905340 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.968701 47922893484928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:08.969612 47135500350336 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6hblv0i8
W0618 12:12:08.969703 47922893484928 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpq5ff_926
I0618 12:12:08.970592 47135500350336 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6hblv0i8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adedf577e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.970695 47922893484928 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpq5ff_926', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9633a14e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.970984 47135500350336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:08.971091 47922893484928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:08.973153 47987916338048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.973160 47539619054464 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.975701 47135500350336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.975736 47922893484928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.978236 47682866148224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881528.897185 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.897924 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.898606 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.981775 47546950001536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.891897 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.892807 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.893666 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.981903 47560655082368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:08.982880 47546950001536 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxhl6u6mi
W0618 12:12:08.982982 47560655082368 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpq2u3_p5_
I0618 12:12:08.983978 47546950001536 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxhl6u6mi', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3eaba6ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.984090 47560655082368 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpq2u3_p5_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b41dc89ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.984428 47546950001536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:08.984542 47560655082368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:08.985319 47027993117568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:08.985555 47535813768064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:08.986209 47375075767168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:08.989723 47560655082368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.989726 47546950001536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:08.991352 47189802836864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:08.991658 46982294557568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881528.903099 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.903608 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.904063 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.993242 47069528408960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.906948 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.907365 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.907734 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:08.993346 47374503973760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:08.994248 47069528408960 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0rg3qt6r
W0618 12:12:08.994295 47374503973760 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgl4yi5_w
I0618 12:12:08.995244 47069528408960 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0rg3qt6r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acf831b9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:08.995280 47374503973760 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgl4yi5_w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1685116e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:08.995268 47539619054464 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:08.995328 47987916338048 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:08.995044 47135500350336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:08.995119 47922893484928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:12:08.995642 47069528408960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:08.995673 47374503973760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:08.995615 47189802836864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:08.995971 46982294557568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:09.000296 47374503973760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:09.000325 47069528408960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:09.000647 47189802836864 estimator.py:1111] Calling model_fn.
W0618 12:12:09.000757 47189802836864 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:09.001052 46982294557568 estimator.py:1111] Calling model_fn.
W0618 12:12:09.001160 46982294557568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:09.002110 47189802836864 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:09.002536 46982294557568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:09.011740 47560655082368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:09.011866 47546950001536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:09.019622 47069528408960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:09.019655 47374503973760 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881528.958831 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.959294 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.959697 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:09.020478 47553080722304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:09.021458 47553080722304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_e8yy9kr
I0618 12:12:09.022505 47553080722304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_e8yy9kr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4019124dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:09.022930 47553080722304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881528.958866 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.959248 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.959564 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:09.023062 47807549109120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.958237 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.958630 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.958966 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:09.023205 47468505498496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.964166 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.964617 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.965032 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:09.024392 47572819071872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
W0618 12:12:09.024060 47807549109120 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmptws9_szg
W0618 12:12:09.024227 47468505498496 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfge6zr8a
I0618 12:12:09.025064 47807549109120 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmptws9_szg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7b5891be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:09.025222 47468505498496 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfge6zr8a', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2c67fede48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:09.025470 47807549109120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:09.025616 47468505498496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:09.025973 47682866148224 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:09.025446 47572819071872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmptk2e8q94
I0618 12:12:09.026467 47572819071872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmptk2e8q94', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b44b191ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:09.026872 47572819071872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:09.027684 47553080722304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881528.968371 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.968816 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.969240 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:09.028368 47517177439104 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
:::MLL 1560881528.971138 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881528.971605 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881528.971998 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:09.029439 47669280322432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000006-000003.tfrecord.zz_0_0
I0618 12:12:09.029352 47517177439104 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b37bd11bcc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:09.030100 47807549109120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:09.030216 47468505498496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:09.030255 47682866148224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:09.030466 47517177439104 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:09.030418 47669280322432 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxzolh0b8
I0618 12:12:09.031383 47669280322432 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxzolh0b8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5b271b8e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:09.031514 47572819071872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:09.031781 47669280322432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:09.032984 47027993117568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:09.033330 47535813768064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:09.034316 47375075767168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:09.035017 47517177439104 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:09.035297 47682866148224 estimator.py:1111] Calling model_fn.
W0618 12:12:09.035412 47682866148224 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:09.036353 47669280322432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:09.036760 47682866148224 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:09.037271 47027993117568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:09.037621 47535813768064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:09.038687 47375075767168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:09.042316 47027993117568 estimator.py:1111] Calling model_fn.
W0618 12:12:09.042427 47027993117568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:09.042654 47535813768064 estimator.py:1111] Calling model_fn.
W0618 12:12:09.042760 47535813768064 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:09.042783 47922893484928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:09.042816 47135500350336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:12:09.043779 47375075767168 estimator.py:1111] Calling model_fn.
W0618 12:12:09.043784 47027993117568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:09.043891 47375075767168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:09.044112 47535813768064 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:09.045242 47375075767168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:09.045679 47987916338048 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:09.045823 47539619054464 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:09.047224 47553080722304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:09.047085 47922893484928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:09.047158 47135500350336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:09.049243 47807549109120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:09.049554 47468505498496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:09.049976 47987916338048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tens[2019-06-18 12:12:48] divide_golden_chunk finished: 3.303 seconds
[2019-06-18 12:12:48] generate golden chunk: 3.318 seconds
[2019-06-18 12:12:48] iteration time 15: 49.310 seconds
2019-06-18 12:12:49.623252: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881568.666165 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 16}}
[2019-06-18 12:12:52] minmax time: 3.189 seconds
2019-06-18 12:12:52.822475: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:12:52.827899: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:12:52.832292: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881572.845345 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 15}}
[2019-06-18 12:12:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000017-000011 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=18 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:12:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-17-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=17 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=1023779848 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=2047559679 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=3071339510 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=4095119341 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=5118899172 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=6142679003 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=7166458834 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=8190238665 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=9214018496 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=10237798327 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=11261578158 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=12285357989 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=13309137820 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=14332917651 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=15356697482 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=16380477313 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=17404257144 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=18428036975 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=19451816806 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000016-000011 --seed=20475596637 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:13:04] eval finished: 11.717 seconds
[2019-06-18 12:13:04] Win rate 000016-000011 vs 000014-000010: 0.590
:::MLL 1560881584.626853 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 15}}
[2019-06-18 12:13:04] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-18-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=18 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=1023779849 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=2047559680 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=3071339511 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=4095119342 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=5118899173 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=6142679004 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=7166458835 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=8190238666 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=9214018497 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=10237798328 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=11261578159 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=12285357990 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=13309137821 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=14332917652 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=15356697483 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=16380477314 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=17404257145 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000017-000010 --seed=18428036976 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:13:35] selfplay finished: 30.453 seconds
[2019-06-18 12:13:35] selfplay mn: 30.475 seconds
[2019-06-18 12:13:35] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-18-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779849 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559680 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339511 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119342 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899173 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679004 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458835 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238666 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018497 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798328 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578159 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357990 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137821 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917652 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697483 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477314 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257145 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036976 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816807 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596638 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376469 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156300 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000017-000010/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:13:37] train finished: 44.547 seconds
:::MLL 1560881578.103180 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.104078 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.104928 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.196977 47435684643712 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.115525 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.116251 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.116899 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.197140 47965288010624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.198108 47435684643712 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5typq0k3
W0618 12:12:58.198206 47965288010624 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp49x_uml0
I0618 12:12:58.199244 47435684643712 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5typq0k3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b24c3b84e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.199279 47965288010624 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp49x_uml0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba0128a6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.199693 47435684643712 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:58.199716 47965288010624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.205051 47965288010624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.205079 47435684643712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.227145 47965288010624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.227532 47435684643712 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881578.147542 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.148299 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.148994 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.235970 47895039157120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.138396 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.139288 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.140135 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.235989 47138930987904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.237082 47895039157120 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpocsyuezf
W0618 12:12:58.237114 47138930987904 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8g3cwfvw
I0618 12:12:58.238167 47895039157120 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpocsyuezf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8fb761ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.238201 47138930987904 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8g3cwfvw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adfabd2ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.238604 47895039157120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:58.238646 47138930987904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.243946 47895039157120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.243953 47138930987904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881578.186741 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.187126 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.187447 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.253075 47510612837248 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.254150 47510612837248 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpuvpuzlf5
I0618 12:12:58.255205 47510612837248 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpuvpuzlf5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3635c9de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.255629 47510612837248 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881578.184005 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.184492 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.184812 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.259902 47791191729024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.260407 47510612837248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.260874 47791191729024 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpomsc3y9f
I0618 12:12:58.261852 47791191729024 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpomsc3y9f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b778997fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.262248 47791191729024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.265692 47895039157120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.265844 47138930987904 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.266718 47791191729024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881578.175061 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.175969 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.176871 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.268963 47993830032256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.188910 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.189650 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.190310 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.269040 46986133140352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.270104 47993830032256 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2ted2of1
W0618 12:12:58.270195 46986133140352 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqm_nkq5x
I0618 12:12:58.271190 47993830032256 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2ted2of1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6b7c71e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.271284 46986133140352 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqm_nkq5x', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abc185cbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.271641 47993830032256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:58.271750 46986133140352 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.276865 47993830032256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.276962 46986133140352 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.279853 47510612837248 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.279933 47965288010624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:58.279963 47435684643712 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881578.197679 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.198182 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.198625 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.280359 47773404226432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.202393 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.202791 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.203142 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.280579 46925114766208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.281377 47773404226432 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpeq98cdka
W0618 12:12:58.281562 46925114766208 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpd9583p7q
I0618 12:12:58.282366 47773404226432 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpeq98cdka', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7365602e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.282550 46925114766208 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpd9583p7q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aade3623e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881578.185771 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.186489 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.187161 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.282220 47183509664640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
I0618 12:12:58.282774 47773404226432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881578.182517 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.183276 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.183908 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.282462 47666851304320 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
I0618 12:12:58.282956 46925114766208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.283330 47183509664640 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2uusype6
W0618 12:12:58.283597 47666851304320 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmptojn6e9_
I0618 12:12:58.284355 47183509664640 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2uusype6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aea0ceb7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.284587 47666851304320 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmptojn6e9_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5a96539dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.284750 47183509664640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.284567 47435684643712 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:58.284549 47965288010624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:58.284974 47666851304320 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.285810 47791191729024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.287403 47773404226432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.287500 46925114766208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.289449 47183509664640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.289553 47666851304320 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:58.290001 47965288010624 estimator.py:1111] Calling model_fn.
I0618 12:12:58.290110 47435684643712 estimator.py:1111] Calling model_fn.
W0618 12:12:58.290138 47965288010624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:58.290230 47435684643712 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:58.291605 47965288010624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:58.291732 47435684643712 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881578.199681 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.200608 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.201454 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.297724 46969747534720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.206159 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.206907 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.207612 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.297850 47858027115392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.299246 47993830032256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.299335 46986133140352 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.298857 46969747534720 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpnpg8bi_1
W0618 12:12:58.298911 47858027115392 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphvkhcobs
I0618 12:12:58.299993 47858027115392 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphvkhcobs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b87194b0dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.299993 46969747534720 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpnpg8bi_1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab847b44e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.300443 46969747534720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:58.300451 47858027115392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.305826 47858027115392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.305832 46969747534720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.306601 46925114766208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.306766 47773404226432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.308937 47183509664640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.309191 47666851304320 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.315371 47895039157120 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:58.315711 47138930987904 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881578.252825 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.253217 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.253584 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.317842 47366453912448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.253643 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.254159 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.254479 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.318065 47842044474240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.318892 47366453912448 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpt591tao4
W0618 12:12:58.319107 47842044474240 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpczrt15o6
I0618 12:12:58.319934 47366453912448 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpt591tao4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b14a53f0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:58.319893 47895039157120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:58.320161 47842044474240 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpczrt15o6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8360a74e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:58.320257 47138930987904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:58.320352 47366453912448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:58.320560 47842044474240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.324942 47366453912448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881578.228402 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.229172 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.229863 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.324771 47351555011456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.226756 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.227589 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.228421 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.324766 47073852633984 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.325100 47842044474240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:58.325185 47895039157120 estimator.py:1111] Calling model_fn.
W0618 12:12:58.325300 47895039157120 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:12:58.325577 47138930987904 estimator.py:1111] Calling model_fn.
W0618 12:12:58.325703 47138930987904 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:58.325829 47073852633984 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpv4uopu4q
W0618 12:12:58.325856 47351555011456 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqh8igg4r
W0618 12:12:58.326769 47895039157120 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:12:58.326890 47073852633984 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpv4uopu4q', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad084da0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.326922 47351555011456 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqh8igg4r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b112d340e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:58.327162 47138930987904 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:12:58.327326 47073852633984 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:58.327360 47351555011456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.327674 47510612837248 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:58.328027 47858027115392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.328152 46969747534720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881578.270310 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.270823 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.271285 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.331342 47953228673920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.256914 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.257441 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.257917 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.331709 46951152767872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
:::MLL 1560881578.262129 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.262583 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.262979 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.331729 47053975126912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.332226 47351555011456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.332233 47073852633984 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.332008 47510612837248 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:12:58.332795 47791191729024 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:58.332441 47953228673920 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpu50ysy4r
I0618 12:12:58.333491 47953228673920 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpu50ysy4r', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9d43bf8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.332719 47053975126912 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acbe40f4d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:58.332710 46951152767872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpiiz8frsq
I0618 12:12:58.333675 46951152767872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpiiz8frsq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab3f35eae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:12:58.333826 47053975126912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:58.333913 47953228673920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:12:58.334080 46951152767872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:12:58.337092 47791191729024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:12:58.337092 47510612837248 estimator.py:1111] Calling model_fn.
W0618 12:12:58.337202 47510612837248 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:58.338462 47053975126912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.338649 46951152767872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:12:58.338565 47510612837248 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:58.338871 47953228673920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:12:58.342112 47791191729024 estimator.py:1111] Calling model_fn.
W0618 12:12:58.342220 47791191729024 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:12:58.343580 47791191729024 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:12:58.344222 47366453912448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:12:58.344497 47842044474240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881578.292789 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881578.293219 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881578.293580 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:12:58.347446 47941424550784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000007-000003.tfrecord.zz_0_0
W0618 12:12:58.349162 47993830032256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:12:58.348404 47941424550784 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfsef9o8b
I0618 12:12:58.349362 47941424550784 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfsef9o8b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a842ace48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:12:58.349473 46986133140352 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but havi[2019-06-18 12:13:38] divide_golden_chunk finished: 3.296 seconds
[2019-06-18 12:13:38] generate golden chunk: 3.311 seconds
[2019-06-18 12:13:38] moving /lfs/lfs12/gma_akey/results/epb002/models/000017-000011.index --> /lfs/lfs12/gma_akey/results/epb002/models/000017-000012.index
[2019-06-18 12:13:38] moving /lfs/lfs12/gma_akey/results/epb002/models/000017-000011.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000017-000012.meta
[2019-06-18 12:13:38] moving /lfs/lfs12/gma_akey/results/epb002/models/000017-000011.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb
[2019-06-18 12:13:38] moving /lfs/lfs12/gma_akey/results/epb002/models/000017-000011.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000017-000012.data-00000-of-00001
[2019-06-18 12:13:38] iteration time 16: 49.789 seconds
2019-06-18 12:13:39.462955: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
Got 343213 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000009.tfrecord.zz: 13.835 seconds
Got 377358 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz: 0.312 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000004.tfrecord.zz: 14.768 seconds
Got 380569 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz: 0.306 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000005.tfrecord.zz: 14.493 seconds
Got 347165 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000006.tfrecord.zz: 13.841 seconds
Got 391449 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz: 0.312 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000000.tfrecord.zz: 15.572 seconds
Got 383972 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000002.tfrecord.zz: 13.943 seconds
Got 348159 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000007.tfrecord.zz: 14.455 seconds
Got 346341 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000008.tfrecord.zz: 14.183 seconds
Got 388630 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz: 0.317 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000003.tfrecord.zz: 13.186 seconds
Got 387688 examples
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz: 0.311 seconds
Writing examples to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000000-000001.tfrecord.zz: 15.019 seconds
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/checkpoint_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/checkpointlog.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000001-000001_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000001-000001log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000002-000002_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000002-000002log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000003-000003_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000003-000003log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000004-000003_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000004-000003log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000005-000004_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000005-000004log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000006-000004_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000006-000004log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000007-000004_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000007-000004log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000008-000005_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000008-000005log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000009-000006_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000009-000006log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000010-000007_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000010-000007log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000011-000008_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000011-000008log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000012-000008_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000012-000008log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000013-000009_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000013-000009log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000014-000010_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000014-000010log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000015-000011_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000015-000011log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000016-000011_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000016-000011log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000017-000012_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000017-000012log.txt['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881618.455285 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 17}}
[2019-06-18 12:13:42] minmax time: 3.212 seconds
2019-06-18 12:13:42.685472: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:13:42.690848: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:13:42.695460: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881622.706688 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 16}}
[2019-06-18 12:13:42] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000018-000012 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=19 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:13:42] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-18-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=18 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=1023779849 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=2047559680 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=3071339511 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=4095119342 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=5118899173 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=6142679004 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=7166458835 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=8190238666 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=9214018497 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=10237798328 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=11261578159 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=12285357990 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=13309137821 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=14332917652 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=15356697483 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=16380477314 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=17404257145 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=18428036976 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=19451816807 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000017-000012 --seed=20475596638 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:13:53] eval finished: 10.785 seconds
[2019-06-18 12:13:53] Win rate 000017-000012 vs 000016-000011: 0.610
:::MLL 1560881633.556309 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 16}}
[2019-06-18 12:13:53] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-19-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=19 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=1023779850 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=2047559681 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=3071339512 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=4095119343 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=5118899174 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=6142679005 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=7166458836 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=8190238667 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=9214018498 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=10237798329 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=11261578160 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=12285357991 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=13309137822 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=14332917653 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=15356697484 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=16380477315 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=17404257146 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000018-000011 --seed=18428036977 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:14:24] selfplay finished: 30.953 seconds
[2019-06-18 12:14:24] selfplay mn: 30.972 seconds
[2019-06-18 12:14:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-19-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779850 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559681 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339512 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119343 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899174 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679005 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458836 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238667 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018498 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798329 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578160 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357991 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137822 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917653 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697484 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477315 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257146 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036977 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816808 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596639 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376470 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156301 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000018-000011/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:14:26] train finished: 43.725 seconds
:::MLL 1560881627.932935 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881627.933797 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881627.934534 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.027966 47457671791488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881627.932796 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881627.933613 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881627.934348 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.028102 47609474823040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:48.029109 47457671791488 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpw9pe1agt
W0618 12:13:48.029169 47609474823040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp27y4d19d
I0618 12:13:48.030220 47457671791488 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpw9pe1agt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b29e2419e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.030321 47609474823040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp27y4d19d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4d3a6c0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.030668 47457671791488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.030774 47609474823040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.035949 47457671791488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.036011 47609474823040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.057992 47457671791488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.058076 47609474823040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881628.011013 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.011425 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.011777 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.088324 47370652222336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881627.993316 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881627.994214 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881627.995081 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.088680 47718274753408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.011122 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.011522 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.011870 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.088496 47454637593472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.006165 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.006891 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.007577 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.088941 47146094470016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:48.089367 47370652222336 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpneovq_bu
W0618 12:13:48.089516 47454637593472 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmptv0_tno0
W0618 12:13:48.089802 47718274753408 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxksh30f0
I0618 12:13:48.090403 47370652222336 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpneovq_bu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b159f7c5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:48.090012 47146094470016 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpnh_iz53i
I0618 12:13:48.090583 47454637593472 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmptv0_tno0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b292d676da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.090911 47718274753408 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxksh30f0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b668f673e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.090814 47370652222336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.091106 47146094470016 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpnh_iz53i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae156cceda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.090985 47454637593472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.091355 47718274753408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.091572 47146094470016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.095432 47370652222336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.095523 47454637593472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.096749 47718274753408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.096868 47146094470016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881628.000628 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.001551 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.002382 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.098548 47934257398656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.008731 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.009509 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.010212 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.098909 47901307806592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:48.099680 47934257398656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmclwv1e6
I0618 12:13:48.100818 47934257398656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmclwv1e6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b98d8f8de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:48.100022 47901307806592 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp93zm203i
I0618 12:13:48.101150 47901307806592 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp93zm203i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b912d05fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.101267 47934257398656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.101617 47901307806592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.106619 47934257398656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.106875 47901307806592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.108591 47457671791488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:48.109042 47609474823040 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:48.112882 47457671791488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:48.113371 47609474823040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:48.114413 47370652222336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.114676 47454637593472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:13:48.117988 47457671791488 estimator.py:1111] Calling model_fn.
W0618 12:13:48.118110 47457671791488 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:48.118486 47609474823040 estimator.py:1111] Calling model_fn.
W0618 12:13:48.118762 47718274753408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.118596 47609474823040 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:48.119010 47146094470016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.119463 47457671791488 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:48.119950 47609474823040 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881628.043053 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.043845 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.044518 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.126621 47760883905408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.031757 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.032631 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.033480 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.126580 47368786232192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:48.127660 47760883905408 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpj6c_jxib
W0618 12:13:48.127630 47368786232192 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpauj7drrd
I0618 12:13:48.128634 47368786232192 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpauj7drrd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1530439dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.128649 47760883905408 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpj6c_jxib', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b707b1b3dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:48.128827 47934257398656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.129077 47901307806592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:13:48.129036 47368786232192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.129048 47760883905408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.134019 47760883905408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.134020 47368786232192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881628.067829 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.068288 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.068718 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.136826 47225254384512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:48.137843 47225254384512 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_5kj96e4
I0618 12:13:48.138838 47225254384512 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_5kj96e4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af3c5196e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881628.071064 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.071441 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.071862 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.138896 47589432222592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
I0618 12:13:48.139240 47225254384512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.139862 47589432222592 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpprknj6rx
I0618 12:13:48.140831 47589432222592 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpprknj6rx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b488fca0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881628.044297 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.045062 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.045772 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.140732 47769969361792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.046647 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.047373 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.048081 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.140723 47814057460608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
I0618 12:13:48.141220 47589432222592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.141920 47814057460608 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpid9ae80l
W0618 12:13:48.141949 47769969361792 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1j4e8zwr
I0618 12:13:48.143054 47814057460608 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpid9ae80l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7cdc7f4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.143078 47769969361792 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1j4e8zwr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7298a45e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.143507 47814057460608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.143532 47769969361792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.143934 47225254384512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.145767 47589432222592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881628.076627 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.077120 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.077564 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.147118 47651847005056 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.080246 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.080625 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.080947 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.147246 46924749546368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:48.148149 47651847005056 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp56y_oy2v
W0618 12:13:48.148236 46924749546368 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxovik921
I0618 12:13:48.149150 47651847005056 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp56y_oy2v', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5718001da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:48.149029 47769969361792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:13:48.149228 46924749546368 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxovik921', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aadcd9d6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:48.149055 47814057460608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:13:48.149556 47651847005056 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.149626 46924749546368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.153164 47760883905408 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.153405 47368786232192 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.154232 47651847005056 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.154263 46924749546368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.161552 47370652222336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:48.162149 47454637593472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:48.163056 47225254384512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.164761 47589432222592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.165804 47370652222336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:48.166456 47454637593472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:48.167482 47718274753408 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:48.168159 47146094470016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:13:48.170881 47370652222336 estimator.py:1111] Calling model_fn.
W0618 12:13:48.170989 47370652222336 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:48.171203 47769969361792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.171539 47814057460608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:13:48.171565 47454637593472 estimator.py:1111] Calling model_fn.
W0618 12:13:48.171780 47718274753408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:48.171670 47454637593472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:48.172520 47146094470016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:48.172349 47370652222336 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:48.173011 47454637593472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:48.173426 47651847005056 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:13:48.173453 46924749546368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881628.081942 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.082692 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.083416 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.175422 46980891677568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.084888 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.085668 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.086365 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.175673 47829899420544 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
I0618 12:13:48.176886 47718274753408 estimator.py:1111] Calling model_fn.
W0618 12:13:48.177002 47718274753408 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:48.176403 46980891677568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppgik3a1y
W0618 12:13:48.176665 47829899420544 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1klu9wyz
I0618 12:13:48.177406 46980891677568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppgik3a1y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abadff25e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.177627 47146094470016 estimator.py:1111] Calling model_fn.
I0618 12:13:48.177681 47829899420544 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1klu9wyz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b808cc07e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:13:48.177745 47146094470016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:48.177821 46980891677568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.178087 47829899420544 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:13:48.178365 47718274753408 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:48.179111 47146094470016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:48.180661 47901307806592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:48.180971 47934257398656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:13:48.182708 46980891677568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.183023 47829899420544 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.185361 47901307806592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:13:48.185740 47934257398656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881628.116026 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.116482 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.116843 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.187724 46965774840704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.119112 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.119565 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.119942 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.188470 47485293216640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
W0618 12:13:48.188762 46965774840704 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpl753h2bl
I0618 12:13:48.189752 46965774840704 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpl753h2bl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab75ae9ce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.190160 46965774840704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.189486 47485293216640 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b30509f1d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:13:48.190619 47485293216640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:13:48.190839 47901307806592 estimator.py:1111] Calling model_fn.
W0618 12:13:48.190956 47901307806592 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:13:48.191255 47934257398656 estimator.py:1111] Calling model_fn.
W0618 12:13:48.191374 47934257398656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:13:48.192409 47901307806592 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:48.192832 47934257398656 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:13:48.194851 46965774840704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:13:48.195264 47485293216640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881628.124700 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881628.125113 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881628.125517 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:13:48.195021 47361429324672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000008-000003.tfrecord.zz_0_0
:::MLL 1560881628.125035 glob[2019-06-18 12:14:27] divide_golden_chunk finished: 3.307 seconds
[2019-06-18 12:14:27] generate golden chunk: 3.322 seconds
[2019-06-18 12:14:27] moving /lfs/lfs12/gma_akey/results/epb002/models/000018-000012.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000018-000013.meta
[2019-06-18 12:14:27] moving /lfs/lfs12/gma_akey/results/epb002/models/000018-000012.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000018-000013.data-00000-of-00001
[2019-06-18 12:14:27] moving /lfs/lfs12/gma_akey/results/epb002/models/000018-000012.index --> /lfs/lfs12/gma_akey/results/epb002/models/000018-000013.index
[2019-06-18 12:14:27] moving /lfs/lfs12/gma_akey/results/epb002/models/000018-000012.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb
[2019-06-18 12:14:27] iteration time 17: 49.440 seconds
2019-06-18 12:14:29.002104: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881667.895370 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 18}}
[2019-06-18 12:14:32] minmax time: 3.212 seconds
2019-06-18 12:14:32.224787: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:14:32.230243: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:14:32.234684: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881672.246185 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 17}}
[2019-06-18 12:14:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=20 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:14:32] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-19-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=19 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=1023779850 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=2047559681 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=3071339512 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=4095119343 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=5118899174 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=6142679005 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=7166458836 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=8190238667 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=9214018498 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=10237798329 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=11261578160 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=12285357991 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=13309137822 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=14332917653 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=15356697484 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=16380477315 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=17404257146 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=18428036977 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=19451816808 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000018-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000018-000013 --seed=20475596639 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:14:43] eval finished: 10.907 seconds
[2019-06-18 12:14:43] Win rate 000018-000013 vs 000017-000012: 0.440
:::MLL 1560881683.218633 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 17}}
[2019-06-18 12:14:43] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-20-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=20 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=1023779851 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=2047559682 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=3071339513 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=4095119344 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=5118899175 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=6142679006 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=7166458837 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=8190238668 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=9214018499 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=10237798330 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=11261578161 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=12285357992 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=13309137823 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=14332917654 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=15356697485 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=16380477316 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=17404257147 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000019-000012 --seed=18428036978 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:15:13] selfplay finished: 29.811 seconds
[2019-06-18 12:15:13] selfplay mn: 29.829 seconds
[2019-06-18 12:15:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-20-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779851 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559682 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339513 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119344 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899175 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679006 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458837 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238668 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018499 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798330 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578161 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357992 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137823 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917654 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697485 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477316 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257147 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036978 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816809 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596640 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376471 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156302 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000019-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:15:16] train finished: 43.839 seconds
:::MLL 1560881677.412338 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.413177 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.413967 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.508997 47275673662336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881677.423193 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.423956 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.424648 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.509041 47415121453952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.510117 47275673662336 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp61s5jj8o
W0618 12:14:37.510151 47415121453952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5qxg5byb
I0618 12:14:37.511238 47275673662336 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp61s5jj8o', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff82528e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.511283 47415121453952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5qxg5byb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1ffa0efe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.511689 47275673662336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:37.511728 47415121453952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.516913 47275673662336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.516934 47415121453952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.538861 47415121453952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.538956 47275673662336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881677.474574 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.475465 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.476309 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.570934 47951101514624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.572052 47951101514624 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8qu9k7_h
I0618 12:14:37.573154 47951101514624 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8qu9k7_h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9cc4f5ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.573607 47951101514624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.578903 47951101514624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881677.506380 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.506798 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.507142 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.578969 47166200685440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881677.506243 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.506648 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.507013 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.579041 47789731275648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.580006 47166200685440 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpu9o3rxpd
W0618 12:14:37.580043 47789731275648 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpc630w0wn
I0618 12:14:37.581011 47166200685440 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpu9o3rxpd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae605396e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.581017 47789731275648 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpc630w0wn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b77328b2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.581413 47166200685440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:37.581414 47789731275648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.586075 47166200685440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.586068 47789731275648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.589924 47415121453952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.590109 47275673662336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.594245 47415121453952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:37.594397 47275673662336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:14:37.599367 47415121453952 estimator.py:1111] Calling model_fn.
W0618 12:14:37.599475 47415121453952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:14:37.599493 47275673662336 estimator.py:1111] Calling model_fn.
W0618 12:14:37.599606 47275673662336 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:37.600902 47951101514624 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.600829 47415121453952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:37.600961 47275673662336 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:37.605076 47789731275648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.605219 47166200685440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881677.511538 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.512327 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.513099 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.614091 46982096057216 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.615182 46982096057216 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4hnwkd8s
I0618 12:14:37.616318 46982096057216 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4hnwkd8s', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abb27bbbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.616769 46982096057216 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881677.526517 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.527270 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.527951 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.617072 47612001444736 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881677.517647 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.518570 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.519448 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.617939 47202599560064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.618224 47612001444736 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_9avrdtl
I0618 12:14:37.619373 47612001444736 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_9avrdtl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4dd1053da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.619835 47612001444736 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.619040 47202599560064 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphq9xvlu6
I0618 12:14:37.620155 47202599560064 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphq9xvlu6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aee7ec41e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.620614 47202599560064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.622029 46982096057216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881677.534137 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.534863 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.535561 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.622573 47913487659904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881677.527366 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.528300 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.529163 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.622577 48000800875392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.623654 48000800875392 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp54af00sb
W0618 12:14:37.623687 47913487659904 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphcygnz77
I0618 12:14:37.624670 48000800875392 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp54af00sb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba85745be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.624687 47913487659904 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphcygnz77', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9402ffce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.625093 47913487659904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:37.625072 48000800875392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.625191 47612001444736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.625762 47202599560064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.629854 48000800875392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.629896 47913487659904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881677.541853 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.542688 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.543537 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.642356 46946052371328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881677.543651 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.544458 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.545147 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.642738 47437627007872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.643378 46982096057216 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.643510 46946052371328 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7ld4c8or
I0618 12:14:37.644633 46946052371328 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7ld4c8or', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab2c35ccdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:14:37.643859 47437627007872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmped8ut4gc
I0618 12:14:37.644974 47437627007872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmped8ut4gc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b25377e6dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.645087 46946052371328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:37.645427 47437627007872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.647363 47612001444736 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.647790 47202599560064 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881677.555843 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.556360 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.556803 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.648591 47743410672512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.649194 48000800875392 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.649249 47913487659904 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.649605 47743410672512 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxtk_imnt
W0618 12:14:37.650406 46946052371328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:14:37.650749 47743410672512 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxtk_imnt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6c699eee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:14:37.650642 47437627007872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:14:37.651191 47743410672512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881677.564485 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.564886 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.565220 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.652773 47697284019072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.653069 47951101514624 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.652997 47789731275648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.653075 47166200685440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.653872 47697284019072 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpa8rilujf
I0618 12:14:37.655016 47697284019072 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpa8rilujf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b61ac41fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.655469 47697284019072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.655874 47743410672512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.657526 47951101514624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:37.657330 47789731275648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:37.657408 47166200685440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:37.660323 47697284019072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:14:37.662675 47951101514624 estimator.py:1111] Calling model_fn.
I0618 12:14:37.662423 47789731275648 estimator.py:1111] Calling model_fn.
W0618 12:14:37.662534 47789731275648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:14:37.662529 47166200685440 estimator.py:1111] Calling model_fn.
W0618 12:14:37.662789 47951101514624 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:37.662643 47166200685440 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:14:37.663885 47789731275648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:37.664153 47951101514624 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:14:37.664001 47166200685440 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881677.598859 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.599237 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.599565 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.668956 47539769840512 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881677.597271 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.597652 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.597975 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.669272 47873154896768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.669969 47539769840512 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp86mjx_67
I0618 12:14:37.670951 47539769840512 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp86mjx_67', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3cffae5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:14:37.670250 47873154896768 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7nfbsan2
I0618 12:14:37.671228 47873154896768 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7nfbsan2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8a9efaae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.671351 47539769840512 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:37.671628 47873154896768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.672270 46946052371328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.672623 47437627007872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.674994 47743410672512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.676002 47539769840512 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.676191 47873154896768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.680148 47697284019072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881677.616798 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.617296 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.617688 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.690254 47645508232064 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
W0618 12:14:37.691966 46982096057216 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.691253 47645508232064 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpob7z_n59
I0618 12:14:37.692253 47645508232064 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpob7z_n59', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b559e2e3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881677.620758 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.621215 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.621595 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.692288 47911127143296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
I0618 12:14:37.692652 47645508232064 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881677.618011 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.618622 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.619130 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.692924 47481462309760 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
:::MLL 1560881677.618005 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881677.618617 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881677.619132 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:14:37.693006 47881343959936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000009-000004.tfrecord.zz_0_0
I0618 12:14:37.693273 47911127143296 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b93764d2d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.694398 47911127143296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.693947 47481462309760 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpl5nuo5bj
W0618 12:14:37.693998 47881343959936 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0usok04u
I0618 12:14:37.694929 47481462309760 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpl5nuo5bj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2f6c480e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:14:37.694982 47881343959936 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0usok04u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8c8715de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:14:37.695232 47539769840512 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:14:37.695288 47873154896768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:14:37.695335 47481462309760 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:14:37.695379 47881343959936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:14:37.696354 46982096057216 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:37.696692 47612001444736 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.696941 47202599560064 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.696944 48000800875392 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.697124 47913487659904 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:14:37.697321 47645508232064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.698960 47911127143296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.699986 47481462309760 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.700009 47881343959936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:14:37.700994 47612001444736 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:37.701275 47202599560064 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:37.701228 48000800875392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:14:37.701420 47913487659904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:14:37.701506 46982096057216 estimator.py:1111] Calling model_fn.
W0618 12:14:37.701617 46982096057216 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.con[2019-06-18 12:15:16] divide_golden_chunk finished: 3.404 seconds
[2019-06-18 12:15:16] generate golden chunk: 3.419 seconds
[2019-06-18 12:15:16] iteration time 18: 48.573 seconds
2019-06-18 12:15:17.514655: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881716.468598 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 19}}
[2019-06-18 12:15:20] minmax time: 3.229 seconds
2019-06-18 12:15:20.753105: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:15:20.758657: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:15:20.763283: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881720.776721 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 18}}
[2019-06-18 12:15:20] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=21 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:15:20] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-20-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=20 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=1023779851 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=2047559682 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=3071339513 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=4095119344 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=5118899175 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=6142679006 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=7166458837 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=8190238668 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=9214018499 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=10237798330 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=11261578161 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=12285357992 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=13309137823 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=14332917654 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=15356697485 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=16380477316 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=17404257147 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=18428036978 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=19451816809 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000019-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000019-000013 --seed=20475596640 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:15:31] eval finished: 11.019 seconds
[2019-06-18 12:15:31] Win rate 000019-000013 vs 000017-000012: 0.470
:::MLL 1560881731.861354 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 18}}
[2019-06-18 12:15:31] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-21-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=21 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=1023779852 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=2047559683 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=3071339514 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=4095119345 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=5118899176 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=6142679007 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=7166458838 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=8190238669 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=9214018500 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=10237798331 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=11261578162 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=12285357993 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=13309137824 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=14332917655 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=15356697486 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=16380477317 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=17404257148 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000020-000012 --seed=18428036979 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:16:02] selfplay finished: 30.431 seconds
[2019-06-18 12:16:02] selfplay mn: 30.452 seconds
[2019-06-18 12:16:02] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-21-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779852 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559683 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339514 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119345 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899176 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679007 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458838 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238669 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018500 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798331 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578162 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357993 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137824 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917655 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697486 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477317 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257148 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036979 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816810 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596641 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376472 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156303 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000020-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:16:04] train finished: 43.856 seconds
:::MLL 1560881726.065230 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.065970 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.066648 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.168225 47565526553472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.062990 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.063719 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.064432 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.168701 46933423276928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.169360 47565526553472 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpojg8fr1l
I0618 12:15:26.170482 47565526553472 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpojg8fr1l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b42fee6ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:26.169772 46933423276928 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmm552d9n
I0618 12:15:26.170887 46933423276928 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmm552d9n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aafd29c1da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.170949 47565526553472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:26.171336 46933423276928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.176449 47565526553472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.176538 46933423276928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881726.085023 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.085725 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.086445 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.179083 47915556062080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.072643 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.073573 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.074430 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.179318 47044872901504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.180160 47915556062080 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqydiq9zd
W0618 12:15:26.180319 47044872901504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmprqd2iuxu
I0618 12:15:26.181160 47915556062080 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqydiq9zd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b947e490e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.181298 47044872901504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmprqd2iuxu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac9c5866e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.181560 47915556062080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:26.181693 47044872901504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.186452 47915556062080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.186454 47044872901504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881726.085943 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.086716 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.087492 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.191326 47058005345152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.088165 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.088935 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.089585 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.191488 47040502371200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.192476 47058005345152 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_tq3wbu3
W0618 12:15:26.192576 47040502371200 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5mqd2z99
I0618 12:15:26.193593 47058005345152 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_tq3wbu3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2accd4479e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.193659 47040502371200 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5mqd2z99', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac8c1056da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.194050 47058005345152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:26.194108 47040502371200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.198462 46933423276928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.198785 47565526553472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.199431 47058005345152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.199477 47040502371200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.205704 47915556062080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.205780 47044872901504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881726.117962 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.118921 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.119824 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.215009 47387765416832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.216138 47387765416832 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpuqasevq2
I0618 12:15:26.217169 47387765416832 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpuqasevq2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b199b82fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.217594 47387765416832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881726.132898 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.133391 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.133819 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.219135 47396182393728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.132908 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.133396 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.133826 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.219180 47394168619904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.220149 47396182393728 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpoda_xad3
W0618 12:15:26.220179 47394168619904 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpn0e5npqf
I0618 12:15:26.221122 47396182393728 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpoda_xad3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1b9133cda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:26.221388 47040502371200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:15:26.221173 47394168619904 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpn0e5npqf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1b192c1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.221515 47396182393728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.221750 47058005345152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:15:26.221564 47394168619904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.223028 47387765416832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881726.150804 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.151644 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.152488 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.224429 47742987572096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.226127 47396182393728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.226176 47394168619904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.225548 47742987572096 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp85z_g83w
I0618 12:15:26.226685 47742987572096 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp85z_g83w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6c5066ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.227146 47742987572096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.232534 47742987572096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881726.162955 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.163430 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.163837 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.240131 47455794234240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.163397 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.163873 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.164226 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.240369 47535733670784 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.163886 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.164309 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.164677 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.241662 47768829510528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.163104 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.163551 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.163946 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.241690 47621024015232 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
I0618 12:15:26.241175 47455794234240 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2972585d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:26.241358 47535733670784 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp896o3fmb
I0618 12:15:26.242296 47455794234240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:26.242332 47535733670784 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp896o3fmb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c0f1b5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.242743 47535733670784 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.242716 47621024015232 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpobfnfftf
W0618 12:15:26.242744 47768829510528 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpskyln00y
I0618 12:15:26.243714 47621024015232 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpobfnfftf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4feacebe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.243724 47768829510528 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpskyln00y', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7254b37e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.244117 47768829510528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:26.244116 47621024015232 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.245138 47396182393728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.245311 47387765416832 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.245335 47394168619904 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.247128 47455794234240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.247462 47535733670784 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.247306 46933423276928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:26.247984 47565526553472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:26.248807 47768829510528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.248837 47621024015232 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881726.067369 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.068082 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.068781 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.248776 47717703238528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.249929 47717703238528 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpku3a722h
I0618 12:15:26.251061 47717703238528 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpku3a722h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b666d569e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.251522 47717703238528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.251617 46933423276928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:26.252366 47565526553472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:26.253357 47915556062080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:26.253603 47044872901504 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:26.255185 47742987572096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.256838 47717703238528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:15:26.256681 46933423276928 estimator.py:1111] Calling model_fn.
W0618 12:15:26.256788 46933423276928 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:15:26.257499 47565526553472 estimator.py:1111] Calling model_fn.
W0618 12:15:26.257611 47565526553472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:26.257681 47915556062080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:26.257908 47044872901504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:26.258146 46933423276928 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:26.258999 47565526553472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881726.069694 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.070397 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.071079 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.261883 47334141526912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
I0618 12:15:26.262742 47915556062080 estimator.py:1111] Calling model_fn.
W0618 12:15:26.262853 47915556062080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:15:26.262958 47044872901504 estimator.py:1111] Calling model_fn.
W0618 12:15:26.263074 47044872901504 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:26.262956 47334141526912 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpglpsczcf
I0618 12:15:26.264030 47334141526912 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpglpsczcf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0d1f475e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:15:26.264227 47915556062080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:15:26.264469 47334141526912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.264430 47044872901504 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:26.266483 47455794234240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.266614 47535733670784 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.268018 47768829510528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.268217 47621024015232 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:15:26.269456 47334141526912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.270660 47040502371200 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:26.270941 47058005345152 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:26.274983 47040502371200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:15:26.275283 47058005345152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881726.115837 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.116279 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.116644 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.276448 47703646921600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.118730 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.119162 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.119535 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.277833 47537128493952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.277461 47703646921600 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp75ea3lq_
W0618 12:15:26.278411 47717703238528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:15:26.278442 47703646921600 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp75ea3lq_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6327843e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.278845 47703646921600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.278819 47537128493952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpddtxtneu
I0618 12:15:26.279797 47537128493952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpddtxtneu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c623eae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.280066 47040502371200 estimator.py:1111] Calling model_fn.
I0618 12:15:26.280185 47537128493952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.280177 47040502371200 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:15:26.280407 47058005345152 estimator.py:1111] Calling model_fn.
W0618 12:15:26.280524 47058005345152 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:15:26.281544 47040502371200 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:26.281911 47058005345152 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:15:26.283500 47703646921600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.284779 47537128493952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881726.186852 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.187801 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.188715 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.287066 47531939144576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.199722 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.200512 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.201222 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.287198 47947988808576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.288077 47531939144576 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpwz6jbjy1
W0618 12:15:26.288185 47947988808576 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpr9xytiks
I0618 12:15:26.289067 47531939144576 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpwz6jbjy1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3b2cef7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.289163 47947988808576 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpr9xytiks', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9c0b6d5e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.289458 47531939144576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:26.289556 47947988808576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.290160 47334141526912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881726.214749 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.215200 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.215580 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.291313 47104060646272 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
:::MLL 1560881726.214825 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881726.215286 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881726.215649 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:15:26.291545 47163670074240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000010-000005.tfrecord.zz_0_0
W0618 12:15:26.292371 47396182393728 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:26.292666 47394168619904 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:15:26.292340 47104060646272 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmptccrz5is
W0618 12:15:26.292534 47163670074240 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp86eqxo7e
I0618 12:15:26.293314 47104060646272 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmptccrz5is', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad78d63ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.293502 47163670074240 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp86eqxo7e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae56e636e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:15:26.293708 47104060646272 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:15:26.293897 47163670074240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:15:26.294355 47531939144576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:15:26.294564 47947988808576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) c[2019-06-18 12:16:05] divide_golden_chunk finished: 3.311 seconds
[2019-06-18 12:16:05] generate golden chunk: 3.325 seconds
[2019-06-18 12:16:05] iteration time 19: 49.172 seconds
2019-06-18 12:16:06.933128: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881765.640913 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 20}}
[2019-06-18 12:16:10] minmax time: 3.232 seconds
2019-06-18 12:16:10.175213: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:16:10.180812: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:16:10.185466: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881770.198644 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 19}}
[2019-06-18 12:16:10] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000021-000013 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=22 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:16:10] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-21-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=21 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=1023779852 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=2047559683 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=3071339514 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=4095119345 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=5118899176 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=6142679007 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=7166458838 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=8190238669 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=9214018500 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=10237798331 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=11261578162 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=12285357993 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=13309137824 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=14332917655 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=15356697486 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=16380477317 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=17404257148 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=18428036979 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=19451816810 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000020-000013 --seed=20475596641 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:16:20] eval finished: 10.443 seconds
[2019-06-18 12:16:20] Win rate 000020-000013 vs 000017-000012: 0.640
:::MLL 1560881780.705009 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 19}}
[2019-06-18 12:16:20] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-22-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=22 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=1023779853 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=2047559684 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=3071339515 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=4095119346 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=5118899177 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=6142679008 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=7166458839 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=8190238670 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=9214018501 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=10237798332 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=11261578163 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=12285357994 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=13309137825 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=14332917656 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=15356697487 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=16380477318 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=17404257149 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000021-000012 --seed=18428036980 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:16:50] selfplay finished: 29.624 seconds
[2019-06-18 12:16:50] selfplay mn: 29.642 seconds
[2019-06-18 12:16:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-22-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779853 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559684 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339515 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119346 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899177 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679008 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458839 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238670 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018501 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798332 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578163 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357994 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137825 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917656 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697487 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477318 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257149 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036980 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816811 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596642 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376473 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156304 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000021-000012/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:16:53] divide_golden_chunk finished: 3.315 seconds
[2019-06-18 12:16:53] generate golden chunk: 3.331 seconds
[2019-06-18 12:16:54] train finished: 44.009 seconds
:::MLL 1560881775.480593 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.481338 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.481994 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.579469 47168608887680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881775.477438 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.478204 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.478886 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.579586 47554244805504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:16:15.580602 47168608887680 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpim1vmpir
W0618 12:16:15.580677 47554244805504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkt92zpsy
I0618 12:16:15.581704 47168608887680 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpim1vmpir', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae694c3be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.581749 47554244805504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkt92zpsy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b405e74ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.582161 47168608887680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.582202 47554244805504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881775.459503 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.460336 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.461138 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.586863 47491518845824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881775.460052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.460922 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.461702 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.586930 47527178580864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:16:15.587447 47554244805504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.587507 47168608887680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.587957 47491518845824 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp548wtbbt
W0618 12:16:15.588015 47527178580864 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp611z5ozs
I0618 12:16:15.589053 47491518845824 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp548wtbbt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b31c3b29e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.589106 47527178580864 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp611z5ozs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3a112f0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.589498 47491518845824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.589556 47527178580864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:15.594785 47491518845824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.594971 47527178580864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.609308 47554244805504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.609930 47168608887680 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.616641 47491518845824 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.616955 47527178580864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881775.519360 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.520233 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.520952 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.620092 47274705326976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881775.518352 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.519169 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.519929 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.620416 47633746097024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:16:15.621150 47274705326976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpbvyk339n
W0618 12:16:15.621440 47633746097024 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpy3zvyib2
I0618 12:16:15.622210 47274705326976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpbvyk339n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff489aee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.622478 47633746097024 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpy3zvyib2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b52e11a4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.622625 47274705326976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.622873 47633746097024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:15.627439 47274705326976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.627580 47633746097024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881775.517692 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.518166 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.518566 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.628916 47080388428672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881775.517885 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.518345 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.518736 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.629083 46965138498432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:16:15.629928 47080388428672 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpiq64p5ya
W0618 12:16:15.630070 46965138498432 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzdoe8ixz
I0618 12:16:15.630925 47080388428672 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpiq64p5ya', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad20a6a4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.631045 46965138498432 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzdoe8ixz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab734fbee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.631330 47080388428672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.631437 46965138498432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881775.531800 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.532629 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.533327 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.635248 47282813907840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881775.530509 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.531342 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.532152 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.635297 47908168835968 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:16:15.636101 46965138498432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.636127 47080388428672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.636408 47282813907840 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpomwd_pvw
W0618 12:16:15.636441 47908168835968 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkjckd8dr
I0618 12:16:15.637553 47282813907840 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpomwd_pvw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b012bea0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.637751 47908168835968 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkjckd8dr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b92c5f8fda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881775.560781 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.561164 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.561484 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.637354 47204544709504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
I0618 12:16:15.638028 47282813907840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.638200 47908168835968 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881775.562366 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.562739 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.563065 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.637938 47783129973632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:16:15.638360 47204544709504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxia384jn
I0618 12:16:15.639345 47204544709504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxia384jn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeef2b4de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:15.638920 47783129973632 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7e0vadks
I0618 12:16:15.639737 47204544709504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.639893 47783129973632 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7e0vadks', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b75a9134e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.640300 47783129973632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:15.643329 47282813907840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.643535 47908168835968 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.644411 47204544709504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.644851 47783129973632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.646708 47274705326976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.647000 47633746097024 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.655302 46965138498432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.655470 47080388428672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.658745 47554244805504 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.659104 47168608887680 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.663060 47554244805504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.663429 47168608887680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.663639 47204544709504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.664037 47783129973632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.664841 47527178580864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.664945 47491518845824 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.665421 47282813907840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.665985 47908168835968 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:16:15.668108 47554244805504 estimator.py:1111] Calling model_fn.
W0618 12:16:15.668221 47554244805504 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:15.668511 47168608887680 estimator.py:1111] Calling model_fn.
W0618 12:16:15.668620 47168608887680 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:15.669139 47527178580864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.669266 47491518845824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.669588 47554244805504 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:15.669985 47168608887680 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:16:15.674195 47527178580864 estimator.py:1111] Calling model_fn.
W0618 12:16:15.674306 47527178580864 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:15.674336 47491518845824 estimator.py:1111] Calling model_fn.
W0618 12:16:15.674448 47491518845824 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:15.675693 47527178580864 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:15.675815 47491518845824 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881775.604536 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.604955 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.605321 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.684474 47584491447168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881775.597088 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.597603 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.598216 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.684628 47225398432640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881775.606965 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.607373 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.607718 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.685537 47257014805376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
:::MLL 1560881775.605533 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.605978 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.606352 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.685735 46956821070720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:16:15.685544 47584491447168 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmph1w808hc
W0618 12:16:15.685640 47225398432640 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7_spsul8
I0618 12:16:15.686617 47584491447168 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmph1w808hc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b47694beda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.686697 47225398432640 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7_spsul8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af3cdaf6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.687043 47584491447168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.687117 47225398432640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.686566 47257014805376 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afb2a2afcc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:16:15.686733 46956821070720 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp434hutwu
I0618 12:16:15.687671 47257014805376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:16:15.687696 46956821070720 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp434hutwu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab5453a1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.688086 46956821070720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:15.692024 47584491447168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.692044 47225398432640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.692367 47257014805376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.692609 46956821070720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.694775 47633746097024 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.695041 47274705326976 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.699069 47633746097024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.699357 47274705326976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881775.569921 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.570768 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.571574 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.700185 47836944634752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
W0618 12:16:15.701246 47836944634752 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0d_mlz5b
I0618 12:16:15.702245 47836944634752 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0d_mlz5b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8230adddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.702652 47836944634752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:15.702904 46965138498432 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.703212 47080388428672 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:16:15.704125 47633746097024 estimator.py:1111] Calling model_fn.
W0618 12:16:15.704234 47633746097024 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881775.571219 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881775.572051 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881775.572801 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:16:15.704101 47675405767552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000011-000006.tfrecord.zz_0_0
I0618 12:16:15.704407 47274705326976 estimator.py:1111] Calling model_fn.
W0618 12:16:15.704516 47274705326976 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:15.705608 47633746097024 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:15.705085 47675405767552 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp02ozb7ra
W0618 12:16:15.705876 47274705326976 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:16:15.706102 47675405767552 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp02ozb7ra', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5c94363dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:16:15.706519 47675405767552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:16:15.707225 46965138498432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.707530 47080388428672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.707514 47836944634752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.711153 47675405767552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:16:15.711183 47204544709504 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.711634 47257014805376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.711731 46956821070720 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.711718 47783129973632 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:16:15.712281 46965138498432 estimator.py:1111] Calling model_fn.
W0618 12:16:15.712394 46965138498432 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:16:15.712599 47080388428672 estimator.py:1111] Calling model_fn.
W0618 12:16:15.712630 47225398432640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.712671 47584491447168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:16:15.712714 47080388428672 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:16:15.713737 46965138498432 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:15.714076 47080388428672 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:16:15.715335 47282813907840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.715774 47908168835968 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:16:15.715472 47204544709504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.716057 47783129973632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:16:15.719796 47282813907840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return[2019-06-18 12:16:54] moving /lfs/lfs12/gma_akey/results/epb002/models/000021-000013.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb
[2019-06-18 12:16:54] moving /lfs/lfs12/gma_akey/results/epb002/models/000021-000013.index --> /lfs/lfs12/gma_akey/results/epb002/models/000021-000014.index
[2019-06-18 12:16:54] moving /lfs/lfs12/gma_akey/results/epb002/models/000021-000013.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000021-000014.data-00000-of-00001
[2019-06-18 12:16:54] moving /lfs/lfs12/gma_akey/results/epb002/models/000021-000013.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000021-000014.meta
[2019-06-18 12:16:54] iteration time 20: 48.636 seconds
2019-06-18 12:16:55.449318: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881814.276967 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 21}}
[2019-06-18 12:16:58] minmax time: 3.222 seconds
2019-06-18 12:16:58.681271: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:16:58.686867: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:16:58.691507: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881818.702960 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 20}}
[2019-06-18 12:16:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000022-000014 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=23 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:16:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-22-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=22 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=1023779853 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=2047559684 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=3071339515 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=4095119346 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=5118899177 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=6142679008 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=7166458839 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=8190238670 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=9214018501 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=10237798332 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=11261578163 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=12285357994 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=13309137825 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=14332917656 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=15356697487 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=16380477318 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=17404257149 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=18428036980 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=19451816811 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000020-000013.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000021-000014 --seed=20475596642 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:17:09] eval finished: 10.315 seconds
[2019-06-18 12:17:09] Win rate 000021-000014 vs 000020-000013: 0.530
:::MLL 1560881829.082694 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 20}}
[2019-06-18 12:17:09] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-23-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=23 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=1023779854 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=2047559685 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=3071339516 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=4095119347 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=5118899178 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=6142679009 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=7166458840 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=8190238671 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=9214018502 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=10237798333 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=11261578164 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=12285357995 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=13309137826 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=14332917657 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=15356697488 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=16380477319 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=17404257150 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000022-000013 --seed=18428036981 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:17:39] selfplay finished: 30.130 seconds
[2019-06-18 12:17:39] selfplay mn: 30.148 seconds
[2019-06-18 12:17:39] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-23-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=23 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779854 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559685 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339516 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119347 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899178 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679009 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458840 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238671 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018502 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798333 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578164 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357995 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137826 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917657 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697488 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477319 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257150 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036981 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816812 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596643 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376474 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156305 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000022-000013/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:17:42] divide_golden_chunk finished: 3.503 seconds
[2019-06-18 12:17:42] generate golden chunk: 3.518 seconds
[2019-06-18 12:17:42] train finished: 44.112 seconds
:::MLL 1560881823.965027 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881823.965881 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881823.966691 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.100322 47998409425792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881823.965393 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881823.966249 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881823.967043 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.100542 47621411177344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.101460 47998409425792 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5pb4qovn
W0618 12:17:04.101650 47621411177344 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppayw8jj8
I0618 12:17:04.102589 47998409425792 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5pb4qovn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7c8bb0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.102772 47621411177344 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppayw8jj8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5001e25da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.103054 47998409425792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.103241 47621411177344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881823.975221 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881823.976098 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881823.976863 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.105608 47070800888704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881823.981496 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881823.982216 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881823.982871 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.105586 47134814483328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.106798 47070800888704 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6yb22l2u
W0618 12:17:04.106767 47134814483328 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpebuoe8mp
I0618 12:17:04.107891 47070800888704 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6yb22l2u', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acfcef41e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.107890 47134814483328 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpebuoe8mp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adeb6760e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.108334 47070800888704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.108345 47134814483328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:04.108408 47998409425792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.108433 47621411177344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.113733 47134814483328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.113742 47070800888704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.130594 47998409425792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.130638 47621411177344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.135483 47070800888704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.135692 47134814483328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881824.027806 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.028226 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.028586 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.141651 47889410704256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881824.026129 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.026536 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.026905 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.141716 47422354285440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.142670 47889410704256 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpig2zamo7
W0618 12:17:04.142701 47422354285440 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpymzrymrn
I0618 12:17:04.143677 47889410704256 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpig2zamo7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8e67e69e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.143702 47422354285440 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpymzrymrn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b21a92b3da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.144081 47889410704256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.144104 47422354285440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881824.032998 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.033952 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.034854 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.145841 47911025193856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881824.051583 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.052338 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.053026 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.146043 47188359820160 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.146974 47911025193856 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpufh_e25t
W0618 12:17:04.147132 47188359820160 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_0_cqlxb
I0618 12:17:04.148079 47911025193856 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpufh_e25t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9370398e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.148193 47188359820160 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_0_cqlxb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aeb2e02de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.148535 47911025193856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.148656 47188359820160 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:04.148799 47889410704256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.148783 47422354285440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881824.039447 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.039884 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.040257 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.150587 47993164460928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881824.034481 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.034995 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.035444 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.150887 47618290992000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.151608 47993164460928 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmps7oipekj
I0618 12:17:04.152594 47993164460928 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmps7oipekj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6901b4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:04.151888 47618290992000 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpj23nye0d
I0618 12:17:04.152884 47618290992000 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpj23nye0d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4f47e7fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.152996 47993164460928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.153281 47618290992000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:04.153849 47911025193856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.153944 47188359820160 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881824.050986 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.051710 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.052411 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.154626 47919531578240 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881824.048762 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.049511 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.050241 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.154706 46922696078208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.155748 47919531578240 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfx34hxk7
W0618 12:17:04.155810 46922696078208 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpn1n0lbql
I0618 12:17:04.156754 47919531578240 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfx34hxk7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b956b3e9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.156808 46922696078208 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpn1n0lbql', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aad5337fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.157168 47919531578240 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.157209 46922696078208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:04.157675 47993164460928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.157900 47618290992000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881824.025272 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.026171 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.027020 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.160691 47941928047488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.161949 46922696078208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.161954 47919531578240 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.161795 47941928047488 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpyeju91a8
I0618 12:17:04.162907 47941928047488 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpyeju91a8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9aa22dada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.163355 47941928047488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881824.044442 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.045186 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.045867 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.164793 47046386553728 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.165874 47046386553728 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8_px6qt_
I0618 12:17:04.167054 47046386553728 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8_px6qt_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca1fbede10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.167572 47046386553728 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:04.167896 47422354285440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.168141 47889410704256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.168713 47941928047488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.173099 47046386553728 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.175801 47911025193856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.176165 47188359820160 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.176821 47993164460928 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.177086 47618290992000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.178933 47998409425792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.179417 47621411177344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.181292 47919531578240 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.181342 46922696078208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:04.183239 47998409425792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:04.183430 47070800888704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.183871 47134814483328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.183748 47621411177344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:04.187711 47070800888704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:04.188200 47134814483328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:17:04.188323 47998409425792 estimator.py:1111] Calling model_fn.
W0618 12:17:04.188430 47998409425792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:04.188868 47621411177344 estimator.py:1111] Calling model_fn.
W0618 12:17:04.188979 47621411177344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:04.189804 47998409425792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:04.190374 47621411177344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:04.190674 47941928047488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:04.192758 47070800888704 estimator.py:1111] Calling model_fn.
W0618 12:17:04.192865 47070800888704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:04.193283 47134814483328 estimator.py:1111] Calling model_fn.
W0618 12:17:04.193392 47134814483328 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:04.194228 47070800888704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:04.194741 47134814483328 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:04.197307 47046386553728 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881824.104383 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.104802 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.105157 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.204869 47144017601408 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881824.101930 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.102343 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.102705 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.205368 47696728400768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.205891 47144017601408 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmptw5mobbn
I0618 12:17:04.206898 47144017601408 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmptw5mobbn', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae0db026e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:04.206351 47696728400768 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9o1sc9ly
I0618 12:17:04.207303 47144017601408 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.207315 47696728400768 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9o1sc9ly', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b618b23fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.207711 47696728400768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881824.118218 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.118722 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.119166 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.208064 47902849045376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881824.121813 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.122236 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.122596 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.208235 47957235000192 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.209080 47902849045376 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpshbvy6d5
W0618 12:17:04.209216 47957235000192 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpzh9yj7ab
I0618 12:17:04.210068 47902849045376 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpshbvy6d5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9188e36e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.210188 47957235000192 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpzh9yj7ab', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9e328b3e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.210472 47902849045376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.210579 47957235000192 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:04.211910 47144017601408 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.212278 47696728400768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881824.125139 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.125596 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.126000 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.213885 47831535178624 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
:::MLL 1560881824.119275 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881824.119813 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881824.120301 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:04.213891 46976546780032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000012-000007.tfrecord.zz_0_0
W0618 12:17:04.215126 47902849045376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.215185 47957235000192 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.214901 46976546780032 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqefgadk5
I0618 12:17:04.214938 47831535178624 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b80ee401d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:04.215865 46976546780032 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqefgadk5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab9dcf88e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:04.215799 47422354285440 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:17:04.216074 47831535178624 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:04.216268 46976546780032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:04.216394 47889410704256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.220134 47422354285440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:04.220728 47831535178624 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.220888 46976546780032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:04.220773 47889410704256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:04.224590 47993164460928 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.224810 47618290992000 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:17:04.225225 47422354285440 estimator.py:1111] Calling model_fn.
W0618 12:17:04.225335 47422354285440 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:04.225866 47889410704256 estimator.py:1111] Calling model_fn.
W0618 12:17:04.225963 47188359820160 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.225977 47889410704256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:04.226033 47911025193856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.226694 47422354285440 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:04.227337 47889410704256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:04.228911 47993164460928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:04.229146 47618290992000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:04.229182 46922696078208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:04.229464 47919531578240 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.[2019-06-18 12:17:42] moving /lfs/lfs12/gma_akey/results/epb002/models/000022-000014.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000022-000015.data-00000-of-00001
[2019-06-18 12:17:42] moving /lfs/lfs12/gma_akey/results/epb002/models/000022-000014.index --> /lfs/lfs12/gma_akey/results/epb002/models/000022-000015.index
[2019-06-18 12:17:42] moving /lfs/lfs12/gma_akey/results/epb002/models/000022-000014.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000022-000015.meta
[2019-06-18 12:17:42] moving /lfs/lfs12/gma_akey/results/epb002/models/000022-000014.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb
[2019-06-18 12:17:42] iteration time 21: 48.601 seconds
2019-06-18 12:17:44.086907: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881862.878028 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 22}}
[2019-06-18 12:17:47] minmax time: 3.233 seconds
2019-06-18 12:17:47.329815: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:17:47.345525: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:17:47.350092: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881867.361704 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 21}}
[2019-06-18 12:17:47] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000023-000015 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=24 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:17:47] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-23-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=23 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=1023779854 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=2047559685 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=3071339516 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=4095119347 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=5118899178 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=6142679009 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=7166458840 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=8190238671 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=9214018502 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=10237798333 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=11261578164 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=12285357995 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=13309137826 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=14332917657 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=15356697488 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=16380477319 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=17404257150 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=18428036981 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=19451816812 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000021-000014.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000022-000015 --seed=20475596643 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:17:58] eval finished: 10.836 seconds
[2019-06-18 12:17:58] Win rate 000022-000015 vs 000021-000014: 0.620
:::MLL 1560881878.263614 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 21}}
[2019-06-18 12:17:58] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-24-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=24 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=1023779855 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=2047559686 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=3071339517 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=4095119348 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=5118899179 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=6142679010 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=7166458841 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=8190238672 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=9214018503 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=10237798334 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=11261578165 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=12285357996 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=13309137827 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=14332917658 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=15356697489 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=16380477320 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=17404257151 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000023-000014 --seed=18428036982 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:18:28] selfplay finished: 29.787 seconds
[2019-06-18 12:18:28] selfplay mn: 29.808 seconds
[2019-06-18 12:18:28] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-24-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=24 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779855 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559686 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339517 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119348 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899179 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679010 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458841 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238672 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018503 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798334 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578165 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357996 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137827 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917658 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697489 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477320 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257151 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036982 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816813 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596644 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376475 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156306 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000023-000014/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:18:31] train finished: 44.109 seconds
:::MLL 1560881872.752663 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.753533 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.754262 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.887621 47538263622528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881872.752063 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.752936 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.753751 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.887847 47605258896256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.888762 47538263622528 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4gig5_ll
W0618 12:17:52.888954 47605258896256 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpn6wp51x0
I0618 12:17:52.889839 47538263622528 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4gig5_ll', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3ca5e75e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.890088 47605258896256 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpn6wp51x0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4c3f221e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.890341 47538263622528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:52.890554 47605258896256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:52.895617 47538263622528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.895836 47605258896256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881872.795867 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.796619 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.797294 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.902480 47569298129792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881872.791557 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.792459 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.793269 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.903051 47325725639552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881872.773548 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.774459 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.775326 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.903971 47253722452864 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.903502 47569298129792 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6cheee6l
I0618 12:17:52.904482 47569298129792 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6cheee6l', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b43dfb45e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:52.904039 47325725639552 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp12kfgzxg
:::MLL 1560881872.801384 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.802125 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.802831 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.904672 46935696094080 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
I0618 12:17:52.904884 47569298129792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:52.905013 47325725639552 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp12kfgzxg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b29a70e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.905410 47325725639552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:52.905082 47253722452864 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp32ondcez
I0618 12:17:52.906199 47253722452864 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp32ondcez', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afa65edae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:52.905771 46935696094080 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2w1hy2n7
I0618 12:17:52.906663 47253722452864 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:52.906924 46935696094080 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2w1hy2n7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab05a147e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.907375 46935696094080 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:52.909610 47569298129792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.909979 47325725639552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.911902 47253722452864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.912509 46935696094080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.917319 47538263622528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.917718 47605258896256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881872.794128 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.794852 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.795553 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.919851 47088337744768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881872.785979 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.786911 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.787806 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.920339 47055362585472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.920992 47088337744768 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmplii5povz
I0618 12:17:52.922095 47088337744768 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmplii5povz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad3e43b4e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:52.921467 47055362585472 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpvq_p38dw
I0618 12:17:52.922545 47088337744768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:52.922580 47055362585472 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpvq_p38dw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acc36c24e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.923060 47055362585472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881872.758129 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.758956 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.759756 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.924339 47285449827200 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.925498 47285449827200 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpy_709g_i
I0618 12:17:52.926629 47285449827200 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpy_709g_i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b01c906fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.927101 47285449827200 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:52.927901 47088337744768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.928368 47055362585472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.929077 47569298129792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.929373 47325725639552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881872.812526 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.812939 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.813296 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.931638 47627984155520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.932490 47285449827200 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.932659 47627984155520 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpvaz54kcp
I0618 12:17:52.933637 47627984155520 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpvaz54kcp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5189aa0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:52.933858 47253722452864 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:52.934039 47627984155520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881872.811002 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.811425 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.811789 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.933995 47371069842304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.934862 46935696094080 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.935110 47371069842304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3v4iexkd
I0618 12:17:52.936285 47371069842304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3v4iexkd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b15b860bda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.936750 47371069842304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:52.938802 47627984155520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881872.759269 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.760064 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.760732 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.939187 47094464181120 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.940285 47094464181120 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpahat5w4j
I0618 12:17:52.941391 47094464181120 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpahat5w4j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad551654e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:52.941664 47371069842304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:17:52.941836 47094464181120 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:52.947195 47094464181120 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.949870 47088337744768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.951177 47055362585472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.954744 47285449827200 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.958061 47627984155520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881872.875230 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.875680 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.876096 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.959075 47582547927936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881872.875479 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.875936 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.876331 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.959285 47049210626944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.960119 47582547927936 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpqedykco6
W0618 12:17:52.960281 47049210626944 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpsc73w4lz
I0618 12:17:52.961102 47582547927936 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpqedykco6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b46f5743da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.961243 47049210626944 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpsc73w4lz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acac812de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.961495 47582547927936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:52.961648 47049210626944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:17:52.963351 47371069842304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.965406 47605258896256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:52.965394 47538263622528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881872.881114 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.881539 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.881915 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.965571 47094350058368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881872.879297 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.879766 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.880175 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.965573 47332952028032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.966150 47582547927936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.966191 47049210626944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:17:52.966676 47094350058368 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad54a97ed30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:52.966655 47332952028032 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgjg1ddi4
I0618 12:17:52.967693 47332952028032 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgjg1ddi4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0cd860fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:52.967714 47094464181120 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:52.967880 47094350058368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881872.819333 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.819785 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.820177 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.967622 46920623133568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
I0618 12:17:52.968118 47332952028032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881872.820436 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.820847 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.821207 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.968156 47633452499840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.968638 46920623133568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmx4hdqzd
W0618 12:17:52.969710 47605258896256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:17:52.969617 46920623133568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmx4hdqzd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aacd7a96e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:17:52.969704 47538263622528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:52.969148 47633452499840 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpb0q2yjk2
I0618 12:17:52.970016 46920623133568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:52.970134 47633452499840 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpb0q2yjk2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b52cf9a5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.970525 47633452499840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881872.869521 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.869932 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.870291 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.971509 47703336022912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
:::MLL 1560881872.867457 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881872.867872 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881872.868231 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:17:52.971565 46925789954944 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000022-000013.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000013-000007.tfrecord.zz_0_0
W0618 12:17:52.972625 47094350058368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.972783 47332952028032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.972554 47703336022912 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_m932adv
W0618 12:17:52.972588 46925789954944 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3vadd5up
I0618 12:17:52.973533 47703336022912 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_m932adv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6314fc2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.973563 46925789954944 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3vadd5up', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aae0ba0ddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:17:52.973925 47703336022912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:52.973955 46925789954944 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:17:52.974740 47605258896256 estimator.py:1111] Calling model_fn.
W0618 12:17:52.974665 46920623133568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:17:52.974783 47538263622528 estimator.py:1111] Calling model_fn.
W0618 12:17:52.974853 47605258896256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:52.974893 47538263622528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:52.975064 47633452499840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.976218 47605258896256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:52.976261 47538263622528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:52.977273 47325725639552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:52.977615 47569298129792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:52.978586 47703336022912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.978610 46925789954944 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:17:52.981583 47325725639552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:52.981955 47569298129792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:52.983194 47253722452864 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:52.983494 46935696094080 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:52.985229 47582547927936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.985221 47049210626944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:52.986636 47325725639552 estimator.py:1111] Calling model_fn.
W0618 12:17:52.986741 47325725639552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:52.987057 47569298129792 estimator.py:1111] Calling model_fn.
W0618 12:17:52.987168 47569298129792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:52.987486 47253722452864 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:52.987778 46935696094080 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:52.988100 47325725639552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:52.988526 47569298129792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:52.992094 47094350058368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.992103 47332952028032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:17:52.992578 47253722452864 estimator.py:1111] Calling model_fn.
W0618 12:17:52.992688 47253722452864 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:17:52.992804 46935696094080 estimator.py:1111] Calling model_fn.
W0618 12:17:52.992914 46935696094080 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:17:52.994049 47253722452864 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:52.993870 46920623133568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.994256 46935696094080 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:17:52.994132 47633452499840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.997631 47703336022912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.997792 46925789954944 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:17:52.998450 47088337744768 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:52.999884 47055362585472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:53.002749 47088337744768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:17:53.003067 47285449827200 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:17:53.004245 47055362585472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: Data[2019-06-18 12:18:31] divide_golden_chunk finished: 3.433 seconds
[2019-06-18 12:18:31] generate golden chunk: 3.447 seconds
[2019-06-18 12:18:31] moving /lfs/lfs12/gma_akey/results/epb002/models/000023-000015.index --> /lfs/lfs12/gma_akey/results/epb002/models/000023-000016.index
[2019-06-18 12:18:31] moving /lfs/lfs12/gma_akey/results/epb002/models/000023-000015.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000023-000016.data-00000-of-00001
[2019-06-18 12:18:31] moving /lfs/lfs12/gma_akey/results/epb002/models/000023-000015.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb
[2019-06-18 12:18:31] moving /lfs/lfs12/gma_akey/results/epb002/models/000023-000015.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000023-000016.meta
[2019-06-18 12:18:31] iteration time 22: 48.689 seconds
2019-06-18 12:18:32.836847: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881911.567397 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 23}}
[2019-06-18 12:18:36] minmax time: 3.244 seconds
2019-06-18 12:18:36.091573: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:18:36.097015: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:18:36.101659: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881916.113865 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 22}}
[2019-06-18 12:18:36] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000024-000016 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=25 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:18:36] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-24-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=24 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=1023779855 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=2047559686 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=3071339517 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=4095119348 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=5118899179 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=6142679010 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=7166458841 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=8190238672 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=9214018503 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=10237798334 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=11261578165 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=12285357996 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=13309137827 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=14332917658 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=15356697489 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=16380477320 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=17404257151 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=18428036982 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=19451816813 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000022-000015.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000023-000016 --seed=20475596644 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:18:46] eval finished: 10.306 seconds
[2019-06-18 12:18:46] Win rate 000023-000016 vs 000022-000015: 0.530
:::MLL 1560881926.485416 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 22}}
[2019-06-18 12:18:46] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-25-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=25 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=1023779856 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=2047559687 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=3071339518 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=4095119349 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=5118899180 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=6142679011 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=7166458842 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=8190238673 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=9214018504 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=10237798335 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=11261578166 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=12285357997 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=13309137828 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=14332917659 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=15356697490 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=16380477321 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=17404257152 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000024-000015 --seed=18428036983 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:19:16] selfplay finished: 30.073 seconds
[2019-06-18 12:19:16] selfplay mn: 30.092 seconds
[2019-06-18 12:19:16] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-25-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=25 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779856 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559687 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339518 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119349 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899180 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679011 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458842 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238673 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018504 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798335 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578166 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357997 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137828 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917659 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697490 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477321 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257152 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036983 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816814 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596645 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376476 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156307 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000024-000015/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:19:19] divide_golden_chunk finished: 3.302 seconds
[2019-06-18 12:19:19] generate golden chunk: 3.317 seconds
[2019-06-18 12:19:20] train finished: 44.012 seconds
:::MLL 1560881921.457001 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.457894 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.458726 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.565957 46945543000960 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.464059 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.464794 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.465483 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.566354 47095445922688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.567144 46945543000960 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpn3cxfsnw
W0618 12:18:41.567442 47095445922688 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpeha3zut0
I0618 12:18:41.568258 46945543000960 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpn3cxfsnw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab2a5005e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.568561 47095445922688 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpeha3zut0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad58be97e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.568720 46945543000960 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.569003 47095445922688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.574119 46945543000960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.574152 47095445922688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.595947 47095445922688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.596035 46945543000960 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881921.538085 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.538574 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.538996 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.625143 47179217900416 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.538071 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.538559 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.538972 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.625366 47511239934848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.626157 47179217900416 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpx0qg_0bs
W0618 12:18:41.626340 47511239934848 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3dw1urhf
I0618 12:18:41.627124 47179217900416 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpx0qg_0bs', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae90d1c6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.627332 47511239934848 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3dw1urhf', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b365b2a9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.627524 47179217900416 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.627742 47511239934848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.632107 47179217900416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.632290 47511239934848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.646134 46945543000960 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:41.646137 47095445922688 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560881921.515052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.515950 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.516761 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.648175 47254936810368 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.514746 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.515629 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.516441 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.648499 47038685287296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.649299 47254936810368 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpovx4exb2
W0618 12:18:41.649568 47038685287296 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpm3saa60g
I0618 12:18:41.650393 47254936810368 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpovx4exb2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afaae4f4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.650671 47038685287296 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpm3saa60g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac854b6ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:41.650467 46945543000960 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:41.650486 47095445922688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:18:41.650842 47254936810368 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.651111 47038685287296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.651150 47179217900416 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.651417 47511239934848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:18:41.655610 46945543000960 estimator.py:1111] Calling model_fn.
I0618 12:18:41.655603 47095445922688 estimator.py:1111] Calling model_fn.
W0618 12:18:41.655714 47095445922688 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:18:41.655721 46945543000960 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:18:41.656138 47254936810368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.656336 47038685287296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.657095 46945543000960 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:41.657075 47095445922688 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881921.547910 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.548860 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.549722 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.665385 47898419561344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.557512 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.558231 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.558902 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.665966 47958037193600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.666490 47898419561344 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3s1zrkmp
I0618 12:18:41.667625 47898419561344 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3s1zrkmp', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9080dede48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:41.667060 47958037193600 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpsf_n410m
I0618 12:18:41.668121 47898419561344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.668180 47958037193600 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpsf_n410m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9e625bbe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.668644 47958037193600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.673361 47898419561344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.673859 47958037193600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.677958 47038685287296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.677994 47254936810368 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881921.569557 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.570338 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.571017 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.678488 46969788056448 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.563957 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.564897 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.565764 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.678860 47836841718656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.679583 46969788056448 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfg_cie1b
I0618 12:18:41.680572 46969788056448 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfg_cie1b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab84a1e8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:41.679878 47836841718656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpnje6rkjt
I0618 12:18:41.680882 47836841718656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpnje6rkjt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b822a8b8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.680975 46969788056448 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.681282 47836841718656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.685706 46969788056448 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.685967 47836841718656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881921.556161 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.557086 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.557953 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.686856 47075116946304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.560803 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.561551 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.562228 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.687451 47980711961472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.687905 47075116946304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1n_grzwo
I0618 12:18:41.688888 47075116946304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1n_grzwo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad0d035de48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:41.688436 47980711961472 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4pt9m_5p
I0618 12:18:41.689297 47075116946304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.689408 47980711961472 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4pt9m_5p', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba3a9e13e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.689803 47980711961472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.694136 47075116946304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.694565 47980711961472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881921.569014 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.569450 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.569850 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.695051 47655188034432 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.695218 47898419561344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881921.569279 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.569751 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.570121 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.695115 46912659415936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.695979 47958037193600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.696129 46912659415936 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmps73or8ow
W0618 12:18:41.696097 47655188034432 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpucmq5xie
I0618 12:18:41.697094 47655188034432 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpucmq5xie', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b57df244e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.697109 46912659415936 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmps73or8ow', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aaafcfcbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.697495 47655188034432 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.697507 46912659415936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.698769 47511239934848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:41.698802 47179217900416 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:41.702168 46912659415936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.702167 47655188034432 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.703065 47511239934848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:41.703113 47179217900416 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:41.705153 46969788056448 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.705597 47836841718656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:18:41.708181 47511239934848 estimator.py:1111] Calling model_fn.
I0618 12:18:41.708193 47179217900416 estimator.py:1111] Calling model_fn.
W0618 12:18:41.708290 47511239934848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:18:41.708299 47179217900416 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:18:41.709641 47179217900416 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:41.709661 47511239934848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:41.713379 47075116946304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.713769 47980711961472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881921.547978 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.548884 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.549766 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.715395 47809526604672 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.716487 47809526604672 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_nzswf18
I0618 12:18:41.717583 47809526604672 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_nzswf18', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7bce6fee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.718039 47809526604672 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.721088 47655188034432 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.721379 46912659415936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.723306 47809526604672 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881921.627547 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.627957 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.628324 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.724298 47926765757312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.622117 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.622622 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.623063 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.724336 47959735681920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.725765 47038685287296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:41.726063 47254936810368 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:41.725334 47926765757312 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp37olff15
W0618 12:18:41.725366 47959735681920 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3jwneo35
I0618 12:18:41.726325 47926765757312 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp37olff15', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b971a6f6dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.726360 47959735681920 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3jwneo35', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9ec7989dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.726722 47926765757312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.726757 47959735681920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.730057 47038685287296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:41.730375 47254936810368 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:18:41.731348 47926765757312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.731412 47959735681920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:18:41.735137 47038685287296 estimator.py:1111] Calling model_fn.
W0618 12:18:41.735246 47038685287296 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:18:41.735480 47254936810368 estimator.py:1111] Calling model_fn.
W0618 12:18:41.735592 47254936810368 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881921.556593 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.557363 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.558092 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.736085 47318725723008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.632891 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.633383 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.633809 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.736364 47535796843392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.736598 47038685287296 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881921.630794 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.631258 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.631678 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.736387 47051111805824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.736956 47254936810368 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:18:41.737108 47318725723008 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpc4r_q3j8
W0618 12:18:41.737429 47535796843392 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmplxet8sps
I0618 12:18:41.738216 47318725723008 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpc4r_q3j8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b09886ccda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:41.737401 47051111805824 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpwp2qhnj9
I0618 12:18:41.738403 47535796843392 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmplxet8sps', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3c12df4dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.738402 47051111805824 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpwp2qhnj9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2acb39648e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.738657 47318725723008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.738810 47535796843392 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.738810 47051111805824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881921.641846 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.642327 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.642745 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.743136 47401664295808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
:::MLL 1560881921.636336 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881921.636906 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881921.637398 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:18:41.743295 47274587669376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000023-000014.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000014-000008.tfrecord.zz_0_0
W0618 12:18:41.743490 47318725723008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.743521 47535796843392 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.743527 47051111805824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:18:41.744511 47898419561344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:18:41.744891 47958037193600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:18:41.744190 47401664295808 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1cd7f2fd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:18:41.745024 47809526604672 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:18:41.744301 47274587669376 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8f3ruljq
I0618 12:18:41.745266 47274587669376 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8f3ruljq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff41978e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:18:41.745327 47401664295808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:18:41.745662 47274587669376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:18:41.748810 47898419561344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object dir[2019-06-18 12:19:20] moving /lfs/lfs12/gma_akey/results/epb002/models/000024-000016.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000024-000017.data-00000-of-00001
[2019-06-18 12:19:20] moving /lfs/lfs12/gma_akey/results/epb002/models/000024-000016.index --> /lfs/lfs12/gma_akey/results/epb002/models/000024-000017.index
[2019-06-18 12:19:20] moving /lfs/lfs12/gma_akey/results/epb002/models/000024-000016.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000024-000017.meta
[2019-06-18 12:19:20] moving /lfs/lfs12/gma_akey/results/epb002/models/000024-000016.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb
[2019-06-18 12:19:20] iteration time 23: 48.620 seconds
2019-06-18 12:19:21.435929: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560881960.187718 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 24}}
[2019-06-18 12:19:24] minmax time: 3.218 seconds
2019-06-18 12:19:24.663747: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:19:24.669288: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:19:24.674060: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560881964.685582 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 23}}
[2019-06-18 12:19:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000025-000017 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=26 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:19:24] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-25-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=25 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=1023779856 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=2047559687 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=3071339518 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=4095119349 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=5118899180 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=6142679011 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=7166458842 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=8190238673 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=9214018504 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=10237798335 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=11261578166 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=12285357997 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=13309137828 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=14332917659 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=15356697490 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=16380477321 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=17404257152 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=18428036983 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=19451816814 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000023-000016.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000024-000017 --seed=20475596645 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:19:35] eval finished: 11.257 seconds
[2019-06-18 12:19:36] Win rate 000024-000017 vs 000023-000016: 0.580
:::MLL 1560881976.009696 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 23}}
[2019-06-18 12:19:36] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-26-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=26 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=1023779857 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=2047559688 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=3071339519 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=4095119350 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=5118899181 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=6142679012 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=7166458843 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=8190238674 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=9214018505 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=10237798336 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=11261578167 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=12285357998 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=13309137829 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=14332917660 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=15356697491 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=16380477322 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=17404257153 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000025-000016 --seed=18428036984 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:20:05] selfplay finished: 29.519 seconds
[2019-06-18 12:20:05] selfplay mn: 29.538 seconds
[2019-06-18 12:20:05] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-26-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=26 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779857 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559688 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339519 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119350 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899181 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679012 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458843 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238674 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018505 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798336 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578167 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357998 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137829 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917660 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697491 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477322 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257153 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036984 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816815 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596646 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376477 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156308 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000025-000016/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:20:08] train finished: 43.968 seconds
:::MLL 1560881970.015501 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.016380 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.017160 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.133759 47108558672768 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.134896 47108558672768 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1tpxz8y3
I0618 12:19:30.136034 47108558672768 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1tpxz8y3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad8997e2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.136498 47108558672768 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560881970.024271 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.024965 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.025579 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.140391 47238548251520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.141833 47108558672768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.141490 47238548251520 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp8mgv8jut
I0618 12:19:30.142621 47238548251520 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp8mgv8jut', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af6dd79be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.143082 47238548251520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.148265 47238548251520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881970.031994 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.032899 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.033748 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.148667 47097926411136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881970.045110 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.045844 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.046489 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.149433 46984536863616 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.149790 47097926411136 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3rwfk58n
I0618 12:19:30.150897 47097926411136 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3rwfk58n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad61fc2be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:30.150531 46984536863616 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7cqwyvyy
I0618 12:19:30.151345 47097926411136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:30.151617 46984536863616 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7cqwyvyy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abbb9377e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.152065 46984536863616 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.156596 47097926411136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.157213 46984536863616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.163757 47108558672768 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.170433 47238548251520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881970.068150 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.068881 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.069571 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.173703 47949954515840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881970.063353 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.064257 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.065120 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.173719 47145597404032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.174874 47145597404032 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpobmal5j8
W0618 12:19:30.174909 47949954515840 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_ivnu0k8
I0618 12:19:30.176023 47145597404032 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpobmal5j8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae1392c5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.176022 47949954515840 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_ivnu0k8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9c8097de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.176471 47145597404032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:30.176473 47949954515840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.178260 47097926411136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.179002 46984536863616 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.181798 47145597404032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.181796 47949954515840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560881970.091667 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.092085 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.092457 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.192678 47662489334656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881970.092828 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.093246 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.093609 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.193091 47850703319936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.193694 47662489334656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpe6hvtjxl
I0618 12:19:30.194699 47662489334656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpe6hvtjxl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5992554e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:30.194078 47850703319936 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9o9oz5f2
I0618 12:19:30.195064 47850703319936 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9o9oz5f2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8564c2ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.195106 47662489334656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:30.195453 47850703319936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.199732 47662489334656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.200046 47850703319936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.203473 47949954515840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.203833 47145597404032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560881970.092261 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.093178 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.094071 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.203803 47885616513920 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881970.100381 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.101121 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.101812 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.204231 47549352305536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881970.098950 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.099358 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.099719 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.204909 48003706172288 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881970.100469 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.100894 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.101250 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.205273 47458990859136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.204843 47885616513920 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppxg6b63k
I0618 12:19:30.205833 47885616513920 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppxg6b63k', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8d85bfde48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:30.205231 47549352305536 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp62neihxh
I0618 12:19:30.206231 47885616513920 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:30.206236 47549352305536 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp62neihxh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3f3ad73dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:30.205918 48003706172288 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmplzf3gxs4
I0618 12:19:30.206636 47549352305536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:30.206908 48003706172288 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmplzf3gxs4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba904710e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:30.206256 47458990859136 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpc34_idx5
I0618 12:19:30.207245 47458990859136 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpc34_idx5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b2a30e0fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.207300 48003706172288 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:30.207640 47458990859136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.211165 47885616513920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.211439 47549352305536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.212019 48003706172288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.212334 47458990859136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.215975 47108558672768 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.218630 47662489334656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.218971 47850703319936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.220579 47108558672768 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:30.221995 47238548251520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:19:30.226108 47108558672768 estimator.py:1111] Calling model_fn.
W0618 12:19:30.226244 47108558672768 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:30.226727 47238548251520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560881970.114830 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.115741 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.116721 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.227439 47904638354304 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.227731 47108558672768 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881970.124689 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.125446 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.126160 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.227897 47384026952576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
:::MLL 1560881970.137491 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.137927 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.138315 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.228105 47911690081152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.228598 47904638354304 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpz0yqfnk_
I0618 12:19:30.229705 47904638354304 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpz0yqfnk_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b91f38a1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:30.228968 47384026952576 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1m0su64_
W0618 12:19:30.229208 47911690081152 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpwfq6y321
I0618 12:19:30.230068 47384026952576 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1m0su64_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b18bcae5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.230155 47904638354304 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:30.230228 47911690081152 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpwfq6y321', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9397dace10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.230513 47384026952576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:19:30.230640 47911690081152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.230565 47097926411136 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.230874 47885616513920 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.230890 48003706172288 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.231208 47549352305536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.231322 46984536863616 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.231477 47458990859136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:19:30.232176 47238548251520 estimator.py:1111] Calling model_fn.
W0618 12:19:30.232292 47238548251520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560881970.137757 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.138211 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.138581 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.233604 47356373959552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.233760 47238548251520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:30.234570 47356373959552 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5nvxduzq
W0618 12:19:30.235200 47097926411136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:30.235322 47911690081152 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.235407 47904638354304 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:19:30.235556 47356373959552 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5nvxduzq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b124c6f4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:19:30.235692 47384026952576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:19:30.235952 47356373959552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.236017 46984536863616 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:30.240465 47356373959552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:19:30.240680 47097926411136 estimator.py:1111] Calling model_fn.
W0618 12:19:30.240797 47097926411136 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:19:30.241504 46984536863616 estimator.py:1111] Calling model_fn.
W0618 12:19:30.241620 46984536863616 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:30.242259 47097926411136 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:30.243089 46984536863616 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:30.254295 47911690081152 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.255848 47949954515840 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.256283 47145597404032 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.257172 47904638354304 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.257425 47384026952576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.259609 47356373959552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:19:30.260510 47949954515840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:30.260956 47145597404032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:19:30.265987 47949954515840 estimator.py:1111] Calling model_fn.
W0618 12:19:30.266103 47949954515840 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:30.266047 47662489334656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:19:30.266447 47145597404032 estimator.py:1111] Calling model_fn.
W0618 12:19:30.266569 47145597404032 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:30.266535 47850703319936 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.267570 47949954515840 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:30.268021 47145597404032 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560881970.175968 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.176507 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.176985 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.268119 47645526188928 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
W0618 12:19:30.269147 47645526188928 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkjdteyb1
I0618 12:19:30.270129 47645526188928 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkjdteyb1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b559f403e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560881970.182243 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560881970.182683 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560881970.183069 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:19:30.270293 47656759464832 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000024-000015.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000015-000009.tfrecord.zz_0_0
I0618 12:19:30.270533 47645526188928 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.270338 47662489334656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:30.270861 47850703319936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:19:30.271299 47656759464832 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b583cce6d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:19:30.272408 47656759464832 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:19:30.275177 47645526188928 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:19:30.275402 47662489334656 estimator.py:1111] Calling model_fn.
W0618 12:19:30.275512 47662489334656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:19:30.275946 47850703319936 estimator.py:1111] Calling model_fn.
W0618 12:19:30.276060 47850703319936 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:19:30.276973 47656759464832 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:19:30.276874 47662489334656 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:30.277423 47850703319936 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:19:30.278276 48003706172288 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.278855 47458990859136 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.278983 47885616513920 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.279248 47549352305536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:19:30.282576 48003706172288 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:30.283189 47458990859136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:19:30.283257 47885616513920 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return[2019-06-18 12:20:08] divide_golden_chunk finished: 3.248 seconds
[2019-06-18 12:20:08] generate golden chunk: 3.263 seconds
[2019-06-18 12:20:08] moving /lfs/lfs12/gma_akey/results/epb002/models/000025-000017.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000025-000018.data-00000-of-00001
[2019-06-18 12:20:08] moving /lfs/lfs12/gma_akey/results/epb002/models/000025-000017.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb
[2019-06-18 12:20:08] moving /lfs/lfs12/gma_akey/results/epb002/models/000025-000017.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000025-000018.meta
[2019-06-18 12:20:08] moving /lfs/lfs12/gma_akey/results/epb002/models/000025-000017.index --> /lfs/lfs12/gma_akey/results/epb002/models/000025-000018.index
[2019-06-18 12:20:08] iteration time 24: 48.671 seconds
2019-06-18 12:20:10.209155: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882008.858863 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 25}}
[2019-06-18 12:20:13] minmax time: 3.209 seconds
2019-06-18 12:20:13.428149: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:20:13.433555: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:20:13.438110: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882013.449603 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 24}}
[2019-06-18 12:20:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000026-000018 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=27 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:20:13] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-26-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=26 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=1023779857 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=2047559688 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=3071339519 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=4095119350 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=5118899181 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=6142679012 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=7166458843 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=8190238674 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=9214018505 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=10237798336 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=11261578167 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=12285357998 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=13309137829 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=14332917660 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=15356697491 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=16380477322 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=17404257153 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=18428036984 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=19451816815 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000024-000017.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000025-000018 --seed=20475596646 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:20:23] eval finished: 10.019 seconds
[2019-06-18 12:20:23] Win rate 000025-000018 vs 000024-000017: 0.610
:::MLL 1560882023.534036 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 24}}
[2019-06-18 12:20:23] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-27-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=27 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=1023779858 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=2047559689 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=3071339520 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=4095119351 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=5118899182 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=6142679013 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=7166458844 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=8190238675 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=9214018506 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=10237798337 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=11261578168 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=12285357999 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=13309137830 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=14332917661 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=15356697492 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=16380477323 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=17404257154 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000026-000017 --seed=18428036985 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:20:52] selfplay finished: 29.196 seconds
[2019-06-18 12:20:52] selfplay mn: 29.213 seconds
[2019-06-18 12:20:52] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-27-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=27 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779858 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559689 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339520 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119351 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899182 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679013 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458844 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238675 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018506 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798337 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578168 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285357999 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137830 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917661 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697492 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477323 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257154 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036985 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816816 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596647 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376478 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156309 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000026-000017/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:20:55] divide_golden_chunk finished: 3.237 seconds
[2019-06-18 12:20:55] generate golden chunk: 3.251 seconds
[2019-06-18 12:20:57] train finished: 44.151 seconds
:::MLL 1560882018.705493 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.706254 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.706931 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.815332 47831941067648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.700215 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.701125 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.701946 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.815585 47298009457536 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.816465 47831941067648 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpf16hcngu
W0618 12:20:18.816686 47298009457536 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpijkl_ie2
I0618 12:20:18.817573 47831941067648 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpf16hcngu', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8106717e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.817778 47298009457536 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpijkl_ie2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b04b5a3be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.818022 47831941067648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:18.818227 47298009457536 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:18.823453 47298009457536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.823462 47831941067648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.845344 47298009457536 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.845372 47831941067648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882018.757295 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.758218 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.759074 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.878974 47045913387904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.778937 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.779360 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.779718 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.879042 47350785663872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.777369 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.777789 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.778169 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.879226 47109907497856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.880090 47045913387904 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9b5rr__f
W0618 12:20:18.880086 47350785663872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgv4ri1u3
W0618 12:20:18.880244 47109907497856 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp3clms6t0
I0618 12:20:18.881074 47350785663872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgv4ri1u3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b10ff58ada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.881218 47045913387904 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9b5rr__f', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aca038afe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.881223 47109907497856 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp3clms6t0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad8e9e39e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.881469 47350785663872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:18.881672 47045913387904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:18.881622 47109907497856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882018.769637 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.770359 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.771035 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.882071 47126810141568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.883181 47126810141568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphjc3zk8t
I0618 12:20:18.884269 47126810141568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphjc3zk8t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adcd95d4e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.884716 47126810141568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:18.886178 47350785663872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.886221 47109907497856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.887057 47045913387904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.890089 47126810141568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.897487 47298009457536 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:18.897640 47831941067648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:18.901944 47298009457536 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:18.902096 47831941067648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:18.905255 47109907497856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.905291 47350785663872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:20:18.907141 47298009457536 estimator.py:1111] Calling model_fn.
W0618 12:20:18.907269 47298009457536 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:20:18.907274 47831941067648 estimator.py:1111] Calling model_fn.
W0618 12:20:18.907392 47831941067648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:18.908719 47298009457536 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:18.908945 47045913387904 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.908846 47831941067648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:18.912173 47126810141568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882018.764039 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.764824 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.765513 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.926304 47925328085888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.762013 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.762801 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.763569 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.926373 47033644491648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.927435 47925328085888 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpwsn5ih1w
W0618 12:20:18.927511 47033644491648 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9g29aufx
I0618 12:20:18.928539 47925328085888 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpwsn5ih1w', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b96c4be2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.928623 47033644491648 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9g29aufx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac728427e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.928998 47925328085888 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:18.929075 47033644491648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882018.803658 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.804408 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.805105 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.933328 47596566471552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.806097 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.806817 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.807458 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.933694 47523871511424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.934294 47925328085888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.934411 47033644491648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.934507 47596566471552 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfe0l5jmw
W0618 12:20:18.934803 47523871511424 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpavefx0gl
I0618 12:20:18.935634 47596566471552 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfe0l5jmw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4a39063dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.935904 47523871511424 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpavefx0gl', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b394c112dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.936088 47596566471552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:18.936360 47523871511424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882018.832223 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.832725 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.833171 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.938065 47997005423488 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.832303 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.832805 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.833238 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.938329 47854161490816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.939090 47997005423488 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkbe8gmml
W0618 12:20:18.939334 47854161490816 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpb7v5_7lh
I0618 12:20:18.940076 47997005423488 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkbe8gmml', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba7750bae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.940306 47854161490816 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpb7v5_7lh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8632e23e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.940476 47997005423488 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:18.940695 47854161490816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:18.941422 47596566471552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.941810 47523871511424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.945177 47997005423488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.945291 47854161490816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.952676 47109907497856 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:18.953323 47350785663872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:18.955789 47925328085888 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.956395 47033644491648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.956965 47109907497856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:18.957662 47350785663872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:18.958026 47045913387904 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:18.960605 47126810141568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:20:18.962010 47109907497856 estimator.py:1111] Calling model_fn.
W0618 12:20:18.962128 47109907497856 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:18.962366 47045913387904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:20:18.962760 47350785663872 estimator.py:1111] Calling model_fn.
W0618 12:20:18.962871 47350785663872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:18.963009 47596566471552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.963408 47523871511424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.963486 47109907497856 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:18.964238 47997005423488 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.964314 47854161490816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.964239 47350785663872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:18.964925 47126810141568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:20:18.967469 47045913387904 estimator.py:1111] Calling model_fn.
W0618 12:20:18.967580 47045913387904 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560882018.833987 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.834404 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.834938 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.968462 47611488723840 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.836050 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.836474 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.836867 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.968802 47384188683136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.968940 47045913387904 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:20:18.970015 47126810141568 estimator.py:1111] Calling model_fn.
W0618 12:20:18.970129 47126810141568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:18.969522 47611488723840 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpoat66mhe
I0618 12:20:18.970508 47611488723840 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpoat66mhe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4db275ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:20:18.969822 47384188683136 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpdz1ixz7h
I0618 12:20:18.970805 47384188683136 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpdz1ixz7h', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b18c6525e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.970912 47611488723840 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:18.971209 47384188683136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:18.971501 47126810141568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:18.975622 47611488723840 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.975832 47384188683136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882018.811574 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.812451 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.813171 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.977059 47260181943168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.811035 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.811827 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.812609 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.977772 47275366409088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.978089 47260181943168 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1jwotcv7
I0618 12:20:18.979227 47260181943168 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1jwotcv7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afbe6f1ae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.979668 47260181943168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:18.978886 47275366409088 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp1o01kqz5
I0618 12:20:18.980010 47275366409088 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp1o01kqz5', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aff70023e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.980420 47275366409088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:18.984472 47260181943168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.985030 47275366409088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882018.878994 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.879825 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.880263 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.988824 47163988415360 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
:::MLL 1560882018.878707 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882018.879211 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882018.879901 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:20:18.989035 47254814471040 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000025-000016.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000016-000010.tfrecord.zz_0_0
W0618 12:20:18.989861 47163988415360 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpj23gp92i
W0618 12:20:18.990041 47254814471040 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpd0mjhxda
I0618 12:20:18.990859 47163988415360 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpj23gp92i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae5815cedd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.991026 47254814471040 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpd0mjhxda', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afaa7048e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:20:18.991265 47163988415360 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:20:18.991422 47254814471040 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:20:18.994692 47611488723840 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.994848 47384188683136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:18.996039 47163988415360 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:18.996114 47254814471040 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:20:19.003758 47925328085888 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:19.004064 47260181943168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:19.004528 47275366409088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:19.004869 47033644491648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:19.008247 47925328085888 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:19.009455 47033644491648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:19.011491 47997005423488 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:19.011760 47596566471552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:19.011858 47523871511424 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:20:19.011942 47854161490816 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:20:19.013495 47925328085888 estimator.py:1111] Calling model_fn.
W0618 12:20:19.013632 47925328085888 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:20:19.014798 47033644491648 estimator.py:1111] Calling model_fn.
W0618 12:20:19.014946 47033644491648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:20:19.015114 47925328085888 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:20:19.015049 47254814471040 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:19.015241 47163988415360 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:20:19.015786 47997005423488 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:20:19.016080 47596566471552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to i[2019-06-18 12:20:57] moving /lfs/lfs12/gma_akey/results/epb002/models/000026-000018.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000026-000019.meta
[2019-06-18 12:20:57] moving /lfs/lfs12/gma_akey/results/epb002/models/000026-000018.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000026-000019.data-00000-of-00001
[2019-06-18 12:20:57] moving /lfs/lfs12/gma_akey/results/epb002/models/000026-000018.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb
[2019-06-18 12:20:57] moving /lfs/lfs12/gma_akey/results/epb002/models/000026-000018.index --> /lfs/lfs12/gma_akey/results/epb002/models/000026-000019.index
[2019-06-18 12:20:57] iteration time 25: 48.803 seconds
2019-06-18 12:20:59.005164: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882057.662224 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 26}}
[2019-06-18 12:21:02] minmax time: 3.216 seconds
2019-06-18 12:21:02.230776: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:21:02.236339: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:21:02.240827: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882062.252360 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 25}}
[2019-06-18 12:21:02] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=28 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:21:02] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-27-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=27 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=1023779858 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=2047559689 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=3071339520 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=4095119351 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=5118899182 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=6142679013 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=7166458844 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=8190238675 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=9214018506 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=10237798337 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=11261578168 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=12285357999 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=13309137830 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=14332917661 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=15356697492 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=16380477323 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=17404257154 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=18428036985 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=19451816816 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000026-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000026-000019 --seed=20475596647 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:21:12] eval finished: 10.445 seconds
[2019-06-18 12:21:12] Win rate 000026-000019 vs 000025-000018: 0.460
:::MLL 1560882072.760989 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 25}}
[2019-06-18 12:21:12] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-28-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=28 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=1023779859 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=2047559690 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=3071339521 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=4095119352 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=5118899183 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=6142679014 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=7166458845 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=8190238676 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=9214018507 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=10237798338 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=11261578169 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=12285358000 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=13309137831 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=14332917662 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=15356697493 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=16380477324 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=17404257155 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000027-000018 --seed=18428036986 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:21:41] selfplay finished: 28.300 seconds
[2019-06-18 12:21:41] selfplay mn: 28.319 seconds
[2019-06-18 12:21:41] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-28-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=28 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779859 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559690 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339521 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119352 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899183 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679014 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458845 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238676 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018507 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798338 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578169 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285358000 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137831 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917662 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697493 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477324 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257155 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036986 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816817 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596648 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376479 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156310 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000027-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:21:44] divide_golden_chunk finished: 3.198 seconds
[2019-06-18 12:21:44] generate golden chunk: 3.213 seconds
[2019-06-18 12:21:45] train finished: 43.668 seconds
:::MLL 1560882067.512232 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.513116 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.513972 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.625119 47924349399936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.523118 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.523864 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.524555 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.625251 47743999533952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:21:07.626234 47924349399936 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp338m3bkv
W0618 12:21:07.626344 47743999533952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpie59f53b
I0618 12:21:07.627339 47924349399936 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp338m3bkv', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b968a68ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.627410 47743999533952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpie59f53b', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6c8cb83da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.627801 47924349399936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.627861 47743999533952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.633093 47743999533952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.633109 47924349399936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882067.499216 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.499972 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.500666 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.635135 47325509649280 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.492908 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.493807 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.494667 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.635989 47012566266752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:21:07.636281 47325509649280 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpdbeuw0pd
I0618 12:21:07.637400 47325509649280 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpdbeuw0pd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b1cc75e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.637858 47325509649280 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.637119 47012566266752 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpeg5ks9md
I0618 12:21:07.638269 47012566266752 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpeg5ks9md', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac23fe63e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.638726 47012566266752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.643213 47325509649280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.644082 47012566266752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.654760 47743999533952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.655012 47924349399936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.664955 47325509649280 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.666365 47012566266752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882067.586034 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.586486 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.586881 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.687091 47207734518656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.586159 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.586614 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.587008 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.687141 47832330118016 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.565950 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.566371 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.566729 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.688087 47710908871552 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.564703 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.565114 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.565501 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.688229 47234032587648 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:21:07.688152 47207734518656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5w183c7d
W0618 12:21:07.688180 47832330118016 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpueupnnvw
I0618 12:21:07.689154 47832330118016 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpueupnnvw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b811da1ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.689153 47207734518656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5w183c7d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aefb0d57e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560882067.572815 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.573559 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.574271 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.688889 47645670286208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.564225 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.565140 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.565988 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.689053 47786783847296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
I0618 12:21:07.689553 47832330118016 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.689555 47207734518656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.689123 47710908871552 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpae_34zc9
W0618 12:21:07.689199 47234032587648 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpp1cv5bk0
I0618 12:21:07.690108 47710908871552 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpae_34zc9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b64d85c9e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.690181 47234032587648 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpp1cv5bk0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af5d0522e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.690509 47710908871552 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.690574 47234032587648 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.690106 47645670286208 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpdkpzwg6n
W0618 12:21:07.690383 47786783847296 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpab4ehebr
I0618 12:21:07.691221 47645670286208 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpdkpzwg6n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b55a7d6ee48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.691485 47786783847296 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpab4ehebr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7682dd0e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.691670 47645670286208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.691929 47786783847296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.694256 47832330118016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.694279 47207734518656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.695158 47710908871552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.695158 47234032587648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.697044 47645670286208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.697115 47786783847296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.704071 47743999533952 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:07.704866 47924349399936 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:07.708485 47743999533952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.709299 47924349399936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.713248 47832330118016 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.713390 47207734518656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:07.713510 47743999533952 estimator.py:1111] Calling model_fn.
W0618 12:21:07.713620 47743999533952 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:07.714067 47710908871552 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.714134 47234032587648 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:07.714403 47924349399936 estimator.py:1111] Calling model_fn.
W0618 12:21:07.714516 47924349399936 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:07.714980 47743999533952 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.715878 47924349399936 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.716232 47325509649280 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:07.717482 47012566266752 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:07.718959 47786783847296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.719045 47645670286208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.720826 47325509649280 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.722103 47012566266752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560882067.575117 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.576058 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.576914 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.723789 47926075130752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.580289 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.581038 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.581771 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.724190 47250724770688 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:21:07.724876 47926075130752 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmnjs9rn4
I0618 12:21:07.725901 47926075130752 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmnjs9rn4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b96f1455e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:07.725212 47250724770688 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgiecf9vz
I0618 12:21:07.726212 47250724770688 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgiecf9vz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af9b3409dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.726309 47926075130752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.726278 47325509649280 estimator.py:1111] Calling model_fn.
W0618 12:21:07.726396 47325509649280 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:07.726610 47250724770688 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.727630 47012566266752 estimator.py:1111] Calling model_fn.
W0618 12:21:07.727750 47012566266752 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:07.727860 47325509649280 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.729229 47012566266752 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.731074 47926075130752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.731338 47250724770688 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882067.515978 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.516796 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.517562 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.736575 47762218288000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.517294 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.518072 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.518757 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.737116 47353099187072 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:21:07.737708 47762218288000 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2941j4c7
I0618 12:21:07.738801 47762218288000 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2941j4c7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b70caa44da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:07.738187 47353099187072 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpiv50c905
I0618 12:21:07.739257 47762218288000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.739306 47353099187072 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpiv50c905', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b11893e4da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.739755 47353099187072 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.744350 47762218288000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.744953 47353099187072 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882067.644747 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.645230 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.645673 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.748820 46965310718848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.644679 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.645157 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.645591 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.749181 47677362652032 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:21:07.750052 47926075130752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.749853 46965310718848 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7s5jf3mo
W0618 12:21:07.750744 47250724770688 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:07.750839 46965310718848 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7s5jf3mo', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab73f3fddd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:07.750176 47677362652032 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpxi8wwcof
I0618 12:21:07.751151 47677362652032 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpxi8wwcof', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5d08da1e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.751241 46965310718848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.751543 47677362652032 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.755938 46965310718848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.756160 47677362652032 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882067.563450 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.563988 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.564404 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.759720 47790377264000 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
:::MLL 1560882067.564032 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.564489 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.564852 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:07.759933 47957693006720 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000026-000017.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000017-000010.tfrecord.zz_0_0
W0618 12:21:07.761108 47832330118016 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:07.761107 47207734518656 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:07.760737 47790377264000 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgnyxg7g0
W0618 12:21:07.760926 47957693006720 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7124abyq
W0618 12:21:07.761448 47234032587648 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:07.761437 47710908871552 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:21:07.761722 47790377264000 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgnyxg7g0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b77590c3e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.761919 47957693006720 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7124abyq', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9e4dd7de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:07.762118 47790377264000 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:07.762319 47957693006720 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:07.763959 47762218288000 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.764580 47353099187072 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.765432 47832330118016 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.765450 47207734518656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.765720 47234032587648 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.765738 47710908871552 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.766759 47790377264000 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.766954 47957693006720 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:07.768453 47786783847296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:07.768769 47645670286208 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:21:07.770529 47832330118016 estimator.py:1111] Calling model_fn.
I0618 12:21:07.770577 47207734518656 estimator.py:1111] Calling model_fn.
W0618 12:21:07.770636 47832330118016 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:07.770684 47207734518656 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:07.770729 47234032587648 estimator.py:1111] Calling model_fn.
I0618 12:21:07.770785 47710908871552 estimator.py:1111] Calling model_fn.
W0618 12:21:07.770837 47234032587648 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:07.770887 47710908871552 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:07.772002 47832330118016 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.772046 47207734518656 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.772180 47234032587648 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.772244 47710908871552 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.772758 47786783847296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.773117 47645670286208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:07.774907 46965310718848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:07.775244 47677362652032 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:07.777804 47786783847296 estimator.py:1111] Calling model_fn.
W0618 12:21:07.777914 47786783847296 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:07.778217 47645670286208 estimator.py:1111] Calling model_fn.
W0618 12:21:07.778325 47645670286208 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:07.779284 47786783847296 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:07.779708 47645670286208 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882067.657973 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882067.658461 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882067.658876 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': '[2019-06-18 12:21:45] iteration time 26: 48.279 seconds
2019-06-18 12:21:47.371278: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882105.941700 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 27}}
[2019-06-18 12:21:50] minmax time: 3.223 seconds
2019-06-18 12:21:50.605398: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:21:50.610984: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:21:50.615594: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882110.629216 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 26}}
[2019-06-18 12:21:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000028-000019 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=29 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:21:50] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-28-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=28 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=1023779859 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=2047559690 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=3071339521 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=4095119352 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=5118899183 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=6142679014 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=7166458845 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=8190238676 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=9214018507 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=10237798338 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=11261578169 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=12285358000 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=13309137831 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=14332917662 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=15356697493 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=16380477324 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=17404257155 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=18428036986 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=19451816817 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000025-000018.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000027-000019 --seed=20475596648 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:22:00] eval finished: 10.225 seconds
[2019-06-18 12:22:00] Win rate 000027-000019 vs 000025-000018: 0.520
:::MLL 1560882120.919476 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 26}}
[2019-06-18 12:22:00] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-29-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=29 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=1023779860 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=2047559691 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=3071339522 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=4095119353 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=5118899184 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=6142679015 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=7166458846 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=8190238677 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=9214018508 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=10237798339 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=11261578170 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=12285358001 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=13309137832 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=14332917663 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=15356697494 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=16380477325 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=17404257156 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000028-000018 --seed=18428036987 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:22:29] selfplay finished: 28.917 seconds
[2019-06-18 12:22:29] selfplay mn: 28.935 seconds
[2019-06-18 12:22:29] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-29-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=29 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779860 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559691 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339522 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119353 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899184 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679015 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458846 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238677 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018508 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798339 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578170 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285358001 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137832 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917663 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697494 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477325 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257156 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036987 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816818 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596649 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376480 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156311 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000028-000018/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:22:33] divide_golden_chunk finished: 3.319 seconds
[2019-06-18 12:22:33] generate golden chunk: 3.334 seconds
[2019-06-18 12:22:34] train finished: 43.455 seconds
:::MLL 1560882115.920135 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.920890 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.921576 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.065359 47266307793792 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882115.910832 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.911683 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.912474 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.065389 47918209045376 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882115.943028 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.943741 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.944412 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.066504 47361525437312 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882115.935794 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.936655 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.937483 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.066674 47838409155456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
W0618 12:21:56.066463 47266307793792 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpk47owasc
W0618 12:21:56.066498 47918209045376 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpp17sydyw
I0618 12:21:56.067553 47266307793792 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpk47owasc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afd5412ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.067578 47918209045376 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpp17sydyw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b951c6a5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.067997 47266307793792 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:56.068048 47918209045376 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.067646 47361525437312 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpul0aq21m
W0618 12:21:56.067758 47838409155456 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7z9aapi3
I0618 12:21:56.068781 47361525437312 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpul0aq21m', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b137f7cada0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.068843 47838409155456 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7z9aapi3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8287f8be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.069257 47361525437312 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:56.069281 47838409155456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882115.959200 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.959919 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.960614 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.072180 47566412358528 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882115.954924 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.955761 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.956545 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.072520 47931315127168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
W0618 12:21:56.073309 47918209045376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.073326 47266307793792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.073347 47566412358528 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpyns5cx3j
W0618 12:21:56.073641 47931315127168 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2k23pro3
I0618 12:21:56.074440 47566412358528 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpyns5cx3j', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4333b2ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:56.074618 47361525437312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:21:56.074723 47931315127168 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2k23pro3', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9829995e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:56.074625 47838409155456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:21:56.074902 47566412358528 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:56.075166 47931315127168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.080267 47566412358528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.080355 47931315127168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.094853 47918209045376 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.094940 47266307793792 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.096366 47361525437312 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.096545 47838409155456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.102159 47566412358528 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.102170 47931315127168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882115.995174 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.996071 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.996720 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.112787 47847941747584 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882115.999049 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.999785 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.000466 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.113990 47192649184128 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
W0618 12:21:56.113928 47847941747584 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2tnh2pq9
I0618 12:21:56.115008 47847941747584 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2tnh2pq9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b84c0288dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.115418 47847941747584 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882115.982992 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.983410 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.983766 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.115027 47980785865600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
W0618 12:21:56.115027 47192649184128 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmph_vbh2y1
:::MLL 1560882115.982048 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.982470 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.982880 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.115590 47556942078848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
I0618 12:21:56.116016 47192649184128 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmph_vbh2y1', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aec2dad8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.116426 47192649184128 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.116045 47980785865600 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpetdfci_9
I0618 12:21:56.117012 47980785865600 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpetdfci_9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba3ae48ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:56.116564 47556942078848 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpu0ygwcyw
I0618 12:21:56.117414 47980785865600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:56.117533 47556942078848 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpu0ygwcyw', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b40ff39fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.117928 47556942078848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.120211 47847941747584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.121124 47192649184128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.122054 47980785865600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.122488 47556942078848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882115.998935 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.999387 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.999789 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.126031 47260665082752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882115.999052 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882115.999512 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882115.999909 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.127112 47741980644224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
W0618 12:21:56.127058 47260665082752 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpoanutdhm
I0618 12:21:56.128049 47260665082752 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpoanutdhm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afc03bdce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.128450 47260665082752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.128099 47741980644224 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjjisple4
I0618 12:21:56.129075 47741980644224 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjjisple4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6c14625e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.129465 47741980644224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.133077 47260665082752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.134047 47741980644224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882116.045126 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882116.045581 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.045933 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.137306 47586802889600 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882116.045373 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882116.045798 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.046136 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.137480 47373852357504 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882116.004631 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882116.005536 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.006400 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.138193 47685453816704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882116.013540 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882116.014309 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.014967 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.138547 47023710217088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
W0618 12:21:56.138301 47586802889600 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpa5jljya7
W0618 12:21:56.138463 47373852357504 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp68vba_95
I0618 12:21:56.139293 47586802889600 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpa5jljya7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b47f311ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.139439 47373852357504 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp68vba_95', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b165e3a8da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:56.139565 47847941747584 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:56.139698 47586802889600 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:21:56.139835 47373852357504 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.139355 47685453816704 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9riwc5oe
I0618 12:21:56.140465 47685453816704 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9riwc5oe', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5eeb1f7e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:21:56.139661 47023710217088 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpj_ql3evb
I0618 12:21:56.140761 47023710217088 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpj_ql3evb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac4d8216e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.140906 47685453816704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.140914 47192649184128 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.140885 47980785865600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:56.141211 47023710217088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.141446 47556942078848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.143206 47918209045376 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.143750 47266307793792 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.144466 47586802889600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.144488 47373852357504 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.145578 47838409155456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.145643 47361525437312 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.146270 47685453816704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.146427 47023710217088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.147470 47918209045376 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:56.148056 47266307793792 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:56.149863 47838409155456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:56.149928 47361525437312 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:56.151880 47566412358528 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.151932 47260665082752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.152109 47931315127168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
I0618 12:21:56.152489 47918209045376 estimator.py:1111] Calling model_fn.
W0618 12:21:56.152595 47918209045376 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:56.153079 47741980644224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:21:56.153116 47266307793792 estimator.py:1111] Calling model_fn.
W0618 12:21:56.153229 47266307793792 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:56.153969 47918209045376 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:56.154581 47266307793792 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:21:56.154884 47838409155456 estimator.py:1111] Calling model_fn.
I0618 12:21:56.154973 47361525437312 estimator.py:1111] Calling model_fn.
W0618 12:21:56.154996 47838409155456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:56.155081 47361525437312 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:56.156204 47566412358528 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:56.156349 47838409155456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:56.156439 47361525437312 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:56.156460 47931315127168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:21:56.161262 47566412358528 estimator.py:1111] Calling model_fn.
W0618 12:21:56.161372 47566412358528 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:56.161577 47931315127168 estimator.py:1111] Calling model_fn.
W0618 12:21:56.161687 47931315127168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:56.162739 47566412358528 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:56.163052 47931315127168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:56.163449 47586802889600 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.163561 47373852357504 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.168170 47685453816704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:21:56.168228 47023710217088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882116.083098 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882116.083657 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.084135 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.182334 47009582769024 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
W0618 12:21:56.183339 47009582769024 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpc3ftultd
I0618 12:21:56.184341 47009582769024 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpc3ftultd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac18e11be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.184746 47009582769024 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882116.089791 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882116.090245 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.090632 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.184788 47941706621824 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
I0618 12:21:56.185816 47941706621824 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9a94faed30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:21:56.186939 47941706621824 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:21:56.188237 47847941747584 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.188218 47980785865600 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.188465 47556942078848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.189411 47009582769024 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.189550 47192649184128 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:21:56.191556 47941706621824 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:21:56.192536 47847941747584 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:56.192495 47980785865600 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:56.192754 47556942078848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:21:56.193867 47192649184128 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:21:56.197592 47847941747584 estimator.py:1111] Calling model_fn.
W0618 12:21:56.197700 47847941747584 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:56.197510 47980785865600 estimator.py:1111] Calling model_fn.
W0618 12:21:56.197620 47980785865600 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:21:56.197781 47556942078848 estimator.py:1111] Calling model_fn.
W0618 12:21:56.197889 47556942078848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560882116.088479 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882116.088896 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.089272 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.197995 47717696148352 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
:::MLL 1560882116.089666 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882116.090076 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882116.090428 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:21:56.198030 47926512341888 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000027-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000018-000011.tfrecord.zz_0_0
I0618 12:21:56.198962 47192649184128 estimator.py:1111] Calling model_fn.
W0618 12:21:56.199064 47847941747584 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:56.198971 47980785865600 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:21:56.199078 47192649184128 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:21:56.199256 47556942078848 deprecation.py:506] From /home/gma/anacond[2019-06-18 12:22:34] moving /lfs/lfs12/gma_akey/results/epb002/models/000028-000019.index --> /lfs/lfs12/gma_akey/results/epb002/models/000028-000020.index
[2019-06-18 12:22:34] moving /lfs/lfs12/gma_akey/results/epb002/models/000028-000019.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb
[2019-06-18 12:22:34] moving /lfs/lfs12/gma_akey/results/epb002/models/000028-000019.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000028-000020.data-00000-of-00001
[2019-06-18 12:22:34] moving /lfs/lfs12/gma_akey/results/epb002/models/000028-000019.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000028-000020.meta
[2019-06-18 12:22:34] iteration time 27: 48.212 seconds
2019-06-18 12:22:35.648899: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882154.153998 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 28}}
[2019-06-18 12:22:38] minmax time: 3.229 seconds
2019-06-18 12:22:38.888411: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:22:38.893856: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:22:38.898439: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882158.910233 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 27}}
[2019-06-18 12:22:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000029-000020 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=30 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:22:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-29-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=29 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=1023779860 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=2047559691 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=3071339522 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=4095119353 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=5118899184 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=6142679015 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=7166458846 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=8190238677 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=9214018508 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=10237798339 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=11261578170 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=12285358001 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=13309137832 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=14332917663 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=15356697494 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=16380477325 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=17404257156 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=18428036987 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=19451816818 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000027-000019.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000028-000020 --seed=20475596649 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:22:49] eval finished: 10.484 seconds
[2019-06-18 12:22:49] Win rate 000028-000020 vs 000027-000019: 0.650
:::MLL 1560882169.458952 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 27}}
[2019-06-18 12:22:49] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-30-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=30 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=1023779861 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=2047559692 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=3071339523 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=4095119354 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=5118899185 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=6142679016 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=7166458847 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=8190238678 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=9214018509 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=10237798340 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=11261578171 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=12285358002 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=13309137833 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=14332917664 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=15356697495 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=16380477326 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=17404257157 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000029-000019 --seed=18428036988 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:23:19] selfplay finished: 30.277 seconds
[2019-06-18 12:23:19] selfplay mn: 30.296 seconds
[2019-06-18 12:23:19] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-30-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=30 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779861 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559692 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339523 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119354 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899185 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679016 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458847 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238678 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018509 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798340 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578171 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285358002 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137833 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917664 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697495 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477326 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257157 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036988 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816819 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596650 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376481 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156312 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000029-000019/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:23:22] train finished: 43.587 seconds
:::MLL 1560882164.173555 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.174464 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.175248 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.300431 47737227223936 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
:::MLL 1560882164.182254 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.182942 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.183593 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.300806 47149758915456 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
W0618 12:22:44.301564 47737227223936 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpeppje3se
I0618 12:22:44.302754 47737227223936 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpeppje3se', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6af90efda0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:22:44.301914 47149758915456 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp_92qshhk
I0618 12:22:44.303143 47149758915456 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp_92qshhk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae23137fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.303220 47737227223936 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:44.303599 47149758915456 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:44.308521 47737227223936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.308751 47149758915456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.330559 47737227223936 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.330840 47149758915456 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882164.257418 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.257897 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.258332 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.364015 47045132448640 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
:::MLL 1560882164.261032 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.261459 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.261828 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.364626 47125443511168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
W0618 12:22:44.365059 47045132448640 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpn6hxg9sy
I0618 12:22:44.366072 47045132448640 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpn6hxg9sy', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac9d4fece10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:22:44.365611 47125443511168 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp5u_74mda
I0618 12:22:44.366468 47045132448640 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:44.366619 47125443511168 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp5u_74mda', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2adc87e85da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.367030 47125443511168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:44.371118 47045132448640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.371623 47125443511168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882164.241131 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.241858 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.242528 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.372029 47992919143296 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
:::MLL 1560882164.231883 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.232813 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.233701 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.372127 47988937577344 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
W0618 12:22:44.373173 47992919143296 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpbam79sv_
W0618 12:22:44.373293 47988937577344 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfb4il1d7
I0618 12:22:44.374277 47992919143296 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpbam79sv_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba6817c0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.374375 47988937577344 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfb4il1d7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba5942a2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.374721 47992919143296 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:44.374803 47988937577344 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:44.379997 47992919143296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.380046 47988937577344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.379905 47737227223936 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.380206 47149758915456 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.384219 47737227223936 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.384519 47149758915456 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:22:44.389290 47737227223936 estimator.py:1111] Calling model_fn.
W0618 12:22:44.389397 47737227223936 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:44.389592 47149758915456 estimator.py:1111] Calling model_fn.
W0618 12:22:44.389703 47149758915456 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:44.390047 47045132448640 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.390762 47737227223936 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:44.391076 47149758915456 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:44.391190 47125443511168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882164.202739 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.203607 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.204328 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.392569 47932355351424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
:::MLL 1560882164.201662 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.202543 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.203308 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.392685 47408557310848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
W0618 12:22:44.393810 47408557310848 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpo5w4rmkz
W0618 12:22:44.393787 47932355351424 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpnioim0r4
I0618 12:22:44.394895 47408557310848 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpo5w4rmkz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b1e72ce1e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.394992 47932355351424 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpnioim0r4', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b986799ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.395350 47408557310848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:44.395437 47932355351424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882164.272041 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.272969 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.273839 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.395455 47850314011520 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
:::MLL 1560882164.285386 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.286120 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.286798 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.395749 47801616921472 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
W0618 12:22:44.396501 47850314011520 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpt_gtznda
W0618 12:22:44.396754 47801616921472 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpkrehlo49
I0618 12:22:44.397525 47850314011520 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpt_gtznda', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b854d8e6e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.397757 47801616921472 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpkrehlo49', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b79f6fbce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.397932 47850314011520 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:44.398161 47801616921472 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:44.400612 47408557310848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.400668 47932355351424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.401756 47992919143296 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.401800 47988937577344 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.403105 47850314011520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.403154 47801616921472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.421263 47408557310848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.421488 47932355351424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.422840 47801616921472 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.423106 47850314011520 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882164.304235 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.304655 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.305067 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.427163 47502870942592 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
:::MLL 1560882164.301613 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.302037 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.302395 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.427397 47723738174336 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
W0618 12:22:44.428205 47502870942592 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpsyl4dq0n
W0618 12:22:44.428379 47723738174336 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpnce5mw2d
I0618 12:22:44.429196 47502870942592 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpsyl4dq0n', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b346855ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.429371 47723738174336 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpnce5mw2d', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b67d50c5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
:::MLL 1560882164.264643 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.265053 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.265411 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.429248 47247624278912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
I0618 12:22:44.429596 47502870942592 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882164.260081 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.260588 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.261118 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.429516 47293743002496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
I0618 12:22:44.429779 47723738174336 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:44.430299 47247624278912 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpr24bwvom
W0618 12:22:44.430519 47293743002496 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpgud64ajm
I0618 12:22:44.431306 47247624278912 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpr24bwvom', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af8fa72ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.431527 47293743002496 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpgud64ajm', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b03b756ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.431705 47247624278912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:44.431943 47293743002496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:22:44.434192 47502870942592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.434322 47723738174336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.436318 47247624278912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.436566 47293743002496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.437575 47045132448640 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.438830 47125443511168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.441863 47045132448640 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.443214 47125443511168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:22:44.446893 47045132448640 estimator.py:1111] Calling model_fn.
W0618 12:22:44.447003 47045132448640 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:44.448357 47045132448640 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:22:44.448436 47125443511168 estimator.py:1111] Calling model_fn.
W0618 12:22:44.448544 47125443511168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:44.449912 47992919143296 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.449939 47125443511168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:44.450180 47988937577344 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.453041 47502870942592 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.453204 47723738174336 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.454197 47992919143296 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.454488 47988937577344 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.455298 47247624278912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.455682 47293743002496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:22:44.459246 47992919143296 estimator.py:1111] Calling model_fn.
W0618 12:22:44.459352 47992919143296 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:44.459553 47988937577344 estimator.py:1111] Calling model_fn.
W0618 12:22:44.459661 47988937577344 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:44.460721 47992919143296 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:44.461031 47988937577344 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:44.468917 47932355351424 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.468947 47408557310848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.472003 47801616921472 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.472335 47850314011520 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.473211 47932355351424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.473278 47408557310848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560882164.366277 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.366837 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.367270 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.475167 47542280053632 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
:::MLL 1560882164.368280 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882164.368752 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882164.369181 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:22:44.475505 47655956366208 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000028-000018.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000019-000012.tfrecord.zz_0_0
W0618 12:22:44.476316 47801616921472 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.476650 47850314011520 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.476176 47542280053632 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfj3jmyxd
I0618 12:22:44.477169 47542280053632 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfj3jmyxd', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3d954d2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.476518 47655956366208 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b580cf00d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:22:44.477566 47542280053632 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:44.477657 47655956366208 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:22:44.478271 47932355351424 estimator.py:1111] Calling model_fn.
W0618 12:22:44.478381 47932355351424 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:44.478365 47408557310848 estimator.py:1111] Calling model_fn.
W0618 12:22:44.478474 47408557310848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:44.479761 47932355351424 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:44.479841 47408557310848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:22:44.481371 47801616921472 estimator.py:1111] Calling model_fn.
W0618 12:22:44.481481 47801616921472 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:22:44.481730 47850314011520 estimator.py:1111] Calling model_fn.
W0618 12:22:44.481846 47850314011520 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:22:44.482161 47542280053632 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.482330 47655956366208 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:22:44.482846 47801616921472 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:44.483226 47850314011520 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:22:44.500367 47502870942592 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.500384 47723738174336 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.501145 47542280053632 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.501453 47655956366208 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:22:44.502535 47247624278912 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.503270 47293743002496 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:22:44.504664 47502870942592 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.504669 47723738174336 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:22:44.506835 47247624278912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is[2019-06-18 12:23:23] divide_golden_chunk finished: 3.253 seconds
[2019-06-18 12:23:23] generate golden chunk: 3.267 seconds
[2019-06-18 12:23:23] moving /lfs/lfs12/gma_akey/results/epb002/models/000029-000020.data-00000-of-00001 --> /lfs/lfs12/gma_akey/results/epb002/models/000029-000021.data-00000-of-00001
[2019-06-18 12:23:23] moving /lfs/lfs12/gma_akey/results/epb002/models/000029-000020.meta --> /lfs/lfs12/gma_akey/results/epb002/models/000029-000021.meta
[2019-06-18 12:23:23] moving /lfs/lfs12/gma_akey/results/epb002/models/000029-000020.index --> /lfs/lfs12/gma_akey/results/epb002/models/000029-000021.index
[2019-06-18 12:23:23] moving /lfs/lfs12/gma_akey/results/epb002/models/000029-000020.pb --> /lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb
[2019-06-18 12:23:23] iteration time 28: 48.910 seconds
2019-06-18 12:23:24.600140: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882203.063661 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 29}}
[2019-06-18 12:23:27] minmax time: 3.276 seconds
2019-06-18 12:23:27.886958: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:23:27.892425: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:23:27.897020: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882207.908887 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 28}}
[2019-06-18 12:23:27] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=31 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:23:27] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-30-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=30 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=1023779861 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=2047559692 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=3071339523 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=4095119354 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=5118899185 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=6142679016 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=7166458847 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=8190238678 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=9214018509 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=10237798340 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=11261578171 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=12285358002 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=13309137833 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=14332917664 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=15356697495 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=16380477326 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=17404257157 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=18428036988 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=19451816819 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000029-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000029-000021 --seed=20475596650 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:23:38] eval finished: 10.497 seconds
[2019-06-18 12:23:38] Win rate 000029-000021 vs 000028-000020: 0.400
:::MLL 1560882218.473350 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 28}}
[2019-06-18 12:23:38] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-31-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=31 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=1023779862 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=2047559693 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=3071339524 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=4095119355 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=5118899186 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=6142679017 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=7166458848 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=8190238679 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=9214018510 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=10237798341 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=11261578172 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=12285358003 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=13309137834 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=14332917665 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=15356697496 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=16380477327 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=17404257158 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000030-000020 --seed=18428036989 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:24:07] selfplay finished: 29.450 seconds
[2019-06-18 12:24:07] selfplay mn: 29.471 seconds
[2019-06-18 12:24:07] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-31-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=31 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779862 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559693 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339524 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119355 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899186 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679017 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458848 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238679 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018510 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798341 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578172 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285358003 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137834 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917665 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697496 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477327 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257158 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036989 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816820 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596651 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376482 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156313 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000030-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:24:11] divide_golden_chunk finished: 3.243 seconds
[2019-06-18 12:24:11] generate golden chunk: 3.258 seconds
[2019-06-18 12:24:11] train finished: 43.566 seconds
:::MLL 1560882213.198841 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.199700 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.200505 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.376259 47642111927168 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.199010 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.199856 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.200608 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.376311 47098998301568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.377407 47642111927168 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpb7dbfhj_
W0618 12:23:33.377441 47098998301568 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpswvpska2
I0618 12:23:33.378502 47642111927168 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpb7dbfhj_', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b54d3beae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.378556 47098998301568 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpswvpska2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad65fa67e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.378949 47642111927168 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.378997 47098998301568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.384338 47642111927168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.384366 47098998301568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.406070 47098998301568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.406156 47642111927168 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882213.243525 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.244271 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.244959 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.412824 47235283452800 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.238482 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.239393 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.240287 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.413336 47774362768256 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.414144 47235283452800 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp7jki9qsx
W0618 12:23:33.414419 47774362768256 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpq555txb9
I0618 12:23:33.415258 47235283452800 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp7jki9qsx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2af61ae0de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.415515 47774362768256 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpq555txb9', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b739e826e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.415714 47235283452800 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.415964 47774362768256 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.421045 47235283452800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.421119 47774362768256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882213.265364 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.265785 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.266238 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.421555 47351238095744 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.266338 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.266823 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.267224 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.423021 47267326964608 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.422564 47351238095744 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpyhzc3vkt
I0618 12:23:33.423554 47351238095744 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpyhzc3vkt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b111a504e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.423951 47351238095744 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.423987 47267326964608 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpexyvarsz
I0618 12:23:33.424971 47267326964608 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpexyvarsz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2afd90d20e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.425369 47267326964608 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882213.270643 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.271393 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.272180 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.427235 47651003995008 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.283996 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.284926 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.285755 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.428189 47920561279872 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.288406 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.289153 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.289830 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.428368 47755254264704 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.428610 47351238095744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.428381 47651003995008 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmps5p4oarg
I0618 12:23:33.429492 47651003995008 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmps5p4oarg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b56e5c0ee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.429958 47651003995008 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.429916 47267326964608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.429328 47920561279872 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp9vq_ab96
W0618 12:23:33.429456 47755254264704 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppo2vug7e
I0618 12:23:33.430462 47920561279872 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp9vq_ab96', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b95a89eae48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.430619 47755254264704 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppo2vug7e', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6f2b8dce48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.430922 47920561279872 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.431068 47755254264704 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.435321 47651003995008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.436243 47920561279872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.436291 47755254264704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.442737 47235283452800 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.442828 47774362768256 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882213.272578 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.273301 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.273989 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.445040 47098608046976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.446131 47098608046976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4j98qat8
I0618 12:23:33.447232 47098608046976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4j98qat8', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad64863ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:23:33.447532 47351238095744 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:23:33.447695 47098608046976 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.449009 47267326964608 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.452761 47098608046976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.454084 47642111927168 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.454174 47098998301568 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.457076 47651003995008 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.458060 47920561279872 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.458073 47755254264704 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.458390 47642111927168 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.458524 47098998301568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
:::MLL 1560882213.303284 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.303701 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.304037 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.459171 47453337060224 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.301019 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.301429 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.301791 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.459164 47156483380096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.460240 47453337060224 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphssrpe4t
W0618 12:23:33.460271 47156483380096 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpe5dnob68
I0618 12:23:33.461243 47453337060224 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphssrpe4t', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b28dfe2de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.461250 47156483380096 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpe5dnob68', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae3c2072e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.461647 47453337060224 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.461646 47156483380096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.463504 47642111927168 estimator.py:1111] Calling model_fn.
W0618 12:23:33.463613 47642111927168 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:23:33.463648 47098998301568 estimator.py:1111] Calling model_fn.
W0618 12:23:33.463758 47098998301568 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.464983 47642111927168 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.465136 47098998301568 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.466318 47453337060224 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.466318 47156483380096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.474386 47098608046976 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882213.282410 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.283159 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.283870 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.477211 47946366280576 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.275039 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.275987 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.276858 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.477278 47743063860096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.478348 47946366280576 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0ce9l77i
W0618 12:23:33.478385 47743063860096 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpixzha4fj
I0618 12:23:33.479365 47946366280576 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0ce9l77i', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9baab7be48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.479389 47743063860096 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpixzha4fj', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b6c54f2fe48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.479768 47946366280576 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.479789 47743063860096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.484704 47946366280576 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.484727 47743063860096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.485248 47156483380096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.485357 47453337060224 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882213.343010 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.343474 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.343843 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.489506 47095551021952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.345795 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.346210 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.346577 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.489539 47888779346816 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.374529 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.374936 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.375301 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.489925 47093136634752 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.373270 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.373679 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.374040 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.490081 46979726250880 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.490592 47235283452800 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.490732 47774362768256 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.490556 47888779346816 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0zp82h13
W0618 12:23:33.490529 47095551021952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphjgvoyns
I0618 12:23:33.491499 47095551021952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphjgvoyns', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad5922d2e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.491539 47888779346816 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0zp82h13', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b8e4244de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:23:33.490972 47093136634752 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpnw5oe_9c
W0618 12:23:33.491080 46979726250880 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpon7exrlx
I0618 12:23:33.491897 47095551021952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.491943 47888779346816 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.491953 47093136634752 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpnw5oe_9c', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad502448e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.492064 46979726250880 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpon7exrlx', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aba9a7b6dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.492347 47093136634752 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.492464 46979726250880 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.494642 47351238095744 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.494951 47235283452800 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.495056 47774362768256 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.495993 47267326964608 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
:::MLL 1560882213.314933 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.315826 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.316687 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.496322 47147657110400 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.496541 47888779346816 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.496531 47095551021952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882213.323898 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.324675 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.325382 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.496493 47630561014656 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
W0618 12:23:33.497003 47093136634752 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.497119 46979726250880 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.497330 47147657110400 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpa2t2w3b6
W0618 12:23:33.497494 47630561014656 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmphchjuqle
I0618 12:23:33.498322 47147657110400 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpa2t2w3b6', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae1b3f0edd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.498484 47630561014656 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmphchjuqle', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b5223419e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.498728 47147657110400 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:23:33.498892 47630561014656 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.498920 47351238095744 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:23:33.500091 47235283452800 estimator.py:1111] Calling model_fn.
I0618 12:23:33.500179 47774362768256 estimator.py:1111] Calling model_fn.
W0618 12:23:33.500200 47235283452800 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.500290 47774362768256 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.500293 47267326964608 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.501561 47235283452800 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.501656 47774362768256 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.503495 47147657110400 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:23:33.503630 47630561014656 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:23:33.503987 47351238095744 estimator.py:1111] Calling model_fn.
W0618 12:23:33.504101 47351238095744 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.504192 47946366280576 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.504504 47743063860096 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:23:33.505381 47267326964608 estimator.py:1111] Calling model_fn.
W0618 12:23:33.505470 47351238095744 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.505488 47267326964608 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.506016 47651003995008 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.506855 47267326964608 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.507640 47920561279872 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.507759 47755254264704 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.510299 47651003995008 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.511961 47920561279872 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:23:33.512089 47755254264704 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:23:33.515377 47651003995008 estimator.py:1111] Calling model_fn.
W0618 12:23:33.515486 47651003995008 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.515497 47888779346816 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.515595 47095551021952 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.516062 47093136634752 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.516207 46979726250880 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.516870 47651003995008 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0618 12:23:33.517087 47920561279872 estimator.py:1111] Calling model_fn.
W0618 12:23:33.517194 47920561279872 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:23:33.517214 47755254264704 estimator.py:1111] Calling model_fn.
W0618 12:23:33.517324 47755254264704 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:23:33.518570 47920561279872 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.518702 47755254264704 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:23:33.521513 47098608046976 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:23:33.522595 47147657110400 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:23:33.523077 47630561014656 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882213.352849 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.353421 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.353828 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.523551 46966027617152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
:::MLL 1560882213.355047 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882213.355507 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882213.355906 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:23:33.523690 47454891627392 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000029-000019.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000020-000012.tfrecord.zz_0_0
I0618 12:23:33.524600 46966027617152 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab769fadd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:23:33.524682 47454891627392 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpfsuygtvk
I0618 12:23:33.525645 47454891627392 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpfsuygtvk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b293c8b8e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:23:33.525719 46966027617152 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:23:33.525792 47098608046976 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can [2019-06-18 12:24:11] iteration time 29: 48.433 seconds
2019-06-18 12:24:13.059530: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying insert_logging
['/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9']
Reading tf_records from 1 inputs
:::MLL 1560882251.496871 epoch_start: {"value": null, "metadata": {'lineno': 648, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 30}}
[2019-06-18 12:24:16] minmax time: 3.239 seconds
2019-06-18 12:24:16.309103: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying freeze_requantization_ranges
2019-06-18 12:24:16.314715: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fuse_quantized_conv_and_requantize
2019-06-18 12:24:16.319437: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
:::MLL 1560882256.332805 save_model: {"value": null, "metadata": {'lineno': 483, 'file': 'ml_perf/reference_implementation.py', 'iteration': 29}}
[2019-06-18 12:24:16] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-train-None-%r.txt -genv HOROVOD_FUSION_THRESHOLD=134217728 -genv KMP_BLOCKTIME=0 -genv KMP_HW_SUBSET=1T -genv OMP_BIND_PROC=true -genv I_MPI_ASYNC_PROGRESS_PIN=0,1,24,25 -genv OMP_NUM_THREADS=11 \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb039 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb047 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,2 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,13 numactl -l -N 0 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,26 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb002/work_dir --export_path=/lfs/lfs12/gma_akey/results/epb002/models/000031-000021 --window_size=3120 --data_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks --training_seed=32 --freeze=True --num_inter_threads=1 --num_intra_threads=11 --dist_train=True : \
-host epb046 -env KMP_AFFINITY=granularity=fine,compact,1,37 numactl -l -N 1 python3 train.py --conv_width=32 --fc_width=64 --trunk_layers=9 --value_cost_weight=0.25 --summary_steps=64 --shuffle_buffer_size=10000 --filter_amount=0.5 --train_batch_size=8192 --lr_rates=0.32 --lr_rates=0.032 --lr_rates=0.0032 --lr_boundaries=12500 --lr_boundaries=18750 --l2_strength=0.0001 --work_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:24:16] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-eval-31-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=31 : \
-host epb001 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=1023779862 : \
-host epb054 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=2047559693 : \
-host epb053 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=3071339524 : \
-host epb052 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=4095119355 : \
-host epb051 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=5118899186 : \
-host epb050 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=6142679017 : \
-host epb049 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=7166458848 : \
-host epb080 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=8190238679 : \
-host epb081 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=9214018510 : \
-host epb082 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=10237798341 : \
-host epb084 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=11261578172 : \
-host epb085 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=12285358003 : \
-host epb086 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=13309137834 : \
-host epb087 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=14332917665 : \
-host epb088 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=15356697496 : \
-host epb089 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=16380477327 : \
-host epb030 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=17404257158 : \
-host epb031 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=18428036989 : \
-host epb032 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=19451816820 : \
-host epb033 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000030-000021.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/000030-000021 --seed=20475596651 : \
-host epb034 python3 ml_perf/execute.py --num_instance=4 -- bazel-bin/cc/eval --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/eval.flags --model=tf,/lfs/lfs12/gma_akey/resu
[2019-06-18 12:24:26] eval finished: 10.312 seconds
[2019-06-18 12:24:26] Win rate 000030-000021 vs 000028-000020: 0.420
:::MLL 1560882266.709666 epoch_stop: {"value": null, "metadata": {'lineno': 705, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 29}}
[2019-06-18 12:24:26] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-selfplay-32-%r.txt -genv LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow \
-host epb002 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=32 : \
-host epb001 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=1023779863 : \
-host epb054 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=2047559694 : \
-host epb053 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=3071339525 : \
-host epb052 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=4095119356 : \
-host epb051 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=5118899187 : \
-host epb050 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=6142679018 : \
-host epb049 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=7166458849 : \
-host epb080 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=8190238680 : \
-host epb081 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=9214018511 : \
-host epb082 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=10237798342 : \
-host epb084 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=11261578173 : \
-host epb085 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=12285358004 : \
-host epb086 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=13309137835 : \
-host epb087 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=14332917666 : \
-host epb088 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=15356697497 : \
-host epb089 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=16380477328 : \
-host epb030 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=17404257159 : \
-host epb031 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb002/data/holdout/000031-000020 --seed=18428036990 : \
-host epb032 python3 ml_perf/execute.py --num_instance=79 -- bazel-bin/cc/selfplay --flagfile=/lfs/lfs12/gma_akey/results/epb002/flags/selfplay.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000028-000020.pb --output_dir=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020 --holdout_dir=/lfs/lfs12/gma_akey/results/epb00
[2019-06-18 12:24:55] selfplay finished: 28.822 seconds
[2019-06-18 12:24:55] selfplay mn: 28.841 seconds
[2019-06-18 12:24:55] Running: mpiexec -outfile-pattern /lfs/lfs12/gma_akey/results/epb002/mpi/out-divide_golden_chunk-32-%r.txt \
-host epb002 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=32 : \
-host epb001 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=1023779863 : \
-host epb054 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=2047559694 : \
-host epb053 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=3071339525 : \
-host epb052 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=4095119356 : \
-host epb051 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=5118899187 : \
-host epb050 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=6142679018 : \
-host epb049 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=7166458849 : \
-host epb080 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=8190238680 : \
-host epb081 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=9214018511 : \
-host epb082 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=10237798342 : \
-host epb084 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=11261578173 : \
-host epb085 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=12285358004 : \
-host epb086 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=13309137835 : \
-host epb087 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=14332917666 : \
-host epb088 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=15356697497 : \
-host epb089 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=16380477328 : \
-host epb030 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=17404257159 : \
-host epb031 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=18428036990 : \
-host epb032 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=19451816821 : \
-host epb033 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=20475596652 : \
-host epb034 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=21499376483 : \
-host epb035 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000031-000020.tfrecord.zz --out_files_number=12 --physical_cores=48 --base_dir=/lfs/lfs12/gma_akey/results/epb002 --seed=22523156314 : \
-host epb036 python3 ml_perf/divide_golden_chunk.py --read_path=/tmp/minigo/lfs/lfs12/gma_akey/results/epb002/data/selfplay/000031-000020/* --write_path=/lfs/lfs12/gma_akey/results/epb002/data/golde
[2019-06-18 12:24:58] divide_golden_chunk finished: 3.294 seconds
[2019-06-18 12:24:58] generate golden chunk: 3.309 seconds
[2019-06-18 12:24:59] train finished: 43.282 seconds
:::MLL 1560882261.594956 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.595865 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.596694 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.740923 47092935955328 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.742034 47092935955328 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6naq27c2
I0618 12:24:21.743100 47092935955328 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6naq27c2', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ad4f64e6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.743556 47092935955328 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.748934 47092935955328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882261.611261 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.611979 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.612684 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.752654 47780572414848 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.753771 47780572414848 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpn8nj6itt
I0618 12:24:21.754875 47780572414848 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpn8nj6itt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7510a20e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.755317 47780572414848 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.760521 47780572414848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.770555 47092935955328 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.781779 47780572414848 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
:::MLL 1560882261.638269 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.639159 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.639877 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.809104 47761588503424 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882261.678086 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.678571 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.678993 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.809178 47037745050496 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882261.637308 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.638175 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.639014 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.809259 46916733227904 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882261.678127 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.678602 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.679033 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.810621 47957745034112 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.810153 47761588503424 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmponr66j91
W0618 12:24:21.810206 47037745050496 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp66_vged0
W0618 12:24:21.810255 46916733227904 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpg_ftjly7
I0618 12:24:21.811173 47761588503424 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmponr66j91', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b70a51a9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.811261 46916733227904 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpg_ftjly7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2aabefce2e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.811220 47037745050496 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp66_vged0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ac81cac0e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.811583 47761588503424 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:21.811657 46916733227904 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:21.811626 47037745050496 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.811577 47957745034112 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp2svzw0qh
I0618 12:24:21.812565 47957745034112 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp2svzw0qh', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9e50f1be10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.812966 47957745034112 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.816291 47037745050496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.816392 47761588503424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.816369 46916733227904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.817505 47957745034112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.819842 47092935955328 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:21.824189 47092935955328 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:24:21.829309 47092935955328 estimator.py:1111] Calling model_fn.
W0618 12:24:21.829420 47092935955328 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:21.829983 47780572414848 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:21.830791 47092935955328 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882261.600350 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.601236 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.602058 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.831202 47547105915776 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.832282 47547105915776 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpu_u26x86
I0618 12:24:21.833377 47547105915776 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpu_u26x86', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b3eb4f1fe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.833919 47547105915776 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.834334 47780572414848 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:21.835263 47037745050496 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.835378 46916733227904 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.835429 47761588503424 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.836606 47957745034112 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.839190 47547105915776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:24:21.839444 47780572414848 estimator.py:1111] Calling model_fn.
W0618 12:24:21.839555 47780572414848 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:21.840912 47780572414848 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
:::MLL 1560882261.612692 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.613588 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.614360 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.846107 47513413387136 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.847257 47513413387136 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0s6_lwsr
I0618 12:24:21.848395 47513413387136 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0s6_lwsr', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b36dcb6de10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.848881 47513413387136 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882261.600296 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.601172 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.601992 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.849170 47326134125440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.850260 47326134125440 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpa562zbzt
I0618 12:24:21.851341 47326134125440 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpa562zbzt', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0b42001e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.851734 47326134125440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882261.683723 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.684482 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.685133 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.853969 46945524007808 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.854287 47513413387136 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882261.681767 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.682574 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.683326 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.854302 47554821718912 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.855123 46945524007808 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp4ds05jl0
W0618 12:24:21.855425 47554821718912 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0n1vw5au
I0618 12:24:21.856256 46945524007808 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp4ds05jl0', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ab2a3de9e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:24:21.856476 47326134125440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:24:21.856530 47554821718912 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0n1vw5au', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4080d7cdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.856716 46945524007808 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:21.856985 47554821718912 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.859664 47547105915776 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.862052 46945524007808 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.862255 47554821718912 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882261.726558 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.727024 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.727460 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.866708 47957709341568 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882261.724679 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.725142 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.725535 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.867093 47334148621184 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
I0618 12:24:21.867732 47957709341568 estimator.py:201] Using config: {'_model_dir': '/lfs/lfs12/gma_akey/results/epb002/work_dir', '_tf_random_seed': None, '_save_summary_steps': 64, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9e4ed11d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:24:21.868114 47334148621184 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmppprs9dfk
I0618 12:24:21.868858 47957709341568 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:21.869097 47334148621184 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmppprs9dfk', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b0d1fb39e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.869492 47334148621184 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882261.652317 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.652775 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.653154 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.873083 47806393197440 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.873549 47957709341568 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882261.653861 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.654265 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.654621 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.873474 46981101097856 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.874068 47334148621184 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.874103 47806393197440 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpu3yln5pb
I0618 12:24:21.875111 47806393197440 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpu3yln5pb', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b7b13abfe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:24:21.874456 46981101097856 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpco391w7g
:::MLL 1560882261.613373 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.614230 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.614908 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.874996 47622053385088 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
I0618 12:24:21.875450 46981101097856 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpco391w7g', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2abaec6dce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0618 12:24:21.875509 47513413387136 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:24:21.875508 47806393197440 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.875612 47326134125440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:24:21.875848 46981101097856 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.876014 47622053385088 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0wbytuxg
I0618 12:24:21.876994 47622053385088 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0wbytuxg', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b502829ae10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.877402 47622053385088 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.880177 47806393197440 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.880454 46981101097856 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.882079 47622053385088 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.882624 47037745050496 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:21.882955 46945524007808 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.883155 47761588503424 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:21.883232 47554821718912 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.883257 46916733227904 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:21.883963 47957745034112 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:21.886938 47037745050496 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:21.887458 47761588503424 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:21.887602 46916733227904 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:21.888325 47957745034112 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
I0618 12:24:21.892018 47037745050496 estimator.py:1111] Calling model_fn.
W0618 12:24:21.892130 47037745050496 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:21.892502 47957709341568 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:24:21.892536 47761588503424 estimator.py:1111] Calling model_fn.
W0618 12:24:21.892643 47761588503424 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0618 12:24:21.892700 46916733227904 estimator.py:1111] Calling model_fn.
W0618 12:24:21.892805 46916733227904 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0618 12:24:21.892988 47334148621184 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
I0618 12:24:21.893399 47957745034112 estimator.py:1111] Calling model_fn.
W0618 12:24:21.893475 47037745050496 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:21.893510 47957745034112 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560882261.673232 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.673641 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.674005 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.893665 47999578743680 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.894026 47761588503424 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:21.894170 46916733227904 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:21.894862 47957745034112 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:21.894671 47999578743680 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpjgk2_9by
I0618 12:24:21.895669 47999578743680 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpjgk2_9by', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba80e6d6e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.896065 47999578743680 train.py:201] Training, steps = ?, batch = 341 -> ? examples
:::MLL 1560882261.674890 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.675311 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.675680 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.896350 47557906989952 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.897357 47557906989952 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmpmwy2xoo7
I0618 12:24:21.898496 47557906989952 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmpmwy2xoo7', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b4138bd5e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.898961 47557906989952 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.899358 46981101097856 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.899320 47806393197440 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.900931 47999578743680 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.901421 47622053385088 deprecation.py:323] From ./preprocessing.py:144: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0618 12:24:21.903551 47557906989952 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
:::MLL 1560882261.748603 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.749073 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.749791 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.904662 47986536756096 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882261.748621 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.749103 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.749828 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.904732 47430846514048 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.905675 47986536756096 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp0quqdfmz
W0618 12:24:21.906450 47547105915776 deprecation.py:323] From ./preprocessing.py:177: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0618 12:24:21.905738 47430846514048 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp60jh_qjz
I0618 12:24:21.906693 47986536756096 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp0quqdfmz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba505108e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.906736 47430846514048 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp60jh_qjz', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b23a3585e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.907107 47986536756096 train.py:201] Training, steps = ?, batch = 341 -> ? examples
I0618 12:24:21.907142 47430846514048 train.py:201] Training, steps = ?, batch = 341 -> ? examples
W0618 12:24:21.910714 47547105915776 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
W0618 12:24:21.911804 47986536756096 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0618 12:24:21.911826 47430846514048 deprecation.py:323] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0618 12:24:21.915745 47547105915776 estimator.py:1111] Calling model_fn.
W0618 12:24:21.915853 47547105915776 deprecation.py:323] From /global/panfs01/users/gma/submission/benchmarks/minigo/implementations/tensorflow/dual_net.py:489: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
:::MLL 1560882261.712679 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.713436 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.714129 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.916723 47959265121152 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
:::MLL 1560882261.704791 global_batch_size: {"value": 8192, "metadata": {'lineno': 233, 'file': 'train.py'}}
:::MLL 1560882261.705744 opt_base_learning_rate: {"value": [0.32, 0.032, 0.0032], "metadata": {'lineno': 234, 'file': 'train.py'}}
:::MLL 1560882261.706631 opt_learning_rate_decay_boundary_steps: {"value": [12500, 18750], "metadata": {'lineno': 235, 'file': 'train.py'}}
I0618 12:24:21.916849 47766989230976 train.py:238] Training on 3120 records: /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000030-000020.tfrecord.zz_9_9 to /lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/000021-000012.tfrecord.zz_0_0
W0618 12:24:21.917209 47547105915776 deprecation.py:506] From /home/gma/anaconda3/envs/minigo-dev/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0618 12:24:21.917747 47959265121152 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmprc726lxc
W0618 12:24:21.917839 47766989230976 estimator.py:1760] Using temporary folder as model directory: /tmp/96736.tmpdir/tmp6ohicg4z
I0618 12:24:21.918748 47959265121152 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmprc726lxc', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2b9eab8c5dd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0618 12:24:21.918803 47766989230976 estimator.py:201] Using config: {'_model_dir': '/tmp/96736.tmpdir/tmp6ohicg4z', '_tf_random_seed': None, '_save_summary_steps': 1000000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 11
inter_op_parallelism_threads: 1
gpu_options {
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000000, '_train_distribute': None, '_device_fn': None, '_protocol': None[2019-06-18 12:24:59] iteration time 30: 48.139 seconds
:::MLL 1560882299.636053 epoch_stop: {"value": null, "metadata": {'lineno': 737, 'file': 'ml_perf/reference_implementation.py', 'epoch_num': 30}}
[2019-06-18 12:24:59] Total time: 1716.261 seconds

numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000018-000013_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000018-000013log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000019-000013_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000019-000013log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000020-000013_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000020-000013log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000021-000014_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000021-000014log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000022-000015_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000022-000015log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000023-000016_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000023-000016log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000024-000017_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000024-000017log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000025-000018_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000025-000018log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000026-000019_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000026-000019log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000027-000019_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000027-000019log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000028-000020_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000028-000020log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000029-000021_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000029-000021log.txt
numactl -N 0 -l python3 produce_min_max_log.py --input_graph=/lfs/lfs12/gma_akey/results/epb002/models/000030-000021_for_min_max.pb --data_location=/lfs/lfs12/gma_akey/results/epb002/data/golden_chunks/*.zz* --num_steps=5 --batch_size=16 --random_rotation=True 2> /lfs/lfs12/gma_akey/results/epb002/models/000030-000021log.txt
:::MLL 1560882302.260999 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
I0618 12:25:02.261753 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000001-000001.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=1
I0618 12:25:28.710348 47083878216576 utils.py:86] eval finished: 26.448 seconds
I0618 12:25:28.713677 47083878216576 reference_implementation.py:563] Win rate 000001-000001 vs target: 0.050
:::MLL 1560882328.714756 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
:::MLL 1560882328.715101 eval_accuracy: {"value": 0.05, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 0}}
:::MLL 1560882328.715428 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
I0618 12:25:28.715743 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000002-000002.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=2
I0618 12:25:55.473977 47083878216576 utils.py:86] eval finished: 26.758 seconds
I0618 12:25:55.476772 47083878216576 reference_implementation.py:563] Win rate 000002-000002 vs target: 0.080
:::MLL 1560882355.477455 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
:::MLL 1560882355.477786 eval_accuracy: {"value": 0.08, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 1}}
:::MLL 1560882355.478124 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
I0618 12:25:55.478433 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000003-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=3
I0618 12:26:21.425805 47083878216576 utils.py:86] eval finished: 25.947 seconds
I0618 12:26:21.428655 47083878216576 reference_implementation.py:563] Win rate 000003-000003 vs target: 0.060
:::MLL 1560882381.429347 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
:::MLL 1560882381.429682 eval_accuracy: {"value": 0.06, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 2}}
:::MLL 1560882381.430012 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
I0618 12:26:21.430330 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000004-000003.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=4
I0618 12:26:46.758093 47083878216576 utils.py:86] eval finished: 25.328 seconds
I0618 12:26:46.760871 47083878216576 reference_implementation.py:563] Win rate 000004-000003 vs target: 0.120
:::MLL 1560882406.761795 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
:::MLL 1560882406.762124 eval_accuracy: {"value": 0.12, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 3}}
:::MLL 1560882406.762448 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
I0618 12:26:46.762755 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000005-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=5
I0618 12:27:13.989218 47083878216576 utils.py:86] eval finished: 27.226 seconds
I0618 12:27:13.992070 47083878216576 reference_implementation.py:563] Win rate 000005-000004 vs target: 0.090
:::MLL 1560882433.992732 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
:::MLL 1560882433.993058 eval_accuracy: {"value": 0.09, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 4}}
:::MLL 1560882433.993387 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
I0618 12:27:13.993673 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000006-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=6
I0618 12:27:38.645064 47083878216576 utils.py:86] eval finished: 24.651 seconds
I0618 12:27:38.647933 47083878216576 reference_implementation.py:563] Win rate 000006-000004 vs target: 0.160
:::MLL 1560882458.648932 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
:::MLL 1560882458.649251 eval_accuracy: {"value": 0.16, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 5}}
:::MLL 1560882458.649561 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
I0618 12:27:38.649853 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000007-000004.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=7
I0618 12:28:07.942385 47083878216576 utils.py:86] eval finished: 29.292 seconds
I0618 12:28:07.945232 47083878216576 reference_implementation.py:563] Win rate 000007-000004 vs target: 0.030
:::MLL 1560882487.946138 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
:::MLL 1560882487.946454 eval_accuracy: {"value": 0.03, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 6}}
:::MLL 1560882487.946756 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
I0618 12:28:07.947052 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000008-000005.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=8
I0618 12:28:32.025946 47083878216576 utils.py:86] eval finished: 24.079 seconds
I0618 12:28:32.028784 47083878216576 reference_implementation.py:563] Win rate 000008-000005 vs target: 0.210
:::MLL 1560882512.029437 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
:::MLL 1560882512.029742 eval_accuracy: {"value": 0.21, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 7}}
:::MLL 1560882512.030039 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
I0618 12:28:32.030337 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000009-000006.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=9
I0618 12:28:57.457871 47083878216576 utils.py:86] eval finished: 25.427 seconds
I0618 12:28:57.460690 47083878216576 reference_implementation.py:563] Win rate 000009-000006 vs target: 0.090
:::MLL 1560882537.461389 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
:::MLL 1560882537.461709 eval_accuracy: {"value": 0.09, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 8}}
:::MLL 1560882537.462039 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
I0618 12:28:57.462347 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000010-000007.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=10
I0618 12:29:21.340547 47083878216576 utils.py:86] eval finished: 23.878 seconds
I0618 12:29:21.343494 47083878216576 reference_implementation.py:563] Win rate 000010-000007 vs target: 0.360
:::MLL 1560882561.344404 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
:::MLL 1560882561.344714 eval_accuracy: {"value": 0.36, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 9}}
:::MLL 1560882561.345015 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
I0618 12:29:21.345320 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000011-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=11
I0618 12:29:46.971574 47083878216576 utils.py:86] eval finished: 25.626 seconds
I0618 12:29:46.974396 47083878216576 reference_implementation.py:563] Win rate 000011-000008 vs target: 0.210
:::MLL 1560882586.975313 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
:::MLL 1560882586.975626 eval_accuracy: {"value": 0.21, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 10}}
:::MLL 1560882586.975928 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
I0618 12:29:46.976222 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000012-000008.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=12
I0618 12:30:10.040870 47083878216576 utils.py:86] eval finished: 23.064 seconds
I0618 12:30:10.043684 47083878216576 reference_implementation.py:563] Win rate 000012-000008 vs target: 0.290
:::MLL 1560882610.044385 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
:::MLL 1560882610.044709 eval_accuracy: {"value": 0.29, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 11}}
:::MLL 1560882610.045048 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
I0618 12:30:10.045383 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000013-000009.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=13
I0618 12:30:33.542570 47083878216576 utils.py:86] eval finished: 23.497 seconds
I0618 12:30:33.545371 47083878216576 reference_implementation.py:563] Win rate 000013-000009 vs target: 0.310
:::MLL 1560882633.546207 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
:::MLL 1560882633.546522 eval_accuracy: {"value": 0.31, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 12}}
:::MLL 1560882633.546821 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
I0618 12:30:33.547120 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000014-000010.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=14
I0618 12:30:59.149145 47083878216576 utils.py:86] eval finished: 25.602 seconds
I0618 12:30:59.151958 47083878216576 reference_implementation.py:563] Win rate 000014-000010 vs target: 0.270
:::MLL 1560882659.152610 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
:::MLL 1560882659.152920 eval_accuracy: {"value": 0.27, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 13}}
:::MLL 1560882659.153221 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
I0618 12:30:59.153522 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000015-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=15
I0618 12:31:23.368573 47083878216576 utils.py:86] eval finished: 24.215 seconds
I0618 12:31:23.371489 47083878216576 reference_implementation.py:563] Win rate 000015-000011 vs target: 0.290
:::MLL 1560882683.372167 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
:::MLL 1560882683.372506 eval_accuracy: {"value": 0.29, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 14}}
:::MLL 1560882683.372809 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
I0618 12:31:23.373102 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000016-000011.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=16
I0618 12:31:45.824830 47083878216576 utils.py:86] eval finished: 22.452 seconds
I0618 12:31:45.827676 47083878216576 reference_implementation.py:563] Win rate 000016-000011 vs target: 0.370
:::MLL 1560882705.828619 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
:::MLL 1560882705.828953 eval_accuracy: {"value": 0.37, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 15}}
:::MLL 1560882705.829285 eval_start: {"value": null, "metadata": {'lineno': 46, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
I0618 12:31:45.829596 47083878216576 utils.py:81] Running: python3 ml_perf/execute.py --num_instance=100 -- bazel-bin/cc/eval --flagfile=ml_perf/flags/9.mn/eval.flags --model=tf,/lfs/lfs12/gma_akey/results/epb002/models/000017-000012.pb --model_two=tf,/lfs/lfs12/gma_akey/results/epb002/models/target.pb --sgf_dir=/lfs/lfs12/gma_akey/results/epb002/sgf/eval/target --seed=17
I0618 12:32:06.781340 47083878216576 utils.py:86] eval finished: 20.952 seconds
I0618 12:32:06.784119 47083878216576 reference_implementation.py:563] Win rate 000017-000012 vs target: 0.570
:::MLL 1560882726.784768 eval_stop: {"value": null, "metadata": {'lineno': 48, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
:::MLL 1560882726.785077 eval_accuracy: {"value": 0.57, "metadata": {'lineno': 49, 'file': 'ml_perf/eval_models.py', 'epoch_num': 16}}
:::MLL 1560882726.785391 eval_result: {"value": null, "metadata": {'lineno': 52, 'file': 'ml_perf/eval_models.py', 'iteration': 16, 'timestamp': 860.346}}
:::MLL 1560882726.785690 run_stop: {"value": null, "metadata": {'lineno': 53, 'file': 'ml_perf/eval_models.py', 'status': 'success'}}
Model 000017-000012 beat target after 860.346s
~/submission/benchmarks/minigo/clx-8260l-2s-x32
