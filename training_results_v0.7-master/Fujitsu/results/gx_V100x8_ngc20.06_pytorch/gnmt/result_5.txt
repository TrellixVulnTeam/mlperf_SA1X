Beginning trial 1 of 1
Gathering sys log on gx2570-03
:::MLLOG {"namespace": "", "time_ms": 1594174477777, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1594174477812, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Fujitsu", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1594174477812, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1594174477812, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1594174477812, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xGX2570M5", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
Clearing caches
:::MLLOG {"namespace": "", "time_ms": 1594174482249, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/opt/conda/lib/python3.6/site-packages/mlperf_logging/mllog/mllog.py", "lineno": 261}}
Launching on node gx2570-03
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e PGSYSTEM=PG -e 'MULTI_NODE= --master_port=4442' -e LR=2.8e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6053 -e DECAY_INTERVAL=859 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=65 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200708111320210039781 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200708111320210039781 ./run_and_time.sh
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.8e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-07-08 02:14:43 AM
+ REMAIN_STEPS=6053
+ DECAY_INTERVAL=859
+ TARGET=24.0
+ MAX_SEQ_LEN=65
+ NUMEPOCHS=8
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ PGSYSTEM=PG
+ [[ -f config_PG.sh ]]
+ source config_PG.sh
++ export PGNNODES=1
++ PGNNODES=1
+++ sed 's/^config_//'
+++ sed 's/\.sh$//'
++++ readlink -f config_PG.sh
+++ basename /workspace/rnn_translator/config_PG.sh
++ export PGSYSTEM=PG
++ PGSYSTEM=PG
++ export WALLTIME=00:30:00
++ WALLTIME=00:30:00
++ export LR=2.8e-3
++ LR=2.8e-3
++ export TRAIN_BATCH_SIZE=256
++ TRAIN_BATCH_SIZE=256
++ export TEST_BATCH_SIZE=128
++ TEST_BATCH_SIZE=128
++ export WARMUP_STEPS=200
++ WARMUP_STEPS=200
++ export REMAIN_STEPS=6053
++ REMAIN_STEPS=6053
++ export DECAY_INTERVAL=859
++ DECAY_INTERVAL=859
++ export TARGET=24.0
++ TARGET=24.0
++ export MAX_SEQ_LEN=65
++ MAX_SEQ_LEN=65
++ export NUMEPOCHS=8
++ NUMEPOCHS=8
++ export MATH=fp16
++ MATH=fp16
++ export DIST_OPTS=
++ DIST_OPTS=
++ export 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ export PGNGPU=8
++ PGNGPU=8
++ export PGSOCKETCORES=24
++ PGSOCKETCORES=24
++ export PGHT=2
++ PGHT=2
++ export PGNSOCKET=2
++ PGNSOCKET=2
+ declare -a CMD
+ '[' -n '' ']'
+ CMD=('python' '-u' '-m' 'bind_launch' "--nsockets_per_node=${PGNSOCKET}" "--ncores_per_socket=${PGSOCKETCORES}" "--nproc_per_node=${PGNGPU}")
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ PG == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ python -u -m bind_launch --nsockets_per_node=2 --ncores_per_socket=24 --nproc_per_node=8 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 65 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.8e-3 --warmup-steps 200 --remain-steps 6053 --decay-interval 859 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1594174485257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1594174485257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1594174485257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1594174485257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1594174485257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1594174485269, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1594174485271, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1594174485271, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=859, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.0028, math='fp16', max_length_test=150, max_length_train=65, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6053, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1803093307
:::MLLOG {"namespace": "", "time_ms": 1594174494082, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1803093307, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 148710819
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.0028}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.0028
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1594174496969, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1594174496971, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0028, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1594174496971, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1594174496971, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1594174496971, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1594174499508, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1594174499512, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1594174499513, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 65, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3866375 min length: 0 max length: 65 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1594174499773, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1594174499774, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3860480, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1594174499774, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6053, 'decay_interval': 859, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6053
0: Scheduler decay interval: 859
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1594174499775, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1594174499775, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1594174499775, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 859, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1594174499776, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1594174499776, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1594174499776, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6053, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1594174499776, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1594174499776, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1594174499776, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1593250919
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1885]	Time 0.393 (0.393)	Data 2.81e-01 (2.81e-01)	Tok/s 22807 (22807)	Loss/tok 10.6237 (10.6237)	LR 2.865e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/1885]	Time 0.100 (0.148)	Data 2.17e-04 (2.58e-02)	Tok/s 90085 (88096)	Loss/tok 9.4435 (9.9893)	LR 3.607e-05
0: TRAIN [0][20/1885]	Time 0.099 (0.134)	Data 2.13e-04 (1.36e-02)	Tok/s 92008 (90696)	Loss/tok 8.9573 (9.6462)	LR 4.541e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][30/1885]	Time 0.240 (0.133)	Data 2.17e-04 (9.28e-03)	Tok/s 108372 (92588)	Loss/tok 9.5787 (9.4592)	LR 5.460e-05
0: TRAIN [0][40/1885]	Time 0.241 (0.137)	Data 2.14e-04 (7.07e-03)	Tok/s 109281 (94430)	Loss/tok 8.8760 (9.2982)	LR 6.873e-05
0: TRAIN [0][50/1885]	Time 0.190 (0.143)	Data 2.03e-04 (5.73e-03)	Tok/s 107166 (96109)	Loss/tok 8.5644 (9.1334)	LR 8.653e-05
0: TRAIN [0][60/1885]	Time 0.145 (0.146)	Data 2.18e-04 (4.82e-03)	Tok/s 99998 (97281)	Loss/tok 8.2257 (8.9792)	LR 1.089e-04
0: TRAIN [0][70/1885]	Time 0.100 (0.152)	Data 2.15e-04 (4.17e-03)	Tok/s 89792 (98212)	Loss/tok 8.0276 (8.8459)	LR 1.371e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][80/1885]	Time 0.100 (0.148)	Data 2.25e-04 (3.69e-03)	Tok/s 90463 (97682)	Loss/tok 7.9257 (8.8041)	LR 1.687e-04
0: TRAIN [0][90/1885]	Time 0.144 (0.145)	Data 2.14e-04 (3.30e-03)	Tok/s 102055 (97458)	Loss/tok 7.9381 (8.7292)	LR 2.124e-04
0: TRAIN [0][100/1885]	Time 0.100 (0.145)	Data 2.13e-04 (3.00e-03)	Tok/s 90295 (97436)	Loss/tok 7.7481 (8.6567)	LR 2.674e-04
0: TRAIN [0][110/1885]	Time 0.101 (0.144)	Data 2.23e-04 (2.75e-03)	Tok/s 88052 (97549)	Loss/tok 7.6843 (8.5891)	LR 3.366e-04
0: TRAIN [0][120/1885]	Time 0.146 (0.144)	Data 2.15e-04 (2.54e-03)	Tok/s 101369 (97643)	Loss/tok 7.7879 (8.5299)	LR 4.238e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][130/1885]	Time 0.240 (0.146)	Data 1.78e-04 (2.36e-03)	Tok/s 108248 (97949)	Loss/tok 8.2590 (8.4832)	LR 5.214e-04
0: TRAIN [0][140/1885]	Time 0.102 (0.147)	Data 2.17e-04 (2.21e-03)	Tok/s 88683 (97803)	Loss/tok 7.5557 (8.4393)	LR 6.564e-04
0: TRAIN [0][150/1885]	Time 0.061 (0.146)	Data 2.13e-04 (2.07e-03)	Tok/s 74509 (97666)	Loss/tok 6.6704 (8.3968)	LR 8.263e-04
0: TRAIN [0][160/1885]	Time 0.103 (0.144)	Data 2.13e-04 (1.96e-03)	Tok/s 89734 (97409)	Loss/tok 7.3079 (8.3513)	LR 1.040e-03
0: TRAIN [0][170/1885]	Time 0.243 (0.145)	Data 2.18e-04 (1.86e-03)	Tok/s 106146 (97549)	Loss/tok 7.7631 (8.2989)	LR 1.310e-03
0: TRAIN [0][180/1885]	Time 0.103 (0.146)	Data 1.75e-04 (1.77e-03)	Tok/s 88441 (97638)	Loss/tok 6.9640 (8.2418)	LR 1.649e-03
0: TRAIN [0][190/1885]	Time 0.102 (0.145)	Data 2.11e-04 (1.68e-03)	Tok/s 89922 (97421)	Loss/tok 6.8312 (8.1944)	LR 2.076e-03
0: TRAIN [0][200/1885]	Time 0.103 (0.144)	Data 1.50e-04 (1.61e-03)	Tok/s 88455 (97163)	Loss/tok 6.9166 (8.1482)	LR 2.613e-03
0: TRAIN [0][210/1885]	Time 0.243 (0.145)	Data 2.09e-04 (1.54e-03)	Tok/s 106821 (97257)	Loss/tok 7.1749 (8.0905)	LR 2.800e-03
0: TRAIN [0][220/1885]	Time 0.193 (0.144)	Data 2.14e-04 (1.48e-03)	Tok/s 105395 (97151)	Loss/tok 6.8379 (8.0355)	LR 2.800e-03
0: TRAIN [0][230/1885]	Time 0.104 (0.144)	Data 2.03e-04 (1.43e-03)	Tok/s 89754 (97076)	Loss/tok 6.3103 (7.9770)	LR 2.800e-03
0: TRAIN [0][240/1885]	Time 0.145 (0.145)	Data 2.15e-04 (1.38e-03)	Tok/s 100729 (97215)	Loss/tok 6.4550 (7.9078)	LR 2.800e-03
0: TRAIN [0][250/1885]	Time 0.101 (0.145)	Data 2.14e-04 (1.33e-03)	Tok/s 90222 (97115)	Loss/tok 6.0197 (7.8461)	LR 2.800e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][260/1885]	Time 0.103 (0.145)	Data 2.12e-04 (1.29e-03)	Tok/s 86765 (97084)	Loss/tok 5.7820 (7.7828)	LR 2.800e-03
0: TRAIN [0][270/1885]	Time 0.102 (0.144)	Data 2.17e-04 (1.25e-03)	Tok/s 92030 (97001)	Loss/tok 5.7727 (7.7247)	LR 2.800e-03
0: TRAIN [0][280/1885]	Time 0.103 (0.144)	Data 1.44e-04 (1.21e-03)	Tok/s 88352 (96897)	Loss/tok 5.4726 (7.6650)	LR 2.800e-03
0: TRAIN [0][290/1885]	Time 0.146 (0.144)	Data 2.17e-04 (1.18e-03)	Tok/s 101642 (96839)	Loss/tok 5.8033 (7.6063)	LR 2.800e-03
0: TRAIN [0][300/1885]	Time 0.147 (0.144)	Data 2.15e-04 (1.14e-03)	Tok/s 100327 (96913)	Loss/tok 5.6559 (7.5431)	LR 2.800e-03
0: TRAIN [0][310/1885]	Time 0.101 (0.144)	Data 2.14e-04 (1.11e-03)	Tok/s 90285 (96893)	Loss/tok 5.2345 (7.4810)	LR 2.800e-03
0: TRAIN [0][320/1885]	Time 0.147 (0.144)	Data 1.50e-04 (1.09e-03)	Tok/s 99822 (96756)	Loss/tok 5.5588 (7.4279)	LR 2.800e-03
0: TRAIN [0][330/1885]	Time 0.145 (0.144)	Data 2.13e-04 (1.06e-03)	Tok/s 101352 (96713)	Loss/tok 5.4870 (7.3724)	LR 2.800e-03
0: TRAIN [0][340/1885]	Time 0.241 (0.145)	Data 2.15e-04 (1.03e-03)	Tok/s 109545 (96873)	Loss/tok 5.7013 (7.3017)	LR 2.800e-03
0: TRAIN [0][350/1885]	Time 0.104 (0.145)	Data 2.01e-04 (1.01e-03)	Tok/s 88791 (96912)	Loss/tok 4.9487 (7.2443)	LR 2.800e-03
0: TRAIN [0][360/1885]	Time 0.147 (0.145)	Data 2.15e-04 (9.89e-04)	Tok/s 100332 (96991)	Loss/tok 5.1395 (7.1836)	LR 2.800e-03
0: TRAIN [0][370/1885]	Time 0.101 (0.144)	Data 2.12e-04 (9.67e-04)	Tok/s 91212 (96839)	Loss/tok 4.6493 (7.1371)	LR 2.800e-03
0: TRAIN [0][380/1885]	Time 0.245 (0.144)	Data 2.16e-04 (9.48e-04)	Tok/s 106907 (96795)	Loss/tok 5.4690 (7.0847)	LR 2.800e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][390/1885]	Time 0.103 (0.145)	Data 2.15e-04 (9.29e-04)	Tok/s 89854 (96885)	Loss/tok 4.3955 (7.0230)	LR 2.800e-03
0: TRAIN [0][400/1885]	Time 0.103 (0.145)	Data 2.15e-04 (9.11e-04)	Tok/s 85467 (96845)	Loss/tok 4.4341 (6.9713)	LR 2.800e-03
0: TRAIN [0][410/1885]	Time 0.148 (0.145)	Data 2.28e-04 (8.93e-04)	Tok/s 98505 (96862)	Loss/tok 4.7736 (6.9166)	LR 2.800e-03
0: TRAIN [0][420/1885]	Time 0.147 (0.144)	Data 2.15e-04 (8.77e-04)	Tok/s 99342 (96768)	Loss/tok 4.6752 (6.8706)	LR 2.800e-03
0: TRAIN [0][430/1885]	Time 0.147 (0.144)	Data 2.18e-04 (8.61e-04)	Tok/s 99534 (96674)	Loss/tok 4.5637 (6.8247)	LR 2.800e-03
0: TRAIN [0][440/1885]	Time 0.245 (0.144)	Data 2.16e-04 (8.47e-04)	Tok/s 106980 (96616)	Loss/tok 5.0048 (6.7786)	LR 2.800e-03
0: TRAIN [0][450/1885]	Time 0.103 (0.144)	Data 2.14e-04 (8.33e-04)	Tok/s 87105 (96656)	Loss/tok 4.0978 (6.7263)	LR 2.800e-03
0: TRAIN [0][460/1885]	Time 0.194 (0.144)	Data 2.17e-04 (8.19e-04)	Tok/s 105655 (96725)	Loss/tok 4.7656 (6.6733)	LR 2.800e-03
0: TRAIN [0][470/1885]	Time 0.102 (0.144)	Data 1.93e-04 (8.06e-04)	Tok/s 89732 (96589)	Loss/tok 3.9898 (6.6364)	LR 2.800e-03
0: TRAIN [0][480/1885]	Time 0.194 (0.144)	Data 1.87e-04 (7.94e-04)	Tok/s 106756 (96665)	Loss/tok 4.5381 (6.5861)	LR 2.800e-03
0: TRAIN [0][490/1885]	Time 0.103 (0.144)	Data 2.16e-04 (7.82e-04)	Tok/s 87176 (96620)	Loss/tok 4.0311 (6.5446)	LR 2.800e-03
0: TRAIN [0][500/1885]	Time 0.147 (0.144)	Data 2.16e-04 (7.70e-04)	Tok/s 97958 (96590)	Loss/tok 4.3976 (6.5019)	LR 2.800e-03
0: TRAIN [0][510/1885]	Time 0.194 (0.144)	Data 1.66e-04 (7.59e-04)	Tok/s 103878 (96641)	Loss/tok 4.4788 (6.4525)	LR 2.800e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][520/1885]	Time 0.148 (0.144)	Data 2.17e-04 (7.49e-04)	Tok/s 99517 (96647)	Loss/tok 4.3114 (6.4109)	LR 2.800e-03
0: TRAIN [0][530/1885]	Time 0.147 (0.145)	Data 2.18e-04 (7.39e-04)	Tok/s 100664 (96702)	Loss/tok 4.2259 (6.3665)	LR 2.800e-03
0: TRAIN [0][540/1885]	Time 0.104 (0.144)	Data 2.17e-04 (7.29e-04)	Tok/s 87660 (96647)	Loss/tok 4.0254 (6.3313)	LR 2.800e-03
0: TRAIN [0][550/1885]	Time 0.146 (0.144)	Data 2.16e-04 (7.20e-04)	Tok/s 99844 (96573)	Loss/tok 4.2183 (6.2982)	LR 2.800e-03
0: TRAIN [0][560/1885]	Time 0.193 (0.144)	Data 2.19e-04 (7.11e-04)	Tok/s 105579 (96559)	Loss/tok 4.3933 (6.2630)	LR 2.800e-03
0: TRAIN [0][570/1885]	Time 0.193 (0.144)	Data 2.17e-04 (7.02e-04)	Tok/s 105270 (96559)	Loss/tok 4.4267 (6.2256)	LR 2.800e-03
0: TRAIN [0][580/1885]	Time 0.193 (0.144)	Data 2.18e-04 (6.93e-04)	Tok/s 104417 (96486)	Loss/tok 4.4953 (6.1932)	LR 2.800e-03
0: TRAIN [0][590/1885]	Time 0.147 (0.144)	Data 2.15e-04 (6.85e-04)	Tok/s 100896 (96486)	Loss/tok 4.1191 (6.1594)	LR 2.800e-03
0: TRAIN [0][600/1885]	Time 0.148 (0.144)	Data 2.14e-04 (6.77e-04)	Tok/s 99755 (96525)	Loss/tok 3.9715 (6.1243)	LR 2.800e-03
0: TRAIN [0][610/1885]	Time 0.245 (0.144)	Data 2.30e-04 (6.70e-04)	Tok/s 108842 (96521)	Loss/tok 4.4792 (6.0911)	LR 2.800e-03
0: TRAIN [0][620/1885]	Time 0.103 (0.144)	Data 2.29e-04 (6.62e-04)	Tok/s 88871 (96513)	Loss/tok 3.6664 (6.0582)	LR 2.800e-03
0: TRAIN [0][630/1885]	Time 0.146 (0.144)	Data 2.27e-04 (6.55e-04)	Tok/s 99469 (96540)	Loss/tok 4.0928 (6.0264)	LR 2.800e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][640/1885]	Time 0.194 (0.144)	Data 1.87e-04 (6.48e-04)	Tok/s 106353 (96509)	Loss/tok 4.2497 (5.9972)	LR 2.800e-03
0: TRAIN [0][650/1885]	Time 0.102 (0.144)	Data 2.15e-04 (6.41e-04)	Tok/s 88488 (96481)	Loss/tok 3.8063 (5.9681)	LR 2.800e-03
0: TRAIN [0][660/1885]	Time 0.146 (0.144)	Data 2.15e-04 (6.35e-04)	Tok/s 100359 (96504)	Loss/tok 3.9518 (5.9381)	LR 2.800e-03
0: TRAIN [0][670/1885]	Time 0.103 (0.144)	Data 2.14e-04 (6.28e-04)	Tok/s 89269 (96429)	Loss/tok 3.7146 (5.9140)	LR 2.800e-03
0: TRAIN [0][680/1885]	Time 0.146 (0.143)	Data 2.19e-04 (6.22e-04)	Tok/s 101293 (96394)	Loss/tok 3.9151 (5.8886)	LR 2.800e-03
0: TRAIN [0][690/1885]	Time 0.104 (0.144)	Data 2.14e-04 (6.16e-04)	Tok/s 86963 (96413)	Loss/tok 3.6183 (5.8588)	LR 2.800e-03
0: TRAIN [0][700/1885]	Time 0.148 (0.143)	Data 2.17e-04 (6.10e-04)	Tok/s 99884 (96366)	Loss/tok 4.0071 (5.8352)	LR 2.800e-03
0: TRAIN [0][710/1885]	Time 0.247 (0.144)	Data 2.14e-04 (6.05e-04)	Tok/s 105572 (96387)	Loss/tok 4.3663 (5.8068)	LR 2.800e-03
0: TRAIN [0][720/1885]	Time 0.194 (0.143)	Data 2.15e-04 (5.99e-04)	Tok/s 105100 (96347)	Loss/tok 4.1963 (5.7839)	LR 2.800e-03
0: TRAIN [0][730/1885]	Time 0.149 (0.143)	Data 2.16e-04 (5.94e-04)	Tok/s 100833 (96345)	Loss/tok 3.9063 (5.7585)	LR 2.800e-03
0: TRAIN [0][740/1885]	Time 0.103 (0.143)	Data 2.18e-04 (5.89e-04)	Tok/s 87384 (96310)	Loss/tok 3.6211 (5.7359)	LR 2.800e-03
0: TRAIN [0][750/1885]	Time 0.194 (0.143)	Data 2.17e-04 (5.84e-04)	Tok/s 105709 (96278)	Loss/tok 4.0419 (5.7128)	LR 2.800e-03
0: TRAIN [0][760/1885]	Time 0.061 (0.143)	Data 1.50e-04 (5.79e-04)	Tok/s 75850 (96198)	Loss/tok 2.9490 (5.6936)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][770/1885]	Time 0.103 (0.143)	Data 2.16e-04 (5.74e-04)	Tok/s 87589 (96208)	Loss/tok 3.6656 (5.6701)	LR 2.800e-03
0: TRAIN [0][780/1885]	Time 0.146 (0.143)	Data 2.36e-04 (5.69e-04)	Tok/s 99977 (96203)	Loss/tok 4.0039 (5.6480)	LR 2.800e-03
0: TRAIN [0][790/1885]	Time 0.103 (0.143)	Data 2.16e-04 (5.65e-04)	Tok/s 88888 (96187)	Loss/tok 3.5781 (5.6273)	LR 2.800e-03
0: TRAIN [0][800/1885]	Time 0.104 (0.143)	Data 2.14e-04 (5.61e-04)	Tok/s 85099 (96242)	Loss/tok 3.4568 (5.6013)	LR 2.800e-03
0: TRAIN [0][810/1885]	Time 0.147 (0.143)	Data 2.02e-04 (5.56e-04)	Tok/s 99327 (96249)	Loss/tok 3.8851 (5.5791)	LR 2.800e-03
0: TRAIN [0][820/1885]	Time 0.102 (0.143)	Data 2.16e-04 (5.52e-04)	Tok/s 90532 (96187)	Loss/tok 3.5984 (5.5618)	LR 2.800e-03
0: TRAIN [0][830/1885]	Time 0.102 (0.143)	Data 2.18e-04 (5.48e-04)	Tok/s 88576 (96214)	Loss/tok 3.5987 (5.5397)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][840/1885]	Time 0.194 (0.143)	Data 2.17e-04 (5.44e-04)	Tok/s 104627 (96242)	Loss/tok 4.1519 (5.5190)	LR 2.800e-03
0: TRAIN [0][850/1885]	Time 0.102 (0.143)	Data 2.17e-04 (5.40e-04)	Tok/s 89234 (96265)	Loss/tok 3.5298 (5.4987)	LR 2.800e-03
0: TRAIN [0][860/1885]	Time 0.103 (0.143)	Data 2.29e-04 (5.36e-04)	Tok/s 87578 (96230)	Loss/tok 3.5994 (5.4812)	LR 2.800e-03
0: TRAIN [0][870/1885]	Time 0.103 (0.143)	Data 1.96e-04 (5.32e-04)	Tok/s 89375 (96232)	Loss/tok 3.3330 (5.4628)	LR 2.800e-03
0: TRAIN [0][880/1885]	Time 0.194 (0.143)	Data 2.17e-04 (5.29e-04)	Tok/s 102570 (96262)	Loss/tok 4.0941 (5.4426)	LR 2.800e-03
0: TRAIN [0][890/1885]	Time 0.104 (0.144)	Data 2.17e-04 (5.25e-04)	Tok/s 87554 (96281)	Loss/tok 3.5819 (5.4234)	LR 2.800e-03
0: TRAIN [0][900/1885]	Time 0.102 (0.143)	Data 2.15e-04 (5.22e-04)	Tok/s 88560 (96264)	Loss/tok 3.4058 (5.4064)	LR 2.800e-03
0: TRAIN [0][910/1885]	Time 0.195 (0.143)	Data 2.15e-04 (5.18e-04)	Tok/s 104420 (96236)	Loss/tok 3.9646 (5.3899)	LR 2.800e-03
0: TRAIN [0][920/1885]	Time 0.195 (0.143)	Data 2.17e-04 (5.15e-04)	Tok/s 105477 (96238)	Loss/tok 3.8973 (5.3722)	LR 2.800e-03
0: TRAIN [0][930/1885]	Time 0.196 (0.143)	Data 2.20e-04 (5.12e-04)	Tok/s 103613 (96221)	Loss/tok 3.9556 (5.3559)	LR 2.800e-03
0: TRAIN [0][940/1885]	Time 0.149 (0.143)	Data 2.19e-04 (5.08e-04)	Tok/s 97193 (96230)	Loss/tok 3.9620 (5.3389)	LR 2.800e-03
0: TRAIN [0][950/1885]	Time 0.147 (0.143)	Data 2.16e-04 (5.05e-04)	Tok/s 100328 (96182)	Loss/tok 3.7303 (5.3250)	LR 2.800e-03
0: TRAIN [0][960/1885]	Time 0.195 (0.144)	Data 2.16e-04 (5.02e-04)	Tok/s 105160 (96260)	Loss/tok 3.9214 (5.3048)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][970/1885]	Time 0.147 (0.144)	Data 2.15e-04 (4.99e-04)	Tok/s 97350 (96266)	Loss/tok 3.8479 (5.2891)	LR 2.800e-03
0: TRAIN [0][980/1885]	Time 0.245 (0.144)	Data 2.14e-04 (4.96e-04)	Tok/s 107857 (96278)	Loss/tok 4.1007 (5.2728)	LR 2.800e-03
0: TRAIN [0][990/1885]	Time 0.103 (0.144)	Data 2.16e-04 (4.93e-04)	Tok/s 87588 (96296)	Loss/tok 3.4889 (5.2561)	LR 2.800e-03
0: TRAIN [0][1000/1885]	Time 0.103 (0.144)	Data 1.89e-04 (4.90e-04)	Tok/s 87853 (96295)	Loss/tok 3.5365 (5.2409)	LR 2.800e-03
0: TRAIN [0][1010/1885]	Time 0.104 (0.144)	Data 2.16e-04 (4.88e-04)	Tok/s 87575 (96297)	Loss/tok 3.4404 (5.2255)	LR 2.800e-03
0: TRAIN [0][1020/1885]	Time 0.103 (0.144)	Data 2.29e-04 (4.85e-04)	Tok/s 88303 (96321)	Loss/tok 3.3525 (5.2094)	LR 2.800e-03
0: TRAIN [0][1030/1885]	Time 0.104 (0.144)	Data 2.17e-04 (4.82e-04)	Tok/s 89292 (96284)	Loss/tok 3.3141 (5.1972)	LR 2.800e-03
0: TRAIN [0][1040/1885]	Time 0.062 (0.144)	Data 2.17e-04 (4.80e-04)	Tok/s 73093 (96296)	Loss/tok 2.8675 (5.1823)	LR 2.800e-03
0: TRAIN [0][1050/1885]	Time 0.102 (0.144)	Data 2.14e-04 (4.77e-04)	Tok/s 88355 (96280)	Loss/tok 3.4316 (5.1696)	LR 2.800e-03
0: TRAIN [0][1060/1885]	Time 0.105 (0.144)	Data 2.16e-04 (4.75e-04)	Tok/s 85847 (96249)	Loss/tok 3.5087 (5.1572)	LR 2.800e-03
0: TRAIN [0][1070/1885]	Time 0.245 (0.144)	Data 2.16e-04 (4.72e-04)	Tok/s 106421 (96253)	Loss/tok 4.0067 (5.1437)	LR 2.800e-03
0: TRAIN [0][1080/1885]	Time 0.148 (0.144)	Data 2.17e-04 (4.70e-04)	Tok/s 98262 (96260)	Loss/tok 3.7445 (5.1299)	LR 2.800e-03
0: TRAIN [0][1090/1885]	Time 0.103 (0.144)	Data 1.88e-04 (4.68e-04)	Tok/s 89966 (96274)	Loss/tok 3.4651 (5.1163)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1100/1885]	Time 0.147 (0.144)	Data 2.15e-04 (4.65e-04)	Tok/s 99063 (96241)	Loss/tok 3.5659 (5.1051)	LR 2.800e-03
0: TRAIN [0][1110/1885]	Time 0.148 (0.144)	Data 2.01e-04 (4.63e-04)	Tok/s 97796 (96208)	Loss/tok 3.6617 (5.0938)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1120/1885]	Time 0.193 (0.144)	Data 1.44e-04 (4.60e-04)	Tok/s 106434 (96211)	Loss/tok 3.8368 (5.0809)	LR 2.800e-03
0: TRAIN [0][1130/1885]	Time 0.102 (0.144)	Data 2.17e-04 (4.58e-04)	Tok/s 90176 (96204)	Loss/tok 3.3211 (5.0694)	LR 2.800e-03
0: TRAIN [0][1140/1885]	Time 0.103 (0.144)	Data 2.16e-04 (4.56e-04)	Tok/s 88741 (96175)	Loss/tok 3.4860 (5.0583)	LR 2.800e-03
0: TRAIN [0][1150/1885]	Time 0.103 (0.144)	Data 2.17e-04 (4.54e-04)	Tok/s 90171 (96180)	Loss/tok 3.3630 (5.0463)	LR 2.800e-03
0: TRAIN [0][1160/1885]	Time 0.102 (0.144)	Data 2.16e-04 (4.52e-04)	Tok/s 89081 (96151)	Loss/tok 3.3297 (5.0356)	LR 2.800e-03
0: TRAIN [0][1170/1885]	Time 0.103 (0.144)	Data 2.18e-04 (4.50e-04)	Tok/s 88015 (96133)	Loss/tok 3.5562 (5.0246)	LR 2.800e-03
0: TRAIN [0][1180/1885]	Time 0.103 (0.144)	Data 2.17e-04 (4.48e-04)	Tok/s 89372 (96133)	Loss/tok 3.4155 (5.0134)	LR 2.800e-03
0: TRAIN [0][1190/1885]	Time 0.147 (0.144)	Data 2.16e-04 (4.46e-04)	Tok/s 101121 (96123)	Loss/tok 3.7120 (5.0023)	LR 2.800e-03
0: TRAIN [0][1200/1885]	Time 0.192 (0.144)	Data 2.17e-04 (4.44e-04)	Tok/s 106995 (96132)	Loss/tok 3.7872 (4.9911)	LR 2.800e-03
0: TRAIN [0][1210/1885]	Time 0.146 (0.144)	Data 2.14e-04 (4.42e-04)	Tok/s 100252 (96143)	Loss/tok 3.7235 (4.9801)	LR 2.800e-03
0: TRAIN [0][1220/1885]	Time 0.245 (0.144)	Data 2.15e-04 (4.40e-04)	Tok/s 106918 (96146)	Loss/tok 4.0511 (4.9691)	LR 2.800e-03
0: TRAIN [0][1230/1885]	Time 0.103 (0.144)	Data 2.16e-04 (4.38e-04)	Tok/s 87334 (96161)	Loss/tok 3.3970 (4.9580)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1240/1885]	Time 0.193 (0.144)	Data 2.16e-04 (4.36e-04)	Tok/s 106352 (96140)	Loss/tok 3.7149 (4.9480)	LR 2.800e-03
0: TRAIN [0][1250/1885]	Time 0.105 (0.144)	Data 1.64e-04 (4.34e-04)	Tok/s 87988 (96146)	Loss/tok 3.4389 (4.9375)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1260/1885]	Time 0.102 (0.144)	Data 2.16e-04 (4.33e-04)	Tok/s 87881 (96141)	Loss/tok 3.3189 (4.9269)	LR 2.800e-03
0: TRAIN [0][1270/1885]	Time 0.195 (0.144)	Data 2.22e-04 (4.31e-04)	Tok/s 105611 (96150)	Loss/tok 3.8455 (4.9168)	LR 2.800e-03
0: TRAIN [0][1280/1885]	Time 0.195 (0.144)	Data 2.18e-04 (4.29e-04)	Tok/s 104251 (96136)	Loss/tok 3.8260 (4.9073)	LR 2.800e-03
0: TRAIN [0][1290/1885]	Time 0.195 (0.144)	Data 2.15e-04 (4.27e-04)	Tok/s 104417 (96142)	Loss/tok 3.8033 (4.8972)	LR 2.800e-03
0: TRAIN [0][1300/1885]	Time 0.103 (0.143)	Data 2.15e-04 (4.26e-04)	Tok/s 87417 (96117)	Loss/tok 3.3376 (4.8882)	LR 2.800e-03
0: TRAIN [0][1310/1885]	Time 0.061 (0.143)	Data 2.19e-04 (4.24e-04)	Tok/s 73579 (96105)	Loss/tok 2.6860 (4.8787)	LR 2.800e-03
0: TRAIN [0][1320/1885]	Time 0.193 (0.143)	Data 2.21e-04 (4.22e-04)	Tok/s 105364 (96093)	Loss/tok 3.8659 (4.8699)	LR 2.800e-03
0: TRAIN [0][1330/1885]	Time 0.103 (0.143)	Data 1.49e-04 (4.21e-04)	Tok/s 86153 (96084)	Loss/tok 3.1852 (4.8605)	LR 2.800e-03
0: TRAIN [0][1340/1885]	Time 0.195 (0.143)	Data 1.90e-04 (4.19e-04)	Tok/s 104200 (96101)	Loss/tok 3.8031 (4.8505)	LR 2.800e-03
0: TRAIN [0][1350/1885]	Time 0.194 (0.144)	Data 2.18e-04 (4.18e-04)	Tok/s 106794 (96115)	Loss/tok 3.7318 (4.8409)	LR 2.800e-03
0: TRAIN [0][1360/1885]	Time 0.101 (0.143)	Data 1.87e-04 (4.16e-04)	Tok/s 90907 (96076)	Loss/tok 3.4427 (4.8333)	LR 2.800e-03
0: TRAIN [0][1370/1885]	Time 0.246 (0.143)	Data 2.15e-04 (4.15e-04)	Tok/s 106137 (96067)	Loss/tok 4.0196 (4.8249)	LR 2.800e-03
0: TRAIN [0][1380/1885]	Time 0.148 (0.143)	Data 1.80e-04 (4.13e-04)	Tok/s 97618 (96104)	Loss/tok 3.5627 (4.8148)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1390/1885]	Time 0.147 (0.143)	Data 2.16e-04 (4.12e-04)	Tok/s 100423 (96070)	Loss/tok 3.5373 (4.8070)	LR 2.800e-03
0: TRAIN [0][1400/1885]	Time 0.148 (0.143)	Data 2.16e-04 (4.10e-04)	Tok/s 100049 (96065)	Loss/tok 3.4797 (4.7988)	LR 2.800e-03
0: TRAIN [0][1410/1885]	Time 0.196 (0.144)	Data 1.76e-04 (4.09e-04)	Tok/s 105778 (96109)	Loss/tok 3.8302 (4.7886)	LR 2.800e-03
0: TRAIN [0][1420/1885]	Time 0.147 (0.144)	Data 2.17e-04 (4.08e-04)	Tok/s 99444 (96109)	Loss/tok 3.5047 (4.7803)	LR 2.800e-03
0: TRAIN [0][1430/1885]	Time 0.148 (0.144)	Data 2.02e-04 (4.06e-04)	Tok/s 99126 (96124)	Loss/tok 3.6267 (4.7716)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1440/1885]	Time 0.060 (0.144)	Data 2.19e-04 (4.05e-04)	Tok/s 77506 (96115)	Loss/tok 2.6878 (4.7635)	LR 2.800e-03
0: TRAIN [0][1450/1885]	Time 0.101 (0.144)	Data 2.17e-04 (4.03e-04)	Tok/s 89285 (96108)	Loss/tok 3.3159 (4.7560)	LR 2.800e-03
0: TRAIN [0][1460/1885]	Time 0.102 (0.143)	Data 2.19e-04 (4.02e-04)	Tok/s 88931 (96088)	Loss/tok 3.3071 (4.7487)	LR 2.800e-03
0: TRAIN [0][1470/1885]	Time 0.193 (0.143)	Data 2.14e-04 (4.01e-04)	Tok/s 104719 (96085)	Loss/tok 3.7600 (4.7409)	LR 2.800e-03
0: TRAIN [0][1480/1885]	Time 0.195 (0.143)	Data 2.16e-04 (3.99e-04)	Tok/s 104588 (96081)	Loss/tok 3.6694 (4.7334)	LR 2.800e-03
0: TRAIN [0][1490/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.98e-04)	Tok/s 99064 (96097)	Loss/tok 3.5625 (4.7257)	LR 2.800e-03
0: TRAIN [0][1500/1885]	Time 0.148 (0.143)	Data 2.21e-04 (3.97e-04)	Tok/s 98740 (96095)	Loss/tok 3.5099 (4.7183)	LR 2.800e-03
0: TRAIN [0][1510/1885]	Time 0.245 (0.143)	Data 2.14e-04 (3.96e-04)	Tok/s 106323 (96094)	Loss/tok 3.9385 (4.7105)	LR 2.800e-03
0: TRAIN [0][1520/1885]	Time 0.103 (0.143)	Data 2.15e-04 (3.95e-04)	Tok/s 88687 (96093)	Loss/tok 3.4138 (4.7031)	LR 2.800e-03
0: TRAIN [0][1530/1885]	Time 0.148 (0.143)	Data 2.14e-04 (3.93e-04)	Tok/s 98797 (96098)	Loss/tok 3.4441 (4.6955)	LR 2.800e-03
0: TRAIN [0][1540/1885]	Time 0.195 (0.143)	Data 2.21e-04 (3.92e-04)	Tok/s 104834 (96069)	Loss/tok 3.6261 (4.6890)	LR 2.800e-03
0: TRAIN [0][1550/1885]	Time 0.104 (0.143)	Data 2.19e-04 (3.91e-04)	Tok/s 88187 (96096)	Loss/tok 3.2466 (4.6809)	LR 2.800e-03
0: TRAIN [0][1560/1885]	Time 0.195 (0.143)	Data 2.00e-04 (3.90e-04)	Tok/s 106095 (96098)	Loss/tok 3.7165 (4.6736)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1570/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.89e-04)	Tok/s 105544 (96103)	Loss/tok 3.7244 (4.6664)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1580/1885]	Time 0.246 (0.143)	Data 2.14e-04 (3.88e-04)	Tok/s 107386 (96089)	Loss/tok 3.8333 (4.6597)	LR 2.800e-03
0: TRAIN [0][1590/1885]	Time 0.104 (0.143)	Data 2.15e-04 (3.87e-04)	Tok/s 87236 (96086)	Loss/tok 3.3152 (4.6530)	LR 2.800e-03
0: TRAIN [0][1600/1885]	Time 0.194 (0.143)	Data 2.17e-04 (3.85e-04)	Tok/s 107375 (96087)	Loss/tok 3.6894 (4.6458)	LR 2.800e-03
0: TRAIN [0][1610/1885]	Time 0.060 (0.143)	Data 2.16e-04 (3.84e-04)	Tok/s 76143 (96072)	Loss/tok 2.5717 (4.6394)	LR 2.800e-03
0: TRAIN [0][1620/1885]	Time 0.150 (0.144)	Data 2.18e-04 (3.83e-04)	Tok/s 97994 (96117)	Loss/tok 3.5613 (4.6314)	LR 2.800e-03
0: TRAIN [0][1630/1885]	Time 0.195 (0.144)	Data 2.17e-04 (3.82e-04)	Tok/s 104380 (96122)	Loss/tok 3.7247 (4.6247)	LR 2.800e-03
0: TRAIN [0][1640/1885]	Time 0.246 (0.144)	Data 2.17e-04 (3.81e-04)	Tok/s 106477 (96126)	Loss/tok 3.8706 (4.6181)	LR 2.800e-03
0: TRAIN [0][1650/1885]	Time 0.147 (0.144)	Data 2.17e-04 (3.80e-04)	Tok/s 99809 (96110)	Loss/tok 3.5211 (4.6122)	LR 2.800e-03
0: TRAIN [0][1660/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.79e-04)	Tok/s 88381 (96093)	Loss/tok 3.1498 (4.6064)	LR 2.800e-03
0: TRAIN [0][1670/1885]	Time 0.103 (0.144)	Data 1.68e-04 (3.78e-04)	Tok/s 89358 (96100)	Loss/tok 3.2162 (4.6002)	LR 2.800e-03
0: TRAIN [0][1680/1885]	Time 0.194 (0.144)	Data 2.16e-04 (3.77e-04)	Tok/s 105897 (96102)	Loss/tok 3.6066 (4.5937)	LR 2.800e-03
0: TRAIN [0][1690/1885]	Time 0.060 (0.144)	Data 2.16e-04 (3.76e-04)	Tok/s 77058 (96116)	Loss/tok 2.7237 (4.5869)	LR 2.800e-03
0: TRAIN [0][1700/1885]	Time 0.103 (0.144)	Data 2.16e-04 (3.75e-04)	Tok/s 88134 (96110)	Loss/tok 3.2726 (4.5810)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1710/1885]	Time 0.060 (0.144)	Data 2.15e-04 (3.74e-04)	Tok/s 75264 (96102)	Loss/tok 2.7285 (4.5749)	LR 2.800e-03
0: TRAIN [0][1720/1885]	Time 0.061 (0.144)	Data 2.14e-04 (3.73e-04)	Tok/s 74640 (96089)	Loss/tok 2.6834 (4.5691)	LR 2.800e-03
0: TRAIN [0][1730/1885]	Time 0.149 (0.144)	Data 2.14e-04 (3.72e-04)	Tok/s 98651 (96100)	Loss/tok 3.6090 (4.5629)	LR 2.800e-03
0: TRAIN [0][1740/1885]	Time 0.147 (0.143)	Data 2.14e-04 (3.72e-04)	Tok/s 99173 (96092)	Loss/tok 3.5340 (4.5574)	LR 2.800e-03
0: TRAIN [0][1750/1885]	Time 0.103 (0.143)	Data 2.18e-04 (3.71e-04)	Tok/s 86937 (96091)	Loss/tok 3.3122 (4.5517)	LR 2.800e-03
0: TRAIN [0][1760/1885]	Time 0.147 (0.143)	Data 2.14e-04 (3.70e-04)	Tok/s 100080 (96092)	Loss/tok 3.5023 (4.5459)	LR 2.800e-03
0: TRAIN [0][1770/1885]	Time 0.104 (0.144)	Data 2.19e-04 (3.69e-04)	Tok/s 87477 (96098)	Loss/tok 3.2407 (4.5397)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1780/1885]	Time 0.103 (0.143)	Data 2.16e-04 (3.68e-04)	Tok/s 89688 (96085)	Loss/tok 3.2257 (4.5345)	LR 2.800e-03
0: TRAIN [0][1790/1885]	Time 0.149 (0.143)	Data 2.14e-04 (3.67e-04)	Tok/s 97674 (96083)	Loss/tok 3.4868 (4.5291)	LR 2.800e-03
0: TRAIN [0][1800/1885]	Time 0.102 (0.144)	Data 1.76e-04 (3.66e-04)	Tok/s 89469 (96106)	Loss/tok 3.2995 (4.5228)	LR 2.800e-03
0: TRAIN [0][1810/1885]	Time 0.196 (0.144)	Data 2.15e-04 (3.65e-04)	Tok/s 103824 (96113)	Loss/tok 3.7837 (4.5172)	LR 2.800e-03
0: TRAIN [0][1820/1885]	Time 0.197 (0.144)	Data 2.17e-04 (3.64e-04)	Tok/s 103081 (96129)	Loss/tok 3.6447 (4.5112)	LR 2.800e-03
0: TRAIN [0][1830/1885]	Time 0.103 (0.144)	Data 2.21e-04 (3.64e-04)	Tok/s 86917 (96122)	Loss/tok 3.3738 (4.5060)	LR 2.800e-03
0: TRAIN [0][1840/1885]	Time 0.193 (0.144)	Data 2.14e-04 (3.63e-04)	Tok/s 105385 (96119)	Loss/tok 3.6386 (4.5008)	LR 2.800e-03
0: TRAIN [0][1850/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.62e-04)	Tok/s 99133 (96120)	Loss/tok 3.4523 (4.4954)	LR 2.800e-03
0: TRAIN [0][1860/1885]	Time 0.194 (0.144)	Data 2.15e-04 (3.61e-04)	Tok/s 105181 (96104)	Loss/tok 3.6067 (4.4903)	LR 2.800e-03
0: TRAIN [0][1870/1885]	Time 0.102 (0.144)	Data 2.18e-04 (3.60e-04)	Tok/s 89831 (96089)	Loss/tok 3.2505 (4.4852)	LR 2.800e-03
0: TRAIN [0][1880/1885]	Time 0.058 (0.143)	Data 2.13e-04 (3.60e-04)	Tok/s 80044 (96072)	Loss/tok 2.6754 (4.4802)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1594174770736, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1594174770737, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.694 (0.694)	Decoder iters 149.0 (149.0)	Tok/s 23059 (23059)
0: Running moses detokenizer
0: BLEU(score=20.175331905513374, counts=[34266, 15797, 8409, 4721], totals=[63661, 60658, 57655, 54656], precisions=[53.825733180440146, 26.042731379208018, 14.585031653802792, 8.637661007025761], bp=0.9841826025381344, sys_len=63661, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1594174772465, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2018, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1594174772465, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.4798	Test BLEU: 20.18
0: Performance: Epoch: 0	Training: 768188 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1594174772465, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1594174772465, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1594174772465, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3117034106
0: TRAIN [1][0/1885]	Time 0.350 (0.350)	Data 2.12e-01 (2.12e-01)	Tok/s 26191 (26191)	Loss/tok 3.1917 (3.1917)	LR 2.800e-03
0: TRAIN [1][10/1885]	Time 0.245 (0.179)	Data 2.11e-04 (1.95e-02)	Tok/s 104090 (92585)	Loss/tok 3.8200 (3.5194)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][20/1885]	Time 0.147 (0.149)	Data 2.14e-04 (1.03e-02)	Tok/s 100862 (91463)	Loss/tok 3.4156 (3.4642)	LR 2.800e-03
0: TRAIN [1][30/1885]	Time 0.146 (0.144)	Data 2.19e-04 (7.05e-03)	Tok/s 99550 (92827)	Loss/tok 3.4794 (3.4500)	LR 2.800e-03
0: TRAIN [1][40/1885]	Time 0.146 (0.143)	Data 2.13e-04 (5.39e-03)	Tok/s 99493 (93658)	Loss/tok 3.3378 (3.4409)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][50/1885]	Time 0.244 (0.142)	Data 2.14e-04 (4.37e-03)	Tok/s 107448 (93942)	Loss/tok 3.9702 (3.4444)	LR 2.800e-03
0: TRAIN [1][60/1885]	Time 0.101 (0.139)	Data 2.17e-04 (3.69e-03)	Tok/s 91449 (93691)	Loss/tok 3.0937 (3.4252)	LR 2.800e-03
0: TRAIN [1][70/1885]	Time 0.146 (0.139)	Data 2.12e-04 (3.20e-03)	Tok/s 100759 (93664)	Loss/tok 3.4151 (3.4315)	LR 2.800e-03
0: TRAIN [1][80/1885]	Time 0.101 (0.138)	Data 2.30e-04 (2.83e-03)	Tok/s 89938 (94021)	Loss/tok 3.1585 (3.4269)	LR 2.800e-03
0: TRAIN [1][90/1885]	Time 0.102 (0.143)	Data 2.16e-04 (2.54e-03)	Tok/s 88502 (94826)	Loss/tok 3.2497 (3.4544)	LR 2.800e-03
0: TRAIN [1][100/1885]	Time 0.146 (0.141)	Data 1.64e-04 (2.31e-03)	Tok/s 100416 (94829)	Loss/tok 3.3202 (3.4403)	LR 2.800e-03
0: TRAIN [1][110/1885]	Time 0.102 (0.143)	Data 2.16e-04 (2.12e-03)	Tok/s 90645 (95349)	Loss/tok 3.0967 (3.4473)	LR 2.800e-03
0: TRAIN [1][120/1885]	Time 0.147 (0.145)	Data 2.15e-04 (1.97e-03)	Tok/s 100541 (95837)	Loss/tok 3.4130 (3.4526)	LR 2.800e-03
0: TRAIN [1][130/1885]	Time 0.147 (0.144)	Data 2.17e-04 (1.83e-03)	Tok/s 101618 (95860)	Loss/tok 3.4636 (3.4492)	LR 2.800e-03
0: TRAIN [1][140/1885]	Time 0.147 (0.144)	Data 2.16e-04 (1.72e-03)	Tok/s 99932 (95960)	Loss/tok 3.2810 (3.4494)	LR 2.800e-03
0: TRAIN [1][150/1885]	Time 0.100 (0.144)	Data 2.16e-04 (1.62e-03)	Tok/s 88855 (95935)	Loss/tok 3.1719 (3.4472)	LR 2.800e-03
0: TRAIN [1][160/1885]	Time 0.147 (0.143)	Data 2.15e-04 (1.53e-03)	Tok/s 100197 (95976)	Loss/tok 3.3592 (3.4459)	LR 2.800e-03
0: TRAIN [1][170/1885]	Time 0.147 (0.145)	Data 2.22e-04 (1.45e-03)	Tok/s 100024 (96309)	Loss/tok 3.3946 (3.4479)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][180/1885]	Time 0.060 (0.144)	Data 2.25e-04 (1.39e-03)	Tok/s 75795 (96207)	Loss/tok 2.6107 (3.4416)	LR 2.800e-03
0: TRAIN [1][190/1885]	Time 0.147 (0.145)	Data 2.18e-04 (1.32e-03)	Tok/s 100109 (96375)	Loss/tok 3.4223 (3.4456)	LR 2.800e-03
0: TRAIN [1][200/1885]	Time 0.102 (0.146)	Data 2.16e-04 (1.27e-03)	Tok/s 89246 (96581)	Loss/tok 3.1794 (3.4464)	LR 2.800e-03
0: TRAIN [1][210/1885]	Time 0.147 (0.147)	Data 2.14e-04 (1.22e-03)	Tok/s 100539 (96780)	Loss/tok 3.3250 (3.4542)	LR 2.800e-03
0: TRAIN [1][220/1885]	Time 0.102 (0.147)	Data 2.13e-04 (1.17e-03)	Tok/s 87491 (96735)	Loss/tok 3.2138 (3.4506)	LR 2.800e-03
0: TRAIN [1][230/1885]	Time 0.147 (0.147)	Data 2.15e-04 (1.13e-03)	Tok/s 99093 (96744)	Loss/tok 3.3831 (3.4487)	LR 2.800e-03
0: TRAIN [1][240/1885]	Time 0.194 (0.147)	Data 2.17e-04 (1.09e-03)	Tok/s 105076 (96821)	Loss/tok 3.6346 (3.4509)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][250/1885]	Time 0.147 (0.148)	Data 2.18e-04 (1.06e-03)	Tok/s 99619 (96837)	Loss/tok 3.3965 (3.4539)	LR 2.800e-03
0: TRAIN [1][260/1885]	Time 0.101 (0.148)	Data 2.16e-04 (1.03e-03)	Tok/s 88590 (96896)	Loss/tok 3.3336 (3.4550)	LR 2.800e-03
0: TRAIN [1][270/1885]	Time 0.102 (0.147)	Data 2.02e-04 (9.96e-04)	Tok/s 89767 (96831)	Loss/tok 3.2187 (3.4541)	LR 2.800e-03
0: TRAIN [1][280/1885]	Time 0.194 (0.147)	Data 2.20e-04 (9.68e-04)	Tok/s 106116 (96717)	Loss/tok 3.5970 (3.4519)	LR 2.800e-03
0: TRAIN [1][290/1885]	Time 0.146 (0.147)	Data 2.14e-04 (9.42e-04)	Tok/s 98889 (96693)	Loss/tok 3.4163 (3.4504)	LR 2.800e-03
0: TRAIN [1][300/1885]	Time 0.147 (0.146)	Data 2.24e-04 (9.18e-04)	Tok/s 100295 (96674)	Loss/tok 3.3376 (3.4503)	LR 2.800e-03
0: TRAIN [1][310/1885]	Time 0.192 (0.146)	Data 2.17e-04 (8.96e-04)	Tok/s 105508 (96621)	Loss/tok 3.5860 (3.4488)	LR 2.800e-03
0: TRAIN [1][320/1885]	Time 0.146 (0.146)	Data 2.17e-04 (8.75e-04)	Tok/s 99232 (96600)	Loss/tok 3.4115 (3.4490)	LR 2.800e-03
0: TRAIN [1][330/1885]	Time 0.147 (0.147)	Data 2.10e-04 (8.55e-04)	Tok/s 100761 (96717)	Loss/tok 3.3903 (3.4512)	LR 2.800e-03
0: TRAIN [1][340/1885]	Time 0.101 (0.146)	Data 2.21e-04 (8.36e-04)	Tok/s 89597 (96710)	Loss/tok 3.2198 (3.4489)	LR 2.800e-03
0: TRAIN [1][350/1885]	Time 0.146 (0.146)	Data 2.21e-04 (8.18e-04)	Tok/s 101192 (96751)	Loss/tok 3.3400 (3.4476)	LR 2.800e-03
0: TRAIN [1][360/1885]	Time 0.101 (0.147)	Data 2.17e-04 (8.02e-04)	Tok/s 89670 (96910)	Loss/tok 3.0585 (3.4497)	LR 2.800e-03
0: TRAIN [1][370/1885]	Time 0.146 (0.147)	Data 2.18e-04 (7.86e-04)	Tok/s 101544 (96849)	Loss/tok 3.3707 (3.4479)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][380/1885]	Time 0.104 (0.146)	Data 2.19e-04 (7.71e-04)	Tok/s 86943 (96804)	Loss/tok 3.1458 (3.4456)	LR 2.800e-03
0: TRAIN [1][390/1885]	Time 0.146 (0.146)	Data 2.16e-04 (7.57e-04)	Tok/s 101110 (96790)	Loss/tok 3.3479 (3.4456)	LR 2.800e-03
0: TRAIN [1][400/1885]	Time 0.244 (0.147)	Data 2.17e-04 (7.43e-04)	Tok/s 108296 (96864)	Loss/tok 3.6837 (3.4462)	LR 2.800e-03
0: TRAIN [1][410/1885]	Time 0.192 (0.147)	Data 2.17e-04 (7.30e-04)	Tok/s 106420 (96874)	Loss/tok 3.4504 (3.4477)	LR 2.800e-03
0: TRAIN [1][420/1885]	Time 0.102 (0.146)	Data 2.17e-04 (7.18e-04)	Tok/s 88907 (96798)	Loss/tok 3.1674 (3.4441)	LR 2.800e-03
0: TRAIN [1][430/1885]	Time 0.101 (0.146)	Data 2.22e-04 (7.06e-04)	Tok/s 91073 (96828)	Loss/tok 3.1396 (3.4422)	LR 2.800e-03
0: TRAIN [1][440/1885]	Time 0.193 (0.146)	Data 2.21e-04 (6.95e-04)	Tok/s 106300 (96763)	Loss/tok 3.5082 (3.4429)	LR 2.800e-03
0: TRAIN [1][450/1885]	Time 0.146 (0.145)	Data 2.16e-04 (6.85e-04)	Tok/s 98680 (96625)	Loss/tok 3.4063 (3.4402)	LR 2.800e-03
0: TRAIN [1][460/1885]	Time 0.146 (0.145)	Data 2.16e-04 (6.75e-04)	Tok/s 100746 (96662)	Loss/tok 3.2530 (3.4402)	LR 2.800e-03
0: TRAIN [1][470/1885]	Time 0.245 (0.145)	Data 2.18e-04 (6.65e-04)	Tok/s 107064 (96641)	Loss/tok 3.6916 (3.4405)	LR 2.800e-03
0: TRAIN [1][480/1885]	Time 0.194 (0.145)	Data 2.33e-04 (6.56e-04)	Tok/s 105118 (96670)	Loss/tok 3.6149 (3.4393)	LR 2.800e-03
0: TRAIN [1][490/1885]	Time 0.101 (0.145)	Data 2.16e-04 (6.47e-04)	Tok/s 89307 (96608)	Loss/tok 3.1127 (3.4366)	LR 2.800e-03
0: TRAIN [1][500/1885]	Time 0.196 (0.145)	Data 2.16e-04 (6.38e-04)	Tok/s 103433 (96622)	Loss/tok 3.5407 (3.4366)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][510/1885]	Time 0.193 (0.145)	Data 2.20e-04 (6.30e-04)	Tok/s 104748 (96646)	Loss/tok 3.5685 (3.4369)	LR 2.800e-03
0: TRAIN [1][520/1885]	Time 0.147 (0.145)	Data 2.16e-04 (6.22e-04)	Tok/s 100391 (96628)	Loss/tok 3.2942 (3.4358)	LR 2.800e-03
0: TRAIN [1][530/1885]	Time 0.102 (0.145)	Data 2.15e-04 (6.14e-04)	Tok/s 89484 (96657)	Loss/tok 3.2524 (3.4355)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][540/1885]	Time 0.193 (0.145)	Data 2.16e-04 (6.07e-04)	Tok/s 105679 (96644)	Loss/tok 3.5335 (3.4349)	LR 2.800e-03
0: TRAIN [1][550/1885]	Time 0.147 (0.145)	Data 2.12e-04 (6.00e-04)	Tok/s 101020 (96643)	Loss/tok 3.3353 (3.4337)	LR 2.800e-03
0: TRAIN [1][560/1885]	Time 0.245 (0.144)	Data 2.16e-04 (5.93e-04)	Tok/s 106414 (96577)	Loss/tok 3.6458 (3.4317)	LR 2.800e-03
0: TRAIN [1][570/1885]	Time 0.148 (0.144)	Data 2.22e-04 (5.86e-04)	Tok/s 98433 (96533)	Loss/tok 3.4642 (3.4299)	LR 2.800e-03
0: TRAIN [1][580/1885]	Time 0.060 (0.144)	Data 2.14e-04 (5.80e-04)	Tok/s 74758 (96467)	Loss/tok 2.5552 (3.4272)	LR 2.800e-03
0: TRAIN [1][590/1885]	Time 0.102 (0.143)	Data 2.14e-04 (5.74e-04)	Tok/s 88650 (96426)	Loss/tok 3.0546 (3.4264)	LR 2.800e-03
0: TRAIN [1][600/1885]	Time 0.102 (0.143)	Data 2.19e-04 (5.68e-04)	Tok/s 87541 (96340)	Loss/tok 3.1459 (3.4236)	LR 2.800e-03
0: TRAIN [1][610/1885]	Time 0.147 (0.143)	Data 2.15e-04 (5.62e-04)	Tok/s 98859 (96313)	Loss/tok 3.3197 (3.4221)	LR 2.800e-03
0: TRAIN [1][620/1885]	Time 0.101 (0.142)	Data 2.15e-04 (5.56e-04)	Tok/s 89504 (96254)	Loss/tok 3.0813 (3.4194)	LR 2.800e-03
0: TRAIN [1][630/1885]	Time 0.196 (0.142)	Data 8.73e-05 (5.51e-04)	Tok/s 103629 (96249)	Loss/tok 3.4891 (3.4187)	LR 2.800e-03
0: TRAIN [1][640/1885]	Time 0.194 (0.142)	Data 2.15e-04 (5.46e-04)	Tok/s 103452 (96185)	Loss/tok 3.6563 (3.4175)	LR 2.800e-03
0: TRAIN [1][650/1885]	Time 0.195 (0.142)	Data 2.17e-04 (5.40e-04)	Tok/s 106048 (96234)	Loss/tok 3.4371 (3.4187)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][660/1885]	Time 0.246 (0.143)	Data 2.17e-04 (5.35e-04)	Tok/s 105367 (96274)	Loss/tok 3.6730 (3.4196)	LR 2.800e-03
0: TRAIN [1][670/1885]	Time 0.246 (0.142)	Data 2.35e-04 (5.31e-04)	Tok/s 106783 (96261)	Loss/tok 3.8020 (3.4195)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][680/1885]	Time 0.194 (0.142)	Data 2.18e-04 (5.26e-04)	Tok/s 104984 (96260)	Loss/tok 3.5915 (3.4192)	LR 2.800e-03
0: TRAIN [1][690/1885]	Time 0.195 (0.143)	Data 2.17e-04 (5.22e-04)	Tok/s 102901 (96344)	Loss/tok 3.6707 (3.4222)	LR 2.800e-03
0: TRAIN [1][700/1885]	Time 0.101 (0.143)	Data 2.16e-04 (5.17e-04)	Tok/s 92423 (96299)	Loss/tok 3.1667 (3.4208)	LR 2.800e-03
0: TRAIN [1][710/1885]	Time 0.146 (0.143)	Data 2.18e-04 (5.13e-04)	Tok/s 101180 (96295)	Loss/tok 3.3606 (3.4200)	LR 2.800e-03
0: TRAIN [1][720/1885]	Time 0.147 (0.142)	Data 2.22e-04 (5.09e-04)	Tok/s 100261 (96257)	Loss/tok 3.3778 (3.4180)	LR 2.800e-03
0: TRAIN [1][730/1885]	Time 0.101 (0.143)	Data 2.19e-04 (5.05e-04)	Tok/s 90178 (96264)	Loss/tok 3.1170 (3.4184)	LR 2.800e-03
0: TRAIN [1][740/1885]	Time 0.101 (0.142)	Data 2.18e-04 (5.01e-04)	Tok/s 92090 (96204)	Loss/tok 3.2157 (3.4174)	LR 2.800e-03
0: TRAIN [1][750/1885]	Time 0.147 (0.142)	Data 2.28e-04 (4.98e-04)	Tok/s 99564 (96228)	Loss/tok 3.4064 (3.4180)	LR 2.800e-03
0: TRAIN [1][760/1885]	Time 0.245 (0.142)	Data 2.23e-04 (4.94e-04)	Tok/s 107221 (96237)	Loss/tok 3.6948 (3.4182)	LR 2.800e-03
0: TRAIN [1][770/1885]	Time 0.102 (0.142)	Data 2.21e-04 (4.91e-04)	Tok/s 89355 (96235)	Loss/tok 3.2020 (3.4175)	LR 2.800e-03
0: TRAIN [1][780/1885]	Time 0.147 (0.142)	Data 2.19e-04 (4.87e-04)	Tok/s 100423 (96229)	Loss/tok 3.2932 (3.4161)	LR 2.800e-03
0: TRAIN [1][790/1885]	Time 0.101 (0.142)	Data 2.15e-04 (4.84e-04)	Tok/s 90127 (96240)	Loss/tok 3.0864 (3.4156)	LR 2.800e-03
0: TRAIN [1][800/1885]	Time 0.147 (0.143)	Data 2.16e-04 (4.80e-04)	Tok/s 99729 (96296)	Loss/tok 3.4181 (3.4177)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][810/1885]	Time 0.146 (0.143)	Data 2.16e-04 (4.77e-04)	Tok/s 99811 (96340)	Loss/tok 3.3202 (3.4177)	LR 2.800e-03
0: TRAIN [1][820/1885]	Time 0.101 (0.143)	Data 2.19e-04 (4.74e-04)	Tok/s 90316 (96367)	Loss/tok 3.0223 (3.4180)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][830/1885]	Time 0.245 (0.143)	Data 2.18e-04 (4.71e-04)	Tok/s 107685 (96414)	Loss/tok 3.5445 (3.4200)	LR 2.800e-03
0: TRAIN [1][840/1885]	Time 0.247 (0.143)	Data 2.19e-04 (4.68e-04)	Tok/s 106295 (96392)	Loss/tok 3.6673 (3.4194)	LR 2.800e-03
0: TRAIN [1][850/1885]	Time 0.245 (0.144)	Data 2.19e-04 (4.65e-04)	Tok/s 105738 (96428)	Loss/tok 3.7321 (3.4198)	LR 2.800e-03
0: TRAIN [1][860/1885]	Time 0.147 (0.144)	Data 2.18e-04 (4.62e-04)	Tok/s 97911 (96444)	Loss/tok 3.4299 (3.4197)	LR 2.800e-03
0: TRAIN [1][870/1885]	Time 0.101 (0.144)	Data 2.18e-04 (4.60e-04)	Tok/s 88716 (96441)	Loss/tok 3.1941 (3.4194)	LR 2.800e-03
0: TRAIN [1][880/1885]	Time 0.195 (0.144)	Data 2.23e-04 (4.57e-04)	Tok/s 106017 (96444)	Loss/tok 3.5439 (3.4196)	LR 2.800e-03
0: TRAIN [1][890/1885]	Time 0.147 (0.144)	Data 2.29e-04 (4.54e-04)	Tok/s 98793 (96469)	Loss/tok 3.3980 (3.4187)	LR 2.800e-03
0: TRAIN [1][900/1885]	Time 0.101 (0.144)	Data 2.17e-04 (4.52e-04)	Tok/s 90446 (96447)	Loss/tok 3.1955 (3.4180)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][910/1885]	Time 0.146 (0.144)	Data 3.04e-04 (4.49e-04)	Tok/s 101411 (96486)	Loss/tok 3.4282 (3.4181)	LR 2.800e-03
0: TRAIN [1][920/1885]	Time 0.101 (0.144)	Data 2.19e-04 (4.47e-04)	Tok/s 90702 (96477)	Loss/tok 3.1372 (3.4184)	LR 2.800e-03
0: TRAIN [1][930/1885]	Time 0.102 (0.144)	Data 2.18e-04 (4.44e-04)	Tok/s 88376 (96527)	Loss/tok 3.1233 (3.4195)	LR 2.800e-03
0: TRAIN [1][940/1885]	Time 0.193 (0.144)	Data 2.22e-04 (4.42e-04)	Tok/s 104932 (96549)	Loss/tok 3.5093 (3.4194)	LR 2.800e-03
0: TRAIN [1][950/1885]	Time 0.194 (0.144)	Data 2.19e-04 (4.40e-04)	Tok/s 104262 (96524)	Loss/tok 3.5413 (3.4183)	LR 2.800e-03
0: TRAIN [1][960/1885]	Time 0.101 (0.144)	Data 2.18e-04 (4.37e-04)	Tok/s 89752 (96512)	Loss/tok 3.2806 (3.4176)	LR 2.800e-03
0: TRAIN [1][970/1885]	Time 0.147 (0.144)	Data 2.19e-04 (4.35e-04)	Tok/s 99104 (96506)	Loss/tok 3.2710 (3.4170)	LR 2.800e-03
0: TRAIN [1][980/1885]	Time 0.101 (0.144)	Data 2.16e-04 (4.33e-04)	Tok/s 89668 (96486)	Loss/tok 3.0667 (3.4161)	LR 2.800e-03
0: TRAIN [1][990/1885]	Time 0.102 (0.144)	Data 2.23e-04 (4.31e-04)	Tok/s 88541 (96494)	Loss/tok 3.2837 (3.4151)	LR 2.800e-03
0: TRAIN [1][1000/1885]	Time 0.193 (0.144)	Data 2.16e-04 (4.29e-04)	Tok/s 105962 (96489)	Loss/tok 3.4936 (3.4140)	LR 2.800e-03
0: TRAIN [1][1010/1885]	Time 0.147 (0.144)	Data 2.17e-04 (4.26e-04)	Tok/s 100895 (96492)	Loss/tok 3.2728 (3.4136)	LR 2.800e-03
0: TRAIN [1][1020/1885]	Time 0.101 (0.144)	Data 2.17e-04 (4.24e-04)	Tok/s 88819 (96493)	Loss/tok 3.0526 (3.4130)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1030/1885]	Time 0.148 (0.144)	Data 2.20e-04 (4.22e-04)	Tok/s 99697 (96516)	Loss/tok 3.3156 (3.4134)	LR 2.800e-03
0: TRAIN [1][1040/1885]	Time 0.195 (0.144)	Data 2.22e-04 (4.21e-04)	Tok/s 105624 (96564)	Loss/tok 3.5217 (3.4147)	LR 2.800e-03
0: TRAIN [1][1050/1885]	Time 0.147 (0.144)	Data 2.23e-04 (4.19e-04)	Tok/s 98240 (96594)	Loss/tok 3.2764 (3.4150)	LR 2.800e-03
0: TRAIN [1][1060/1885]	Time 0.194 (0.144)	Data 2.21e-04 (4.17e-04)	Tok/s 104874 (96594)	Loss/tok 3.4800 (3.4141)	LR 2.800e-03
0: TRAIN [1][1070/1885]	Time 0.146 (0.144)	Data 2.17e-04 (4.15e-04)	Tok/s 99352 (96618)	Loss/tok 3.4923 (3.4149)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1080/1885]	Time 0.101 (0.144)	Data 2.19e-04 (4.13e-04)	Tok/s 88785 (96611)	Loss/tok 3.2732 (3.4145)	LR 2.800e-03
0: TRAIN [1][1090/1885]	Time 0.246 (0.144)	Data 2.16e-04 (4.12e-04)	Tok/s 105817 (96619)	Loss/tok 3.6844 (3.4141)	LR 2.800e-03
0: TRAIN [1][1100/1885]	Time 0.195 (0.144)	Data 2.18e-04 (4.10e-04)	Tok/s 104058 (96624)	Loss/tok 3.6342 (3.4142)	LR 2.800e-03
0: TRAIN [1][1110/1885]	Time 0.059 (0.145)	Data 3.12e-04 (4.08e-04)	Tok/s 78504 (96640)	Loss/tok 2.5172 (3.4150)	LR 2.800e-03
0: TRAIN [1][1120/1885]	Time 0.147 (0.145)	Data 2.19e-04 (4.06e-04)	Tok/s 100753 (96662)	Loss/tok 3.2108 (3.4150)	LR 2.800e-03
0: TRAIN [1][1130/1885]	Time 0.059 (0.145)	Data 2.19e-04 (4.05e-04)	Tok/s 78899 (96630)	Loss/tok 2.5970 (3.4142)	LR 2.800e-03
0: TRAIN [1][1140/1885]	Time 0.146 (0.144)	Data 2.20e-04 (4.03e-04)	Tok/s 101888 (96603)	Loss/tok 3.3457 (3.4131)	LR 2.800e-03
0: TRAIN [1][1150/1885]	Time 0.103 (0.144)	Data 2.25e-04 (4.02e-04)	Tok/s 89673 (96597)	Loss/tok 3.0561 (3.4123)	LR 2.800e-03
0: TRAIN [1][1160/1885]	Time 0.102 (0.144)	Data 2.19e-04 (4.00e-04)	Tok/s 89164 (96585)	Loss/tok 3.0825 (3.4117)	LR 2.800e-03
0: TRAIN [1][1170/1885]	Time 0.101 (0.144)	Data 2.20e-04 (3.99e-04)	Tok/s 88524 (96607)	Loss/tok 3.2198 (3.4119)	LR 2.800e-03
0: TRAIN [1][1180/1885]	Time 0.148 (0.144)	Data 2.18e-04 (3.97e-04)	Tok/s 98163 (96627)	Loss/tok 3.2919 (3.4118)	LR 2.800e-03
0: TRAIN [1][1190/1885]	Time 0.147 (0.144)	Data 3.06e-04 (3.96e-04)	Tok/s 98690 (96636)	Loss/tok 3.2302 (3.4108)	LR 2.800e-03
0: TRAIN [1][1200/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.94e-04)	Tok/s 100190 (96656)	Loss/tok 3.3569 (3.4109)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1210/1885]	Time 0.102 (0.144)	Data 2.17e-04 (3.93e-04)	Tok/s 88889 (96669)	Loss/tok 3.1368 (3.4104)	LR 2.800e-03
0: TRAIN [1][1220/1885]	Time 0.101 (0.144)	Data 2.16e-04 (3.91e-04)	Tok/s 89449 (96653)	Loss/tok 3.1701 (3.4096)	LR 2.800e-03
0: TRAIN [1][1230/1885]	Time 0.195 (0.145)	Data 2.18e-04 (3.90e-04)	Tok/s 102397 (96679)	Loss/tok 3.5563 (3.4102)	LR 2.800e-03
0: TRAIN [1][1240/1885]	Time 0.102 (0.144)	Data 2.25e-04 (3.89e-04)	Tok/s 88789 (96667)	Loss/tok 3.1288 (3.4100)	LR 2.800e-03
0: TRAIN [1][1250/1885]	Time 0.101 (0.144)	Data 2.27e-04 (3.87e-04)	Tok/s 89662 (96631)	Loss/tok 3.2139 (3.4091)	LR 2.800e-03
0: TRAIN [1][1260/1885]	Time 0.102 (0.144)	Data 3.09e-04 (3.86e-04)	Tok/s 90482 (96645)	Loss/tok 3.1173 (3.4086)	LR 2.800e-03
0: TRAIN [1][1270/1885]	Time 0.101 (0.144)	Data 2.17e-04 (3.85e-04)	Tok/s 90976 (96647)	Loss/tok 3.1086 (3.4084)	LR 2.800e-03
0: TRAIN [1][1280/1885]	Time 0.101 (0.144)	Data 2.22e-04 (3.84e-04)	Tok/s 89360 (96639)	Loss/tok 3.1177 (3.4080)	LR 2.800e-03
0: TRAIN [1][1290/1885]	Time 0.245 (0.144)	Data 2.17e-04 (3.82e-04)	Tok/s 107345 (96622)	Loss/tok 3.6167 (3.4076)	LR 2.800e-03
0: TRAIN [1][1300/1885]	Time 0.102 (0.144)	Data 2.20e-04 (3.81e-04)	Tok/s 89032 (96627)	Loss/tok 3.1247 (3.4071)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1310/1885]	Time 0.195 (0.144)	Data 2.23e-04 (3.80e-04)	Tok/s 104202 (96645)	Loss/tok 3.5062 (3.4074)	LR 2.800e-03
0: TRAIN [1][1320/1885]	Time 0.146 (0.144)	Data 2.26e-04 (3.79e-04)	Tok/s 101071 (96658)	Loss/tok 3.4664 (3.4073)	LR 2.800e-03
0: TRAIN [1][1330/1885]	Time 0.060 (0.144)	Data 2.18e-04 (3.78e-04)	Tok/s 76389 (96638)	Loss/tok 2.6238 (3.4072)	LR 2.800e-03
0: TRAIN [1][1340/1885]	Time 0.102 (0.144)	Data 3.27e-04 (3.77e-04)	Tok/s 89297 (96612)	Loss/tok 3.1111 (3.4065)	LR 2.800e-03
0: TRAIN [1][1350/1885]	Time 0.146 (0.144)	Data 2.94e-04 (3.75e-04)	Tok/s 101684 (96575)	Loss/tok 3.2915 (3.4057)	LR 2.800e-03
0: TRAIN [1][1360/1885]	Time 0.102 (0.144)	Data 2.19e-04 (3.74e-04)	Tok/s 87462 (96562)	Loss/tok 3.0852 (3.4055)	LR 2.800e-03
0: TRAIN [1][1370/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.73e-04)	Tok/s 90293 (96531)	Loss/tok 3.0384 (3.4042)	LR 2.800e-03
0: TRAIN [1][1380/1885]	Time 0.101 (0.144)	Data 2.60e-04 (3.72e-04)	Tok/s 91192 (96511)	Loss/tok 3.0404 (3.4035)	LR 2.800e-03
0: TRAIN [1][1390/1885]	Time 0.245 (0.144)	Data 2.17e-04 (3.71e-04)	Tok/s 105746 (96522)	Loss/tok 3.7000 (3.4037)	LR 2.800e-03
0: TRAIN [1][1400/1885]	Time 0.146 (0.144)	Data 2.15e-04 (3.70e-04)	Tok/s 100241 (96504)	Loss/tok 3.4299 (3.4029)	LR 2.800e-03
0: TRAIN [1][1410/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.69e-04)	Tok/s 100079 (96510)	Loss/tok 3.3697 (3.4033)	LR 2.800e-03
0: TRAIN [1][1420/1885]	Time 0.102 (0.143)	Data 2.47e-04 (3.68e-04)	Tok/s 88184 (96468)	Loss/tok 3.0936 (3.4025)	LR 2.800e-03
0: TRAIN [1][1430/1885]	Time 0.145 (0.144)	Data 2.19e-04 (3.67e-04)	Tok/s 101256 (96492)	Loss/tok 3.2506 (3.4032)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1440/1885]	Time 0.102 (0.144)	Data 2.18e-04 (3.66e-04)	Tok/s 89215 (96497)	Loss/tok 3.1603 (3.4028)	LR 2.800e-03
0: TRAIN [1][1450/1885]	Time 0.101 (0.144)	Data 2.16e-04 (3.65e-04)	Tok/s 90705 (96511)	Loss/tok 3.1116 (3.4023)	LR 2.800e-03
0: TRAIN [1][1460/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.64e-04)	Tok/s 90628 (96496)	Loss/tok 2.9666 (3.4018)	LR 2.800e-03
0: TRAIN [1][1470/1885]	Time 0.194 (0.144)	Data 2.20e-04 (3.63e-04)	Tok/s 104848 (96528)	Loss/tok 3.4342 (3.4021)	LR 2.800e-03
0: TRAIN [1][1480/1885]	Time 0.059 (0.144)	Data 2.22e-04 (3.62e-04)	Tok/s 76148 (96519)	Loss/tok 2.6603 (3.4017)	LR 2.800e-03
0: TRAIN [1][1490/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.61e-04)	Tok/s 88354 (96506)	Loss/tok 3.0501 (3.4016)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1500/1885]	Time 0.246 (0.144)	Data 2.16e-04 (3.60e-04)	Tok/s 106222 (96492)	Loss/tok 3.6652 (3.4014)	LR 2.800e-03
0: TRAIN [1][1510/1885]	Time 0.101 (0.144)	Data 2.16e-04 (3.59e-04)	Tok/s 89478 (96482)	Loss/tok 3.0279 (3.4008)	LR 2.800e-03
0: TRAIN [1][1520/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.58e-04)	Tok/s 99989 (96459)	Loss/tok 3.3962 (3.4000)	LR 2.800e-03
0: TRAIN [1][1530/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.57e-04)	Tok/s 88557 (96424)	Loss/tok 3.0579 (3.3993)	LR 2.800e-03
0: TRAIN [1][1540/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.56e-04)	Tok/s 100059 (96424)	Loss/tok 3.4026 (3.3991)	LR 2.800e-03
0: TRAIN [1][1550/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.55e-04)	Tok/s 89316 (96415)	Loss/tok 3.1792 (3.3989)	LR 2.800e-03
0: TRAIN [1][1560/1885]	Time 0.194 (0.143)	Data 2.20e-04 (3.55e-04)	Tok/s 105564 (96441)	Loss/tok 3.5131 (3.3995)	LR 2.800e-03
0: TRAIN [1][1570/1885]	Time 0.195 (0.143)	Data 2.18e-04 (3.54e-04)	Tok/s 105501 (96436)	Loss/tok 3.4720 (3.3989)	LR 2.800e-03
0: TRAIN [1][1580/1885]	Time 0.195 (0.143)	Data 2.32e-04 (3.53e-04)	Tok/s 105466 (96411)	Loss/tok 3.3026 (3.3979)	LR 2.800e-03
0: TRAIN [1][1590/1885]	Time 0.148 (0.143)	Data 2.17e-04 (3.52e-04)	Tok/s 98591 (96432)	Loss/tok 3.3015 (3.3984)	LR 2.800e-03
0: TRAIN [1][1600/1885]	Time 0.059 (0.143)	Data 2.17e-04 (3.51e-04)	Tok/s 78422 (96434)	Loss/tok 2.6189 (3.3982)	LR 2.800e-03
0: TRAIN [1][1610/1885]	Time 0.194 (0.143)	Data 2.24e-04 (3.50e-04)	Tok/s 105215 (96436)	Loss/tok 3.6484 (3.3981)	LR 2.800e-03
0: TRAIN [1][1620/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.50e-04)	Tok/s 90274 (96438)	Loss/tok 3.0627 (3.3975)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1630/1885]	Time 0.245 (0.143)	Data 2.20e-04 (3.49e-04)	Tok/s 106459 (96393)	Loss/tok 3.7506 (3.3970)	LR 2.800e-03
0: TRAIN [1][1640/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.48e-04)	Tok/s 90145 (96377)	Loss/tok 3.1607 (3.3965)	LR 2.800e-03
0: TRAIN [1][1650/1885]	Time 0.195 (0.143)	Data 2.19e-04 (3.47e-04)	Tok/s 104283 (96390)	Loss/tok 3.4890 (3.3965)	LR 2.800e-03
0: TRAIN [1][1660/1885]	Time 0.197 (0.143)	Data 2.01e-04 (3.46e-04)	Tok/s 103859 (96399)	Loss/tok 3.4571 (3.3967)	LR 2.800e-03
0: TRAIN [1][1670/1885]	Time 0.102 (0.143)	Data 1.79e-04 (3.46e-04)	Tok/s 91279 (96408)	Loss/tok 3.1244 (3.3965)	LR 2.800e-03
0: TRAIN [1][1680/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.45e-04)	Tok/s 90857 (96397)	Loss/tok 3.0650 (3.3958)	LR 2.800e-03
0: TRAIN [1][1690/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.44e-04)	Tok/s 89899 (96390)	Loss/tok 2.9961 (3.3956)	LR 2.800e-03
0: TRAIN [1][1700/1885]	Time 0.195 (0.143)	Data 2.21e-04 (3.43e-04)	Tok/s 106312 (96394)	Loss/tok 3.4360 (3.3950)	LR 2.800e-03
0: TRAIN [1][1710/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.43e-04)	Tok/s 88901 (96385)	Loss/tok 3.1398 (3.3943)	LR 2.800e-03
0: TRAIN [1][1720/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.42e-04)	Tok/s 100861 (96384)	Loss/tok 3.2664 (3.3937)	LR 2.800e-03
0: TRAIN [1][1730/1885]	Time 0.194 (0.143)	Data 2.30e-04 (3.41e-04)	Tok/s 106484 (96404)	Loss/tok 3.3230 (3.3932)	LR 2.800e-03
0: TRAIN [1][1740/1885]	Time 0.147 (0.143)	Data 2.24e-04 (3.41e-04)	Tok/s 99857 (96368)	Loss/tok 3.2566 (3.3923)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1750/1885]	Time 0.148 (0.143)	Data 2.18e-04 (3.40e-04)	Tok/s 99720 (96356)	Loss/tok 3.1640 (3.3918)	LR 2.800e-03
0: TRAIN [1][1760/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.39e-04)	Tok/s 89567 (96333)	Loss/tok 3.1662 (3.3907)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1770/1885]	Time 0.101 (0.143)	Data 3.12e-04 (3.39e-04)	Tok/s 89407 (96326)	Loss/tok 3.0194 (3.3904)	LR 2.800e-03
0: TRAIN [1][1780/1885]	Time 0.244 (0.143)	Data 2.23e-04 (3.38e-04)	Tok/s 107523 (96347)	Loss/tok 3.5612 (3.3908)	LR 2.800e-03
0: TRAIN [1][1790/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.37e-04)	Tok/s 89926 (96336)	Loss/tok 3.0034 (3.3904)	LR 2.800e-03
0: TRAIN [1][1800/1885]	Time 0.193 (0.143)	Data 2.18e-04 (3.37e-04)	Tok/s 107204 (96343)	Loss/tok 3.4630 (3.3902)	LR 2.800e-03
0: TRAIN [1][1810/1885]	Time 0.147 (0.143)	Data 2.17e-04 (3.36e-04)	Tok/s 100241 (96343)	Loss/tok 3.3799 (3.3901)	LR 2.800e-03
0: TRAIN [1][1820/1885]	Time 0.145 (0.143)	Data 2.97e-04 (3.35e-04)	Tok/s 99933 (96343)	Loss/tok 3.2885 (3.3897)	LR 2.800e-03
0: TRAIN [1][1830/1885]	Time 0.193 (0.143)	Data 2.15e-04 (3.35e-04)	Tok/s 106355 (96359)	Loss/tok 3.4514 (3.3895)	LR 2.800e-03
0: TRAIN [1][1840/1885]	Time 0.245 (0.143)	Data 2.21e-04 (3.34e-04)	Tok/s 106135 (96359)	Loss/tok 3.6282 (3.3894)	LR 2.800e-03
0: TRAIN [1][1850/1885]	Time 0.147 (0.143)	Data 1.89e-04 (3.34e-04)	Tok/s 99418 (96369)	Loss/tok 3.3743 (3.3893)	LR 2.800e-03
0: TRAIN [1][1860/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.33e-04)	Tok/s 88115 (96371)	Loss/tok 3.0758 (3.3891)	LR 2.800e-03
0: TRAIN [1][1870/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.33e-04)	Tok/s 89450 (96368)	Loss/tok 3.1205 (3.3888)	LR 2.800e-03
0: TRAIN [1][1880/1885]	Time 0.244 (0.143)	Data 2.17e-04 (3.32e-04)	Tok/s 106301 (96360)	Loss/tok 3.6237 (3.3885)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1594175042688, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1594175042689, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.657 (0.657)	Decoder iters 133.0 (133.0)	Tok/s 24959 (24959)
0: Running moses detokenizer
0: BLEU(score=22.07758644593491, counts=[35803, 17221, 9482, 5474], totals=[65178, 62175, 59172, 56175], precisions=[54.931111724815125, 27.697627663852032, 16.02447103359697, 9.744548286604362], bp=1.0, sys_len=65178, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1594175044372, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2208, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1594175044372, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3877	Test BLEU: 22.08
0: Performance: Epoch: 1	Training: 770794 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1594175044372, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1594175044373, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1594175044373, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3730515442
0: TRAIN [2][0/1885]	Time 0.467 (0.467)	Data 2.26e-01 (2.26e-01)	Tok/s 56203 (56203)	Loss/tok 3.4197 (3.4197)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][10/1885]	Time 0.102 (0.160)	Data 2.13e-04 (2.07e-02)	Tok/s 89495 (91005)	Loss/tok 3.1010 (3.2511)	LR 2.800e-03
0: TRAIN [2][20/1885]	Time 0.194 (0.145)	Data 2.13e-04 (1.09e-02)	Tok/s 106132 (92457)	Loss/tok 3.3934 (3.2101)	LR 2.800e-03
0: TRAIN [2][30/1885]	Time 0.102 (0.145)	Data 2.17e-04 (7.48e-03)	Tok/s 89994 (94004)	Loss/tok 2.8434 (3.1937)	LR 2.800e-03
0: TRAIN [2][40/1885]	Time 0.195 (0.147)	Data 2.14e-04 (5.71e-03)	Tok/s 104263 (95081)	Loss/tok 3.3617 (3.2119)	LR 2.800e-03
0: TRAIN [2][50/1885]	Time 0.194 (0.142)	Data 2.16e-04 (4.63e-03)	Tok/s 105084 (94857)	Loss/tok 3.3540 (3.2007)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][60/1885]	Time 0.146 (0.148)	Data 1.94e-04 (3.91e-03)	Tok/s 99886 (95950)	Loss/tok 3.2453 (3.2249)	LR 2.800e-03
0: TRAIN [2][70/1885]	Time 0.148 (0.145)	Data 2.12e-04 (3.39e-03)	Tok/s 98852 (95637)	Loss/tok 3.2429 (3.2146)	LR 2.800e-03
0: TRAIN [2][80/1885]	Time 0.147 (0.145)	Data 2.17e-04 (3.00e-03)	Tok/s 98866 (95714)	Loss/tok 3.1455 (3.2198)	LR 2.800e-03
0: TRAIN [2][90/1885]	Time 0.195 (0.147)	Data 2.15e-04 (2.69e-03)	Tok/s 105288 (96153)	Loss/tok 3.2791 (3.2277)	LR 2.800e-03
0: TRAIN [2][100/1885]	Time 0.101 (0.149)	Data 2.14e-04 (2.45e-03)	Tok/s 89386 (96658)	Loss/tok 2.8104 (3.2449)	LR 2.800e-03
0: TRAIN [2][110/1885]	Time 0.102 (0.147)	Data 2.14e-04 (2.24e-03)	Tok/s 89046 (96369)	Loss/tok 3.0306 (3.2406)	LR 2.800e-03
0: TRAIN [2][120/1885]	Time 0.195 (0.148)	Data 2.12e-04 (2.08e-03)	Tok/s 104865 (96517)	Loss/tok 3.3860 (3.2487)	LR 2.800e-03
0: TRAIN [2][130/1885]	Time 0.194 (0.148)	Data 2.14e-04 (1.93e-03)	Tok/s 104008 (96483)	Loss/tok 3.2887 (3.2530)	LR 2.800e-03
0: TRAIN [2][140/1885]	Time 0.060 (0.147)	Data 2.15e-04 (1.81e-03)	Tok/s 76509 (96322)	Loss/tok 2.4813 (3.2503)	LR 2.800e-03
0: TRAIN [2][150/1885]	Time 0.101 (0.145)	Data 2.13e-04 (1.71e-03)	Tok/s 90774 (96106)	Loss/tok 3.0282 (3.2478)	LR 2.800e-03
0: TRAIN [2][160/1885]	Time 0.060 (0.143)	Data 2.14e-04 (1.61e-03)	Tok/s 75505 (95780)	Loss/tok 2.3373 (3.2416)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][170/1885]	Time 0.146 (0.144)	Data 2.14e-04 (1.53e-03)	Tok/s 98916 (96045)	Loss/tok 3.2894 (3.2452)	LR 2.800e-03
0: TRAIN [2][180/1885]	Time 0.195 (0.144)	Data 2.14e-04 (1.46e-03)	Tok/s 105612 (96020)	Loss/tok 3.4617 (3.2460)	LR 2.800e-03
0: TRAIN [2][190/1885]	Time 0.103 (0.143)	Data 2.15e-04 (1.39e-03)	Tok/s 88303 (95913)	Loss/tok 2.9582 (3.2431)	LR 2.800e-03
0: TRAIN [2][200/1885]	Time 0.147 (0.142)	Data 2.13e-04 (1.33e-03)	Tok/s 99913 (95854)	Loss/tok 3.1889 (3.2411)	LR 2.800e-03
0: TRAIN [2][210/1885]	Time 0.101 (0.143)	Data 2.13e-04 (1.28e-03)	Tok/s 90967 (96041)	Loss/tok 3.0707 (3.2460)	LR 2.800e-03
0: TRAIN [2][220/1885]	Time 0.102 (0.143)	Data 2.13e-04 (1.23e-03)	Tok/s 87689 (96063)	Loss/tok 3.1381 (3.2460)	LR 2.800e-03
0: TRAIN [2][230/1885]	Time 0.194 (0.144)	Data 2.02e-04 (1.19e-03)	Tok/s 104503 (96196)	Loss/tok 3.4065 (3.2458)	LR 2.800e-03
0: TRAIN [2][240/1885]	Time 0.102 (0.144)	Data 2.16e-04 (1.15e-03)	Tok/s 89476 (96195)	Loss/tok 2.9594 (3.2467)	LR 2.800e-03
0: TRAIN [2][250/1885]	Time 0.147 (0.143)	Data 2.16e-04 (1.11e-03)	Tok/s 99096 (96224)	Loss/tok 3.1161 (3.2435)	LR 2.800e-03
0: TRAIN [2][260/1885]	Time 0.101 (0.142)	Data 2.15e-04 (1.08e-03)	Tok/s 90652 (96115)	Loss/tok 3.0534 (3.2407)	LR 2.800e-03
0: TRAIN [2][270/1885]	Time 0.148 (0.142)	Data 2.17e-04 (1.04e-03)	Tok/s 100013 (96125)	Loss/tok 3.2241 (3.2415)	LR 2.800e-03
0: TRAIN [2][280/1885]	Time 0.194 (0.143)	Data 2.13e-04 (1.01e-03)	Tok/s 105741 (96292)	Loss/tok 3.4418 (3.2449)	LR 2.800e-03
0: TRAIN [2][290/1885]	Time 0.147 (0.144)	Data 2.31e-04 (9.87e-04)	Tok/s 99741 (96366)	Loss/tok 3.2272 (3.2470)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][300/1885]	Time 0.145 (0.143)	Data 2.20e-04 (9.62e-04)	Tok/s 98966 (96393)	Loss/tok 3.2789 (3.2468)	LR 2.800e-03
0: TRAIN [2][310/1885]	Time 0.060 (0.143)	Data 2.17e-04 (9.38e-04)	Tok/s 74637 (96140)	Loss/tok 2.5645 (3.2445)	LR 2.800e-03
0: TRAIN [2][320/1885]	Time 0.145 (0.142)	Data 2.15e-04 (9.15e-04)	Tok/s 100944 (96128)	Loss/tok 3.2610 (3.2440)	LR 2.800e-03
0: TRAIN [2][330/1885]	Time 0.102 (0.143)	Data 2.25e-04 (8.94e-04)	Tok/s 90776 (96237)	Loss/tok 3.1660 (3.2459)	LR 2.800e-03
0: TRAIN [2][340/1885]	Time 0.193 (0.143)	Data 2.17e-04 (8.74e-04)	Tok/s 104617 (96357)	Loss/tok 3.4215 (3.2496)	LR 2.800e-03
0: TRAIN [2][350/1885]	Time 0.102 (0.143)	Data 2.18e-04 (8.55e-04)	Tok/s 91074 (96355)	Loss/tok 3.0752 (3.2489)	LR 2.800e-03
0: TRAIN [2][360/1885]	Time 0.060 (0.142)	Data 2.15e-04 (8.38e-04)	Tok/s 77802 (96185)	Loss/tok 2.6477 (3.2463)	LR 2.800e-03
0: TRAIN [2][370/1885]	Time 0.101 (0.142)	Data 2.14e-04 (8.21e-04)	Tok/s 90761 (96154)	Loss/tok 2.9457 (3.2462)	LR 2.800e-03
0: TRAIN [2][380/1885]	Time 0.146 (0.142)	Data 2.12e-04 (8.05e-04)	Tok/s 99459 (96149)	Loss/tok 3.2698 (3.2465)	LR 2.800e-03
0: TRAIN [2][390/1885]	Time 0.193 (0.142)	Data 2.16e-04 (7.90e-04)	Tok/s 105641 (96217)	Loss/tok 3.4191 (3.2469)	LR 2.800e-03
0: TRAIN [2][400/1885]	Time 0.193 (0.142)	Data 2.19e-04 (7.75e-04)	Tok/s 106030 (96232)	Loss/tok 3.3462 (3.2471)	LR 2.800e-03
0: TRAIN [2][410/1885]	Time 0.060 (0.142)	Data 2.13e-04 (7.62e-04)	Tok/s 76507 (96246)	Loss/tok 2.5654 (3.2471)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][420/1885]	Time 0.246 (0.143)	Data 2.30e-04 (7.49e-04)	Tok/s 105438 (96315)	Loss/tok 3.6102 (3.2490)	LR 2.800e-03
0: TRAIN [2][430/1885]	Time 0.197 (0.143)	Data 2.14e-04 (7.36e-04)	Tok/s 104920 (96348)	Loss/tok 3.3659 (3.2498)	LR 2.800e-03
0: TRAIN [2][440/1885]	Time 0.060 (0.143)	Data 2.14e-04 (7.24e-04)	Tok/s 77396 (96342)	Loss/tok 2.5892 (3.2504)	LR 2.800e-03
0: TRAIN [2][450/1885]	Time 0.146 (0.143)	Data 2.14e-04 (7.13e-04)	Tok/s 101991 (96399)	Loss/tok 3.2488 (3.2520)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][460/1885]	Time 0.060 (0.144)	Data 2.35e-04 (7.02e-04)	Tok/s 76965 (96431)	Loss/tok 2.6136 (3.2547)	LR 2.800e-03
0: TRAIN [2][470/1885]	Time 0.196 (0.144)	Data 2.14e-04 (6.92e-04)	Tok/s 104095 (96511)	Loss/tok 3.2952 (3.2571)	LR 2.800e-03
0: TRAIN [2][480/1885]	Time 0.147 (0.144)	Data 2.17e-04 (6.82e-04)	Tok/s 100125 (96519)	Loss/tok 3.2567 (3.2587)	LR 2.800e-03
0: TRAIN [2][490/1885]	Time 0.194 (0.144)	Data 2.14e-04 (6.72e-04)	Tok/s 105538 (96529)	Loss/tok 3.2921 (3.2584)	LR 2.800e-03
0: TRAIN [2][500/1885]	Time 0.148 (0.144)	Data 2.22e-04 (6.63e-04)	Tok/s 98436 (96525)	Loss/tok 3.2471 (3.2578)	LR 2.800e-03
0: TRAIN [2][510/1885]	Time 0.244 (0.144)	Data 2.18e-04 (6.55e-04)	Tok/s 106107 (96535)	Loss/tok 3.6272 (3.2608)	LR 2.800e-03
0: TRAIN [2][520/1885]	Time 0.194 (0.144)	Data 2.28e-04 (6.46e-04)	Tok/s 104952 (96542)	Loss/tok 3.3658 (3.2607)	LR 2.800e-03
0: TRAIN [2][530/1885]	Time 0.146 (0.145)	Data 2.16e-04 (6.38e-04)	Tok/s 100387 (96602)	Loss/tok 3.2950 (3.2622)	LR 2.800e-03
0: TRAIN [2][540/1885]	Time 0.245 (0.145)	Data 2.16e-04 (6.30e-04)	Tok/s 106374 (96607)	Loss/tok 3.5708 (3.2629)	LR 2.800e-03
0: TRAIN [2][550/1885]	Time 0.102 (0.144)	Data 2.26e-04 (6.23e-04)	Tok/s 88173 (96559)	Loss/tok 3.0034 (3.2620)	LR 2.800e-03
0: TRAIN [2][560/1885]	Time 0.146 (0.144)	Data 2.15e-04 (6.16e-04)	Tok/s 99930 (96523)	Loss/tok 3.2581 (3.2609)	LR 2.800e-03
0: TRAIN [2][570/1885]	Time 0.148 (0.144)	Data 2.16e-04 (6.08e-04)	Tok/s 99059 (96483)	Loss/tok 3.2128 (3.2597)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][580/1885]	Time 0.148 (0.144)	Data 2.16e-04 (6.02e-04)	Tok/s 100222 (96443)	Loss/tok 3.0853 (3.2586)	LR 2.800e-03
0: TRAIN [2][590/1885]	Time 0.101 (0.143)	Data 2.13e-04 (5.95e-04)	Tok/s 90002 (96416)	Loss/tok 2.8213 (3.2573)	LR 2.800e-03
0: TRAIN [2][600/1885]	Time 0.102 (0.144)	Data 1.77e-04 (5.89e-04)	Tok/s 87694 (96423)	Loss/tok 2.9551 (3.2583)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][610/1885]	Time 0.148 (0.144)	Data 1.97e-04 (5.82e-04)	Tok/s 99008 (96405)	Loss/tok 3.1958 (3.2581)	LR 2.800e-03
0: TRAIN [2][620/1885]	Time 0.148 (0.143)	Data 2.17e-04 (5.76e-04)	Tok/s 98662 (96431)	Loss/tok 3.2199 (3.2575)	LR 2.800e-03
0: TRAIN [2][630/1885]	Time 0.101 (0.143)	Data 2.15e-04 (5.71e-04)	Tok/s 90020 (96412)	Loss/tok 2.9361 (3.2576)	LR 2.800e-03
0: TRAIN [2][640/1885]	Time 0.059 (0.143)	Data 2.15e-04 (5.65e-04)	Tok/s 76544 (96396)	Loss/tok 2.4426 (3.2572)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][650/1885]	Time 0.147 (0.144)	Data 2.14e-04 (5.60e-04)	Tok/s 100144 (96454)	Loss/tok 3.2032 (3.2588)	LR 2.800e-03
0: TRAIN [2][660/1885]	Time 0.102 (0.144)	Data 2.01e-04 (5.55e-04)	Tok/s 88585 (96495)	Loss/tok 3.0159 (3.2606)	LR 2.800e-03
0: TRAIN [2][670/1885]	Time 0.148 (0.144)	Data 2.02e-04 (5.49e-04)	Tok/s 99191 (96557)	Loss/tok 3.2841 (3.2616)	LR 2.800e-03
0: TRAIN [2][680/1885]	Time 0.100 (0.144)	Data 2.15e-04 (5.45e-04)	Tok/s 91652 (96537)	Loss/tok 3.0852 (3.2620)	LR 2.800e-03
0: TRAIN [2][690/1885]	Time 0.146 (0.144)	Data 2.13e-04 (5.40e-04)	Tok/s 100088 (96472)	Loss/tok 3.2272 (3.2604)	LR 2.800e-03
0: TRAIN [2][700/1885]	Time 0.061 (0.143)	Data 2.12e-04 (5.35e-04)	Tok/s 75379 (96373)	Loss/tok 2.5490 (3.2599)	LR 2.800e-03
0: TRAIN [2][710/1885]	Time 0.146 (0.143)	Data 2.15e-04 (5.31e-04)	Tok/s 101466 (96349)	Loss/tok 3.2102 (3.2596)	LR 2.800e-03
0: TRAIN [2][720/1885]	Time 0.146 (0.143)	Data 2.18e-04 (5.26e-04)	Tok/s 100385 (96305)	Loss/tok 3.3335 (3.2583)	LR 2.800e-03
0: TRAIN [2][730/1885]	Time 0.102 (0.143)	Data 2.15e-04 (5.22e-04)	Tok/s 88146 (96239)	Loss/tok 2.8834 (3.2570)	LR 2.800e-03
0: TRAIN [2][740/1885]	Time 0.194 (0.143)	Data 2.16e-04 (5.18e-04)	Tok/s 105233 (96282)	Loss/tok 3.4414 (3.2581)	LR 2.800e-03
0: TRAIN [2][750/1885]	Time 0.102 (0.143)	Data 2.13e-04 (5.14e-04)	Tok/s 88105 (96291)	Loss/tok 2.9620 (3.2585)	LR 2.800e-03
0: TRAIN [2][760/1885]	Time 0.194 (0.143)	Data 2.16e-04 (5.10e-04)	Tok/s 105146 (96351)	Loss/tok 3.3892 (3.2592)	LR 2.800e-03
0: TRAIN [2][770/1885]	Time 0.244 (0.143)	Data 2.16e-04 (5.06e-04)	Tok/s 107064 (96377)	Loss/tok 3.6050 (3.2602)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][780/1885]	Time 0.146 (0.143)	Data 2.17e-04 (5.02e-04)	Tok/s 102031 (96315)	Loss/tok 3.1182 (3.2594)	LR 2.800e-03
0: TRAIN [2][790/1885]	Time 0.102 (0.143)	Data 2.16e-04 (4.99e-04)	Tok/s 86238 (96316)	Loss/tok 3.0044 (3.2603)	LR 2.800e-03
0: TRAIN [2][800/1885]	Time 0.102 (0.143)	Data 2.12e-04 (4.95e-04)	Tok/s 89910 (96363)	Loss/tok 2.9933 (3.2615)	LR 2.800e-03
0: TRAIN [2][810/1885]	Time 0.245 (0.144)	Data 2.30e-04 (4.92e-04)	Tok/s 105782 (96374)	Loss/tok 3.5699 (3.2624)	LR 2.800e-03
0: TRAIN [2][820/1885]	Time 0.194 (0.144)	Data 2.04e-04 (4.88e-04)	Tok/s 104018 (96403)	Loss/tok 3.4046 (3.2634)	LR 2.800e-03
0: TRAIN [2][830/1885]	Time 0.146 (0.144)	Data 2.16e-04 (4.85e-04)	Tok/s 101329 (96411)	Loss/tok 3.1731 (3.2625)	LR 2.800e-03
0: TRAIN [2][840/1885]	Time 0.194 (0.144)	Data 2.19e-04 (4.82e-04)	Tok/s 105214 (96381)	Loss/tok 3.3967 (3.2626)	LR 2.800e-03
0: TRAIN [2][850/1885]	Time 0.146 (0.144)	Data 2.12e-04 (4.78e-04)	Tok/s 100448 (96378)	Loss/tok 3.3186 (3.2625)	LR 2.800e-03
0: TRAIN [2][860/1885]	Time 0.146 (0.143)	Data 2.15e-04 (4.75e-04)	Tok/s 100411 (96346)	Loss/tok 3.1676 (3.2612)	LR 2.800e-03
0: TRAIN [2][870/1885]	Time 0.101 (0.143)	Data 2.16e-04 (4.72e-04)	Tok/s 89554 (96341)	Loss/tok 3.1452 (3.2614)	LR 2.800e-03
0: TRAIN [2][880/1885]	Time 0.146 (0.144)	Data 2.15e-04 (4.69e-04)	Tok/s 99164 (96383)	Loss/tok 3.2562 (3.2623)	LR 2.800e-03
0: TRAIN [2][890/1885]	Time 0.101 (0.144)	Data 2.18e-04 (4.67e-04)	Tok/s 89930 (96435)	Loss/tok 3.0245 (3.2643)	LR 2.800e-03
0: TRAIN [2][900/1885]	Time 0.147 (0.144)	Data 2.16e-04 (4.64e-04)	Tok/s 100872 (96483)	Loss/tok 3.2346 (3.2648)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][910/1885]	Time 0.103 (0.144)	Data 2.13e-04 (4.61e-04)	Tok/s 88626 (96504)	Loss/tok 2.9489 (3.2650)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][920/1885]	Time 0.102 (0.144)	Data 2.18e-04 (4.58e-04)	Tok/s 88625 (96529)	Loss/tok 3.1671 (3.2651)	LR 2.800e-03
0: TRAIN [2][930/1885]	Time 0.147 (0.144)	Data 2.22e-04 (4.56e-04)	Tok/s 100216 (96554)	Loss/tok 3.1727 (3.2652)	LR 2.800e-03
0: TRAIN [2][940/1885]	Time 0.102 (0.144)	Data 2.18e-04 (4.53e-04)	Tok/s 87549 (96545)	Loss/tok 3.0323 (3.2655)	LR 2.800e-03
0: TRAIN [2][950/1885]	Time 0.194 (0.144)	Data 2.16e-04 (4.51e-04)	Tok/s 104639 (96509)	Loss/tok 3.4447 (3.2649)	LR 2.800e-03
0: TRAIN [2][960/1885]	Time 0.147 (0.144)	Data 2.14e-04 (4.48e-04)	Tok/s 101621 (96556)	Loss/tok 3.1365 (3.2662)	LR 2.800e-03
0: TRAIN [2][970/1885]	Time 0.146 (0.144)	Data 2.13e-04 (4.46e-04)	Tok/s 101023 (96506)	Loss/tok 3.2636 (3.2651)	LR 2.800e-03
0: TRAIN [2][980/1885]	Time 0.193 (0.144)	Data 2.13e-04 (4.43e-04)	Tok/s 107336 (96500)	Loss/tok 3.3597 (3.2648)	LR 2.800e-03
0: TRAIN [2][990/1885]	Time 0.101 (0.144)	Data 2.16e-04 (4.41e-04)	Tok/s 89703 (96494)	Loss/tok 3.0355 (3.2655)	LR 2.800e-03
0: TRAIN [2][1000/1885]	Time 0.102 (0.144)	Data 2.30e-04 (4.39e-04)	Tok/s 90721 (96524)	Loss/tok 2.9130 (3.2668)	LR 2.800e-03
0: TRAIN [2][1010/1885]	Time 0.194 (0.144)	Data 2.14e-04 (4.37e-04)	Tok/s 106105 (96533)	Loss/tok 3.3172 (3.2666)	LR 2.800e-03
0: TRAIN [2][1020/1885]	Time 0.147 (0.144)	Data 2.15e-04 (4.35e-04)	Tok/s 99424 (96535)	Loss/tok 3.2023 (3.2660)	LR 2.800e-03
0: TRAIN [2][1030/1885]	Time 0.146 (0.144)	Data 2.16e-04 (4.32e-04)	Tok/s 101591 (96510)	Loss/tok 3.1434 (3.2657)	LR 2.800e-03
0: TRAIN [2][1040/1885]	Time 0.146 (0.144)	Data 2.15e-04 (4.30e-04)	Tok/s 100718 (96483)	Loss/tok 3.3258 (3.2652)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1050/1885]	Time 0.059 (0.144)	Data 2.19e-04 (4.28e-04)	Tok/s 75708 (96413)	Loss/tok 2.4819 (3.2640)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1060/1885]	Time 0.103 (0.144)	Data 1.76e-04 (4.26e-04)	Tok/s 88582 (96428)	Loss/tok 2.9821 (3.2635)	LR 2.800e-03
0: TRAIN [2][1070/1885]	Time 0.101 (0.143)	Data 2.15e-04 (4.24e-04)	Tok/s 90582 (96402)	Loss/tok 2.9397 (3.2631)	LR 2.800e-03
0: TRAIN [2][1080/1885]	Time 0.146 (0.144)	Data 2.18e-04 (4.22e-04)	Tok/s 98939 (96407)	Loss/tok 3.3135 (3.2649)	LR 2.800e-03
0: TRAIN [2][1090/1885]	Time 0.193 (0.143)	Data 2.14e-04 (4.20e-04)	Tok/s 105595 (96362)	Loss/tok 3.3000 (3.2636)	LR 2.800e-03
0: TRAIN [2][1100/1885]	Time 0.194 (0.143)	Data 2.13e-04 (4.18e-04)	Tok/s 106969 (96350)	Loss/tok 3.1831 (3.2630)	LR 2.800e-03
0: TRAIN [2][1110/1885]	Time 0.194 (0.143)	Data 2.15e-04 (4.17e-04)	Tok/s 104046 (96352)	Loss/tok 3.4536 (3.2634)	LR 2.800e-03
0: TRAIN [2][1120/1885]	Time 0.245 (0.143)	Data 2.14e-04 (4.15e-04)	Tok/s 106496 (96348)	Loss/tok 3.5306 (3.2642)	LR 2.800e-03
0: TRAIN [2][1130/1885]	Time 0.102 (0.143)	Data 2.12e-04 (4.13e-04)	Tok/s 88741 (96351)	Loss/tok 3.0493 (3.2646)	LR 2.800e-03
0: TRAIN [2][1140/1885]	Time 0.148 (0.143)	Data 2.18e-04 (4.11e-04)	Tok/s 99544 (96301)	Loss/tok 3.2603 (3.2640)	LR 2.800e-03
0: TRAIN [2][1150/1885]	Time 0.148 (0.143)	Data 1.88e-04 (4.10e-04)	Tok/s 98816 (96288)	Loss/tok 3.2204 (3.2645)	LR 2.800e-03
0: TRAIN [2][1160/1885]	Time 0.060 (0.143)	Data 2.15e-04 (4.08e-04)	Tok/s 75416 (96290)	Loss/tok 2.5402 (3.2650)	LR 2.800e-03
0: TRAIN [2][1170/1885]	Time 0.102 (0.143)	Data 2.15e-04 (4.06e-04)	Tok/s 89248 (96266)	Loss/tok 3.0340 (3.2646)	LR 2.800e-03
0: TRAIN [2][1180/1885]	Time 0.193 (0.143)	Data 2.17e-04 (4.04e-04)	Tok/s 106211 (96254)	Loss/tok 3.3846 (3.2645)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1190/1885]	Time 0.195 (0.143)	Data 2.13e-04 (4.03e-04)	Tok/s 102743 (96267)	Loss/tok 3.4704 (3.2647)	LR 2.800e-03
0: TRAIN [2][1200/1885]	Time 0.194 (0.143)	Data 2.15e-04 (4.01e-04)	Tok/s 105628 (96294)	Loss/tok 3.4363 (3.2648)	LR 2.800e-03
0: TRAIN [2][1210/1885]	Time 0.102 (0.143)	Data 2.15e-04 (4.00e-04)	Tok/s 87969 (96283)	Loss/tok 2.9362 (3.2642)	LR 2.800e-03
0: TRAIN [2][1220/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.98e-04)	Tok/s 90955 (96271)	Loss/tok 2.9876 (3.2642)	LR 2.800e-03
0: TRAIN [2][1230/1885]	Time 0.246 (0.143)	Data 2.13e-04 (3.97e-04)	Tok/s 106257 (96262)	Loss/tok 3.5860 (3.2649)	LR 2.800e-03
0: TRAIN [2][1240/1885]	Time 0.146 (0.143)	Data 2.17e-04 (3.95e-04)	Tok/s 100344 (96269)	Loss/tok 3.1411 (3.2644)	LR 2.800e-03
0: TRAIN [2][1250/1885]	Time 0.147 (0.143)	Data 2.15e-04 (3.94e-04)	Tok/s 98582 (96283)	Loss/tok 3.2436 (3.2642)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1260/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.92e-04)	Tok/s 101230 (96259)	Loss/tok 3.2193 (3.2639)	LR 2.800e-03
0: TRAIN [2][1270/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.91e-04)	Tok/s 89679 (96274)	Loss/tok 3.0278 (3.2640)	LR 2.800e-03
0: TRAIN [2][1280/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.90e-04)	Tok/s 89420 (96256)	Loss/tok 3.0932 (3.2645)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1290/1885]	Time 0.195 (0.143)	Data 2.27e-04 (3.88e-04)	Tok/s 105297 (96299)	Loss/tok 3.4183 (3.2660)	LR 2.800e-03
0: TRAIN [2][1300/1885]	Time 0.102 (0.143)	Data 1.75e-04 (3.87e-04)	Tok/s 91901 (96316)	Loss/tok 3.1349 (3.2661)	LR 2.800e-03
0: TRAIN [2][1310/1885]	Time 0.194 (0.143)	Data 2.18e-04 (3.86e-04)	Tok/s 104452 (96298)	Loss/tok 3.4396 (3.2654)	LR 2.800e-03
0: TRAIN [2][1320/1885]	Time 0.059 (0.143)	Data 2.15e-04 (3.84e-04)	Tok/s 77180 (96309)	Loss/tok 2.5818 (3.2654)	LR 2.800e-03
0: TRAIN [2][1330/1885]	Time 0.146 (0.143)	Data 2.12e-04 (3.83e-04)	Tok/s 98452 (96287)	Loss/tok 3.1937 (3.2648)	LR 2.800e-03
0: TRAIN [2][1340/1885]	Time 0.101 (0.143)	Data 2.15e-04 (3.82e-04)	Tok/s 90236 (96261)	Loss/tok 2.9560 (3.2641)	LR 2.800e-03
0: TRAIN [2][1350/1885]	Time 0.193 (0.143)	Data 2.13e-04 (3.80e-04)	Tok/s 106022 (96273)	Loss/tok 3.4042 (3.2649)	LR 2.800e-03
0: TRAIN [2][1360/1885]	Time 0.145 (0.143)	Data 2.12e-04 (3.79e-04)	Tok/s 102203 (96271)	Loss/tok 3.3265 (3.2647)	LR 2.800e-03
0: TRAIN [2][1370/1885]	Time 0.060 (0.143)	Data 2.14e-04 (3.78e-04)	Tok/s 77127 (96272)	Loss/tok 2.6740 (3.2650)	LR 2.800e-03
0: TRAIN [2][1380/1885]	Time 0.102 (0.143)	Data 2.27e-04 (3.77e-04)	Tok/s 90216 (96255)	Loss/tok 2.9758 (3.2646)	LR 2.800e-03
0: TRAIN [2][1390/1885]	Time 0.146 (0.143)	Data 2.14e-04 (3.76e-04)	Tok/s 99788 (96261)	Loss/tok 3.2197 (3.2644)	LR 2.800e-03
0: TRAIN [2][1400/1885]	Time 0.147 (0.143)	Data 1.76e-04 (3.74e-04)	Tok/s 99844 (96238)	Loss/tok 3.3313 (3.2638)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1410/1885]	Time 0.195 (0.143)	Data 2.13e-04 (3.73e-04)	Tok/s 105124 (96254)	Loss/tok 3.3494 (3.2640)	LR 2.800e-03
0: TRAIN [2][1420/1885]	Time 0.194 (0.143)	Data 2.18e-04 (3.72e-04)	Tok/s 105006 (96252)	Loss/tok 3.4524 (3.2639)	LR 2.800e-03
0: TRAIN [2][1430/1885]	Time 0.147 (0.143)	Data 2.13e-04 (3.71e-04)	Tok/s 102101 (96240)	Loss/tok 3.2093 (3.2637)	LR 2.800e-03
0: TRAIN [2][1440/1885]	Time 0.194 (0.143)	Data 2.13e-04 (3.70e-04)	Tok/s 104914 (96252)	Loss/tok 3.4218 (3.2638)	LR 2.800e-03
0: TRAIN [2][1450/1885]	Time 0.146 (0.143)	Data 2.15e-04 (3.69e-04)	Tok/s 99859 (96243)	Loss/tok 3.3030 (3.2634)	LR 2.800e-03
0: TRAIN [2][1460/1885]	Time 0.148 (0.143)	Data 2.28e-04 (3.68e-04)	Tok/s 99509 (96250)	Loss/tok 3.1882 (3.2633)	LR 2.800e-03
0: TRAIN [2][1470/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.67e-04)	Tok/s 100813 (96241)	Loss/tok 3.3034 (3.2630)	LR 2.800e-03
0: TRAIN [2][1480/1885]	Time 0.101 (0.143)	Data 2.15e-04 (3.66e-04)	Tok/s 90404 (96252)	Loss/tok 3.1770 (3.2630)	LR 2.800e-03
0: TRAIN [2][1490/1885]	Time 0.193 (0.143)	Data 2.16e-04 (3.65e-04)	Tok/s 103626 (96272)	Loss/tok 3.5929 (3.2636)	LR 2.800e-03
0: TRAIN [2][1500/1885]	Time 0.147 (0.143)	Data 2.15e-04 (3.64e-04)	Tok/s 99070 (96279)	Loss/tok 3.2415 (3.2639)	LR 2.800e-03
0: TRAIN [2][1510/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.63e-04)	Tok/s 89717 (96270)	Loss/tok 2.9481 (3.2635)	LR 2.800e-03
0: TRAIN [2][1520/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.62e-04)	Tok/s 88700 (96269)	Loss/tok 3.0154 (3.2627)	LR 2.800e-03
0: TRAIN [2][1530/1885]	Time 0.147 (0.143)	Data 2.14e-04 (3.61e-04)	Tok/s 100769 (96296)	Loss/tok 3.1731 (3.2631)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1540/1885]	Time 0.061 (0.143)	Data 1.86e-04 (3.60e-04)	Tok/s 74041 (96286)	Loss/tok 2.4163 (3.2627)	LR 2.800e-03
0: TRAIN [2][1550/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.59e-04)	Tok/s 89948 (96275)	Loss/tok 2.9394 (3.2624)	LR 2.800e-03
0: TRAIN [2][1560/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.58e-04)	Tok/s 89218 (96302)	Loss/tok 3.0344 (3.2635)	LR 2.800e-03
0: TRAIN [2][1570/1885]	Time 0.102 (0.143)	Data 2.25e-04 (3.57e-04)	Tok/s 89233 (96292)	Loss/tok 3.0981 (3.2637)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1580/1885]	Time 0.145 (0.143)	Data 2.27e-04 (3.56e-04)	Tok/s 99715 (96285)	Loss/tok 3.2477 (3.2638)	LR 2.800e-03
0: TRAIN [2][1590/1885]	Time 0.246 (0.143)	Data 2.16e-04 (3.55e-04)	Tok/s 104884 (96310)	Loss/tok 3.6692 (3.2647)	LR 2.800e-03
0: TRAIN [2][1600/1885]	Time 0.245 (0.143)	Data 1.86e-04 (3.54e-04)	Tok/s 108133 (96339)	Loss/tok 3.6459 (3.2658)	LR 2.800e-03
0: TRAIN [2][1610/1885]	Time 0.244 (0.144)	Data 2.15e-04 (3.53e-04)	Tok/s 107240 (96367)	Loss/tok 3.4809 (3.2661)	LR 2.800e-03
0: TRAIN [2][1620/1885]	Time 0.102 (0.143)	Data 2.14e-04 (3.52e-04)	Tok/s 89436 (96359)	Loss/tok 2.9037 (3.2658)	LR 2.800e-03
0: TRAIN [2][1630/1885]	Time 0.146 (0.144)	Data 2.21e-04 (3.52e-04)	Tok/s 100900 (96383)	Loss/tok 3.1065 (3.2660)	LR 2.800e-03
0: TRAIN [2][1640/1885]	Time 0.147 (0.144)	Data 2.12e-04 (3.51e-04)	Tok/s 99808 (96371)	Loss/tok 3.1849 (3.2655)	LR 2.800e-03
0: TRAIN [2][1650/1885]	Time 0.245 (0.144)	Data 2.15e-04 (3.50e-04)	Tok/s 107048 (96399)	Loss/tok 3.5295 (3.2661)	LR 2.800e-03
0: TRAIN [2][1660/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.49e-04)	Tok/s 88880 (96399)	Loss/tok 3.0067 (3.2655)	LR 2.800e-03
0: TRAIN [2][1670/1885]	Time 0.147 (0.144)	Data 2.13e-04 (3.48e-04)	Tok/s 100498 (96405)	Loss/tok 3.1493 (3.2653)	LR 2.800e-03
0: TRAIN [2][1680/1885]	Time 0.194 (0.144)	Data 2.17e-04 (3.48e-04)	Tok/s 104157 (96414)	Loss/tok 3.3628 (3.2652)	LR 2.800e-03
0: TRAIN [2][1690/1885]	Time 0.102 (0.144)	Data 2.14e-04 (3.47e-04)	Tok/s 90039 (96406)	Loss/tok 2.9787 (3.2650)	LR 2.800e-03
0: TRAIN [2][1700/1885]	Time 0.101 (0.144)	Data 2.14e-04 (3.46e-04)	Tok/s 88517 (96416)	Loss/tok 2.9972 (3.2653)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1710/1885]	Time 0.194 (0.144)	Data 2.15e-04 (3.45e-04)	Tok/s 103370 (96408)	Loss/tok 3.4452 (3.2648)	LR 2.800e-03
0: TRAIN [2][1720/1885]	Time 0.148 (0.144)	Data 2.14e-04 (3.44e-04)	Tok/s 98675 (96415)	Loss/tok 3.2659 (3.2643)	LR 2.800e-03
0: TRAIN [2][1730/1885]	Time 0.194 (0.144)	Data 2.15e-04 (3.44e-04)	Tok/s 104820 (96433)	Loss/tok 3.4184 (3.2648)	LR 2.800e-03
0: TRAIN [2][1740/1885]	Time 0.147 (0.144)	Data 2.17e-04 (3.43e-04)	Tok/s 98651 (96425)	Loss/tok 3.2486 (3.2644)	LR 2.800e-03
0: TRAIN [2][1750/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.42e-04)	Tok/s 86547 (96423)	Loss/tok 2.9927 (3.2643)	LR 2.800e-03
0: TRAIN [2][1760/1885]	Time 0.101 (0.143)	Data 2.14e-04 (3.42e-04)	Tok/s 87510 (96395)	Loss/tok 2.9998 (3.2637)	LR 2.800e-03
0: TRAIN [2][1770/1885]	Time 0.101 (0.143)	Data 2.14e-04 (3.41e-04)	Tok/s 90244 (96380)	Loss/tok 2.9554 (3.2636)	LR 2.800e-03
0: TRAIN [2][1780/1885]	Time 0.147 (0.143)	Data 2.12e-04 (3.40e-04)	Tok/s 99025 (96392)	Loss/tok 3.2345 (3.2635)	LR 2.800e-03
0: TRAIN [2][1790/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.39e-04)	Tok/s 88505 (96380)	Loss/tok 3.0178 (3.2633)	LR 2.800e-03
0: TRAIN [2][1800/1885]	Time 0.147 (0.143)	Data 2.15e-04 (3.39e-04)	Tok/s 99847 (96359)	Loss/tok 3.2420 (3.2631)	LR 2.800e-03
0: TRAIN [2][1810/1885]	Time 0.147 (0.143)	Data 2.15e-04 (3.38e-04)	Tok/s 100978 (96375)	Loss/tok 3.1982 (3.2632)	LR 2.800e-03
0: TRAIN [2][1820/1885]	Time 0.147 (0.143)	Data 2.12e-04 (3.37e-04)	Tok/s 99348 (96353)	Loss/tok 3.2332 (3.2625)	LR 2.800e-03
0: TRAIN [2][1830/1885]	Time 0.060 (0.143)	Data 2.14e-04 (3.37e-04)	Tok/s 75798 (96358)	Loss/tok 2.6005 (3.2626)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1840/1885]	Time 0.195 (0.143)	Data 2.22e-04 (3.36e-04)	Tok/s 104417 (96366)	Loss/tok 3.3781 (3.2624)	LR 2.800e-03
0: TRAIN [2][1850/1885]	Time 0.194 (0.143)	Data 2.15e-04 (3.35e-04)	Tok/s 105501 (96323)	Loss/tok 3.2883 (3.2616)	LR 2.800e-03
0: TRAIN [2][1860/1885]	Time 0.195 (0.143)	Data 2.15e-04 (3.35e-04)	Tok/s 104635 (96328)	Loss/tok 3.2967 (3.2616)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1870/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.34e-04)	Tok/s 99840 (96321)	Loss/tok 3.1145 (3.2615)	LR 2.800e-03
0: TRAIN [2][1880/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.33e-04)	Tok/s 105906 (96331)	Loss/tok 3.2460 (3.2614)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1594175314665, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1594175314666, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.571 (0.571)	Decoder iters 95.0 (95.0)	Tok/s 28455 (28455)
0: Running moses detokenizer
0: BLEU(score=22.786414872241625, counts=[36500, 17884, 9999, 5837], totals=[65908, 62905, 59902, 56903], precisions=[55.3802269830673, 28.430172482314603, 16.692264031251042, 10.257807145493208], bp=1.0, sys_len=65908, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1594175316244, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2279, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1594175316244, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2615	Test BLEU: 22.79
0: Performance: Epoch: 2	Training: 770576 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1594175316245, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1594175316245, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1594175316245, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2313652261
0: TRAIN [3][0/1885]	Time 0.422 (0.422)	Data 2.24e-01 (2.24e-01)	Tok/s 47923 (47923)	Loss/tok 3.2585 (3.2585)	LR 2.800e-03
0: TRAIN [3][10/1885]	Time 0.147 (0.156)	Data 2.16e-04 (2.06e-02)	Tok/s 100058 (89866)	Loss/tok 3.1144 (3.1276)	LR 2.800e-03
0: TRAIN [3][20/1885]	Time 0.102 (0.150)	Data 1.68e-04 (1.09e-02)	Tok/s 88113 (93090)	Loss/tok 2.9323 (3.1448)	LR 2.800e-03
0: TRAIN [3][30/1885]	Time 0.148 (0.151)	Data 2.20e-04 (7.44e-03)	Tok/s 98364 (94833)	Loss/tok 3.0665 (3.1648)	LR 2.800e-03
0: TRAIN [3][40/1885]	Time 0.194 (0.147)	Data 2.15e-04 (5.68e-03)	Tok/s 106323 (95124)	Loss/tok 3.3682 (3.1562)	LR 2.800e-03
0: TRAIN [3][50/1885]	Time 0.145 (0.143)	Data 2.14e-04 (4.61e-03)	Tok/s 101252 (95041)	Loss/tok 3.1973 (3.1478)	LR 2.800e-03
0: TRAIN [3][60/1885]	Time 0.147 (0.141)	Data 1.09e-04 (3.87e-03)	Tok/s 99248 (94591)	Loss/tok 3.1733 (3.1505)	LR 2.800e-03
0: TRAIN [3][70/1885]	Time 0.195 (0.142)	Data 1.99e-04 (3.35e-03)	Tok/s 105198 (94844)	Loss/tok 3.3117 (3.1587)	LR 2.800e-03
0: TRAIN [3][80/1885]	Time 0.101 (0.141)	Data 2.17e-04 (2.96e-03)	Tok/s 88794 (95012)	Loss/tok 2.9383 (3.1555)	LR 2.800e-03
0: TRAIN [3][90/1885]	Time 0.247 (0.144)	Data 2.15e-04 (2.66e-03)	Tok/s 106211 (95504)	Loss/tok 3.4081 (3.1742)	LR 2.800e-03
0: TRAIN [3][100/1885]	Time 0.246 (0.145)	Data 1.76e-04 (2.42e-03)	Tok/s 107360 (95830)	Loss/tok 3.4543 (3.1769)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][110/1885]	Time 0.100 (0.144)	Data 2.15e-04 (2.22e-03)	Tok/s 90590 (95702)	Loss/tok 2.8801 (3.1719)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][120/1885]	Time 0.101 (0.144)	Data 2.14e-04 (2.05e-03)	Tok/s 90446 (95795)	Loss/tok 2.9829 (3.1809)	LR 2.800e-03
0: TRAIN [3][130/1885]	Time 0.195 (0.145)	Data 1.08e-04 (1.90e-03)	Tok/s 104866 (96156)	Loss/tok 3.2858 (3.1787)	LR 2.800e-03
0: TRAIN [3][140/1885]	Time 0.194 (0.144)	Data 1.15e-04 (1.78e-03)	Tok/s 105493 (96104)	Loss/tok 3.2448 (3.1750)	LR 2.800e-03
0: TRAIN [3][150/1885]	Time 0.102 (0.144)	Data 2.33e-04 (1.67e-03)	Tok/s 88788 (96056)	Loss/tok 2.9841 (3.1775)	LR 2.800e-03
0: TRAIN [3][160/1885]	Time 0.102 (0.143)	Data 2.16e-04 (1.58e-03)	Tok/s 89326 (95893)	Loss/tok 2.9492 (3.1707)	LR 2.800e-03
0: TRAIN [3][170/1885]	Time 0.197 (0.145)	Data 2.12e-04 (1.50e-03)	Tok/s 103004 (96169)	Loss/tok 3.4243 (3.1793)	LR 2.800e-03
0: TRAIN [3][180/1885]	Time 0.148 (0.143)	Data 2.01e-04 (1.43e-03)	Tok/s 99971 (95909)	Loss/tok 3.0985 (3.1749)	LR 2.800e-03
0: TRAIN [3][190/1885]	Time 0.059 (0.143)	Data 2.16e-04 (1.36e-03)	Tok/s 78759 (95872)	Loss/tok 2.5658 (3.1759)	LR 2.800e-03
0: TRAIN [3][200/1885]	Time 0.196 (0.143)	Data 2.12e-04 (1.31e-03)	Tok/s 104970 (95935)	Loss/tok 3.3060 (3.1762)	LR 2.800e-03
0: TRAIN [3][210/1885]	Time 0.147 (0.142)	Data 2.14e-04 (1.26e-03)	Tok/s 99905 (95738)	Loss/tok 3.0468 (3.1691)	LR 2.800e-03
0: TRAIN [3][220/1885]	Time 0.147 (0.142)	Data 1.77e-04 (1.21e-03)	Tok/s 99554 (95682)	Loss/tok 3.1503 (3.1707)	LR 2.800e-03
0: TRAIN [3][230/1885]	Time 0.195 (0.142)	Data 2.02e-04 (1.16e-03)	Tok/s 103201 (95766)	Loss/tok 3.2744 (3.1696)	LR 2.800e-03
0: TRAIN [3][240/1885]	Time 0.147 (0.142)	Data 2.15e-04 (1.13e-03)	Tok/s 100111 (95735)	Loss/tok 3.1381 (3.1700)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][250/1885]	Time 0.147 (0.142)	Data 2.15e-04 (1.09e-03)	Tok/s 99438 (95744)	Loss/tok 3.0859 (3.1681)	LR 2.800e-03
0: TRAIN [3][260/1885]	Time 0.195 (0.143)	Data 2.17e-04 (1.06e-03)	Tok/s 104596 (95973)	Loss/tok 3.1516 (3.1708)	LR 2.800e-03
0: TRAIN [3][270/1885]	Time 0.193 (0.143)	Data 2.16e-04 (1.02e-03)	Tok/s 104778 (96017)	Loss/tok 3.5510 (3.1739)	LR 2.800e-03
0: TRAIN [3][280/1885]	Time 0.147 (0.144)	Data 2.15e-04 (9.95e-04)	Tok/s 99724 (96135)	Loss/tok 2.9966 (3.1756)	LR 2.800e-03
0: TRAIN [3][290/1885]	Time 0.101 (0.144)	Data 2.16e-04 (9.69e-04)	Tok/s 90162 (96192)	Loss/tok 2.9402 (3.1796)	LR 2.800e-03
0: TRAIN [3][300/1885]	Time 0.245 (0.144)	Data 2.16e-04 (9.43e-04)	Tok/s 105973 (96270)	Loss/tok 3.4728 (3.1805)	LR 2.800e-03
0: TRAIN [3][310/1885]	Time 0.103 (0.144)	Data 2.04e-04 (9.20e-04)	Tok/s 88426 (96273)	Loss/tok 2.8882 (3.1787)	LR 2.800e-03
0: TRAIN [3][320/1885]	Time 0.195 (0.144)	Data 2.13e-04 (8.98e-04)	Tok/s 105891 (96179)	Loss/tok 3.3275 (3.1770)	LR 2.800e-03
0: TRAIN [3][330/1885]	Time 0.102 (0.143)	Data 2.14e-04 (8.77e-04)	Tok/s 89473 (96207)	Loss/tok 3.0359 (3.1755)	LR 2.800e-03
0: TRAIN [3][340/1885]	Time 0.194 (0.144)	Data 2.22e-04 (8.58e-04)	Tok/s 104464 (96356)	Loss/tok 3.3085 (3.1782)	LR 2.800e-03
0: TRAIN [3][350/1885]	Time 0.103 (0.145)	Data 1.52e-04 (8.39e-04)	Tok/s 87103 (96429)	Loss/tok 3.0859 (3.1812)	LR 2.800e-03
0: TRAIN [3][360/1885]	Time 0.146 (0.144)	Data 2.17e-04 (8.22e-04)	Tok/s 101390 (96290)	Loss/tok 3.1611 (3.1793)	LR 2.800e-03
0: TRAIN [3][370/1885]	Time 0.146 (0.145)	Data 1.08e-04 (8.05e-04)	Tok/s 100820 (96400)	Loss/tok 3.1553 (3.1822)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][380/1885]	Time 0.147 (0.144)	Data 1.08e-04 (7.87e-04)	Tok/s 99384 (96344)	Loss/tok 3.0824 (3.1803)	LR 2.800e-03
0: TRAIN [3][390/1885]	Time 0.100 (0.144)	Data 1.08e-04 (7.69e-04)	Tok/s 89285 (96329)	Loss/tok 2.8527 (3.1804)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][400/1885]	Time 0.060 (0.144)	Data 2.19e-04 (7.54e-04)	Tok/s 77144 (96299)	Loss/tok 2.4366 (3.1807)	LR 2.800e-03
0: TRAIN [3][410/1885]	Time 0.194 (0.145)	Data 2.18e-04 (7.41e-04)	Tok/s 105784 (96418)	Loss/tok 3.2189 (3.1822)	LR 2.800e-03
0: TRAIN [3][420/1885]	Time 0.195 (0.145)	Data 2.16e-04 (7.28e-04)	Tok/s 103724 (96460)	Loss/tok 3.4011 (3.1815)	LR 2.800e-03
0: TRAIN [3][430/1885]	Time 0.148 (0.145)	Data 2.14e-04 (7.17e-04)	Tok/s 97915 (96516)	Loss/tok 3.1371 (3.1826)	LR 2.800e-03
0: TRAIN [3][440/1885]	Time 0.102 (0.144)	Data 2.12e-04 (7.05e-04)	Tok/s 89660 (96387)	Loss/tok 2.9706 (3.1812)	LR 1.400e-03
0: TRAIN [3][450/1885]	Time 0.103 (0.144)	Data 2.07e-04 (6.94e-04)	Tok/s 87251 (96299)	Loss/tok 2.9553 (3.1801)	LR 1.400e-03
0: TRAIN [3][460/1885]	Time 0.147 (0.144)	Data 2.15e-04 (6.84e-04)	Tok/s 100045 (96299)	Loss/tok 3.0372 (3.1785)	LR 1.400e-03
0: TRAIN [3][470/1885]	Time 0.246 (0.144)	Data 2.20e-04 (6.74e-04)	Tok/s 109295 (96332)	Loss/tok 3.3260 (3.1796)	LR 1.400e-03
0: TRAIN [3][480/1885]	Time 0.102 (0.144)	Data 2.21e-04 (6.64e-04)	Tok/s 87943 (96332)	Loss/tok 2.8463 (3.1791)	LR 1.400e-03
0: TRAIN [3][490/1885]	Time 0.101 (0.144)	Data 2.17e-04 (6.55e-04)	Tok/s 91348 (96291)	Loss/tok 2.9096 (3.1785)	LR 1.400e-03
0: TRAIN [3][500/1885]	Time 0.246 (0.144)	Data 2.17e-04 (6.46e-04)	Tok/s 105871 (96322)	Loss/tok 3.3514 (3.1791)	LR 1.400e-03
0: TRAIN [3][510/1885]	Time 0.146 (0.144)	Data 2.16e-04 (6.38e-04)	Tok/s 100788 (96268)	Loss/tok 3.1267 (3.1770)	LR 1.400e-03
0: TRAIN [3][520/1885]	Time 0.146 (0.144)	Data 2.15e-04 (6.29e-04)	Tok/s 100760 (96268)	Loss/tok 3.2240 (3.1765)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][530/1885]	Time 0.147 (0.144)	Data 2.17e-04 (6.22e-04)	Tok/s 99106 (96279)	Loss/tok 3.1548 (3.1760)	LR 1.400e-03
0: TRAIN [3][540/1885]	Time 0.147 (0.143)	Data 2.12e-04 (6.14e-04)	Tok/s 102130 (96299)	Loss/tok 3.0395 (3.1740)	LR 1.400e-03
0: TRAIN [3][550/1885]	Time 0.102 (0.143)	Data 2.19e-04 (6.07e-04)	Tok/s 89095 (96298)	Loss/tok 2.9563 (3.1732)	LR 1.400e-03
0: TRAIN [3][560/1885]	Time 0.101 (0.143)	Data 2.16e-04 (6.00e-04)	Tok/s 90285 (96239)	Loss/tok 2.9285 (3.1717)	LR 1.400e-03
0: TRAIN [3][570/1885]	Time 0.102 (0.143)	Data 2.16e-04 (5.93e-04)	Tok/s 89401 (96261)	Loss/tok 2.8693 (3.1709)	LR 1.400e-03
0: TRAIN [3][580/1885]	Time 0.060 (0.143)	Data 2.16e-04 (5.86e-04)	Tok/s 74747 (96241)	Loss/tok 2.5001 (3.1707)	LR 1.400e-03
0: TRAIN [3][590/1885]	Time 0.146 (0.143)	Data 2.16e-04 (5.80e-04)	Tok/s 101079 (96274)	Loss/tok 3.1494 (3.1707)	LR 1.400e-03
0: TRAIN [3][600/1885]	Time 0.102 (0.143)	Data 2.20e-04 (5.74e-04)	Tok/s 88346 (96279)	Loss/tok 2.9123 (3.1716)	LR 1.400e-03
0: TRAIN [3][610/1885]	Time 0.101 (0.143)	Data 2.13e-04 (5.68e-04)	Tok/s 89296 (96283)	Loss/tok 2.9910 (3.1707)	LR 1.400e-03
0: TRAIN [3][620/1885]	Time 0.101 (0.143)	Data 2.34e-04 (5.62e-04)	Tok/s 90172 (96271)	Loss/tok 2.8252 (3.1696)	LR 1.400e-03
0: TRAIN [3][630/1885]	Time 0.102 (0.143)	Data 2.16e-04 (5.57e-04)	Tok/s 88987 (96280)	Loss/tok 2.8224 (3.1695)	LR 1.400e-03
0: TRAIN [3][640/1885]	Time 0.247 (0.143)	Data 2.18e-04 (5.52e-04)	Tok/s 105520 (96251)	Loss/tok 3.5657 (3.1694)	LR 1.400e-03
0: TRAIN [3][650/1885]	Time 0.101 (0.143)	Data 2.14e-04 (5.46e-04)	Tok/s 89297 (96223)	Loss/tok 2.8659 (3.1678)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][660/1885]	Time 0.248 (0.143)	Data 1.06e-04 (5.40e-04)	Tok/s 107285 (96241)	Loss/tok 3.4011 (3.1676)	LR 1.400e-03
0: TRAIN [3][670/1885]	Time 0.147 (0.143)	Data 1.09e-04 (5.33e-04)	Tok/s 100230 (96269)	Loss/tok 3.1858 (3.1679)	LR 1.400e-03
0: TRAIN [3][680/1885]	Time 0.148 (0.143)	Data 1.06e-04 (5.27e-04)	Tok/s 98282 (96307)	Loss/tok 3.1144 (3.1686)	LR 1.400e-03
0: TRAIN [3][690/1885]	Time 0.103 (0.143)	Data 1.07e-04 (5.21e-04)	Tok/s 87029 (96331)	Loss/tok 2.8578 (3.1677)	LR 1.400e-03
0: TRAIN [3][700/1885]	Time 0.102 (0.143)	Data 1.06e-04 (5.16e-04)	Tok/s 90066 (96318)	Loss/tok 2.8589 (3.1676)	LR 1.400e-03
0: TRAIN [3][710/1885]	Time 0.146 (0.144)	Data 1.05e-04 (5.11e-04)	Tok/s 101064 (96347)	Loss/tok 3.0881 (3.1680)	LR 1.400e-03
0: TRAIN [3][720/1885]	Time 0.147 (0.143)	Data 1.08e-04 (5.05e-04)	Tok/s 97417 (96335)	Loss/tok 3.2010 (3.1672)	LR 1.400e-03
0: TRAIN [3][730/1885]	Time 0.195 (0.143)	Data 1.09e-04 (5.00e-04)	Tok/s 104288 (96348)	Loss/tok 3.2101 (3.1670)	LR 1.400e-03
0: TRAIN [3][740/1885]	Time 0.148 (0.143)	Data 2.16e-04 (4.96e-04)	Tok/s 100469 (96346)	Loss/tok 3.1481 (3.1664)	LR 1.400e-03
0: TRAIN [3][750/1885]	Time 0.101 (0.143)	Data 2.18e-04 (4.92e-04)	Tok/s 87949 (96307)	Loss/tok 2.9314 (3.1649)	LR 1.400e-03
0: TRAIN [3][760/1885]	Time 0.102 (0.143)	Data 2.31e-04 (4.88e-04)	Tok/s 89326 (96275)	Loss/tok 2.9137 (3.1640)	LR 1.400e-03
0: TRAIN [3][770/1885]	Time 0.246 (0.143)	Data 2.15e-04 (4.85e-04)	Tok/s 105427 (96299)	Loss/tok 3.3147 (3.1641)	LR 1.400e-03
0: TRAIN [3][780/1885]	Time 0.148 (0.144)	Data 2.20e-04 (4.81e-04)	Tok/s 98207 (96364)	Loss/tok 3.0615 (3.1649)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][790/1885]	Time 0.195 (0.144)	Data 2.19e-04 (4.78e-04)	Tok/s 104838 (96403)	Loss/tok 3.2859 (3.1664)	LR 1.400e-03
0: TRAIN [3][800/1885]	Time 0.197 (0.144)	Data 2.18e-04 (4.75e-04)	Tok/s 103781 (96386)	Loss/tok 3.2246 (3.1656)	LR 1.400e-03
0: TRAIN [3][810/1885]	Time 0.146 (0.144)	Data 2.15e-04 (4.71e-04)	Tok/s 100632 (96364)	Loss/tok 3.0851 (3.1656)	LR 1.400e-03
0: TRAIN [3][820/1885]	Time 0.102 (0.144)	Data 2.16e-04 (4.68e-04)	Tok/s 90992 (96325)	Loss/tok 2.8287 (3.1643)	LR 1.400e-03
0: TRAIN [3][830/1885]	Time 0.148 (0.144)	Data 2.32e-04 (4.65e-04)	Tok/s 99714 (96342)	Loss/tok 3.1501 (3.1640)	LR 1.400e-03
0: TRAIN [3][840/1885]	Time 0.195 (0.144)	Data 2.14e-04 (4.62e-04)	Tok/s 104716 (96354)	Loss/tok 3.1494 (3.1636)	LR 1.400e-03
0: TRAIN [3][850/1885]	Time 0.102 (0.144)	Data 2.06e-04 (4.59e-04)	Tok/s 87459 (96358)	Loss/tok 2.9210 (3.1634)	LR 1.400e-03
0: TRAIN [3][860/1885]	Time 0.147 (0.144)	Data 2.16e-04 (4.56e-04)	Tok/s 99192 (96373)	Loss/tok 3.2373 (3.1634)	LR 1.400e-03
0: TRAIN [3][870/1885]	Time 0.148 (0.144)	Data 1.91e-04 (4.53e-04)	Tok/s 98187 (96334)	Loss/tok 3.1496 (3.1629)	LR 1.400e-03
0: TRAIN [3][880/1885]	Time 0.147 (0.144)	Data 2.07e-04 (4.51e-04)	Tok/s 99558 (96307)	Loss/tok 2.9972 (3.1621)	LR 1.400e-03
0: TRAIN [3][890/1885]	Time 0.104 (0.143)	Data 1.87e-04 (4.48e-04)	Tok/s 88645 (96294)	Loss/tok 2.9423 (3.1617)	LR 1.400e-03
0: TRAIN [3][900/1885]	Time 0.146 (0.143)	Data 2.15e-04 (4.45e-04)	Tok/s 100250 (96270)	Loss/tok 3.1134 (3.1610)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][910/1885]	Time 0.247 (0.143)	Data 2.15e-04 (4.43e-04)	Tok/s 105782 (96285)	Loss/tok 3.4173 (3.1612)	LR 1.400e-03
0: TRAIN [3][920/1885]	Time 0.146 (0.143)	Data 2.15e-04 (4.40e-04)	Tok/s 101565 (96279)	Loss/tok 3.0314 (3.1601)	LR 1.400e-03
0: TRAIN [3][930/1885]	Time 0.246 (0.144)	Data 1.07e-04 (4.38e-04)	Tok/s 106649 (96315)	Loss/tok 3.3829 (3.1603)	LR 1.400e-03
0: TRAIN [3][940/1885]	Time 0.147 (0.143)	Data 1.08e-04 (4.34e-04)	Tok/s 99610 (96301)	Loss/tok 3.0344 (3.1594)	LR 1.400e-03
0: TRAIN [3][950/1885]	Time 0.194 (0.144)	Data 1.08e-04 (4.31e-04)	Tok/s 105915 (96314)	Loss/tok 3.2187 (3.1589)	LR 1.400e-03
0: TRAIN [3][960/1885]	Time 0.101 (0.143)	Data 1.06e-04 (4.28e-04)	Tok/s 89747 (96259)	Loss/tok 2.9354 (3.1574)	LR 1.400e-03
0: TRAIN [3][970/1885]	Time 0.245 (0.143)	Data 1.09e-04 (4.24e-04)	Tok/s 105385 (96261)	Loss/tok 3.5096 (3.1586)	LR 1.400e-03
0: TRAIN [3][980/1885]	Time 0.061 (0.143)	Data 1.08e-04 (4.21e-04)	Tok/s 75674 (96270)	Loss/tok 2.4281 (3.1593)	LR 1.400e-03
0: TRAIN [3][990/1885]	Time 0.102 (0.144)	Data 1.07e-04 (4.18e-04)	Tok/s 88624 (96254)	Loss/tok 2.9081 (3.1595)	LR 1.400e-03
0: TRAIN [3][1000/1885]	Time 0.148 (0.144)	Data 1.05e-04 (4.15e-04)	Tok/s 97664 (96271)	Loss/tok 3.1654 (3.1596)	LR 1.400e-03
0: TRAIN [3][1010/1885]	Time 0.060 (0.143)	Data 2.16e-04 (4.12e-04)	Tok/s 76964 (96248)	Loss/tok 2.4384 (3.1589)	LR 1.400e-03
0: TRAIN [3][1020/1885]	Time 0.101 (0.143)	Data 2.17e-04 (4.10e-04)	Tok/s 90639 (96255)	Loss/tok 2.9392 (3.1584)	LR 1.400e-03
0: TRAIN [3][1030/1885]	Time 0.146 (0.144)	Data 2.15e-04 (4.09e-04)	Tok/s 100720 (96273)	Loss/tok 3.1574 (3.1585)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1040/1885]	Time 0.149 (0.143)	Data 2.12e-04 (4.06e-04)	Tok/s 98824 (96270)	Loss/tok 3.1001 (3.1580)	LR 1.400e-03
0: TRAIN [3][1050/1885]	Time 0.196 (0.143)	Data 1.90e-04 (4.04e-04)	Tok/s 103720 (96274)	Loss/tok 3.3605 (3.1578)	LR 1.400e-03
0: TRAIN [3][1060/1885]	Time 0.101 (0.143)	Data 2.27e-04 (4.02e-04)	Tok/s 89512 (96269)	Loss/tok 2.8355 (3.1572)	LR 1.400e-03
0: TRAIN [3][1070/1885]	Time 0.102 (0.143)	Data 2.04e-04 (4.01e-04)	Tok/s 88779 (96224)	Loss/tok 2.9081 (3.1570)	LR 1.400e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1080/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.99e-04)	Tok/s 91166 (96230)	Loss/tok 2.8727 (3.1574)	LR 1.400e-03
0: TRAIN [3][1090/1885]	Time 0.102 (0.143)	Data 2.13e-04 (3.97e-04)	Tok/s 89052 (96223)	Loss/tok 2.9481 (3.1574)	LR 1.400e-03
0: TRAIN [3][1100/1885]	Time 0.102 (0.143)	Data 2.15e-04 (3.96e-04)	Tok/s 90519 (96195)	Loss/tok 2.9465 (3.1569)	LR 1.400e-03
0: TRAIN [3][1110/1885]	Time 0.103 (0.143)	Data 2.28e-04 (3.94e-04)	Tok/s 88144 (96190)	Loss/tok 3.1250 (3.1573)	LR 1.400e-03
0: TRAIN [3][1120/1885]	Time 0.148 (0.143)	Data 2.14e-04 (3.92e-04)	Tok/s 97959 (96170)	Loss/tok 3.1149 (3.1566)	LR 1.400e-03
0: TRAIN [3][1130/1885]	Time 0.100 (0.143)	Data 2.18e-04 (3.91e-04)	Tok/s 92492 (96160)	Loss/tok 2.9412 (3.1570)	LR 1.400e-03
0: TRAIN [3][1140/1885]	Time 0.197 (0.143)	Data 2.29e-04 (3.89e-04)	Tok/s 104555 (96194)	Loss/tok 3.2506 (3.1576)	LR 1.400e-03
0: TRAIN [3][1150/1885]	Time 0.194 (0.143)	Data 2.18e-04 (3.87e-04)	Tok/s 104870 (96201)	Loss/tok 3.1807 (3.1577)	LR 1.400e-03
0: TRAIN [3][1160/1885]	Time 0.148 (0.144)	Data 2.17e-04 (3.86e-04)	Tok/s 98831 (96199)	Loss/tok 3.1250 (3.1574)	LR 1.400e-03
0: TRAIN [3][1170/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.84e-04)	Tok/s 99895 (96177)	Loss/tok 3.1788 (3.1570)	LR 1.400e-03
0: TRAIN [3][1180/1885]	Time 0.193 (0.144)	Data 2.17e-04 (3.83e-04)	Tok/s 106005 (96201)	Loss/tok 3.1662 (3.1575)	LR 1.400e-03
0: TRAIN [3][1190/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.81e-04)	Tok/s 90008 (96192)	Loss/tok 2.8604 (3.1569)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1200/1885]	Time 0.247 (0.144)	Data 2.16e-04 (3.80e-04)	Tok/s 105310 (96221)	Loss/tok 3.4942 (3.1579)	LR 1.400e-03
0: TRAIN [3][1210/1885]	Time 0.246 (0.144)	Data 2.18e-04 (3.79e-04)	Tok/s 106398 (96203)	Loss/tok 3.3203 (3.1583)	LR 1.400e-03
0: TRAIN [3][1220/1885]	Time 0.195 (0.144)	Data 2.20e-04 (3.77e-04)	Tok/s 104825 (96181)	Loss/tok 3.2159 (3.1577)	LR 1.400e-03
0: TRAIN [3][1230/1885]	Time 0.247 (0.144)	Data 2.03e-04 (3.76e-04)	Tok/s 105896 (96173)	Loss/tok 3.4094 (3.1574)	LR 1.400e-03
0: TRAIN [3][1240/1885]	Time 0.102 (0.143)	Data 2.15e-04 (3.75e-04)	Tok/s 87475 (96102)	Loss/tok 2.8629 (3.1562)	LR 1.400e-03
0: TRAIN [3][1250/1885]	Time 0.102 (0.143)	Data 1.90e-04 (3.73e-04)	Tok/s 89169 (96085)	Loss/tok 2.8484 (3.1556)	LR 1.400e-03
0: TRAIN [3][1260/1885]	Time 0.245 (0.143)	Data 1.89e-04 (3.72e-04)	Tok/s 107114 (96105)	Loss/tok 3.4370 (3.1561)	LR 1.400e-03
0: TRAIN [3][1270/1885]	Time 0.193 (0.143)	Data 2.16e-04 (3.71e-04)	Tok/s 107241 (96112)	Loss/tok 3.2651 (3.1562)	LR 1.400e-03
0: TRAIN [3][1280/1885]	Time 0.246 (0.144)	Data 2.26e-04 (3.70e-04)	Tok/s 107825 (96141)	Loss/tok 3.2819 (3.1565)	LR 1.400e-03
0: TRAIN [3][1290/1885]	Time 0.194 (0.144)	Data 1.06e-04 (3.68e-04)	Tok/s 104552 (96167)	Loss/tok 3.3939 (3.1575)	LR 1.400e-03
0: TRAIN [3][1300/1885]	Time 0.195 (0.144)	Data 1.06e-04 (3.66e-04)	Tok/s 104531 (96175)	Loss/tok 3.2731 (3.1574)	LR 7.000e-04
0: TRAIN [3][1310/1885]	Time 0.103 (0.144)	Data 2.19e-04 (3.65e-04)	Tok/s 89149 (96175)	Loss/tok 2.8409 (3.1571)	LR 7.000e-04
0: TRAIN [3][1320/1885]	Time 0.102 (0.144)	Data 2.02e-04 (3.64e-04)	Tok/s 88291 (96165)	Loss/tok 2.8742 (3.1575)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1330/1885]	Time 0.247 (0.144)	Data 2.16e-04 (3.62e-04)	Tok/s 106714 (96161)	Loss/tok 3.3308 (3.1572)	LR 7.000e-04
0: TRAIN [3][1340/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.61e-04)	Tok/s 99036 (96156)	Loss/tok 3.0851 (3.1567)	LR 7.000e-04
0: TRAIN [3][1350/1885]	Time 0.061 (0.144)	Data 2.17e-04 (3.60e-04)	Tok/s 74950 (96160)	Loss/tok 2.3937 (3.1567)	LR 7.000e-04
0: TRAIN [3][1360/1885]	Time 0.245 (0.144)	Data 2.15e-04 (3.59e-04)	Tok/s 106076 (96129)	Loss/tok 3.2985 (3.1559)	LR 7.000e-04
0: TRAIN [3][1370/1885]	Time 0.148 (0.144)	Data 2.15e-04 (3.58e-04)	Tok/s 98138 (96137)	Loss/tok 2.9732 (3.1556)	LR 7.000e-04
0: TRAIN [3][1380/1885]	Time 0.146 (0.144)	Data 2.18e-04 (3.57e-04)	Tok/s 100009 (96117)	Loss/tok 3.1034 (3.1550)	LR 7.000e-04
0: TRAIN [3][1390/1885]	Time 0.101 (0.144)	Data 2.16e-04 (3.56e-04)	Tok/s 90661 (96110)	Loss/tok 2.8861 (3.1545)	LR 7.000e-04
0: TRAIN [3][1400/1885]	Time 0.147 (0.144)	Data 2.18e-04 (3.55e-04)	Tok/s 100409 (96111)	Loss/tok 3.1567 (3.1541)	LR 7.000e-04
0: TRAIN [3][1410/1885]	Time 0.149 (0.144)	Data 2.15e-04 (3.54e-04)	Tok/s 98683 (96144)	Loss/tok 3.1240 (3.1544)	LR 7.000e-04
0: TRAIN [3][1420/1885]	Time 0.147 (0.144)	Data 2.21e-04 (3.53e-04)	Tok/s 99603 (96127)	Loss/tok 3.0450 (3.1544)	LR 7.000e-04
0: TRAIN [3][1430/1885]	Time 0.195 (0.144)	Data 2.19e-04 (3.52e-04)	Tok/s 105454 (96159)	Loss/tok 3.2104 (3.1546)	LR 7.000e-04
0: TRAIN [3][1440/1885]	Time 0.147 (0.144)	Data 2.14e-04 (3.51e-04)	Tok/s 100812 (96158)	Loss/tok 3.0014 (3.1544)	LR 7.000e-04
0: TRAIN [3][1450/1885]	Time 0.101 (0.144)	Data 2.00e-04 (3.50e-04)	Tok/s 90599 (96173)	Loss/tok 2.7994 (3.1548)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1460/1885]	Time 0.245 (0.144)	Data 2.16e-04 (3.49e-04)	Tok/s 106978 (96172)	Loss/tok 3.3913 (3.1547)	LR 7.000e-04
0: TRAIN [3][1470/1885]	Time 0.148 (0.144)	Data 2.19e-04 (3.48e-04)	Tok/s 98474 (96168)	Loss/tok 3.1006 (3.1543)	LR 7.000e-04
0: TRAIN [3][1480/1885]	Time 0.148 (0.144)	Data 1.88e-04 (3.47e-04)	Tok/s 97681 (96187)	Loss/tok 3.0785 (3.1540)	LR 7.000e-04
0: TRAIN [3][1490/1885]	Time 0.101 (0.144)	Data 2.16e-04 (3.46e-04)	Tok/s 91969 (96183)	Loss/tok 2.9332 (3.1531)	LR 7.000e-04
0: TRAIN [3][1500/1885]	Time 0.148 (0.144)	Data 2.14e-04 (3.45e-04)	Tok/s 98550 (96171)	Loss/tok 3.1223 (3.1529)	LR 7.000e-04
0: TRAIN [3][1510/1885]	Time 0.196 (0.144)	Data 2.18e-04 (3.44e-04)	Tok/s 104729 (96161)	Loss/tok 3.1387 (3.1528)	LR 7.000e-04
0: TRAIN [3][1520/1885]	Time 0.102 (0.144)	Data 1.99e-04 (3.43e-04)	Tok/s 88128 (96138)	Loss/tok 2.8774 (3.1523)	LR 7.000e-04
0: TRAIN [3][1530/1885]	Time 0.062 (0.144)	Data 2.16e-04 (3.42e-04)	Tok/s 76026 (96102)	Loss/tok 2.4203 (3.1513)	LR 7.000e-04
0: TRAIN [3][1540/1885]	Time 0.102 (0.144)	Data 2.19e-04 (3.42e-04)	Tok/s 88095 (96092)	Loss/tok 2.9487 (3.1513)	LR 7.000e-04
0: TRAIN [3][1550/1885]	Time 0.148 (0.144)	Data 2.18e-04 (3.41e-04)	Tok/s 99633 (96128)	Loss/tok 3.1393 (3.1512)	LR 7.000e-04
0: TRAIN [3][1560/1885]	Time 0.149 (0.144)	Data 2.05e-04 (3.40e-04)	Tok/s 98209 (96138)	Loss/tok 3.0723 (3.1513)	LR 7.000e-04
0: TRAIN [3][1570/1885]	Time 0.147 (0.144)	Data 2.17e-04 (3.39e-04)	Tok/s 99652 (96138)	Loss/tok 3.0871 (3.1509)	LR 7.000e-04
0: TRAIN [3][1580/1885]	Time 0.062 (0.144)	Data 2.17e-04 (3.38e-04)	Tok/s 74161 (96117)	Loss/tok 2.4272 (3.1504)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1590/1885]	Time 0.103 (0.144)	Data 2.17e-04 (3.38e-04)	Tok/s 88459 (96097)	Loss/tok 2.8470 (3.1496)	LR 7.000e-04
0: TRAIN [3][1600/1885]	Time 0.147 (0.144)	Data 2.17e-04 (3.37e-04)	Tok/s 98967 (96103)	Loss/tok 3.1306 (3.1492)	LR 7.000e-04
0: TRAIN [3][1610/1885]	Time 0.146 (0.144)	Data 2.27e-04 (3.36e-04)	Tok/s 101595 (96116)	Loss/tok 2.9812 (3.1489)	LR 7.000e-04
0: TRAIN [3][1620/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.35e-04)	Tok/s 101513 (96115)	Loss/tok 3.1072 (3.1486)	LR 7.000e-04
0: TRAIN [3][1630/1885]	Time 0.102 (0.144)	Data 2.20e-04 (3.35e-04)	Tok/s 87686 (96126)	Loss/tok 2.9819 (3.1487)	LR 7.000e-04
0: TRAIN [3][1640/1885]	Time 0.195 (0.144)	Data 2.20e-04 (3.34e-04)	Tok/s 104075 (96126)	Loss/tok 3.2096 (3.1484)	LR 7.000e-04
0: TRAIN [3][1650/1885]	Time 0.102 (0.144)	Data 2.18e-04 (3.33e-04)	Tok/s 87785 (96113)	Loss/tok 2.8719 (3.1477)	LR 7.000e-04
0: TRAIN [3][1660/1885]	Time 0.195 (0.144)	Data 2.18e-04 (3.32e-04)	Tok/s 104316 (96137)	Loss/tok 3.1917 (3.1475)	LR 7.000e-04
0: TRAIN [3][1670/1885]	Time 0.102 (0.144)	Data 2.18e-04 (3.32e-04)	Tok/s 88564 (96136)	Loss/tok 2.9527 (3.1472)	LR 7.000e-04
0: TRAIN [3][1680/1885]	Time 0.103 (0.144)	Data 2.17e-04 (3.31e-04)	Tok/s 87809 (96108)	Loss/tok 2.8907 (3.1464)	LR 7.000e-04
0: TRAIN [3][1690/1885]	Time 0.148 (0.144)	Data 2.16e-04 (3.30e-04)	Tok/s 98076 (96105)	Loss/tok 3.1223 (3.1464)	LR 7.000e-04
0: TRAIN [3][1700/1885]	Time 0.247 (0.144)	Data 2.20e-04 (3.30e-04)	Tok/s 105061 (96121)	Loss/tok 3.4489 (3.1468)	LR 7.000e-04
0: TRAIN [3][1710/1885]	Time 0.149 (0.144)	Data 2.16e-04 (3.29e-04)	Tok/s 99314 (96127)	Loss/tok 3.0979 (3.1467)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1720/1885]	Time 0.060 (0.144)	Data 2.19e-04 (3.28e-04)	Tok/s 77508 (96089)	Loss/tok 2.3612 (3.1460)	LR 7.000e-04
0: TRAIN [3][1730/1885]	Time 0.102 (0.144)	Data 2.17e-04 (3.28e-04)	Tok/s 88160 (96067)	Loss/tok 2.8621 (3.1454)	LR 7.000e-04
0: TRAIN [3][1740/1885]	Time 0.147 (0.144)	Data 2.18e-04 (3.27e-04)	Tok/s 100956 (96059)	Loss/tok 3.0068 (3.1449)	LR 7.000e-04
0: TRAIN [3][1750/1885]	Time 0.103 (0.144)	Data 2.20e-04 (3.26e-04)	Tok/s 87847 (96072)	Loss/tok 2.9292 (3.1447)	LR 7.000e-04
0: TRAIN [3][1760/1885]	Time 0.147 (0.144)	Data 2.18e-04 (3.26e-04)	Tok/s 100444 (96084)	Loss/tok 3.0978 (3.1445)	LR 7.000e-04
0: TRAIN [3][1770/1885]	Time 0.194 (0.144)	Data 2.14e-04 (3.25e-04)	Tok/s 106209 (96063)	Loss/tok 3.1828 (3.1438)	LR 7.000e-04
0: TRAIN [3][1780/1885]	Time 0.149 (0.143)	Data 2.15e-04 (3.25e-04)	Tok/s 98745 (96050)	Loss/tok 3.1428 (3.1434)	LR 7.000e-04
0: TRAIN [3][1790/1885]	Time 0.247 (0.143)	Data 2.21e-04 (3.24e-04)	Tok/s 106198 (96058)	Loss/tok 3.4571 (3.1434)	LR 7.000e-04
0: TRAIN [3][1800/1885]	Time 0.246 (0.144)	Data 2.17e-04 (3.23e-04)	Tok/s 106942 (96070)	Loss/tok 3.3487 (3.1431)	LR 7.000e-04
0: TRAIN [3][1810/1885]	Time 0.148 (0.144)	Data 2.18e-04 (3.23e-04)	Tok/s 99107 (96072)	Loss/tok 3.0448 (3.1428)	LR 7.000e-04
0: TRAIN [3][1820/1885]	Time 0.102 (0.144)	Data 2.19e-04 (3.22e-04)	Tok/s 89055 (96085)	Loss/tok 2.8717 (3.1430)	LR 7.000e-04
0: TRAIN [3][1830/1885]	Time 0.245 (0.144)	Data 2.26e-04 (3.22e-04)	Tok/s 106931 (96074)	Loss/tok 3.3975 (3.1429)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1840/1885]	Time 0.060 (0.144)	Data 2.20e-04 (3.21e-04)	Tok/s 76954 (96080)	Loss/tok 2.4075 (3.1431)	LR 7.000e-04
0: TRAIN [3][1850/1885]	Time 0.247 (0.144)	Data 1.08e-04 (3.20e-04)	Tok/s 104768 (96096)	Loss/tok 3.4334 (3.1430)	LR 7.000e-04
0: TRAIN [3][1860/1885]	Time 0.103 (0.144)	Data 1.08e-04 (3.19e-04)	Tok/s 87814 (96075)	Loss/tok 2.7725 (3.1426)	LR 7.000e-04
0: TRAIN [3][1870/1885]	Time 0.103 (0.143)	Data 1.09e-04 (3.18e-04)	Tok/s 87837 (96044)	Loss/tok 2.8142 (3.1423)	LR 7.000e-04
0: TRAIN [3][1880/1885]	Time 0.060 (0.143)	Data 2.19e-04 (3.17e-04)	Tok/s 76560 (96027)	Loss/tok 2.4139 (3.1423)	LR 7.000e-04
:::MLLOG {"namespace": "", "time_ms": 1594175587331, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1594175587332, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.583 (0.583)	Decoder iters 98.0 (98.0)	Tok/s 28324 (28324)
0: Running moses detokenizer
0: BLEU(score=24.07743695014936, counts=[37253, 18700, 10652, 6309], totals=[65688, 62685, 59682, 56684], precisions=[56.71203263914261, 29.831698173406718, 17.847927348279214, 11.130124902970856], bp=1.0, sys_len=65688, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1594175588921, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2408, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1594175588921, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1412	Test BLEU: 24.08
0: Performance: Epoch: 3	Training: 768286 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1594175588921, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1594175588921, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-07-08 02:33:14 AM
RESULT,RNN_TRANSLATOR,,1111,Fujitsu,2020-07-08 02:14:43 AM
