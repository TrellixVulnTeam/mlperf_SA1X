Beginning trial 1 of 1
Gathering sys log on gx2570-03
:::MLLOG {"namespace": "", "time_ms": 1593834273685, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593834273721, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Fujitsu", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593834273722, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593834273722, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593834273722, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xGX2570M5", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
Clearing caches
:::MLLOG {"namespace": "", "time_ms": 1593834277002, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/opt/conda/lib/python3.6/site-packages/mlperf_logging/mllog/mllog.py", "lineno": 261}}
Launching on node gx2570-03
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e PGSYSTEM=PG -e 'MULTI_NODE= --master_port=4260' -e LR=2.8e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6053 -e DECAY_INTERVAL=859 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=65 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200704114252288086675 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200704114252288086675 ./run_and_time.sh
STARTING TIMING RUN AT 2020-07-04 03:44:38 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.8e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6053
+ DECAY_INTERVAL=859
+ TARGET=24.0
+ MAX_SEQ_LEN=65
+ NUMEPOCHS=8
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ PGSYSTEM=PG
+ [[ -f config_PG.sh ]]
+ source config_PG.sh
++ export PGNNODES=1
++ PGNNODES=1
+++ sed 's/^config_//'
+++ sed 's/\.sh$//'
++++ readlink -f config_PG.sh
+++ basename /workspace/rnn_translator/config_PG.sh
++ export PGSYSTEM=PG
++ PGSYSTEM=PG
++ export WALLTIME=00:30:00
++ WALLTIME=00:30:00
++ export LR=2.8e-3
++ LR=2.8e-3
++ export TRAIN_BATCH_SIZE=256
++ TRAIN_BATCH_SIZE=256
++ export TEST_BATCH_SIZE=128
++ TEST_BATCH_SIZE=128
++ export WARMUP_STEPS=200
++ WARMUP_STEPS=200
++ export REMAIN_STEPS=6053
++ REMAIN_STEPS=6053
++ export DECAY_INTERVAL=859
++ DECAY_INTERVAL=859
++ export TARGET=24.0
++ TARGET=24.0
++ export MAX_SEQ_LEN=65
++ MAX_SEQ_LEN=65
++ export NUMEPOCHS=8
++ NUMEPOCHS=8
++ export MATH=fp16
++ MATH=fp16
++ export DIST_OPTS=
++ DIST_OPTS=
++ export 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ export PGNGPU=8
++ PGNGPU=8
++ export PGSOCKETCORES=24
++ PGSOCKETCORES=24
++ export PGHT=2
++ PGHT=2
++ export PGNSOCKET=2
++ PGNSOCKET=2
+ declare -a CMD
+ '[' -n '' ']'
+ CMD=('python' '-u' '-m' 'bind_launch' "--nsockets_per_node=${PGNSOCKET}" "--ncores_per_socket=${PGSOCKETCORES}" "--nproc_per_node=${PGNGPU}")
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ PG == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ python -u -m bind_launch --nsockets_per_node=2 --ncores_per_socket=24 --nproc_per_node=8 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 65 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.8e-3 --warmup-steps 200 --remain-steps 6053 --decay-interval 859 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593834280043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593834280043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593834280043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593834280043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593834280096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593834280098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593834280102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593834280104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=859, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.0028, math='fp16', max_length_test=150, max_length_train=65, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6053, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3104303208
:::MLLOG {"namespace": "", "time_ms": 1593834288215, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3104303208, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 184328564
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.0028}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.0028
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593834291104, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593834291105, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0028, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593834291105, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593834291105, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593834291105, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593834293608, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593834293621, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593834293622, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 65, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3866375 min length: 0 max length: 65 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593834293893, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593834293894, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3860480, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593834293894, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6053, 'decay_interval': 859, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6053
0: Scheduler decay interval: 859
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593834293894, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593834293895, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593834293895, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 859, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593834293895, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593834293895, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593834293895, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6053, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593834293895, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593834293896, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593834293896, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2127186755
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1885]	Time 0.453 (0.453)	Data 2.55e-01 (2.55e-01)	Tok/s 45522 (45522)	Loss/tok 10.6743 (10.6743)	LR 2.865e-05
0: TRAIN [0][10/1885]	Time 0.100 (0.180)	Data 2.23e-04 (2.33e-02)	Tok/s 90756 (94472)	Loss/tok 9.4440 (9.9948)	LR 3.607e-05
0: TRAIN [0][20/1885]	Time 0.146 (0.164)	Data 2.22e-04 (1.23e-02)	Tok/s 100506 (96323)	Loss/tok 9.1967 (9.6952)	LR 4.541e-05
0: TRAIN [0][30/1885]	Time 0.100 (0.152)	Data 2.22e-04 (8.42e-03)	Tok/s 89099 (95971)	Loss/tok 8.7655 (9.5029)	LR 5.717e-05
0: TRAIN [0][40/1885]	Time 0.145 (0.153)	Data 2.17e-04 (6.42e-03)	Tok/s 100231 (97091)	Loss/tok 8.5749 (9.2942)	LR 7.197e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][50/1885]	Time 0.191 (0.152)	Data 2.17e-04 (5.20e-03)	Tok/s 107037 (97541)	Loss/tok 8.4079 (9.1300)	LR 8.854e-05
0: TRAIN [0][60/1885]	Time 0.145 (0.150)	Data 2.20e-04 (4.39e-03)	Tok/s 101101 (97254)	Loss/tok 8.2164 (8.9946)	LR 1.115e-04
0: TRAIN [0][70/1885]	Time 0.191 (0.149)	Data 2.21e-04 (3.80e-03)	Tok/s 108789 (97346)	Loss/tok 8.1706 (8.8713)	LR 1.403e-04
0: TRAIN [0][80/1885]	Time 0.145 (0.147)	Data 2.18e-04 (3.36e-03)	Tok/s 101352 (97224)	Loss/tok 7.9786 (8.7711)	LR 1.767e-04
0: TRAIN [0][90/1885]	Time 0.192 (0.145)	Data 2.20e-04 (3.01e-03)	Tok/s 106111 (96979)	Loss/tok 8.1178 (8.6901)	LR 2.224e-04
0: TRAIN [0][100/1885]	Time 0.242 (0.148)	Data 2.20e-04 (2.74e-03)	Tok/s 108693 (97608)	Loss/tok 8.0547 (8.6032)	LR 2.800e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][110/1885]	Time 0.242 (0.148)	Data 2.20e-04 (2.51e-03)	Tok/s 107180 (97723)	Loss/tok 8.0833 (8.5436)	LR 3.445e-04
0: TRAIN [0][120/1885]	Time 0.243 (0.147)	Data 2.19e-04 (2.32e-03)	Tok/s 108330 (97713)	Loss/tok 7.9787 (8.4937)	LR 4.337e-04
0: TRAIN [0][130/1885]	Time 0.145 (0.148)	Data 2.19e-04 (2.16e-03)	Tok/s 101243 (97880)	Loss/tok 7.8388 (8.4429)	LR 5.460e-04
0: TRAIN [0][140/1885]	Time 0.244 (0.147)	Data 2.18e-04 (2.02e-03)	Tok/s 107526 (97677)	Loss/tok 8.1106 (8.3977)	LR 6.873e-04
0: TRAIN [0][150/1885]	Time 0.193 (0.147)	Data 2.16e-04 (1.90e-03)	Tok/s 105762 (97887)	Loss/tok 7.7960 (8.3486)	LR 8.653e-04
0: TRAIN [0][160/1885]	Time 0.147 (0.148)	Data 2.17e-04 (1.80e-03)	Tok/s 100120 (98069)	Loss/tok 7.5205 (8.2997)	LR 1.089e-03
0: TRAIN [0][170/1885]	Time 0.148 (0.147)	Data 2.21e-04 (1.70e-03)	Tok/s 98870 (97980)	Loss/tok 7.4024 (8.2530)	LR 1.371e-03
0: TRAIN [0][180/1885]	Time 0.102 (0.147)	Data 2.18e-04 (1.62e-03)	Tok/s 90678 (97991)	Loss/tok 6.8647 (8.2000)	LR 1.726e-03
0: TRAIN [0][190/1885]	Time 0.195 (0.147)	Data 2.16e-04 (1.55e-03)	Tok/s 104115 (98004)	Loss/tok 7.1977 (8.1413)	LR 2.173e-03
0: TRAIN [0][200/1885]	Time 0.101 (0.146)	Data 2.16e-04 (1.48e-03)	Tok/s 89414 (97849)	Loss/tok 6.5702 (8.0862)	LR 2.736e-03
0: TRAIN [0][210/1885]	Time 0.195 (0.146)	Data 2.27e-04 (1.42e-03)	Tok/s 103847 (97715)	Loss/tok 6.8730 (8.0294)	LR 2.800e-03
0: TRAIN [0][220/1885]	Time 0.145 (0.146)	Data 2.17e-04 (1.37e-03)	Tok/s 99767 (97719)	Loss/tok 6.5456 (7.9628)	LR 2.800e-03
0: TRAIN [0][230/1885]	Time 0.193 (0.146)	Data 2.24e-04 (1.32e-03)	Tok/s 106837 (97533)	Loss/tok 6.5790 (7.9046)	LR 2.800e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][240/1885]	Time 0.101 (0.146)	Data 2.18e-04 (1.27e-03)	Tok/s 88421 (97586)	Loss/tok 6.0432 (7.8396)	LR 2.800e-03
0: TRAIN [0][250/1885]	Time 0.102 (0.146)	Data 2.15e-04 (1.23e-03)	Tok/s 88277 (97649)	Loss/tok 5.7847 (7.7719)	LR 2.800e-03
0: TRAIN [0][260/1885]	Time 0.101 (0.147)	Data 2.17e-04 (1.19e-03)	Tok/s 91713 (97841)	Loss/tok 5.7516 (7.6948)	LR 2.800e-03
0: TRAIN [0][270/1885]	Time 0.193 (0.147)	Data 2.20e-04 (1.15e-03)	Tok/s 104768 (97744)	Loss/tok 6.1698 (7.6365)	LR 2.800e-03
0: TRAIN [0][280/1885]	Time 0.146 (0.147)	Data 2.17e-04 (1.12e-03)	Tok/s 100049 (97727)	Loss/tok 5.6639 (7.5745)	LR 2.800e-03
0: TRAIN [0][290/1885]	Time 0.147 (0.147)	Data 2.20e-04 (1.09e-03)	Tok/s 98987 (97762)	Loss/tok 5.7251 (7.5118)	LR 2.800e-03
0: TRAIN [0][300/1885]	Time 0.196 (0.147)	Data 2.16e-04 (1.06e-03)	Tok/s 103361 (97699)	Loss/tok 5.7324 (7.4550)	LR 2.800e-03
0: TRAIN [0][310/1885]	Time 0.192 (0.147)	Data 2.19e-04 (1.03e-03)	Tok/s 106542 (97720)	Loss/tok 5.7096 (7.3907)	LR 2.800e-03
0: TRAIN [0][320/1885]	Time 0.148 (0.147)	Data 2.18e-04 (1.01e-03)	Tok/s 98096 (97635)	Loss/tok 5.3903 (7.3332)	LR 2.800e-03
0: TRAIN [0][330/1885]	Time 0.102 (0.146)	Data 2.17e-04 (9.84e-04)	Tok/s 89786 (97500)	Loss/tok 4.9615 (7.2812)	LR 2.800e-03
0: TRAIN [0][340/1885]	Time 0.101 (0.146)	Data 2.18e-04 (9.62e-04)	Tok/s 91365 (97500)	Loss/tok 4.7488 (7.2211)	LR 2.800e-03
0: TRAIN [0][350/1885]	Time 0.100 (0.145)	Data 2.17e-04 (9.41e-04)	Tok/s 89546 (97361)	Loss/tok 4.7341 (7.1694)	LR 2.800e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][360/1885]	Time 0.102 (0.146)	Data 2.21e-04 (9.21e-04)	Tok/s 89983 (97401)	Loss/tok 4.4786 (7.1063)	LR 2.800e-03
0: TRAIN [0][370/1885]	Time 0.102 (0.146)	Data 2.19e-04 (9.02e-04)	Tok/s 87146 (97347)	Loss/tok 4.6059 (7.0517)	LR 2.800e-03
0: TRAIN [0][380/1885]	Time 0.102 (0.145)	Data 2.16e-04 (8.84e-04)	Tok/s 88543 (97315)	Loss/tok 4.3600 (6.9967)	LR 2.800e-03
0: TRAIN [0][390/1885]	Time 0.100 (0.145)	Data 2.20e-04 (8.67e-04)	Tok/s 90570 (97299)	Loss/tok 4.2975 (6.9418)	LR 2.800e-03
0: TRAIN [0][400/1885]	Time 0.102 (0.145)	Data 2.21e-04 (8.50e-04)	Tok/s 90117 (97261)	Loss/tok 4.3038 (6.8891)	LR 2.800e-03
0: TRAIN [0][410/1885]	Time 0.244 (0.146)	Data 2.17e-04 (8.35e-04)	Tok/s 106620 (97321)	Loss/tok 4.9467 (6.8261)	LR 2.800e-03
0: TRAIN [0][420/1885]	Time 0.102 (0.146)	Data 2.16e-04 (8.20e-04)	Tok/s 88465 (97341)	Loss/tok 4.3100 (6.7726)	LR 2.800e-03
0: TRAIN [0][430/1885]	Time 0.101 (0.145)	Data 2.19e-04 (8.06e-04)	Tok/s 90274 (97215)	Loss/tok 4.1825 (6.7278)	LR 2.800e-03
0: TRAIN [0][440/1885]	Time 0.193 (0.145)	Data 2.16e-04 (7.93e-04)	Tok/s 106298 (97208)	Loss/tok 4.7301 (6.6783)	LR 2.800e-03
0: TRAIN [0][450/1885]	Time 0.147 (0.145)	Data 2.33e-04 (7.80e-04)	Tok/s 100396 (97235)	Loss/tok 4.4779 (6.6277)	LR 2.800e-03
0: TRAIN [0][460/1885]	Time 0.147 (0.146)	Data 2.19e-04 (7.68e-04)	Tok/s 100651 (97272)	Loss/tok 4.3759 (6.5774)	LR 2.800e-03
0: TRAIN [0][470/1885]	Time 0.100 (0.145)	Data 2.16e-04 (7.56e-04)	Tok/s 91155 (97236)	Loss/tok 4.0638 (6.5338)	LR 2.800e-03
0: TRAIN [0][480/1885]	Time 0.195 (0.145)	Data 2.14e-04 (7.45e-04)	Tok/s 104891 (97222)	Loss/tok 4.6123 (6.4901)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][490/1885]	Time 0.146 (0.145)	Data 2.20e-04 (7.34e-04)	Tok/s 99391 (97200)	Loss/tok 4.2629 (6.4479)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][500/1885]	Time 0.246 (0.145)	Data 2.21e-04 (7.24e-04)	Tok/s 105156 (97227)	Loss/tok 4.7405 (6.4031)	LR 2.800e-03
0: TRAIN [0][510/1885]	Time 0.147 (0.145)	Data 2.22e-04 (7.14e-04)	Tok/s 98769 (97235)	Loss/tok 4.2652 (6.3594)	LR 2.800e-03
0: TRAIN [0][520/1885]	Time 0.102 (0.146)	Data 2.20e-04 (7.04e-04)	Tok/s 87512 (97258)	Loss/tok 3.9746 (6.3162)	LR 2.800e-03
0: TRAIN [0][530/1885]	Time 0.147 (0.146)	Data 2.19e-04 (6.95e-04)	Tok/s 100966 (97257)	Loss/tok 4.1828 (6.2772)	LR 2.800e-03
0: TRAIN [0][540/1885]	Time 0.147 (0.145)	Data 2.16e-04 (6.86e-04)	Tok/s 98627 (97228)	Loss/tok 4.2386 (6.2408)	LR 2.800e-03
0: TRAIN [0][550/1885]	Time 0.147 (0.146)	Data 2.20e-04 (6.78e-04)	Tok/s 100694 (97240)	Loss/tok 4.1235 (6.2024)	LR 2.800e-03
0: TRAIN [0][560/1885]	Time 0.102 (0.146)	Data 2.29e-04 (6.70e-04)	Tok/s 89330 (97249)	Loss/tok 3.7198 (6.1636)	LR 2.800e-03
0: TRAIN [0][570/1885]	Time 0.101 (0.145)	Data 2.15e-04 (6.62e-04)	Tok/s 89838 (97204)	Loss/tok 3.7273 (6.1321)	LR 2.800e-03
0: TRAIN [0][580/1885]	Time 0.245 (0.145)	Data 2.15e-04 (6.54e-04)	Tok/s 106795 (97115)	Loss/tok 4.4674 (6.1023)	LR 2.800e-03
0: TRAIN [0][590/1885]	Time 0.102 (0.145)	Data 2.16e-04 (6.47e-04)	Tok/s 88296 (97051)	Loss/tok 3.7356 (6.0721)	LR 2.800e-03
0: TRAIN [0][600/1885]	Time 0.149 (0.145)	Data 2.15e-04 (6.40e-04)	Tok/s 97751 (97090)	Loss/tok 3.9585 (6.0370)	LR 2.800e-03
0: TRAIN [0][610/1885]	Time 0.194 (0.145)	Data 2.16e-04 (6.33e-04)	Tok/s 105573 (97110)	Loss/tok 4.2177 (6.0027)	LR 2.800e-03
0: TRAIN [0][620/1885]	Time 0.194 (0.145)	Data 2.17e-04 (6.26e-04)	Tok/s 105424 (97140)	Loss/tok 4.3412 (5.9693)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][630/1885]	Time 0.060 (0.145)	Data 2.15e-04 (6.19e-04)	Tok/s 75610 (97105)	Loss/tok 2.9685 (5.9407)	LR 2.800e-03
0: TRAIN [0][640/1885]	Time 0.061 (0.145)	Data 2.20e-04 (6.13e-04)	Tok/s 75008 (97096)	Loss/tok 2.9948 (5.9112)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][650/1885]	Time 0.245 (0.145)	Data 2.20e-04 (6.07e-04)	Tok/s 106528 (97046)	Loss/tok 4.3834 (5.8849)	LR 2.800e-03
0: TRAIN [0][660/1885]	Time 0.194 (0.145)	Data 2.16e-04 (6.01e-04)	Tok/s 105924 (97042)	Loss/tok 4.1596 (5.8556)	LR 2.800e-03
0: TRAIN [0][670/1885]	Time 0.194 (0.145)	Data 2.14e-04 (5.95e-04)	Tok/s 103809 (97057)	Loss/tok 4.4440 (5.8270)	LR 2.800e-03
0: TRAIN [0][680/1885]	Time 0.061 (0.145)	Data 1.94e-04 (5.89e-04)	Tok/s 74885 (97014)	Loss/tok 3.1936 (5.8020)	LR 2.800e-03
0: TRAIN [0][690/1885]	Time 0.195 (0.145)	Data 2.27e-04 (5.84e-04)	Tok/s 104475 (97024)	Loss/tok 4.2318 (5.7753)	LR 2.800e-03
0: TRAIN [0][700/1885]	Time 0.103 (0.145)	Data 2.18e-04 (5.79e-04)	Tok/s 87122 (97042)	Loss/tok 3.7994 (5.7488)	LR 2.800e-03
0: TRAIN [0][710/1885]	Time 0.195 (0.145)	Data 2.16e-04 (5.74e-04)	Tok/s 104433 (97014)	Loss/tok 4.1259 (5.7252)	LR 2.800e-03
0: TRAIN [0][720/1885]	Time 0.101 (0.145)	Data 2.15e-04 (5.69e-04)	Tok/s 91312 (96991)	Loss/tok 3.6039 (5.7016)	LR 2.800e-03
0: TRAIN [0][730/1885]	Time 0.061 (0.145)	Data 2.00e-04 (5.64e-04)	Tok/s 75511 (96918)	Loss/tok 2.8918 (5.6813)	LR 2.800e-03
0: TRAIN [0][740/1885]	Time 0.195 (0.145)	Data 2.16e-04 (5.59e-04)	Tok/s 103299 (96953)	Loss/tok 4.0625 (5.6551)	LR 2.800e-03
0: TRAIN [0][750/1885]	Time 0.102 (0.145)	Data 2.14e-04 (5.54e-04)	Tok/s 88141 (96968)	Loss/tok 3.4770 (5.6318)	LR 2.800e-03
0: TRAIN [0][760/1885]	Time 0.145 (0.145)	Data 2.19e-04 (5.50e-04)	Tok/s 100616 (96963)	Loss/tok 3.9311 (5.6095)	LR 2.800e-03
0: TRAIN [0][770/1885]	Time 0.193 (0.145)	Data 2.16e-04 (5.46e-04)	Tok/s 104977 (96927)	Loss/tok 4.0193 (5.5888)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][780/1885]	Time 0.194 (0.145)	Data 2.15e-04 (5.41e-04)	Tok/s 104702 (96906)	Loss/tok 4.1279 (5.5669)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][790/1885]	Time 0.246 (0.145)	Data 2.16e-04 (5.37e-04)	Tok/s 106273 (96943)	Loss/tok 4.2703 (5.5436)	LR 2.800e-03
0: TRAIN [0][800/1885]	Time 0.147 (0.145)	Data 2.17e-04 (5.33e-04)	Tok/s 100372 (96957)	Loss/tok 3.7856 (5.5221)	LR 2.800e-03
0: TRAIN [0][810/1885]	Time 0.101 (0.145)	Data 2.20e-04 (5.29e-04)	Tok/s 89469 (96975)	Loss/tok 3.4458 (5.5002)	LR 2.800e-03
0: TRAIN [0][820/1885]	Time 0.102 (0.145)	Data 2.16e-04 (5.26e-04)	Tok/s 89627 (96949)	Loss/tok 3.5035 (5.4817)	LR 2.800e-03
0: TRAIN [0][830/1885]	Time 0.102 (0.145)	Data 2.16e-04 (5.22e-04)	Tok/s 88970 (96975)	Loss/tok 3.6332 (5.4601)	LR 2.800e-03
0: TRAIN [0][840/1885]	Time 0.245 (0.145)	Data 2.03e-04 (5.18e-04)	Tok/s 108954 (96968)	Loss/tok 4.0855 (5.4403)	LR 2.800e-03
0: TRAIN [0][850/1885]	Time 0.146 (0.145)	Data 2.16e-04 (5.15e-04)	Tok/s 101197 (96927)	Loss/tok 3.7738 (5.4239)	LR 2.800e-03
0: TRAIN [0][860/1885]	Time 0.103 (0.145)	Data 2.15e-04 (5.11e-04)	Tok/s 87175 (96917)	Loss/tok 3.5089 (5.4061)	LR 2.800e-03
0: TRAIN [0][870/1885]	Time 0.101 (0.144)	Data 2.19e-04 (5.08e-04)	Tok/s 89445 (96908)	Loss/tok 3.4144 (5.3883)	LR 2.800e-03
0: TRAIN [0][880/1885]	Time 0.103 (0.144)	Data 2.17e-04 (5.04e-04)	Tok/s 86654 (96909)	Loss/tok 3.4628 (5.3702)	LR 2.800e-03
0: TRAIN [0][890/1885]	Time 0.148 (0.144)	Data 2.16e-04 (5.01e-04)	Tok/s 98029 (96898)	Loss/tok 3.7894 (5.3535)	LR 2.800e-03
0: TRAIN [0][900/1885]	Time 0.246 (0.145)	Data 2.16e-04 (4.98e-04)	Tok/s 105567 (96937)	Loss/tok 4.2289 (5.3343)	LR 2.800e-03
0: TRAIN [0][910/1885]	Time 0.148 (0.145)	Data 2.15e-04 (4.95e-04)	Tok/s 97681 (96980)	Loss/tok 3.7840 (5.3159)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][920/1885]	Time 0.104 (0.145)	Data 1.86e-04 (4.92e-04)	Tok/s 87459 (96991)	Loss/tok 3.4646 (5.2991)	LR 2.800e-03
0: TRAIN [0][930/1885]	Time 0.103 (0.145)	Data 2.14e-04 (4.89e-04)	Tok/s 89182 (96961)	Loss/tok 3.4441 (5.2845)	LR 2.800e-03
0: TRAIN [0][940/1885]	Time 0.059 (0.145)	Data 2.12e-04 (4.86e-04)	Tok/s 77850 (96951)	Loss/tok 2.8258 (5.2688)	LR 2.800e-03
0: TRAIN [0][950/1885]	Time 0.193 (0.145)	Data 2.36e-04 (4.83e-04)	Tok/s 105695 (96948)	Loss/tok 4.0885 (5.2534)	LR 2.800e-03
0: TRAIN [0][960/1885]	Time 0.193 (0.145)	Data 2.25e-04 (4.80e-04)	Tok/s 107258 (96944)	Loss/tok 4.0146 (5.2387)	LR 2.800e-03
0: TRAIN [0][970/1885]	Time 0.102 (0.145)	Data 2.16e-04 (4.77e-04)	Tok/s 89233 (96908)	Loss/tok 3.5201 (5.2249)	LR 2.800e-03
0: TRAIN [0][980/1885]	Time 0.102 (0.144)	Data 2.27e-04 (4.75e-04)	Tok/s 88804 (96858)	Loss/tok 3.5028 (5.2122)	LR 2.800e-03
0: TRAIN [0][990/1885]	Time 0.102 (0.144)	Data 2.15e-04 (4.72e-04)	Tok/s 91138 (96846)	Loss/tok 3.4537 (5.1982)	LR 2.800e-03
0: TRAIN [0][1000/1885]	Time 0.102 (0.144)	Data 2.30e-04 (4.69e-04)	Tok/s 89361 (96820)	Loss/tok 3.3512 (5.1848)	LR 2.800e-03
0: TRAIN [0][1010/1885]	Time 0.194 (0.144)	Data 2.16e-04 (4.67e-04)	Tok/s 104729 (96835)	Loss/tok 3.9158 (5.1701)	LR 2.800e-03
0: TRAIN [0][1020/1885]	Time 0.059 (0.144)	Data 2.17e-04 (4.64e-04)	Tok/s 76777 (96779)	Loss/tok 2.8867 (5.1579)	LR 2.800e-03
0: TRAIN [0][1030/1885]	Time 0.101 (0.144)	Data 2.18e-04 (4.62e-04)	Tok/s 89930 (96773)	Loss/tok 3.4506 (5.1439)	LR 2.800e-03
0: TRAIN [0][1040/1885]	Time 0.197 (0.144)	Data 2.32e-04 (4.60e-04)	Tok/s 105292 (96780)	Loss/tok 3.8682 (5.1298)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1050/1885]	Time 0.194 (0.144)	Data 2.04e-04 (4.57e-04)	Tok/s 105248 (96752)	Loss/tok 4.0312 (5.1179)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1060/1885]	Time 0.146 (0.144)	Data 2.17e-04 (4.55e-04)	Tok/s 100012 (96765)	Loss/tok 3.6840 (5.1049)	LR 2.800e-03
0: TRAIN [0][1070/1885]	Time 0.194 (0.144)	Data 2.19e-04 (4.53e-04)	Tok/s 105165 (96795)	Loss/tok 3.8512 (5.0908)	LR 2.800e-03
0: TRAIN [0][1080/1885]	Time 0.059 (0.144)	Data 2.19e-04 (4.51e-04)	Tok/s 76648 (96778)	Loss/tok 2.8738 (5.0789)	LR 2.800e-03
0: TRAIN [0][1090/1885]	Time 0.244 (0.144)	Data 2.23e-04 (4.48e-04)	Tok/s 106734 (96758)	Loss/tok 4.1884 (5.0674)	LR 2.800e-03
0: TRAIN [0][1100/1885]	Time 0.195 (0.144)	Data 2.17e-04 (4.46e-04)	Tok/s 103595 (96759)	Loss/tok 3.7924 (5.0548)	LR 2.800e-03
0: TRAIN [0][1110/1885]	Time 0.194 (0.144)	Data 1.77e-04 (4.44e-04)	Tok/s 104348 (96746)	Loss/tok 3.8504 (5.0423)	LR 2.800e-03
0: TRAIN [0][1120/1885]	Time 0.194 (0.144)	Data 2.16e-04 (4.42e-04)	Tok/s 105079 (96745)	Loss/tok 4.0040 (5.0304)	LR 2.800e-03
0: TRAIN [0][1130/1885]	Time 0.103 (0.145)	Data 1.80e-04 (4.40e-04)	Tok/s 87794 (96788)	Loss/tok 3.4051 (5.0167)	LR 2.800e-03
0: TRAIN [0][1140/1885]	Time 0.147 (0.145)	Data 2.15e-04 (4.38e-04)	Tok/s 97199 (96787)	Loss/tok 3.7665 (5.0051)	LR 2.800e-03
0: TRAIN [0][1150/1885]	Time 0.147 (0.145)	Data 2.16e-04 (4.36e-04)	Tok/s 101016 (96794)	Loss/tok 3.6780 (4.9938)	LR 2.800e-03
0: TRAIN [0][1160/1885]	Time 0.246 (0.145)	Data 2.18e-04 (4.34e-04)	Tok/s 106630 (96828)	Loss/tok 4.0635 (4.9806)	LR 2.800e-03
0: TRAIN [0][1170/1885]	Time 0.101 (0.145)	Data 2.21e-04 (4.32e-04)	Tok/s 90971 (96825)	Loss/tok 3.4013 (4.9695)	LR 2.800e-03
0: TRAIN [0][1180/1885]	Time 0.101 (0.145)	Data 2.19e-04 (4.30e-04)	Tok/s 88544 (96817)	Loss/tok 3.2555 (4.9588)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1190/1885]	Time 0.147 (0.145)	Data 2.16e-04 (4.28e-04)	Tok/s 99640 (96819)	Loss/tok 3.7501 (4.9481)	LR 2.800e-03
0: TRAIN [0][1200/1885]	Time 0.101 (0.145)	Data 2.16e-04 (4.27e-04)	Tok/s 88140 (96770)	Loss/tok 3.4274 (4.9390)	LR 2.800e-03
0: TRAIN [0][1210/1885]	Time 0.103 (0.145)	Data 2.16e-04 (4.25e-04)	Tok/s 89527 (96764)	Loss/tok 3.2950 (4.9286)	LR 2.800e-03
0: TRAIN [0][1220/1885]	Time 0.147 (0.145)	Data 2.19e-04 (4.23e-04)	Tok/s 99362 (96762)	Loss/tok 3.6497 (4.9185)	LR 2.800e-03
0: TRAIN [0][1230/1885]	Time 0.101 (0.144)	Data 2.13e-04 (4.21e-04)	Tok/s 91304 (96734)	Loss/tok 3.3542 (4.9092)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1240/1885]	Time 0.195 (0.145)	Data 2.32e-04 (4.20e-04)	Tok/s 105707 (96751)	Loss/tok 3.8975 (4.8985)	LR 2.800e-03
0: TRAIN [0][1250/1885]	Time 0.194 (0.145)	Data 2.17e-04 (4.18e-04)	Tok/s 104374 (96771)	Loss/tok 3.8584 (4.8882)	LR 2.800e-03
0: TRAIN [0][1260/1885]	Time 0.148 (0.145)	Data 2.17e-04 (4.17e-04)	Tok/s 99332 (96759)	Loss/tok 3.6416 (4.8788)	LR 2.800e-03
0: TRAIN [0][1270/1885]	Time 0.103 (0.145)	Data 1.89e-04 (4.15e-04)	Tok/s 88514 (96764)	Loss/tok 3.2617 (4.8686)	LR 2.800e-03
0: TRAIN [0][1280/1885]	Time 0.146 (0.145)	Data 2.14e-04 (4.13e-04)	Tok/s 99293 (96763)	Loss/tok 3.6097 (4.8588)	LR 2.800e-03
0: TRAIN [0][1290/1885]	Time 0.101 (0.145)	Data 2.18e-04 (4.12e-04)	Tok/s 91791 (96766)	Loss/tok 3.4074 (4.8491)	LR 2.800e-03
0: TRAIN [0][1300/1885]	Time 0.102 (0.145)	Data 2.18e-04 (4.10e-04)	Tok/s 90339 (96739)	Loss/tok 3.3727 (4.8404)	LR 2.800e-03
0: TRAIN [0][1310/1885]	Time 0.102 (0.145)	Data 2.16e-04 (4.09e-04)	Tok/s 88563 (96752)	Loss/tok 3.3591 (4.8307)	LR 2.800e-03
0: TRAIN [0][1320/1885]	Time 0.104 (0.145)	Data 2.03e-04 (4.07e-04)	Tok/s 90241 (96739)	Loss/tok 3.4230 (4.8222)	LR 2.800e-03
0: TRAIN [0][1330/1885]	Time 0.147 (0.145)	Data 2.16e-04 (4.06e-04)	Tok/s 99780 (96750)	Loss/tok 3.5097 (4.8128)	LR 2.800e-03
0: TRAIN [0][1340/1885]	Time 0.245 (0.145)	Data 2.24e-04 (4.05e-04)	Tok/s 106698 (96735)	Loss/tok 3.9736 (4.8043)	LR 2.800e-03
0: TRAIN [0][1350/1885]	Time 0.147 (0.145)	Data 2.16e-04 (4.03e-04)	Tok/s 100325 (96737)	Loss/tok 3.6773 (4.7953)	LR 2.800e-03
0: TRAIN [0][1360/1885]	Time 0.246 (0.145)	Data 1.65e-04 (4.02e-04)	Tok/s 106672 (96730)	Loss/tok 3.9649 (4.7867)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1370/1885]	Time 0.103 (0.144)	Data 2.14e-04 (4.00e-04)	Tok/s 88633 (96703)	Loss/tok 3.5104 (4.7788)	LR 2.800e-03
0: TRAIN [0][1380/1885]	Time 0.101 (0.144)	Data 2.20e-04 (3.99e-04)	Tok/s 91627 (96690)	Loss/tok 3.3358 (4.7708)	LR 2.800e-03
0: TRAIN [0][1390/1885]	Time 0.104 (0.144)	Data 1.68e-04 (3.98e-04)	Tok/s 88014 (96649)	Loss/tok 3.3224 (4.7641)	LR 2.800e-03
0: TRAIN [0][1400/1885]	Time 0.195 (0.144)	Data 2.16e-04 (3.96e-04)	Tok/s 105973 (96643)	Loss/tok 3.6322 (4.7558)	LR 2.800e-03
0: TRAIN [0][1410/1885]	Time 0.100 (0.144)	Data 2.14e-04 (3.95e-04)	Tok/s 92344 (96662)	Loss/tok 3.2095 (4.7471)	LR 2.800e-03
0: TRAIN [0][1420/1885]	Time 0.195 (0.144)	Data 2.16e-04 (3.94e-04)	Tok/s 104246 (96656)	Loss/tok 3.6341 (4.7391)	LR 2.800e-03
0: TRAIN [0][1430/1885]	Time 0.196 (0.144)	Data 2.18e-04 (3.92e-04)	Tok/s 103721 (96640)	Loss/tok 3.7767 (4.7317)	LR 2.800e-03
0: TRAIN [0][1440/1885]	Time 0.245 (0.144)	Data 2.18e-04 (3.91e-04)	Tok/s 106676 (96637)	Loss/tok 3.9339 (4.7242)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1450/1885]	Time 0.102 (0.144)	Data 2.20e-04 (3.90e-04)	Tok/s 87464 (96618)	Loss/tok 3.3265 (4.7169)	LR 2.800e-03
0: TRAIN [0][1460/1885]	Time 0.104 (0.144)	Data 1.50e-04 (3.89e-04)	Tok/s 87305 (96612)	Loss/tok 3.4064 (4.7095)	LR 2.800e-03
0: TRAIN [0][1470/1885]	Time 0.102 (0.144)	Data 1.05e-04 (3.88e-04)	Tok/s 89122 (96619)	Loss/tok 3.2028 (4.7018)	LR 2.800e-03
0: TRAIN [0][1480/1885]	Time 0.149 (0.144)	Data 1.05e-04 (3.86e-04)	Tok/s 99314 (96606)	Loss/tok 3.5368 (4.6946)	LR 2.800e-03
0: TRAIN [0][1490/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.84e-04)	Tok/s 91320 (96604)	Loss/tok 3.3492 (4.6874)	LR 2.800e-03
0: TRAIN [0][1500/1885]	Time 0.060 (0.144)	Data 2.17e-04 (3.83e-04)	Tok/s 76273 (96582)	Loss/tok 2.6427 (4.6806)	LR 2.800e-03
0: TRAIN [0][1510/1885]	Time 0.102 (0.144)	Data 2.02e-04 (3.82e-04)	Tok/s 91366 (96568)	Loss/tok 3.3285 (4.6738)	LR 2.800e-03
0: TRAIN [0][1520/1885]	Time 0.147 (0.143)	Data 2.12e-04 (3.81e-04)	Tok/s 99064 (96526)	Loss/tok 3.5612 (4.6676)	LR 2.800e-03
0: TRAIN [0][1530/1885]	Time 0.193 (0.144)	Data 2.15e-04 (3.80e-04)	Tok/s 104725 (96547)	Loss/tok 3.7209 (4.6597)	LR 2.800e-03
0: TRAIN [0][1540/1885]	Time 0.102 (0.144)	Data 2.25e-04 (3.78e-04)	Tok/s 89686 (96539)	Loss/tok 3.4195 (4.6533)	LR 2.800e-03
0: TRAIN [0][1550/1885]	Time 0.060 (0.143)	Data 2.16e-04 (3.77e-04)	Tok/s 76438 (96509)	Loss/tok 2.6488 (4.6470)	LR 2.800e-03
0: TRAIN [0][1560/1885]	Time 0.148 (0.144)	Data 1.87e-04 (3.76e-04)	Tok/s 99095 (96531)	Loss/tok 3.6326 (4.6393)	LR 2.800e-03
0: TRAIN [0][1570/1885]	Time 0.193 (0.143)	Data 2.18e-04 (3.75e-04)	Tok/s 106398 (96502)	Loss/tok 3.7085 (4.6333)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1580/1885]	Time 0.196 (0.144)	Data 1.06e-04 (3.74e-04)	Tok/s 105354 (96515)	Loss/tok 3.6945 (4.6263)	LR 2.800e-03
0: TRAIN [0][1590/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.73e-04)	Tok/s 100765 (96488)	Loss/tok 3.6287 (4.6201)	LR 2.800e-03
0: TRAIN [0][1600/1885]	Time 0.104 (0.143)	Data 2.27e-04 (3.72e-04)	Tok/s 89299 (96481)	Loss/tok 3.2677 (4.6136)	LR 2.800e-03
0: TRAIN [0][1610/1885]	Time 0.148 (0.143)	Data 2.20e-04 (3.71e-04)	Tok/s 98509 (96468)	Loss/tok 3.6174 (4.6076)	LR 2.800e-03
0: TRAIN [0][1620/1885]	Time 0.148 (0.143)	Data 2.16e-04 (3.70e-04)	Tok/s 98500 (96448)	Loss/tok 3.5746 (4.6016)	LR 2.800e-03
0: TRAIN [0][1630/1885]	Time 0.148 (0.143)	Data 2.16e-04 (3.69e-04)	Tok/s 98935 (96445)	Loss/tok 3.4665 (4.5952)	LR 2.800e-03
0: TRAIN [0][1640/1885]	Time 0.103 (0.143)	Data 2.17e-04 (3.68e-04)	Tok/s 87341 (96437)	Loss/tok 3.2508 (4.5888)	LR 2.800e-03
0: TRAIN [0][1650/1885]	Time 0.245 (0.143)	Data 2.16e-04 (3.67e-04)	Tok/s 106839 (96430)	Loss/tok 3.8534 (4.5830)	LR 2.800e-03
0: TRAIN [0][1660/1885]	Time 0.103 (0.143)	Data 2.16e-04 (3.66e-04)	Tok/s 87248 (96447)	Loss/tok 3.3396 (4.5758)	LR 2.800e-03
0: TRAIN [0][1670/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.65e-04)	Tok/s 89622 (96445)	Loss/tok 3.3113 (4.5697)	LR 2.800e-03
0: TRAIN [0][1680/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.64e-04)	Tok/s 87062 (96422)	Loss/tok 3.0580 (4.5642)	LR 2.800e-03
0: TRAIN [0][1690/1885]	Time 0.146 (0.143)	Data 2.16e-04 (3.64e-04)	Tok/s 101168 (96443)	Loss/tok 3.5082 (4.5573)	LR 2.800e-03
0: TRAIN [0][1700/1885]	Time 0.245 (0.143)	Data 2.16e-04 (3.63e-04)	Tok/s 107190 (96460)	Loss/tok 3.8560 (4.5510)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1710/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.62e-04)	Tok/s 88046 (96433)	Loss/tok 3.2737 (4.5456)	LR 2.800e-03
0: TRAIN [0][1720/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.61e-04)	Tok/s 89129 (96421)	Loss/tok 3.2603 (4.5402)	LR 2.800e-03
0: TRAIN [0][1730/1885]	Time 0.100 (0.143)	Data 1.86e-04 (3.60e-04)	Tok/s 91394 (96409)	Loss/tok 3.3775 (4.5349)	LR 2.800e-03
0: TRAIN [0][1740/1885]	Time 0.102 (0.143)	Data 2.24e-04 (3.59e-04)	Tok/s 90849 (96410)	Loss/tok 3.2356 (4.5289)	LR 2.800e-03
0: TRAIN [0][1750/1885]	Time 0.101 (0.143)	Data 1.74e-04 (3.58e-04)	Tok/s 90204 (96413)	Loss/tok 3.2820 (4.5230)	LR 2.800e-03
0: TRAIN [0][1760/1885]	Time 0.060 (0.143)	Data 1.48e-04 (3.57e-04)	Tok/s 75194 (96361)	Loss/tok 2.6713 (4.5187)	LR 2.800e-03
0: TRAIN [0][1770/1885]	Time 0.103 (0.143)	Data 2.16e-04 (3.57e-04)	Tok/s 88531 (96364)	Loss/tok 3.2903 (4.5130)	LR 2.800e-03
0: TRAIN [0][1780/1885]	Time 0.145 (0.143)	Data 2.16e-04 (3.56e-04)	Tok/s 100655 (96352)	Loss/tok 3.6636 (4.5078)	LR 2.800e-03
0: TRAIN [0][1790/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.55e-04)	Tok/s 88191 (96348)	Loss/tok 3.1619 (4.5024)	LR 2.800e-03
0: TRAIN [0][1800/1885]	Time 0.102 (0.143)	Data 1.75e-04 (3.54e-04)	Tok/s 87808 (96364)	Loss/tok 3.1902 (4.4964)	LR 2.800e-03
0: TRAIN [0][1810/1885]	Time 0.102 (0.143)	Data 2.13e-04 (3.53e-04)	Tok/s 89504 (96369)	Loss/tok 3.2224 (4.4911)	LR 2.800e-03
0: TRAIN [0][1820/1885]	Time 0.195 (0.143)	Data 2.15e-04 (3.52e-04)	Tok/s 105775 (96373)	Loss/tok 3.6456 (4.4855)	LR 2.800e-03
0: TRAIN [0][1830/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.52e-04)	Tok/s 99155 (96376)	Loss/tok 3.5332 (4.4805)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [0][1840/1885]	Time 0.150 (0.143)	Data 1.59e-04 (3.51e-04)	Tok/s 98781 (96392)	Loss/tok 3.3512 (4.4746)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1850/1885]	Time 0.194 (0.143)	Data 1.91e-04 (3.50e-04)	Tok/s 104472 (96389)	Loss/tok 3.5988 (4.4693)	LR 2.800e-03
0: TRAIN [0][1860/1885]	Time 0.247 (0.143)	Data 2.21e-04 (3.49e-04)	Tok/s 106649 (96386)	Loss/tok 3.7558 (4.4642)	LR 2.800e-03
0: TRAIN [0][1870/1885]	Time 0.146 (0.143)	Data 2.16e-04 (3.49e-04)	Tok/s 100008 (96376)	Loss/tok 3.5102 (4.4593)	LR 2.800e-03
0: TRAIN [0][1880/1885]	Time 0.103 (0.143)	Data 2.03e-04 (3.48e-04)	Tok/s 89578 (96363)	Loss/tok 3.2533 (4.4544)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593834564089, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593834564089, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.717 (0.717)	Decoder iters 149.0 (149.0)	Tok/s 23937 (23937)
0: Running moses detokenizer
0: BLEU(score=18.4035786489685, counts=[35222, 16059, 8566, 4730], totals=[71443, 68440, 65438, 62440], precisions=[49.300841230071526, 23.464348334307424, 13.09025336960176, 7.575272261370916], bp=1.0, sys_len=71443, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593834566025, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.184, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593834566025, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.4528	Test BLEU: 18.40
0: Performance: Epoch: 0	Training: 770753 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593834566026, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593834566026, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593834566026, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 352373654
0: TRAIN [1][0/1885]	Time 0.417 (0.417)	Data 2.27e-01 (2.27e-01)	Tok/s 49110 (49110)	Loss/tok 3.6068 (3.6068)	LR 2.800e-03
0: TRAIN [1][10/1885]	Time 0.146 (0.165)	Data 2.20e-04 (2.08e-02)	Tok/s 99275 (90689)	Loss/tok 3.4366 (3.4202)	LR 2.800e-03
0: TRAIN [1][20/1885]	Time 0.149 (0.167)	Data 2.17e-04 (1.10e-02)	Tok/s 98408 (94918)	Loss/tok 3.3999 (3.4772)	LR 2.800e-03
0: TRAIN [1][30/1885]	Time 0.105 (0.157)	Data 2.01e-04 (7.52e-03)	Tok/s 86424 (94184)	Loss/tok 3.1463 (3.4572)	LR 2.800e-03
0: TRAIN [1][40/1885]	Time 0.146 (0.157)	Data 2.16e-04 (5.74e-03)	Tok/s 100190 (94953)	Loss/tok 3.5135 (3.4686)	LR 2.800e-03
0: TRAIN [1][50/1885]	Time 0.104 (0.155)	Data 2.16e-04 (4.65e-03)	Tok/s 85278 (94873)	Loss/tok 3.1599 (3.4566)	LR 2.800e-03
0: TRAIN [1][60/1885]	Time 0.104 (0.158)	Data 2.17e-04 (3.92e-03)	Tok/s 87969 (95634)	Loss/tok 3.0342 (3.4678)	LR 2.800e-03
0: TRAIN [1][70/1885]	Time 0.148 (0.159)	Data 2.15e-04 (3.40e-03)	Tok/s 99660 (96264)	Loss/tok 3.3811 (3.4680)	LR 2.800e-03
0: TRAIN [1][80/1885]	Time 0.245 (0.158)	Data 2.14e-04 (3.01e-03)	Tok/s 106631 (96193)	Loss/tok 3.7623 (3.4643)	LR 2.800e-03
0: TRAIN [1][90/1885]	Time 0.148 (0.158)	Data 2.16e-04 (2.70e-03)	Tok/s 99819 (96413)	Loss/tok 3.3652 (3.4694)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][100/1885]	Time 0.196 (0.157)	Data 2.14e-04 (2.45e-03)	Tok/s 103719 (96514)	Loss/tok 3.5861 (3.4619)	LR 2.800e-03
0: TRAIN [1][110/1885]	Time 0.149 (0.154)	Data 2.17e-04 (2.25e-03)	Tok/s 97053 (96095)	Loss/tok 3.4395 (3.4520)	LR 2.800e-03
0: TRAIN [1][120/1885]	Time 0.104 (0.152)	Data 2.15e-04 (2.08e-03)	Tok/s 86992 (95545)	Loss/tok 3.1559 (3.4459)	LR 2.800e-03
0: TRAIN [1][130/1885]	Time 0.105 (0.151)	Data 2.15e-04 (1.93e-03)	Tok/s 87189 (95424)	Loss/tok 3.1017 (3.4398)	LR 2.800e-03
0: TRAIN [1][140/1885]	Time 0.103 (0.149)	Data 2.12e-04 (1.81e-03)	Tok/s 87203 (95162)	Loss/tok 3.1752 (3.4312)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][150/1885]	Time 0.103 (0.148)	Data 2.18e-04 (1.70e-03)	Tok/s 87233 (94962)	Loss/tok 3.0721 (3.4287)	LR 2.800e-03
0: TRAIN [1][160/1885]	Time 0.104 (0.148)	Data 1.88e-04 (1.61e-03)	Tok/s 85716 (94935)	Loss/tok 3.2305 (3.4296)	LR 2.800e-03
0: TRAIN [1][170/1885]	Time 0.104 (0.148)	Data 2.26e-04 (1.53e-03)	Tok/s 88045 (94981)	Loss/tok 3.1660 (3.4301)	LR 2.800e-03
0: TRAIN [1][180/1885]	Time 0.195 (0.149)	Data 2.16e-04 (1.45e-03)	Tok/s 105279 (95254)	Loss/tok 3.6041 (3.4334)	LR 2.800e-03
0: TRAIN [1][190/1885]	Time 0.105 (0.150)	Data 1.65e-04 (1.39e-03)	Tok/s 86461 (95483)	Loss/tok 3.2738 (3.4378)	LR 2.800e-03
0: TRAIN [1][200/1885]	Time 0.148 (0.148)	Data 1.88e-04 (1.33e-03)	Tok/s 99557 (95188)	Loss/tok 3.3657 (3.4303)	LR 2.800e-03
0: TRAIN [1][210/1885]	Time 0.197 (0.149)	Data 2.11e-04 (1.28e-03)	Tok/s 102189 (95418)	Loss/tok 3.5085 (3.4346)	LR 2.800e-03
0: TRAIN [1][220/1885]	Time 0.248 (0.149)	Data 1.67e-04 (1.23e-03)	Tok/s 104916 (95394)	Loss/tok 3.6877 (3.4353)	LR 2.800e-03
0: TRAIN [1][230/1885]	Time 0.101 (0.148)	Data 2.16e-04 (1.18e-03)	Tok/s 89115 (95316)	Loss/tok 3.1831 (3.4348)	LR 2.800e-03
0: TRAIN [1][240/1885]	Time 0.195 (0.149)	Data 2.14e-04 (1.14e-03)	Tok/s 104752 (95395)	Loss/tok 3.5093 (3.4366)	LR 2.800e-03
0: TRAIN [1][250/1885]	Time 0.247 (0.149)	Data 1.65e-04 (1.11e-03)	Tok/s 106508 (95293)	Loss/tok 3.7316 (3.4377)	LR 2.800e-03
0: TRAIN [1][260/1885]	Time 0.105 (0.150)	Data 2.22e-04 (1.07e-03)	Tok/s 85606 (95429)	Loss/tok 3.1498 (3.4445)	LR 2.800e-03
0: TRAIN [1][270/1885]	Time 0.105 (0.149)	Data 2.15e-04 (1.04e-03)	Tok/s 86968 (95279)	Loss/tok 3.0348 (3.4394)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][280/1885]	Time 0.195 (0.149)	Data 2.13e-04 (1.01e-03)	Tok/s 104751 (95301)	Loss/tok 3.5406 (3.4393)	LR 2.800e-03
0: TRAIN [1][290/1885]	Time 0.061 (0.148)	Data 2.14e-04 (9.81e-04)	Tok/s 76109 (95267)	Loss/tok 2.6698 (3.4368)	LR 2.800e-03
0: TRAIN [1][300/1885]	Time 0.149 (0.149)	Data 2.02e-04 (9.55e-04)	Tok/s 97434 (95350)	Loss/tok 3.4245 (3.4403)	LR 2.800e-03
0: TRAIN [1][310/1885]	Time 0.103 (0.148)	Data 2.20e-04 (9.31e-04)	Tok/s 86320 (95246)	Loss/tok 3.1671 (3.4389)	LR 2.800e-03
0: TRAIN [1][320/1885]	Time 0.196 (0.148)	Data 2.18e-04 (9.09e-04)	Tok/s 105096 (95166)	Loss/tok 3.4213 (3.4381)	LR 2.800e-03
0: TRAIN [1][330/1885]	Time 0.148 (0.148)	Data 2.16e-04 (8.88e-04)	Tok/s 98579 (95137)	Loss/tok 3.4116 (3.4385)	LR 2.800e-03
0: TRAIN [1][340/1885]	Time 0.195 (0.149)	Data 2.01e-04 (8.67e-04)	Tok/s 105928 (95308)	Loss/tok 3.5250 (3.4433)	LR 2.800e-03
0: TRAIN [1][350/1885]	Time 0.147 (0.149)	Data 2.18e-04 (8.48e-04)	Tok/s 99517 (95269)	Loss/tok 3.4167 (3.4442)	LR 2.800e-03
0: TRAIN [1][360/1885]	Time 0.149 (0.148)	Data 2.21e-04 (8.31e-04)	Tok/s 98316 (95104)	Loss/tok 3.3526 (3.4405)	LR 2.800e-03
0: TRAIN [1][370/1885]	Time 0.149 (0.147)	Data 2.18e-04 (8.14e-04)	Tok/s 99259 (94930)	Loss/tok 3.4303 (3.4360)	LR 2.800e-03
0: TRAIN [1][380/1885]	Time 0.105 (0.147)	Data 2.19e-04 (7.98e-04)	Tok/s 87589 (94872)	Loss/tok 3.1543 (3.4343)	LR 2.800e-03
0: TRAIN [1][390/1885]	Time 0.149 (0.146)	Data 1.67e-04 (7.82e-04)	Tok/s 98211 (94840)	Loss/tok 3.4230 (3.4323)	LR 2.800e-03
0: TRAIN [1][400/1885]	Time 0.104 (0.146)	Data 1.76e-04 (7.68e-04)	Tok/s 87590 (94866)	Loss/tok 3.1756 (3.4310)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][410/1885]	Time 0.150 (0.146)	Data 2.20e-04 (7.54e-04)	Tok/s 97933 (94848)	Loss/tok 3.2885 (3.4287)	LR 2.800e-03
0: TRAIN [1][420/1885]	Time 0.248 (0.146)	Data 1.44e-04 (7.41e-04)	Tok/s 105339 (94843)	Loss/tok 3.7000 (3.4299)	LR 2.800e-03
0: TRAIN [1][430/1885]	Time 0.149 (0.147)	Data 2.18e-04 (7.28e-04)	Tok/s 100111 (94913)	Loss/tok 3.3264 (3.4295)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][440/1885]	Time 0.196 (0.147)	Data 1.88e-04 (7.17e-04)	Tok/s 104644 (95007)	Loss/tok 3.4944 (3.4302)	LR 2.800e-03
0: TRAIN [1][450/1885]	Time 0.148 (0.147)	Data 1.67e-04 (7.05e-04)	Tok/s 98938 (94993)	Loss/tok 3.4218 (3.4300)	LR 2.800e-03
0: TRAIN [1][460/1885]	Time 0.150 (0.147)	Data 2.17e-04 (6.94e-04)	Tok/s 98851 (95041)	Loss/tok 3.2977 (3.4295)	LR 2.800e-03
0: TRAIN [1][470/1885]	Time 0.105 (0.147)	Data 2.15e-04 (6.84e-04)	Tok/s 86541 (95088)	Loss/tok 3.0852 (3.4295)	LR 2.800e-03
0: TRAIN [1][480/1885]	Time 0.196 (0.147)	Data 2.18e-04 (6.74e-04)	Tok/s 102750 (95049)	Loss/tok 3.6501 (3.4291)	LR 2.800e-03
0: TRAIN [1][490/1885]	Time 0.150 (0.147)	Data 2.18e-04 (6.64e-04)	Tok/s 97506 (94993)	Loss/tok 3.3555 (3.4269)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][500/1885]	Time 0.105 (0.146)	Data 2.19e-04 (6.55e-04)	Tok/s 85366 (94862)	Loss/tok 3.0840 (3.4250)	LR 2.800e-03
0: TRAIN [1][510/1885]	Time 0.148 (0.146)	Data 2.16e-04 (6.46e-04)	Tok/s 98847 (94835)	Loss/tok 3.4950 (3.4242)	LR 2.800e-03
0: TRAIN [1][520/1885]	Time 0.246 (0.146)	Data 2.18e-04 (6.38e-04)	Tok/s 106116 (94927)	Loss/tok 3.7760 (3.4277)	LR 2.800e-03
0: TRAIN [1][530/1885]	Time 0.196 (0.147)	Data 2.18e-04 (6.30e-04)	Tok/s 102653 (94985)	Loss/tok 3.5722 (3.4297)	LR 2.800e-03
0: TRAIN [1][540/1885]	Time 0.195 (0.147)	Data 2.16e-04 (6.22e-04)	Tok/s 104491 (95025)	Loss/tok 3.5656 (3.4311)	LR 2.800e-03
0: TRAIN [1][550/1885]	Time 0.105 (0.146)	Data 2.18e-04 (6.14e-04)	Tok/s 86696 (94940)	Loss/tok 3.0680 (3.4300)	LR 2.800e-03
0: TRAIN [1][560/1885]	Time 0.103 (0.147)	Data 2.18e-04 (6.07e-04)	Tok/s 87522 (94974)	Loss/tok 3.0540 (3.4296)	LR 2.800e-03
0: TRAIN [1][570/1885]	Time 0.149 (0.146)	Data 2.16e-04 (6.00e-04)	Tok/s 97505 (94957)	Loss/tok 3.2504 (3.4275)	LR 2.800e-03
0: TRAIN [1][580/1885]	Time 0.245 (0.146)	Data 2.14e-04 (5.93e-04)	Tok/s 108037 (94960)	Loss/tok 3.7448 (3.4293)	LR 2.800e-03
0: TRAIN [1][590/1885]	Time 0.105 (0.146)	Data 2.18e-04 (5.87e-04)	Tok/s 86483 (94845)	Loss/tok 3.1402 (3.4265)	LR 2.800e-03
0: TRAIN [1][600/1885]	Time 0.150 (0.146)	Data 2.14e-04 (5.80e-04)	Tok/s 98686 (94828)	Loss/tok 3.4617 (3.4252)	LR 2.800e-03
0: TRAIN [1][610/1885]	Time 0.246 (0.146)	Data 1.66e-04 (5.74e-04)	Tok/s 106401 (94801)	Loss/tok 3.6965 (3.4245)	LR 2.800e-03
0: TRAIN [1][620/1885]	Time 0.149 (0.146)	Data 2.18e-04 (5.68e-04)	Tok/s 99224 (94860)	Loss/tok 3.3769 (3.4253)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][630/1885]	Time 0.149 (0.146)	Data 2.23e-04 (5.62e-04)	Tok/s 99508 (94893)	Loss/tok 3.3574 (3.4251)	LR 2.800e-03
0: TRAIN [1][640/1885]	Time 0.150 (0.146)	Data 2.17e-04 (5.56e-04)	Tok/s 97513 (94971)	Loss/tok 3.3982 (3.4255)	LR 2.800e-03
0: TRAIN [1][650/1885]	Time 0.063 (0.146)	Data 2.18e-04 (5.51e-04)	Tok/s 71892 (94967)	Loss/tok 2.5811 (3.4243)	LR 2.800e-03
0: TRAIN [1][660/1885]	Time 0.148 (0.146)	Data 1.65e-04 (5.46e-04)	Tok/s 100365 (94974)	Loss/tok 3.2758 (3.4235)	LR 2.800e-03
0: TRAIN [1][670/1885]	Time 0.063 (0.146)	Data 2.14e-04 (5.40e-04)	Tok/s 71762 (94922)	Loss/tok 2.5558 (3.4215)	LR 2.800e-03
0: TRAIN [1][680/1885]	Time 0.148 (0.146)	Data 2.17e-04 (5.35e-04)	Tok/s 98941 (94880)	Loss/tok 3.3745 (3.4199)	LR 2.800e-03
0: TRAIN [1][690/1885]	Time 0.149 (0.146)	Data 2.14e-04 (5.30e-04)	Tok/s 98737 (94897)	Loss/tok 3.2843 (3.4198)	LR 2.800e-03
0: TRAIN [1][700/1885]	Time 0.105 (0.145)	Data 2.13e-04 (5.25e-04)	Tok/s 86092 (94837)	Loss/tok 3.1597 (3.4183)	LR 2.800e-03
0: TRAIN [1][710/1885]	Time 0.150 (0.146)	Data 2.19e-04 (5.21e-04)	Tok/s 96904 (94910)	Loss/tok 3.3785 (3.4201)	LR 2.800e-03
0: TRAIN [1][720/1885]	Time 0.148 (0.145)	Data 1.38e-04 (5.16e-04)	Tok/s 98441 (94841)	Loss/tok 3.5188 (3.4191)	LR 2.800e-03
0: TRAIN [1][730/1885]	Time 0.104 (0.145)	Data 2.19e-04 (5.12e-04)	Tok/s 86805 (94811)	Loss/tok 3.0705 (3.4188)	LR 2.800e-03
0: TRAIN [1][740/1885]	Time 0.104 (0.145)	Data 1.80e-04 (5.08e-04)	Tok/s 86880 (94844)	Loss/tok 3.1737 (3.4184)	LR 2.800e-03
0: TRAIN [1][750/1885]	Time 0.194 (0.145)	Data 2.22e-04 (5.04e-04)	Tok/s 104733 (94839)	Loss/tok 3.5878 (3.4189)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][760/1885]	Time 0.102 (0.145)	Data 2.16e-04 (5.00e-04)	Tok/s 87486 (94822)	Loss/tok 3.0323 (3.4179)	LR 2.800e-03
0: TRAIN [1][770/1885]	Time 0.149 (0.145)	Data 2.23e-04 (4.96e-04)	Tok/s 97565 (94844)	Loss/tok 3.3037 (3.4181)	LR 2.800e-03
0: TRAIN [1][780/1885]	Time 0.198 (0.146)	Data 1.52e-04 (4.92e-04)	Tok/s 102268 (94857)	Loss/tok 3.3988 (3.4178)	LR 2.800e-03
0: TRAIN [1][790/1885]	Time 0.105 (0.146)	Data 2.18e-04 (4.88e-04)	Tok/s 86098 (94875)	Loss/tok 3.1796 (3.4178)	LR 2.800e-03
0: TRAIN [1][800/1885]	Time 0.103 (0.146)	Data 2.18e-04 (4.84e-04)	Tok/s 89428 (94872)	Loss/tok 3.2188 (3.4168)	LR 2.800e-03
0: TRAIN [1][810/1885]	Time 0.245 (0.145)	Data 2.18e-04 (4.81e-04)	Tok/s 106822 (94790)	Loss/tok 3.7608 (3.4160)	LR 2.800e-03
0: TRAIN [1][820/1885]	Time 0.148 (0.145)	Data 2.17e-04 (4.77e-04)	Tok/s 100495 (94811)	Loss/tok 3.3382 (3.4153)	LR 2.800e-03
0: TRAIN [1][830/1885]	Time 0.195 (0.145)	Data 2.30e-04 (4.74e-04)	Tok/s 103780 (94802)	Loss/tok 3.4614 (3.4146)	LR 2.800e-03
0: TRAIN [1][840/1885]	Time 0.195 (0.145)	Data 2.17e-04 (4.71e-04)	Tok/s 103931 (94814)	Loss/tok 3.5396 (3.4137)	LR 2.800e-03
0: TRAIN [1][850/1885]	Time 0.148 (0.145)	Data 2.04e-04 (4.68e-04)	Tok/s 99175 (94746)	Loss/tok 3.2737 (3.4122)	LR 2.800e-03
0: TRAIN [1][860/1885]	Time 0.105 (0.145)	Data 2.14e-04 (4.65e-04)	Tok/s 87668 (94742)	Loss/tok 3.1159 (3.4112)	LR 2.800e-03
0: TRAIN [1][870/1885]	Time 0.195 (0.145)	Data 2.14e-04 (4.62e-04)	Tok/s 104492 (94782)	Loss/tok 3.5250 (3.4115)	LR 2.800e-03
0: TRAIN [1][880/1885]	Time 0.148 (0.145)	Data 2.18e-04 (4.59e-04)	Tok/s 98858 (94781)	Loss/tok 3.3853 (3.4108)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][890/1885]	Time 0.104 (0.145)	Data 2.19e-04 (4.56e-04)	Tok/s 87811 (94783)	Loss/tok 3.0987 (3.4111)	LR 2.800e-03
0: TRAIN [1][900/1885]	Time 0.149 (0.145)	Data 2.16e-04 (4.53e-04)	Tok/s 97261 (94781)	Loss/tok 3.3381 (3.4111)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][910/1885]	Time 0.148 (0.145)	Data 1.87e-04 (4.50e-04)	Tok/s 99168 (94731)	Loss/tok 3.3673 (3.4098)	LR 2.800e-03
0: TRAIN [1][920/1885]	Time 0.106 (0.145)	Data 1.69e-04 (4.48e-04)	Tok/s 86602 (94715)	Loss/tok 3.1616 (3.4091)	LR 2.800e-03
0: TRAIN [1][930/1885]	Time 0.196 (0.145)	Data 2.02e-04 (4.45e-04)	Tok/s 104766 (94739)	Loss/tok 3.5299 (3.4099)	LR 2.800e-03
0: TRAIN [1][940/1885]	Time 0.150 (0.145)	Data 2.18e-04 (4.42e-04)	Tok/s 97719 (94748)	Loss/tok 3.3537 (3.4101)	LR 2.800e-03
0: TRAIN [1][950/1885]	Time 0.101 (0.145)	Data 2.17e-04 (4.40e-04)	Tok/s 88902 (94748)	Loss/tok 3.0963 (3.4094)	LR 2.800e-03
0: TRAIN [1][960/1885]	Time 0.195 (0.145)	Data 2.18e-04 (4.37e-04)	Tok/s 104426 (94719)	Loss/tok 3.4400 (3.4086)	LR 2.800e-03
0: TRAIN [1][970/1885]	Time 0.246 (0.145)	Data 2.19e-04 (4.35e-04)	Tok/s 105167 (94748)	Loss/tok 3.7597 (3.4092)	LR 2.800e-03
0: TRAIN [1][980/1885]	Time 0.102 (0.145)	Data 2.18e-04 (4.33e-04)	Tok/s 88485 (94711)	Loss/tok 3.1828 (3.4077)	LR 2.800e-03
0: TRAIN [1][990/1885]	Time 0.147 (0.144)	Data 2.17e-04 (4.30e-04)	Tok/s 100363 (94694)	Loss/tok 3.4259 (3.4071)	LR 2.800e-03
0: TRAIN [1][1000/1885]	Time 0.146 (0.145)	Data 2.16e-04 (4.28e-04)	Tok/s 99150 (94733)	Loss/tok 3.4344 (3.4072)	LR 2.800e-03
0: TRAIN [1][1010/1885]	Time 0.146 (0.145)	Data 2.15e-04 (4.26e-04)	Tok/s 102275 (94789)	Loss/tok 3.3263 (3.4071)	LR 2.800e-03
0: TRAIN [1][1020/1885]	Time 0.192 (0.145)	Data 2.19e-04 (4.24e-04)	Tok/s 103837 (94816)	Loss/tok 3.6185 (3.4077)	LR 2.800e-03
0: TRAIN [1][1030/1885]	Time 0.100 (0.145)	Data 2.13e-04 (4.22e-04)	Tok/s 91822 (94838)	Loss/tok 3.0503 (3.4074)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1040/1885]	Time 0.148 (0.145)	Data 2.15e-04 (4.20e-04)	Tok/s 101336 (94847)	Loss/tok 3.3851 (3.4068)	LR 2.800e-03
0: TRAIN [1][1050/1885]	Time 0.102 (0.145)	Data 2.15e-04 (4.18e-04)	Tok/s 88012 (94891)	Loss/tok 3.1113 (3.4068)	LR 2.800e-03
0: TRAIN [1][1060/1885]	Time 0.102 (0.145)	Data 2.11e-04 (4.16e-04)	Tok/s 87931 (94893)	Loss/tok 3.1289 (3.4061)	LR 2.800e-03
0: TRAIN [1][1070/1885]	Time 0.102 (0.145)	Data 2.30e-04 (4.14e-04)	Tok/s 86969 (94941)	Loss/tok 3.1648 (3.4068)	LR 2.800e-03
0: TRAIN [1][1080/1885]	Time 0.146 (0.145)	Data 2.13e-04 (4.12e-04)	Tok/s 102727 (94970)	Loss/tok 3.4410 (3.4069)	LR 2.800e-03
0: TRAIN [1][1090/1885]	Time 0.059 (0.145)	Data 2.14e-04 (4.10e-04)	Tok/s 75959 (94954)	Loss/tok 2.4864 (3.4053)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1100/1885]	Time 0.062 (0.145)	Data 1.42e-04 (4.09e-04)	Tok/s 75287 (94957)	Loss/tok 2.6452 (3.4055)	LR 2.800e-03
0: TRAIN [1][1110/1885]	Time 0.102 (0.145)	Data 2.13e-04 (4.07e-04)	Tok/s 88409 (94948)	Loss/tok 3.1651 (3.4049)	LR 2.800e-03
0: TRAIN [1][1120/1885]	Time 0.147 (0.145)	Data 2.14e-04 (4.05e-04)	Tok/s 98693 (94935)	Loss/tok 3.3530 (3.4038)	LR 2.800e-03
0: TRAIN [1][1130/1885]	Time 0.194 (0.145)	Data 2.15e-04 (4.03e-04)	Tok/s 105014 (94966)	Loss/tok 3.5583 (3.4041)	LR 2.800e-03
0: TRAIN [1][1140/1885]	Time 0.148 (0.145)	Data 2.17e-04 (4.01e-04)	Tok/s 100916 (94998)	Loss/tok 3.3481 (3.4039)	LR 2.800e-03
0: TRAIN [1][1150/1885]	Time 0.102 (0.145)	Data 2.32e-04 (4.00e-04)	Tok/s 87800 (95011)	Loss/tok 3.0062 (3.4040)	LR 2.800e-03
0: TRAIN [1][1160/1885]	Time 0.102 (0.145)	Data 2.15e-04 (3.98e-04)	Tok/s 89721 (95047)	Loss/tok 3.1112 (3.4038)	LR 2.800e-03
0: TRAIN [1][1170/1885]	Time 0.102 (0.145)	Data 2.12e-04 (3.96e-04)	Tok/s 90354 (95027)	Loss/tok 3.0717 (3.4024)	LR 2.800e-03
0: TRAIN [1][1180/1885]	Time 0.102 (0.145)	Data 2.14e-04 (3.95e-04)	Tok/s 88661 (95002)	Loss/tok 3.0472 (3.4020)	LR 2.800e-03
0: TRAIN [1][1190/1885]	Time 0.147 (0.144)	Data 2.00e-04 (3.93e-04)	Tok/s 101300 (94988)	Loss/tok 3.3628 (3.4007)	LR 2.800e-03
0: TRAIN [1][1200/1885]	Time 0.147 (0.144)	Data 2.11e-04 (3.92e-04)	Tok/s 100842 (94987)	Loss/tok 3.3224 (3.3998)	LR 2.800e-03
0: TRAIN [1][1210/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.90e-04)	Tok/s 89201 (94955)	Loss/tok 2.9844 (3.3986)	LR 2.800e-03
0: TRAIN [1][1220/1885]	Time 0.145 (0.144)	Data 2.13e-04 (3.89e-04)	Tok/s 100647 (94981)	Loss/tok 3.2848 (3.3989)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1230/1885]	Time 0.103 (0.144)	Data 2.10e-04 (3.87e-04)	Tok/s 88209 (94962)	Loss/tok 3.0838 (3.3977)	LR 2.800e-03
0: TRAIN [1][1240/1885]	Time 0.101 (0.144)	Data 2.20e-04 (3.86e-04)	Tok/s 89338 (94929)	Loss/tok 3.1281 (3.3968)	LR 2.800e-03
0: TRAIN [1][1250/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.85e-04)	Tok/s 90301 (94915)	Loss/tok 3.0824 (3.3958)	LR 2.800e-03
0: TRAIN [1][1260/1885]	Time 0.104 (0.143)	Data 2.26e-04 (3.83e-04)	Tok/s 87463 (94896)	Loss/tok 3.1062 (3.3955)	LR 2.800e-03
0: TRAIN [1][1270/1885]	Time 0.147 (0.143)	Data 2.17e-04 (3.82e-04)	Tok/s 100459 (94927)	Loss/tok 3.3009 (3.3951)	LR 2.800e-03
0: TRAIN [1][1280/1885]	Time 0.147 (0.144)	Data 2.15e-04 (3.81e-04)	Tok/s 100268 (94950)	Loss/tok 3.3472 (3.3951)	LR 2.800e-03
0: TRAIN [1][1290/1885]	Time 0.194 (0.144)	Data 2.12e-04 (3.79e-04)	Tok/s 105687 (94988)	Loss/tok 3.4464 (3.3944)	LR 2.800e-03
0: TRAIN [1][1300/1885]	Time 0.193 (0.144)	Data 2.16e-04 (3.78e-04)	Tok/s 107305 (95003)	Loss/tok 3.3851 (3.3938)	LR 2.800e-03
0: TRAIN [1][1310/1885]	Time 0.103 (0.144)	Data 1.35e-04 (3.77e-04)	Tok/s 86643 (95007)	Loss/tok 3.0419 (3.3935)	LR 2.800e-03
0: TRAIN [1][1320/1885]	Time 0.102 (0.144)	Data 2.31e-04 (3.75e-04)	Tok/s 89334 (95008)	Loss/tok 3.0956 (3.3927)	LR 2.800e-03
0: TRAIN [1][1330/1885]	Time 0.146 (0.144)	Data 2.16e-04 (3.74e-04)	Tok/s 100153 (95055)	Loss/tok 3.2611 (3.3927)	LR 2.800e-03
0: TRAIN [1][1340/1885]	Time 0.195 (0.144)	Data 2.16e-04 (3.73e-04)	Tok/s 104317 (95118)	Loss/tok 3.4954 (3.3937)	LR 2.800e-03
0: TRAIN [1][1350/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.72e-04)	Tok/s 88409 (95097)	Loss/tok 3.0073 (3.3928)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1360/1885]	Time 0.195 (0.144)	Data 1.05e-04 (3.71e-04)	Tok/s 105449 (95098)	Loss/tok 3.4620 (3.3921)	LR 2.800e-03
0: TRAIN [1][1370/1885]	Time 0.102 (0.144)	Data 1.05e-04 (3.69e-04)	Tok/s 87803 (95098)	Loss/tok 3.0507 (3.3917)	LR 2.800e-03
0: TRAIN [1][1380/1885]	Time 0.196 (0.144)	Data 1.08e-04 (3.67e-04)	Tok/s 104376 (95095)	Loss/tok 3.4581 (3.3917)	LR 2.800e-03
0: TRAIN [1][1390/1885]	Time 0.147 (0.144)	Data 2.20e-04 (3.65e-04)	Tok/s 99478 (95072)	Loss/tok 3.2820 (3.3912)	LR 2.800e-03
0: TRAIN [1][1400/1885]	Time 0.148 (0.144)	Data 2.19e-04 (3.64e-04)	Tok/s 100231 (95093)	Loss/tok 3.3889 (3.3915)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1410/1885]	Time 0.102 (0.144)	Data 2.17e-04 (3.63e-04)	Tok/s 88027 (95104)	Loss/tok 2.9977 (3.3917)	LR 2.800e-03
0: TRAIN [1][1420/1885]	Time 0.101 (0.144)	Data 2.17e-04 (3.62e-04)	Tok/s 90075 (95120)	Loss/tok 3.0291 (3.3916)	LR 2.800e-03
0: TRAIN [1][1430/1885]	Time 0.102 (0.144)	Data 2.19e-04 (3.61e-04)	Tok/s 89632 (95162)	Loss/tok 3.1459 (3.3917)	LR 2.800e-03
0: TRAIN [1][1440/1885]	Time 0.101 (0.144)	Data 2.16e-04 (3.60e-04)	Tok/s 88915 (95169)	Loss/tok 3.1120 (3.3911)	LR 2.800e-03
0: TRAIN [1][1450/1885]	Time 0.102 (0.144)	Data 2.15e-04 (3.59e-04)	Tok/s 88785 (95182)	Loss/tok 3.0387 (3.3907)	LR 2.800e-03
0: TRAIN [1][1460/1885]	Time 0.102 (0.144)	Data 2.02e-04 (3.58e-04)	Tok/s 87364 (95174)	Loss/tok 3.1140 (3.3902)	LR 2.800e-03
0: TRAIN [1][1470/1885]	Time 0.246 (0.144)	Data 2.15e-04 (3.57e-04)	Tok/s 107109 (95174)	Loss/tok 3.6965 (3.3898)	LR 2.800e-03
0: TRAIN [1][1480/1885]	Time 0.195 (0.144)	Data 2.03e-04 (3.56e-04)	Tok/s 103761 (95155)	Loss/tok 3.4975 (3.3891)	LR 2.800e-03
0: TRAIN [1][1490/1885]	Time 0.148 (0.144)	Data 1.05e-04 (3.55e-04)	Tok/s 99234 (95175)	Loss/tok 3.3147 (3.3894)	LR 2.800e-03
0: TRAIN [1][1500/1885]	Time 0.148 (0.144)	Data 2.10e-04 (3.54e-04)	Tok/s 99913 (95169)	Loss/tok 3.2833 (3.3887)	LR 2.800e-03
0: TRAIN [1][1510/1885]	Time 0.103 (0.144)	Data 2.13e-04 (3.53e-04)	Tok/s 88059 (95164)	Loss/tok 3.1369 (3.3881)	LR 2.800e-03
0: TRAIN [1][1520/1885]	Time 0.147 (0.144)	Data 2.14e-04 (3.52e-04)	Tok/s 101793 (95192)	Loss/tok 3.2629 (3.3880)	LR 2.800e-03
0: TRAIN [1][1530/1885]	Time 0.147 (0.144)	Data 1.74e-04 (3.51e-04)	Tok/s 99428 (95221)	Loss/tok 3.3275 (3.3881)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1540/1885]	Time 0.149 (0.144)	Data 2.15e-04 (3.50e-04)	Tok/s 100141 (95256)	Loss/tok 3.3594 (3.3881)	LR 2.800e-03
0: TRAIN [1][1550/1885]	Time 0.101 (0.144)	Data 2.15e-04 (3.49e-04)	Tok/s 88610 (95266)	Loss/tok 3.1213 (3.3873)	LR 2.800e-03
0: TRAIN [1][1560/1885]	Time 0.101 (0.144)	Data 2.14e-04 (3.48e-04)	Tok/s 90871 (95271)	Loss/tok 3.0864 (3.3868)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1570/1885]	Time 0.102 (0.144)	Data 2.12e-04 (3.47e-04)	Tok/s 88275 (95286)	Loss/tok 2.9903 (3.3863)	LR 2.800e-03
0: TRAIN [1][1580/1885]	Time 0.148 (0.144)	Data 2.30e-04 (3.46e-04)	Tok/s 98880 (95295)	Loss/tok 3.2245 (3.3861)	LR 2.800e-03
0: TRAIN [1][1590/1885]	Time 0.102 (0.144)	Data 2.13e-04 (3.45e-04)	Tok/s 89015 (95293)	Loss/tok 3.1018 (3.3859)	LR 2.800e-03
0: TRAIN [1][1600/1885]	Time 0.194 (0.144)	Data 2.14e-04 (3.45e-04)	Tok/s 105218 (95310)	Loss/tok 3.4719 (3.3852)	LR 2.800e-03
0: TRAIN [1][1610/1885]	Time 0.102 (0.144)	Data 2.10e-04 (3.44e-04)	Tok/s 89390 (95323)	Loss/tok 3.1627 (3.3852)	LR 2.800e-03
0: TRAIN [1][1620/1885]	Time 0.244 (0.144)	Data 2.17e-04 (3.43e-04)	Tok/s 107669 (95351)	Loss/tok 3.6379 (3.3855)	LR 2.800e-03
0: TRAIN [1][1630/1885]	Time 0.101 (0.144)	Data 2.14e-04 (3.42e-04)	Tok/s 89891 (95349)	Loss/tok 3.0989 (3.3854)	LR 2.800e-03
0: TRAIN [1][1640/1885]	Time 0.101 (0.144)	Data 2.23e-04 (3.42e-04)	Tok/s 87184 (95373)	Loss/tok 3.0764 (3.3858)	LR 2.800e-03
0: TRAIN [1][1650/1885]	Time 0.147 (0.144)	Data 2.11e-04 (3.41e-04)	Tok/s 99937 (95409)	Loss/tok 3.3621 (3.3866)	LR 2.800e-03
0: TRAIN [1][1660/1885]	Time 0.101 (0.144)	Data 2.12e-04 (3.40e-04)	Tok/s 90470 (95411)	Loss/tok 3.1252 (3.3866)	LR 2.800e-03
0: TRAIN [1][1670/1885]	Time 0.102 (0.144)	Data 2.11e-04 (3.39e-04)	Tok/s 89942 (95432)	Loss/tok 3.0611 (3.3862)	LR 2.800e-03
0: TRAIN [1][1680/1885]	Time 0.102 (0.144)	Data 2.12e-04 (3.39e-04)	Tok/s 87767 (95437)	Loss/tok 3.0339 (3.3857)	LR 2.800e-03
0: TRAIN [1][1690/1885]	Time 0.147 (0.144)	Data 1.03e-04 (3.38e-04)	Tok/s 99589 (95447)	Loss/tok 3.2879 (3.3854)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1700/1885]	Time 0.059 (0.144)	Data 2.18e-04 (3.37e-04)	Tok/s 77096 (95437)	Loss/tok 2.4725 (3.3848)	LR 2.800e-03
0: TRAIN [1][1710/1885]	Time 0.193 (0.144)	Data 2.15e-04 (3.36e-04)	Tok/s 105436 (95440)	Loss/tok 3.4953 (3.3845)	LR 2.800e-03
0: TRAIN [1][1720/1885]	Time 0.147 (0.144)	Data 2.16e-04 (3.36e-04)	Tok/s 100458 (95428)	Loss/tok 3.3651 (3.3839)	LR 2.800e-03
0: TRAIN [1][1730/1885]	Time 0.194 (0.144)	Data 2.13e-04 (3.35e-04)	Tok/s 105986 (95435)	Loss/tok 3.3299 (3.3832)	LR 2.800e-03
0: TRAIN [1][1740/1885]	Time 0.060 (0.144)	Data 2.14e-04 (3.34e-04)	Tok/s 75256 (95432)	Loss/tok 2.6701 (3.3827)	LR 2.800e-03
0: TRAIN [1][1750/1885]	Time 0.146 (0.144)	Data 2.27e-04 (3.33e-04)	Tok/s 100343 (95429)	Loss/tok 3.3177 (3.3821)	LR 2.800e-03
0: TRAIN [1][1760/1885]	Time 0.196 (0.144)	Data 1.96e-04 (3.33e-04)	Tok/s 102441 (95435)	Loss/tok 3.4781 (3.3815)	LR 2.800e-03
0: TRAIN [1][1770/1885]	Time 0.146 (0.144)	Data 2.10e-04 (3.32e-04)	Tok/s 100120 (95440)	Loss/tok 3.4264 (3.3812)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1780/1885]	Time 0.101 (0.144)	Data 2.12e-04 (3.31e-04)	Tok/s 90910 (95454)	Loss/tok 3.0572 (3.3812)	LR 2.800e-03
0: TRAIN [1][1790/1885]	Time 0.146 (0.144)	Data 2.13e-04 (3.31e-04)	Tok/s 100528 (95457)	Loss/tok 3.2948 (3.3807)	LR 2.800e-03
0: TRAIN [1][1800/1885]	Time 0.101 (0.144)	Data 2.12e-04 (3.30e-04)	Tok/s 89305 (95469)	Loss/tok 3.0761 (3.3808)	LR 2.800e-03
0: TRAIN [1][1810/1885]	Time 0.148 (0.144)	Data 2.12e-04 (3.29e-04)	Tok/s 98790 (95487)	Loss/tok 3.2275 (3.3811)	LR 2.800e-03
0: TRAIN [1][1820/1885]	Time 0.194 (0.144)	Data 1.87e-04 (3.29e-04)	Tok/s 104268 (95499)	Loss/tok 3.5901 (3.3812)	LR 2.800e-03
0: TRAIN [1][1830/1885]	Time 0.103 (0.144)	Data 2.16e-04 (3.28e-04)	Tok/s 90677 (95507)	Loss/tok 3.1077 (3.3811)	LR 2.800e-03
0: TRAIN [1][1840/1885]	Time 0.101 (0.144)	Data 2.17e-04 (3.28e-04)	Tok/s 90037 (95513)	Loss/tok 3.1578 (3.3810)	LR 2.800e-03
0: TRAIN [1][1850/1885]	Time 0.060 (0.144)	Data 2.13e-04 (3.27e-04)	Tok/s 75358 (95500)	Loss/tok 2.6248 (3.3805)	LR 2.800e-03
0: TRAIN [1][1860/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.26e-04)	Tok/s 90122 (95493)	Loss/tok 3.2411 (3.3802)	LR 2.800e-03
0: TRAIN [1][1870/1885]	Time 0.061 (0.144)	Data 2.12e-04 (3.26e-04)	Tok/s 75550 (95481)	Loss/tok 2.6341 (3.3795)	LR 2.800e-03
0: TRAIN [1][1880/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.25e-04)	Tok/s 100451 (95501)	Loss/tok 3.3149 (3.3796)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593834838359, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593834838359, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.568 (0.568)	Decoder iters 100.0 (100.0)	Tok/s 28997 (28997)
0: Running moses detokenizer
0: BLEU(score=21.708301004083168, counts=[35894, 17231, 9457, 5413], totals=[66042, 63039, 60037, 57039], precisions=[54.3502619545138, 27.333872681990513, 15.75195296233989, 9.489998071494941], bp=1.0, sys_len=66042, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593834840026, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21710000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593834840026, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3825	Test BLEU: 21.71
0: Performance: Epoch: 1	Training: 763801 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593834840027, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593834840027, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593834840027, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 163573839
0: TRAIN [2][0/1885]	Time 0.348 (0.348)	Data 2.19e-01 (2.19e-01)	Tok/s 26574 (26574)	Loss/tok 3.0546 (3.0546)	LR 2.800e-03
0: TRAIN [2][10/1885]	Time 0.103 (0.163)	Data 2.18e-04 (2.01e-02)	Tok/s 87138 (89376)	Loss/tok 2.9440 (3.2596)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][20/1885]	Time 0.061 (0.141)	Data 2.15e-04 (1.06e-02)	Tok/s 75297 (90128)	Loss/tok 2.4175 (3.2147)	LR 2.800e-03
0: TRAIN [2][30/1885]	Time 0.102 (0.139)	Data 2.14e-04 (7.27e-03)	Tok/s 89470 (91481)	Loss/tok 3.0003 (3.2093)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][40/1885]	Time 0.194 (0.141)	Data 2.11e-04 (5.55e-03)	Tok/s 105390 (92796)	Loss/tok 3.4458 (3.2353)	LR 2.800e-03
0: TRAIN [2][50/1885]	Time 0.147 (0.142)	Data 2.16e-04 (4.50e-03)	Tok/s 99325 (93588)	Loss/tok 3.1959 (3.2383)	LR 2.800e-03
0: TRAIN [2][60/1885]	Time 0.193 (0.143)	Data 1.45e-04 (3.80e-03)	Tok/s 107558 (94106)	Loss/tok 3.3909 (3.2404)	LR 2.800e-03
0: TRAIN [2][70/1885]	Time 0.247 (0.144)	Data 1.86e-04 (3.29e-03)	Tok/s 106796 (94295)	Loss/tok 3.4925 (3.2497)	LR 2.800e-03
0: TRAIN [2][80/1885]	Time 0.147 (0.144)	Data 2.15e-04 (2.91e-03)	Tok/s 98804 (94738)	Loss/tok 3.2182 (3.2523)	LR 2.800e-03
0: TRAIN [2][90/1885]	Time 0.194 (0.146)	Data 2.16e-04 (2.62e-03)	Tok/s 104604 (95137)	Loss/tok 3.3904 (3.2566)	LR 2.800e-03
0: TRAIN [2][100/1885]	Time 0.147 (0.143)	Data 2.14e-04 (2.38e-03)	Tok/s 98605 (94903)	Loss/tok 3.2793 (3.2490)	LR 2.800e-03
0: TRAIN [2][110/1885]	Time 0.195 (0.143)	Data 2.12e-04 (2.18e-03)	Tok/s 103991 (95038)	Loss/tok 3.4374 (3.2507)	LR 2.800e-03
0: TRAIN [2][120/1885]	Time 0.195 (0.144)	Data 2.11e-04 (2.02e-03)	Tok/s 105927 (94986)	Loss/tok 3.2367 (3.2529)	LR 2.800e-03
0: TRAIN [2][130/1885]	Time 0.147 (0.144)	Data 2.10e-04 (1.88e-03)	Tok/s 100264 (95117)	Loss/tok 3.1093 (3.2549)	LR 2.800e-03
0: TRAIN [2][140/1885]	Time 0.197 (0.144)	Data 1.49e-04 (1.76e-03)	Tok/s 104476 (95257)	Loss/tok 3.3758 (3.2535)	LR 2.800e-03
0: TRAIN [2][150/1885]	Time 0.103 (0.146)	Data 2.11e-04 (1.66e-03)	Tok/s 87375 (95614)	Loss/tok 3.0072 (3.2605)	LR 2.800e-03
0: TRAIN [2][160/1885]	Time 0.148 (0.146)	Data 2.12e-04 (1.57e-03)	Tok/s 100120 (95691)	Loss/tok 3.2138 (3.2596)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][170/1885]	Time 0.103 (0.146)	Data 1.75e-04 (1.49e-03)	Tok/s 89317 (95717)	Loss/tok 3.1014 (3.2610)	LR 2.800e-03
0: TRAIN [2][180/1885]	Time 0.103 (0.146)	Data 2.13e-04 (1.42e-03)	Tok/s 86821 (95834)	Loss/tok 2.9865 (3.2628)	LR 2.800e-03
0: TRAIN [2][190/1885]	Time 0.146 (0.146)	Data 2.11e-04 (1.35e-03)	Tok/s 100735 (95799)	Loss/tok 3.1902 (3.2601)	LR 2.800e-03
0: TRAIN [2][200/1885]	Time 0.149 (0.145)	Data 2.12e-04 (1.30e-03)	Tok/s 98437 (95706)	Loss/tok 3.2089 (3.2554)	LR 2.800e-03
0: TRAIN [2][210/1885]	Time 0.101 (0.144)	Data 2.16e-04 (1.25e-03)	Tok/s 91451 (95618)	Loss/tok 2.9543 (3.2542)	LR 2.800e-03
0: TRAIN [2][220/1885]	Time 0.147 (0.143)	Data 2.13e-04 (1.20e-03)	Tok/s 99305 (95433)	Loss/tok 3.2187 (3.2516)	LR 2.800e-03
0: TRAIN [2][230/1885]	Time 0.146 (0.143)	Data 2.19e-04 (1.16e-03)	Tok/s 101070 (95426)	Loss/tok 3.1724 (3.2531)	LR 2.800e-03
0: TRAIN [2][240/1885]	Time 0.246 (0.144)	Data 2.13e-04 (1.12e-03)	Tok/s 106497 (95528)	Loss/tok 3.5520 (3.2543)	LR 2.800e-03
0: TRAIN [2][250/1885]	Time 0.147 (0.144)	Data 2.11e-04 (1.08e-03)	Tok/s 98619 (95571)	Loss/tok 3.2738 (3.2538)	LR 2.800e-03
0: TRAIN [2][260/1885]	Time 0.147 (0.145)	Data 2.13e-04 (1.05e-03)	Tok/s 100926 (95734)	Loss/tok 3.1523 (3.2585)	LR 2.800e-03
0: TRAIN [2][270/1885]	Time 0.196 (0.144)	Data 2.16e-04 (1.02e-03)	Tok/s 102992 (95707)	Loss/tok 3.4640 (3.2576)	LR 2.800e-03
0: TRAIN [2][280/1885]	Time 0.147 (0.144)	Data 2.14e-04 (9.87e-04)	Tok/s 100363 (95801)	Loss/tok 3.2064 (3.2561)	LR 2.800e-03
0: TRAIN [2][290/1885]	Time 0.193 (0.144)	Data 2.15e-04 (9.60e-04)	Tok/s 105450 (95789)	Loss/tok 3.4562 (3.2576)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][300/1885]	Time 0.147 (0.145)	Data 2.12e-04 (9.35e-04)	Tok/s 100225 (95893)	Loss/tok 3.2936 (3.2584)	LR 2.800e-03
0: TRAIN [2][310/1885]	Time 0.147 (0.145)	Data 2.16e-04 (9.12e-04)	Tok/s 99339 (96017)	Loss/tok 3.1389 (3.2620)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][320/1885]	Time 0.247 (0.145)	Data 2.11e-04 (8.90e-04)	Tok/s 105626 (95981)	Loss/tok 3.3845 (3.2624)	LR 2.800e-03
0: TRAIN [2][330/1885]	Time 0.146 (0.145)	Data 2.13e-04 (8.70e-04)	Tok/s 102258 (95989)	Loss/tok 3.1075 (3.2627)	LR 2.800e-03
0: TRAIN [2][340/1885]	Time 0.194 (0.146)	Data 2.12e-04 (8.51e-04)	Tok/s 106808 (96091)	Loss/tok 3.3381 (3.2648)	LR 2.800e-03
0: TRAIN [2][350/1885]	Time 0.102 (0.146)	Data 2.13e-04 (8.32e-04)	Tok/s 88663 (96041)	Loss/tok 2.8779 (3.2628)	LR 2.800e-03
0: TRAIN [2][360/1885]	Time 0.146 (0.145)	Data 2.11e-04 (8.15e-04)	Tok/s 100691 (96004)	Loss/tok 3.1411 (3.2611)	LR 2.800e-03
0: TRAIN [2][370/1885]	Time 0.146 (0.145)	Data 2.14e-04 (7.99e-04)	Tok/s 101096 (95915)	Loss/tok 3.1395 (3.2593)	LR 2.800e-03
0: TRAIN [2][380/1885]	Time 0.059 (0.144)	Data 2.13e-04 (7.83e-04)	Tok/s 75030 (95838)	Loss/tok 2.5079 (3.2569)	LR 2.800e-03
0: TRAIN [2][390/1885]	Time 0.147 (0.144)	Data 2.15e-04 (7.69e-04)	Tok/s 98957 (95788)	Loss/tok 3.2002 (3.2559)	LR 2.800e-03
0: TRAIN [2][400/1885]	Time 0.147 (0.144)	Data 2.14e-04 (7.55e-04)	Tok/s 100578 (95782)	Loss/tok 3.1909 (3.2556)	LR 2.800e-03
0: TRAIN [2][410/1885]	Time 0.102 (0.143)	Data 2.18e-04 (7.42e-04)	Tok/s 88611 (95739)	Loss/tok 3.0734 (3.2553)	LR 2.800e-03
0: TRAIN [2][420/1885]	Time 0.245 (0.144)	Data 2.15e-04 (7.29e-04)	Tok/s 106875 (95855)	Loss/tok 3.5617 (3.2576)	LR 2.800e-03
0: TRAIN [2][430/1885]	Time 0.196 (0.145)	Data 2.15e-04 (7.17e-04)	Tok/s 105009 (95988)	Loss/tok 3.3630 (3.2591)	LR 2.800e-03
0: TRAIN [2][440/1885]	Time 0.103 (0.145)	Data 2.16e-04 (7.05e-04)	Tok/s 89234 (95978)	Loss/tok 2.9753 (3.2584)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][450/1885]	Time 0.147 (0.145)	Data 2.13e-04 (6.94e-04)	Tok/s 99138 (95995)	Loss/tok 3.2648 (3.2594)	LR 2.800e-03
0: TRAIN [2][460/1885]	Time 0.147 (0.144)	Data 2.29e-04 (6.84e-04)	Tok/s 99707 (95941)	Loss/tok 3.4086 (3.2582)	LR 2.800e-03
0: TRAIN [2][470/1885]	Time 0.245 (0.145)	Data 2.17e-04 (6.74e-04)	Tok/s 107386 (95993)	Loss/tok 3.5750 (3.2603)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][480/1885]	Time 0.145 (0.145)	Data 1.96e-04 (6.64e-04)	Tok/s 100840 (96066)	Loss/tok 3.2015 (3.2614)	LR 2.800e-03
0: TRAIN [2][490/1885]	Time 0.103 (0.145)	Data 2.12e-04 (6.55e-04)	Tok/s 87019 (96042)	Loss/tok 2.9697 (3.2605)	LR 2.800e-03
0: TRAIN [2][500/1885]	Time 0.194 (0.145)	Data 2.13e-04 (6.46e-04)	Tok/s 104222 (96055)	Loss/tok 3.3985 (3.2610)	LR 2.800e-03
0: TRAIN [2][510/1885]	Time 0.103 (0.144)	Data 2.13e-04 (6.38e-04)	Tok/s 88193 (95989)	Loss/tok 2.9856 (3.2591)	LR 2.800e-03
0: TRAIN [2][520/1885]	Time 0.147 (0.144)	Data 2.09e-04 (6.30e-04)	Tok/s 100527 (95941)	Loss/tok 3.1793 (3.2572)	LR 2.800e-03
0: TRAIN [2][530/1885]	Time 0.146 (0.144)	Data 2.16e-04 (6.22e-04)	Tok/s 101193 (95974)	Loss/tok 3.2680 (3.2569)	LR 2.800e-03
0: TRAIN [2][540/1885]	Time 0.246 (0.144)	Data 2.14e-04 (6.14e-04)	Tok/s 106608 (96040)	Loss/tok 3.4476 (3.2580)	LR 2.800e-03
0: TRAIN [2][550/1885]	Time 0.245 (0.145)	Data 2.11e-04 (6.07e-04)	Tok/s 106890 (96121)	Loss/tok 3.4785 (3.2590)	LR 2.800e-03
0: TRAIN [2][560/1885]	Time 0.060 (0.145)	Data 2.13e-04 (6.00e-04)	Tok/s 74441 (96108)	Loss/tok 2.4968 (3.2592)	LR 2.800e-03
0: TRAIN [2][570/1885]	Time 0.196 (0.145)	Data 2.12e-04 (5.93e-04)	Tok/s 105094 (96153)	Loss/tok 3.3701 (3.2595)	LR 2.800e-03
0: TRAIN [2][580/1885]	Time 0.060 (0.144)	Data 2.14e-04 (5.86e-04)	Tok/s 77495 (96088)	Loss/tok 2.3672 (3.2572)	LR 2.800e-03
0: TRAIN [2][590/1885]	Time 0.246 (0.145)	Data 2.17e-04 (5.80e-04)	Tok/s 104511 (96148)	Loss/tok 3.5360 (3.2586)	LR 2.800e-03
0: TRAIN [2][600/1885]	Time 0.102 (0.145)	Data 2.15e-04 (5.74e-04)	Tok/s 89341 (96111)	Loss/tok 2.9572 (3.2579)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][610/1885]	Time 0.147 (0.144)	Data 2.16e-04 (5.68e-04)	Tok/s 99935 (96072)	Loss/tok 3.2449 (3.2574)	LR 2.800e-03
0: TRAIN [2][620/1885]	Time 0.103 (0.144)	Data 2.11e-04 (5.62e-04)	Tok/s 88030 (96079)	Loss/tok 3.0388 (3.2574)	LR 2.800e-03
0: TRAIN [2][630/1885]	Time 0.147 (0.144)	Data 2.16e-04 (5.56e-04)	Tok/s 99497 (96064)	Loss/tok 3.1604 (3.2566)	LR 2.800e-03
0: TRAIN [2][640/1885]	Time 0.102 (0.144)	Data 2.01e-04 (5.51e-04)	Tok/s 88206 (96025)	Loss/tok 3.0934 (3.2554)	LR 2.800e-03
0: TRAIN [2][650/1885]	Time 0.060 (0.143)	Data 2.11e-04 (5.46e-04)	Tok/s 75066 (95961)	Loss/tok 2.5115 (3.2542)	LR 2.800e-03
0: TRAIN [2][660/1885]	Time 0.102 (0.143)	Data 2.14e-04 (5.41e-04)	Tok/s 89229 (95964)	Loss/tok 3.0816 (3.2539)	LR 2.800e-03
0: TRAIN [2][670/1885]	Time 0.195 (0.143)	Data 2.12e-04 (5.36e-04)	Tok/s 103725 (95984)	Loss/tok 3.4240 (3.2539)	LR 2.800e-03
0: TRAIN [2][680/1885]	Time 0.102 (0.143)	Data 2.15e-04 (5.31e-04)	Tok/s 88314 (95918)	Loss/tok 2.9592 (3.2527)	LR 2.800e-03
0: TRAIN [2][690/1885]	Time 0.103 (0.143)	Data 2.13e-04 (5.27e-04)	Tok/s 88411 (95925)	Loss/tok 3.0985 (3.2535)	LR 2.800e-03
0: TRAIN [2][700/1885]	Time 0.194 (0.143)	Data 2.14e-04 (5.22e-04)	Tok/s 105955 (95834)	Loss/tok 3.4314 (3.2519)	LR 2.800e-03
0: TRAIN [2][710/1885]	Time 0.147 (0.143)	Data 2.13e-04 (5.18e-04)	Tok/s 97883 (95849)	Loss/tok 3.3622 (3.2523)	LR 2.800e-03
0: TRAIN [2][720/1885]	Time 0.103 (0.143)	Data 2.15e-04 (5.13e-04)	Tok/s 87898 (95863)	Loss/tok 3.0395 (3.2532)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][730/1885]	Time 0.147 (0.143)	Data 2.12e-04 (5.09e-04)	Tok/s 100897 (95897)	Loss/tok 3.1817 (3.2529)	LR 2.800e-03
0: TRAIN [2][740/1885]	Time 0.147 (0.143)	Data 2.21e-04 (5.05e-04)	Tok/s 100325 (95867)	Loss/tok 3.2883 (3.2524)	LR 2.800e-03
0: TRAIN [2][750/1885]	Time 0.103 (0.142)	Data 2.30e-04 (5.02e-04)	Tok/s 88558 (95843)	Loss/tok 3.0341 (3.2517)	LR 2.800e-03
0: TRAIN [2][760/1885]	Time 0.147 (0.142)	Data 1.98e-04 (4.98e-04)	Tok/s 98638 (95854)	Loss/tok 3.1971 (3.2520)	LR 2.800e-03
0: TRAIN [2][770/1885]	Time 0.147 (0.142)	Data 2.15e-04 (4.94e-04)	Tok/s 98652 (95846)	Loss/tok 3.2143 (3.2518)	LR 2.800e-03
0: TRAIN [2][780/1885]	Time 0.194 (0.142)	Data 2.16e-04 (4.90e-04)	Tok/s 105289 (95860)	Loss/tok 3.3307 (3.2517)	LR 2.800e-03
0: TRAIN [2][790/1885]	Time 0.060 (0.142)	Data 2.14e-04 (4.87e-04)	Tok/s 76428 (95870)	Loss/tok 2.5760 (3.2512)	LR 2.800e-03
0: TRAIN [2][800/1885]	Time 0.103 (0.142)	Data 2.17e-04 (4.83e-04)	Tok/s 86447 (95844)	Loss/tok 3.0531 (3.2508)	LR 2.800e-03
0: TRAIN [2][810/1885]	Time 0.060 (0.142)	Data 2.15e-04 (4.80e-04)	Tok/s 77912 (95825)	Loss/tok 2.5769 (3.2510)	LR 2.800e-03
0: TRAIN [2][820/1885]	Time 0.246 (0.143)	Data 1.98e-04 (4.77e-04)	Tok/s 105402 (95897)	Loss/tok 3.6827 (3.2533)	LR 2.800e-03
0: TRAIN [2][830/1885]	Time 0.195 (0.142)	Data 2.28e-04 (4.74e-04)	Tok/s 104274 (95853)	Loss/tok 3.4084 (3.2529)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][840/1885]	Time 0.060 (0.143)	Data 2.17e-04 (4.70e-04)	Tok/s 74244 (95883)	Loss/tok 2.5535 (3.2550)	LR 2.800e-03
0: TRAIN [2][850/1885]	Time 0.102 (0.143)	Data 2.15e-04 (4.67e-04)	Tok/s 88835 (95894)	Loss/tok 2.9597 (3.2559)	LR 2.800e-03
0: TRAIN [2][860/1885]	Time 0.102 (0.143)	Data 2.10e-04 (4.64e-04)	Tok/s 89726 (95855)	Loss/tok 3.0525 (3.2553)	LR 2.800e-03
0: TRAIN [2][870/1885]	Time 0.102 (0.142)	Data 2.16e-04 (4.62e-04)	Tok/s 88845 (95832)	Loss/tok 3.0352 (3.2544)	LR 2.800e-03
0: TRAIN [2][880/1885]	Time 0.147 (0.142)	Data 2.13e-04 (4.59e-04)	Tok/s 99908 (95834)	Loss/tok 3.1382 (3.2532)	LR 2.800e-03
0: TRAIN [2][890/1885]	Time 0.102 (0.142)	Data 2.14e-04 (4.56e-04)	Tok/s 89023 (95833)	Loss/tok 2.9186 (3.2525)	LR 2.800e-03
0: TRAIN [2][900/1885]	Time 0.103 (0.142)	Data 2.02e-04 (4.53e-04)	Tok/s 87115 (95842)	Loss/tok 3.0063 (3.2526)	LR 2.800e-03
0: TRAIN [2][910/1885]	Time 0.102 (0.143)	Data 2.16e-04 (4.51e-04)	Tok/s 88475 (95892)	Loss/tok 2.9568 (3.2546)	LR 2.800e-03
0: TRAIN [2][920/1885]	Time 0.147 (0.143)	Data 2.26e-04 (4.48e-04)	Tok/s 101273 (95936)	Loss/tok 3.2685 (3.2555)	LR 2.800e-03
0: TRAIN [2][930/1885]	Time 0.101 (0.143)	Data 2.15e-04 (4.45e-04)	Tok/s 89758 (95937)	Loss/tok 2.8635 (3.2559)	LR 2.800e-03
0: TRAIN [2][940/1885]	Time 0.245 (0.143)	Data 2.17e-04 (4.43e-04)	Tok/s 107016 (95956)	Loss/tok 3.5275 (3.2565)	LR 2.800e-03
0: TRAIN [2][950/1885]	Time 0.101 (0.143)	Data 2.11e-04 (4.41e-04)	Tok/s 90425 (95918)	Loss/tok 2.9579 (3.2554)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][960/1885]	Time 0.193 (0.143)	Data 2.14e-04 (4.38e-04)	Tok/s 105125 (95955)	Loss/tok 3.3498 (3.2554)	LR 2.800e-03
0: TRAIN [2][970/1885]	Time 0.148 (0.143)	Data 2.16e-04 (4.36e-04)	Tok/s 99547 (95965)	Loss/tok 3.2079 (3.2553)	LR 2.800e-03
0: TRAIN [2][980/1885]	Time 0.148 (0.143)	Data 2.28e-04 (4.34e-04)	Tok/s 100107 (95958)	Loss/tok 3.2337 (3.2546)	LR 2.800e-03
0: TRAIN [2][990/1885]	Time 0.060 (0.143)	Data 2.11e-04 (4.31e-04)	Tok/s 76587 (95976)	Loss/tok 2.6226 (3.2556)	LR 2.800e-03
0: TRAIN [2][1000/1885]	Time 0.196 (0.143)	Data 2.18e-04 (4.29e-04)	Tok/s 104477 (95989)	Loss/tok 3.4567 (3.2562)	LR 2.800e-03
0: TRAIN [2][1010/1885]	Time 0.147 (0.143)	Data 2.18e-04 (4.27e-04)	Tok/s 99436 (96042)	Loss/tok 3.3141 (3.2579)	LR 2.800e-03
0: TRAIN [2][1020/1885]	Time 0.147 (0.144)	Data 2.14e-04 (4.25e-04)	Tok/s 99324 (96102)	Loss/tok 3.1547 (3.2584)	LR 2.800e-03
0: TRAIN [2][1030/1885]	Time 0.195 (0.144)	Data 2.25e-04 (4.23e-04)	Tok/s 104221 (96137)	Loss/tok 3.3817 (3.2586)	LR 2.800e-03
0: TRAIN [2][1040/1885]	Time 0.146 (0.144)	Data 2.14e-04 (4.21e-04)	Tok/s 101778 (96116)	Loss/tok 3.2300 (3.2579)	LR 2.800e-03
0: TRAIN [2][1050/1885]	Time 0.101 (0.144)	Data 2.12e-04 (4.19e-04)	Tok/s 92473 (96131)	Loss/tok 3.0046 (3.2579)	LR 2.800e-03
0: TRAIN [2][1060/1885]	Time 0.194 (0.144)	Data 2.14e-04 (4.17e-04)	Tok/s 105292 (96129)	Loss/tok 3.4351 (3.2579)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1070/1885]	Time 0.148 (0.144)	Data 2.12e-04 (4.15e-04)	Tok/s 98592 (96106)	Loss/tok 3.2211 (3.2572)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1080/1885]	Time 0.247 (0.143)	Data 2.13e-04 (4.13e-04)	Tok/s 104922 (96064)	Loss/tok 3.5942 (3.2568)	LR 2.800e-03
0: TRAIN [2][1090/1885]	Time 0.194 (0.143)	Data 2.13e-04 (4.12e-04)	Tok/s 106355 (96052)	Loss/tok 3.2814 (3.2565)	LR 2.800e-03
0: TRAIN [2][1100/1885]	Time 0.196 (0.143)	Data 2.16e-04 (4.10e-04)	Tok/s 105531 (96055)	Loss/tok 3.3141 (3.2564)	LR 2.800e-03
0: TRAIN [2][1110/1885]	Time 0.103 (0.143)	Data 1.62e-04 (4.08e-04)	Tok/s 88027 (96029)	Loss/tok 3.0348 (3.2561)	LR 2.800e-03
0: TRAIN [2][1120/1885]	Time 0.101 (0.143)	Data 2.11e-04 (4.06e-04)	Tok/s 88648 (96019)	Loss/tok 3.0138 (3.2557)	LR 2.800e-03
0: TRAIN [2][1130/1885]	Time 0.194 (0.143)	Data 2.17e-04 (4.04e-04)	Tok/s 106318 (96037)	Loss/tok 3.3684 (3.2560)	LR 2.800e-03
0: TRAIN [2][1140/1885]	Time 0.147 (0.143)	Data 2.17e-04 (4.03e-04)	Tok/s 97553 (96046)	Loss/tok 3.2580 (3.2561)	LR 2.800e-03
0: TRAIN [2][1150/1885]	Time 0.147 (0.143)	Data 2.13e-04 (4.01e-04)	Tok/s 99872 (96015)	Loss/tok 3.3103 (3.2554)	LR 2.800e-03
0: TRAIN [2][1160/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.99e-04)	Tok/s 89917 (96007)	Loss/tok 2.9862 (3.2553)	LR 2.800e-03
0: TRAIN [2][1170/1885]	Time 0.102 (0.143)	Data 2.15e-04 (3.98e-04)	Tok/s 87673 (95996)	Loss/tok 3.0573 (3.2548)	LR 2.800e-03
0: TRAIN [2][1180/1885]	Time 0.101 (0.143)	Data 2.12e-04 (3.96e-04)	Tok/s 88193 (96026)	Loss/tok 3.0587 (3.2549)	LR 2.800e-03
0: TRAIN [2][1190/1885]	Time 0.102 (0.143)	Data 2.12e-04 (3.95e-04)	Tok/s 89243 (95981)	Loss/tok 3.0721 (3.2539)	LR 2.800e-03
0: TRAIN [2][1200/1885]	Time 0.244 (0.143)	Data 2.12e-04 (3.93e-04)	Tok/s 106427 (96010)	Loss/tok 3.5894 (3.2545)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1210/1885]	Time 0.102 (0.143)	Data 2.11e-04 (3.92e-04)	Tok/s 88981 (96006)	Loss/tok 3.0988 (3.2545)	LR 2.800e-03
0: TRAIN [2][1220/1885]	Time 0.104 (0.143)	Data 2.15e-04 (3.90e-04)	Tok/s 86223 (95997)	Loss/tok 2.9601 (3.2545)	LR 2.800e-03
0: TRAIN [2][1230/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.89e-04)	Tok/s 99653 (95993)	Loss/tok 3.0889 (3.2539)	LR 2.800e-03
0: TRAIN [2][1240/1885]	Time 0.102 (0.143)	Data 2.15e-04 (3.87e-04)	Tok/s 88464 (95990)	Loss/tok 3.0374 (3.2543)	LR 2.800e-03
0: TRAIN [2][1250/1885]	Time 0.149 (0.143)	Data 2.12e-04 (3.86e-04)	Tok/s 97874 (95978)	Loss/tok 3.2908 (3.2536)	LR 2.800e-03
0: TRAIN [2][1260/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.85e-04)	Tok/s 89276 (95955)	Loss/tok 2.9999 (3.2538)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1270/1885]	Time 0.101 (0.143)	Data 2.15e-04 (3.83e-04)	Tok/s 89055 (95934)	Loss/tok 3.0970 (3.2541)	LR 2.800e-03
0: TRAIN [2][1280/1885]	Time 0.245 (0.143)	Data 2.10e-04 (3.82e-04)	Tok/s 106807 (95945)	Loss/tok 3.4569 (3.2545)	LR 2.800e-03
0: TRAIN [2][1290/1885]	Time 0.102 (0.143)	Data 2.09e-04 (3.81e-04)	Tok/s 89546 (95919)	Loss/tok 3.0243 (3.2546)	LR 2.800e-03
0: TRAIN [2][1300/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.79e-04)	Tok/s 98713 (95928)	Loss/tok 3.1783 (3.2548)	LR 2.800e-03
0: TRAIN [2][1310/1885]	Time 0.246 (0.143)	Data 2.17e-04 (3.78e-04)	Tok/s 105665 (95935)	Loss/tok 3.6200 (3.2551)	LR 2.800e-03
0: TRAIN [2][1320/1885]	Time 0.146 (0.143)	Data 2.17e-04 (3.77e-04)	Tok/s 102073 (95940)	Loss/tok 3.2053 (3.2550)	LR 2.800e-03
0: TRAIN [2][1330/1885]	Time 0.101 (0.143)	Data 2.13e-04 (3.76e-04)	Tok/s 89799 (95959)	Loss/tok 2.9528 (3.2560)	LR 2.800e-03
0: TRAIN [2][1340/1885]	Time 0.147 (0.143)	Data 2.17e-04 (3.74e-04)	Tok/s 98586 (95969)	Loss/tok 3.3148 (3.2563)	LR 2.800e-03
0: TRAIN [2][1350/1885]	Time 0.195 (0.143)	Data 2.14e-04 (3.73e-04)	Tok/s 103813 (95946)	Loss/tok 3.2851 (3.2554)	LR 2.800e-03
0: TRAIN [2][1360/1885]	Time 0.195 (0.143)	Data 2.14e-04 (3.72e-04)	Tok/s 103439 (95946)	Loss/tok 3.4313 (3.2552)	LR 2.800e-03
0: TRAIN [2][1370/1885]	Time 0.101 (0.143)	Data 2.11e-04 (3.71e-04)	Tok/s 89839 (95931)	Loss/tok 3.1278 (3.2553)	LR 2.800e-03
0: TRAIN [2][1380/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.70e-04)	Tok/s 89999 (95921)	Loss/tok 3.0227 (3.2551)	LR 2.800e-03
0: TRAIN [2][1390/1885]	Time 0.148 (0.143)	Data 2.15e-04 (3.69e-04)	Tok/s 98851 (95950)	Loss/tok 3.1629 (3.2557)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1400/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.67e-04)	Tok/s 99907 (95947)	Loss/tok 3.3339 (3.2555)	LR 2.800e-03
0: TRAIN [2][1410/1885]	Time 0.147 (0.143)	Data 2.15e-04 (3.66e-04)	Tok/s 99739 (95942)	Loss/tok 3.2728 (3.2551)	LR 2.800e-03
0: TRAIN [2][1420/1885]	Time 0.193 (0.143)	Data 2.16e-04 (3.65e-04)	Tok/s 105808 (95948)	Loss/tok 3.4294 (3.2549)	LR 2.800e-03
0: TRAIN [2][1430/1885]	Time 0.146 (0.143)	Data 2.14e-04 (3.64e-04)	Tok/s 101280 (95968)	Loss/tok 3.2970 (3.2549)	LR 2.800e-03
0: TRAIN [2][1440/1885]	Time 0.146 (0.143)	Data 2.13e-04 (3.63e-04)	Tok/s 100626 (95970)	Loss/tok 3.3167 (3.2548)	LR 2.800e-03
0: TRAIN [2][1450/1885]	Time 0.195 (0.143)	Data 2.21e-04 (3.62e-04)	Tok/s 103972 (96007)	Loss/tok 3.4259 (3.2553)	LR 2.800e-03
0: TRAIN [2][1460/1885]	Time 0.147 (0.143)	Data 2.14e-04 (3.61e-04)	Tok/s 99577 (96028)	Loss/tok 3.3305 (3.2558)	LR 2.800e-03
0: TRAIN [2][1470/1885]	Time 0.148 (0.143)	Data 2.16e-04 (3.60e-04)	Tok/s 100091 (96014)	Loss/tok 3.1408 (3.2550)	LR 2.800e-03
0: TRAIN [2][1480/1885]	Time 0.104 (0.143)	Data 1.98e-04 (3.59e-04)	Tok/s 86697 (96043)	Loss/tok 3.0819 (3.2559)	LR 2.800e-03
0: TRAIN [2][1490/1885]	Time 0.103 (0.143)	Data 2.13e-04 (3.58e-04)	Tok/s 87006 (96017)	Loss/tok 3.1322 (3.2559)	LR 2.800e-03
0: TRAIN [2][1500/1885]	Time 0.102 (0.143)	Data 2.11e-04 (3.57e-04)	Tok/s 88861 (96007)	Loss/tok 3.0028 (3.2559)	LR 2.800e-03
0: TRAIN [2][1510/1885]	Time 0.103 (0.143)	Data 2.12e-04 (3.56e-04)	Tok/s 87341 (96011)	Loss/tok 2.9515 (3.2558)	LR 2.800e-03
0: TRAIN [2][1520/1885]	Time 0.104 (0.143)	Data 1.85e-04 (3.55e-04)	Tok/s 87423 (96009)	Loss/tok 3.0652 (3.2557)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1530/1885]	Time 0.148 (0.143)	Data 2.00e-04 (3.54e-04)	Tok/s 98665 (96034)	Loss/tok 3.2277 (3.2564)	LR 2.800e-03
0: TRAIN [2][1540/1885]	Time 0.196 (0.143)	Data 1.46e-04 (3.53e-04)	Tok/s 104177 (96055)	Loss/tok 3.1242 (3.2562)	LR 2.800e-03
0: TRAIN [2][1550/1885]	Time 0.103 (0.143)	Data 2.11e-04 (3.52e-04)	Tok/s 86373 (96039)	Loss/tok 2.9623 (3.2554)	LR 2.800e-03
0: TRAIN [2][1560/1885]	Time 0.148 (0.143)	Data 2.12e-04 (3.51e-04)	Tok/s 98963 (96035)	Loss/tok 3.2683 (3.2558)	LR 2.800e-03
0: TRAIN [2][1570/1885]	Time 0.104 (0.143)	Data 1.71e-04 (3.50e-04)	Tok/s 86671 (96045)	Loss/tok 2.9838 (3.2561)	LR 2.800e-03
0: TRAIN [2][1580/1885]	Time 0.147 (0.143)	Data 1.65e-04 (3.49e-04)	Tok/s 99682 (96054)	Loss/tok 3.1911 (3.2564)	LR 2.800e-03
0: TRAIN [2][1590/1885]	Time 0.196 (0.143)	Data 2.13e-04 (3.48e-04)	Tok/s 105284 (96056)	Loss/tok 3.4384 (3.2562)	LR 2.800e-03
0: TRAIN [2][1600/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.47e-04)	Tok/s 91358 (96071)	Loss/tok 3.0425 (3.2562)	LR 2.800e-03
0: TRAIN [2][1610/1885]	Time 0.104 (0.143)	Data 1.85e-04 (3.46e-04)	Tok/s 88759 (96061)	Loss/tok 2.9681 (3.2557)	LR 2.800e-03
0: TRAIN [2][1620/1885]	Time 0.102 (0.143)	Data 2.15e-04 (3.45e-04)	Tok/s 88446 (96049)	Loss/tok 2.9505 (3.2550)	LR 2.800e-03
0: TRAIN [2][1630/1885]	Time 0.101 (0.143)	Data 2.15e-04 (3.45e-04)	Tok/s 90229 (96056)	Loss/tok 3.0318 (3.2551)	LR 2.800e-03
0: TRAIN [2][1640/1885]	Time 0.145 (0.143)	Data 2.46e-04 (3.44e-04)	Tok/s 101380 (96058)	Loss/tok 3.2474 (3.2554)	LR 2.800e-03
0: TRAIN [2][1650/1885]	Time 0.194 (0.143)	Data 1.49e-04 (3.43e-04)	Tok/s 105677 (96041)	Loss/tok 3.3965 (3.2550)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1660/1885]	Time 0.195 (0.143)	Data 1.78e-04 (3.42e-04)	Tok/s 105636 (96062)	Loss/tok 3.3452 (3.2553)	LR 2.800e-03
0: TRAIN [2][1670/1885]	Time 0.196 (0.143)	Data 1.82e-04 (3.41e-04)	Tok/s 103334 (96063)	Loss/tok 3.4273 (3.2551)	LR 2.800e-03
0: TRAIN [2][1680/1885]	Time 0.146 (0.143)	Data 2.13e-04 (3.40e-04)	Tok/s 99645 (96042)	Loss/tok 3.1896 (3.2544)	LR 2.800e-03
0: TRAIN [2][1690/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.39e-04)	Tok/s 98407 (96049)	Loss/tok 3.1941 (3.2540)	LR 2.800e-03
0: TRAIN [2][1700/1885]	Time 0.101 (0.143)	Data 2.15e-04 (3.39e-04)	Tok/s 90140 (96022)	Loss/tok 3.0205 (3.2538)	LR 2.800e-03
0: TRAIN [2][1710/1885]	Time 0.060 (0.143)	Data 1.77e-04 (3.38e-04)	Tok/s 76798 (96018)	Loss/tok 2.4955 (3.2536)	LR 2.800e-03
0: TRAIN [2][1720/1885]	Time 0.102 (0.143)	Data 1.86e-04 (3.37e-04)	Tok/s 89129 (96042)	Loss/tok 2.9142 (3.2544)	LR 2.800e-03
0: TRAIN [2][1730/1885]	Time 0.148 (0.143)	Data 2.14e-04 (3.36e-04)	Tok/s 99902 (96044)	Loss/tok 3.1399 (3.2541)	LR 2.800e-03
0: TRAIN [2][1740/1885]	Time 0.149 (0.143)	Data 1.38e-04 (3.35e-04)	Tok/s 97474 (96046)	Loss/tok 3.2062 (3.2544)	LR 2.800e-03
0: TRAIN [2][1750/1885]	Time 0.147 (0.143)	Data 2.12e-04 (3.35e-04)	Tok/s 98758 (96019)	Loss/tok 3.1713 (3.2537)	LR 2.800e-03
0: TRAIN [2][1760/1885]	Time 0.194 (0.143)	Data 2.11e-04 (3.34e-04)	Tok/s 103247 (96036)	Loss/tok 3.4826 (3.2544)	LR 2.800e-03
0: TRAIN [2][1770/1885]	Time 0.102 (0.143)	Data 2.13e-04 (3.33e-04)	Tok/s 88139 (96041)	Loss/tok 2.9839 (3.2544)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1780/1885]	Time 0.145 (0.143)	Data 1.98e-04 (3.32e-04)	Tok/s 101315 (96053)	Loss/tok 3.2025 (3.2550)	LR 2.800e-03
0: TRAIN [2][1790/1885]	Time 0.101 (0.143)	Data 3.04e-04 (3.32e-04)	Tok/s 90785 (96061)	Loss/tok 2.9598 (3.2557)	LR 2.800e-03
0: TRAIN [2][1800/1885]	Time 0.246 (0.144)	Data 2.13e-04 (3.31e-04)	Tok/s 106952 (96066)	Loss/tok 3.5422 (3.2559)	LR 2.800e-03
0: TRAIN [2][1810/1885]	Time 0.248 (0.144)	Data 1.87e-04 (3.30e-04)	Tok/s 104385 (96064)	Loss/tok 3.6153 (3.2561)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1820/1885]	Time 0.148 (0.144)	Data 2.15e-04 (3.30e-04)	Tok/s 99919 (96081)	Loss/tok 3.2370 (3.2564)	LR 2.800e-03
0: TRAIN [2][1830/1885]	Time 0.102 (0.144)	Data 1.88e-04 (3.29e-04)	Tok/s 89681 (96079)	Loss/tok 2.9307 (3.2563)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1840/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.28e-04)	Tok/s 89012 (96064)	Loss/tok 2.9877 (3.2562)	LR 2.800e-03
0: TRAIN [2][1850/1885]	Time 0.147 (0.144)	Data 2.16e-04 (3.28e-04)	Tok/s 100009 (96073)	Loss/tok 3.2435 (3.2563)	LR 2.800e-03
0: TRAIN [2][1860/1885]	Time 0.146 (0.144)	Data 2.12e-04 (3.27e-04)	Tok/s 98126 (96083)	Loss/tok 3.2743 (3.2562)	LR 2.800e-03
0: TRAIN [2][1870/1885]	Time 0.102 (0.143)	Data 2.14e-04 (3.26e-04)	Tok/s 88539 (96057)	Loss/tok 3.0808 (3.2557)	LR 2.800e-03
0: TRAIN [2][1880/1885]	Time 0.146 (0.143)	Data 2.13e-04 (3.26e-04)	Tok/s 100377 (96047)	Loss/tok 3.2503 (3.2554)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593835111166, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593835111167, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.672 (0.672)	Decoder iters 149.0 (149.0)	Tok/s 24250 (24250)
0: Running moses detokenizer
0: BLEU(score=22.78816475320877, counts=[36623, 17869, 9962, 5806], totals=[65803, 62800, 59798, 56800], precisions=[55.65551722565841, 28.453821656050955, 16.659420047493228, 10.221830985915492], bp=1.0, sys_len=65803, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593835112815, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2279, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593835112816, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2553	Test BLEU: 22.79
0: Performance: Epoch: 2	Training: 768041 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593835112816, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593835112816, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593835112816, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3550389791
0: TRAIN [3][0/1885]	Time 0.346 (0.346)	Data 2.27e-01 (2.27e-01)	Tok/s 26570 (26570)	Loss/tok 2.8634 (2.8634)	LR 2.800e-03
0: TRAIN [3][10/1885]	Time 0.101 (0.129)	Data 2.13e-04 (2.08e-02)	Tok/s 92656 (84227)	Loss/tok 2.9271 (2.9623)	LR 2.800e-03
0: TRAIN [3][20/1885]	Time 0.147 (0.136)	Data 2.14e-04 (1.10e-02)	Tok/s 98740 (90451)	Loss/tok 3.1685 (3.0501)	LR 2.800e-03
0: TRAIN [3][30/1885]	Time 0.146 (0.140)	Data 2.12e-04 (7.53e-03)	Tok/s 99132 (92510)	Loss/tok 3.2752 (3.1050)	LR 2.800e-03
0: TRAIN [3][40/1885]	Time 0.102 (0.141)	Data 2.13e-04 (5.74e-03)	Tok/s 89319 (93287)	Loss/tok 2.9550 (3.1238)	LR 2.800e-03
0: TRAIN [3][50/1885]	Time 0.102 (0.140)	Data 2.13e-04 (4.66e-03)	Tok/s 90259 (93604)	Loss/tok 2.9638 (3.1307)	LR 2.800e-03
0: TRAIN [3][60/1885]	Time 0.147 (0.147)	Data 2.16e-04 (3.93e-03)	Tok/s 98070 (94879)	Loss/tok 3.2088 (3.1601)	LR 2.800e-03
0: TRAIN [3][70/1885]	Time 0.193 (0.148)	Data 2.12e-04 (3.40e-03)	Tok/s 104492 (95257)	Loss/tok 3.3920 (3.1683)	LR 2.800e-03
0: TRAIN [3][80/1885]	Time 0.146 (0.149)	Data 2.15e-04 (3.01e-03)	Tok/s 100397 (95657)	Loss/tok 3.1049 (3.1708)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][90/1885]	Time 0.148 (0.151)	Data 2.14e-04 (2.70e-03)	Tok/s 100360 (96188)	Loss/tok 3.0622 (3.1696)	LR 2.800e-03
0: TRAIN [3][100/1885]	Time 0.196 (0.150)	Data 2.13e-04 (2.45e-03)	Tok/s 104740 (96148)	Loss/tok 3.2630 (3.1714)	LR 2.800e-03
0: TRAIN [3][110/1885]	Time 0.147 (0.149)	Data 2.11e-04 (2.25e-03)	Tok/s 99820 (96004)	Loss/tok 3.2569 (3.1691)	LR 2.800e-03
0: TRAIN [3][120/1885]	Time 0.103 (0.148)	Data 2.11e-04 (2.08e-03)	Tok/s 87953 (95844)	Loss/tok 2.9613 (3.1709)	LR 2.800e-03
0: TRAIN [3][130/1885]	Time 0.148 (0.146)	Data 2.10e-04 (1.94e-03)	Tok/s 98498 (95703)	Loss/tok 3.1256 (3.1639)	LR 2.800e-03
0: TRAIN [3][140/1885]	Time 0.145 (0.145)	Data 2.11e-04 (1.82e-03)	Tok/s 99004 (95484)	Loss/tok 3.1120 (3.1625)	LR 2.800e-03
0: TRAIN [3][150/1885]	Time 0.246 (0.145)	Data 2.14e-04 (1.71e-03)	Tok/s 107731 (95481)	Loss/tok 3.3808 (3.1642)	LR 2.800e-03
0: TRAIN [3][160/1885]	Time 0.148 (0.144)	Data 2.11e-04 (1.62e-03)	Tok/s 99730 (95370)	Loss/tok 3.0883 (3.1611)	LR 2.800e-03
0: TRAIN [3][170/1885]	Time 0.101 (0.143)	Data 2.13e-04 (1.54e-03)	Tok/s 90939 (95270)	Loss/tok 2.9546 (3.1606)	LR 2.800e-03
0: TRAIN [3][180/1885]	Time 0.101 (0.144)	Data 2.21e-04 (1.46e-03)	Tok/s 91873 (95424)	Loss/tok 2.9084 (3.1613)	LR 2.800e-03
0: TRAIN [3][190/1885]	Time 0.059 (0.144)	Data 2.12e-04 (1.40e-03)	Tok/s 79173 (95492)	Loss/tok 2.4612 (3.1667)	LR 2.800e-03
0: TRAIN [3][200/1885]	Time 0.102 (0.145)	Data 1.82e-04 (1.34e-03)	Tok/s 90263 (95625)	Loss/tok 2.8938 (3.1677)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][210/1885]	Time 0.103 (0.145)	Data 2.10e-04 (1.28e-03)	Tok/s 87755 (95717)	Loss/tok 2.9617 (3.1715)	LR 2.800e-03
0: TRAIN [3][220/1885]	Time 0.104 (0.145)	Data 2.10e-04 (1.24e-03)	Tok/s 87253 (95672)	Loss/tok 2.9380 (3.1703)	LR 2.800e-03
0: TRAIN [3][230/1885]	Time 0.148 (0.145)	Data 2.08e-04 (1.19e-03)	Tok/s 98778 (95668)	Loss/tok 3.0573 (3.1690)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][240/1885]	Time 0.147 (0.146)	Data 2.13e-04 (1.15e-03)	Tok/s 99078 (95914)	Loss/tok 3.0458 (3.1746)	LR 2.800e-03
0: TRAIN [3][250/1885]	Time 0.101 (0.146)	Data 2.12e-04 (1.11e-03)	Tok/s 87877 (95981)	Loss/tok 2.9553 (3.1734)	LR 2.800e-03
0: TRAIN [3][260/1885]	Time 0.193 (0.146)	Data 2.11e-04 (1.08e-03)	Tok/s 105945 (96065)	Loss/tok 3.2831 (3.1755)	LR 2.800e-03
0: TRAIN [3][270/1885]	Time 0.195 (0.146)	Data 2.22e-04 (1.05e-03)	Tok/s 104509 (96010)	Loss/tok 3.4179 (3.1758)	LR 2.800e-03
0: TRAIN [3][280/1885]	Time 0.102 (0.146)	Data 2.11e-04 (1.02e-03)	Tok/s 87188 (96069)	Loss/tok 2.8987 (3.1752)	LR 2.800e-03
0: TRAIN [3][290/1885]	Time 0.247 (0.146)	Data 1.40e-04 (9.88e-04)	Tok/s 105475 (96008)	Loss/tok 3.2791 (3.1741)	LR 2.800e-03
0: TRAIN [3][300/1885]	Time 0.102 (0.146)	Data 2.11e-04 (9.62e-04)	Tok/s 88595 (96048)	Loss/tok 2.9200 (3.1733)	LR 2.800e-03
0: TRAIN [3][310/1885]	Time 0.148 (0.147)	Data 2.23e-04 (9.38e-04)	Tok/s 98454 (96168)	Loss/tok 3.0133 (3.1757)	LR 2.800e-03
0: TRAIN [3][320/1885]	Time 0.147 (0.147)	Data 2.11e-04 (9.16e-04)	Tok/s 99418 (96279)	Loss/tok 3.1120 (3.1782)	LR 2.800e-03
0: TRAIN [3][330/1885]	Time 0.061 (0.147)	Data 2.11e-04 (8.94e-04)	Tok/s 75853 (96325)	Loss/tok 2.4361 (3.1795)	LR 2.800e-03
0: TRAIN [3][340/1885]	Time 0.246 (0.147)	Data 2.12e-04 (8.74e-04)	Tok/s 106533 (96256)	Loss/tok 3.4282 (3.1792)	LR 2.800e-03
0: TRAIN [3][350/1885]	Time 0.101 (0.147)	Data 2.25e-04 (8.55e-04)	Tok/s 85802 (96205)	Loss/tok 2.9969 (3.1766)	LR 2.800e-03
0: TRAIN [3][360/1885]	Time 0.148 (0.147)	Data 2.12e-04 (8.37e-04)	Tok/s 98606 (96262)	Loss/tok 3.2088 (3.1761)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][370/1885]	Time 0.195 (0.147)	Data 2.15e-04 (8.21e-04)	Tok/s 104948 (96292)	Loss/tok 3.3748 (3.1770)	LR 2.800e-03
0: TRAIN [3][380/1885]	Time 0.060 (0.146)	Data 2.10e-04 (8.05e-04)	Tok/s 75788 (96198)	Loss/tok 2.4522 (3.1764)	LR 2.800e-03
0: TRAIN [3][390/1885]	Time 0.149 (0.146)	Data 2.13e-04 (7.89e-04)	Tok/s 99551 (96200)	Loss/tok 2.9862 (3.1764)	LR 2.800e-03
0: TRAIN [3][400/1885]	Time 0.102 (0.145)	Data 2.15e-04 (7.75e-04)	Tok/s 88552 (96079)	Loss/tok 2.9867 (3.1744)	LR 2.800e-03
0: TRAIN [3][410/1885]	Time 0.060 (0.145)	Data 2.10e-04 (7.61e-04)	Tok/s 76112 (96052)	Loss/tok 2.4711 (3.1739)	LR 2.800e-03
0: TRAIN [3][420/1885]	Time 0.197 (0.145)	Data 2.12e-04 (7.48e-04)	Tok/s 104110 (96000)	Loss/tok 3.3327 (3.1734)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][430/1885]	Time 0.245 (0.145)	Data 2.14e-04 (7.36e-04)	Tok/s 106468 (95929)	Loss/tok 3.4586 (3.1735)	LR 1.400e-03
0: TRAIN [3][440/1885]	Time 0.146 (0.144)	Data 2.11e-04 (7.24e-04)	Tok/s 101011 (95900)	Loss/tok 3.2206 (3.1731)	LR 1.400e-03
0: TRAIN [3][450/1885]	Time 0.102 (0.144)	Data 2.18e-04 (7.13e-04)	Tok/s 88443 (95897)	Loss/tok 3.0280 (3.1727)	LR 1.400e-03
0: TRAIN [3][460/1885]	Time 0.147 (0.145)	Data 2.18e-04 (7.02e-04)	Tok/s 100949 (95962)	Loss/tok 3.1135 (3.1730)	LR 1.400e-03
0: TRAIN [3][470/1885]	Time 0.196 (0.145)	Data 2.16e-04 (6.91e-04)	Tok/s 106256 (95959)	Loss/tok 3.2912 (3.1746)	LR 1.400e-03
0: TRAIN [3][480/1885]	Time 0.102 (0.145)	Data 2.08e-04 (6.81e-04)	Tok/s 89213 (95951)	Loss/tok 2.9221 (3.1732)	LR 1.400e-03
0: TRAIN [3][490/1885]	Time 0.061 (0.144)	Data 2.13e-04 (6.72e-04)	Tok/s 75092 (95894)	Loss/tok 2.4075 (3.1714)	LR 1.400e-03
0: TRAIN [3][500/1885]	Time 0.147 (0.144)	Data 2.15e-04 (6.63e-04)	Tok/s 100074 (95811)	Loss/tok 3.1368 (3.1702)	LR 1.400e-03
0: TRAIN [3][510/1885]	Time 0.196 (0.144)	Data 2.12e-04 (6.54e-04)	Tok/s 104215 (95785)	Loss/tok 3.2159 (3.1693)	LR 1.400e-03
0: TRAIN [3][520/1885]	Time 0.195 (0.144)	Data 2.09e-04 (6.45e-04)	Tok/s 104822 (95747)	Loss/tok 3.3266 (3.1688)	LR 1.400e-03
0: TRAIN [3][530/1885]	Time 0.194 (0.144)	Data 2.13e-04 (6.37e-04)	Tok/s 103447 (95788)	Loss/tok 3.3336 (3.1697)	LR 1.400e-03
0: TRAIN [3][540/1885]	Time 0.101 (0.143)	Data 2.12e-04 (6.29e-04)	Tok/s 90831 (95699)	Loss/tok 2.9640 (3.1677)	LR 1.400e-03
0: TRAIN [3][550/1885]	Time 0.062 (0.143)	Data 2.10e-04 (6.21e-04)	Tok/s 75261 (95705)	Loss/tok 2.4137 (3.1670)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1885]	Time 0.062 (0.143)	Data 2.10e-04 (6.14e-04)	Tok/s 72246 (95565)	Loss/tok 2.4502 (3.1654)	LR 1.400e-03
0: TRAIN [3][570/1885]	Time 0.196 (0.143)	Data 1.74e-04 (6.07e-04)	Tok/s 104118 (95628)	Loss/tok 3.3321 (3.1661)	LR 1.400e-03
0: TRAIN [3][580/1885]	Time 0.101 (0.143)	Data 2.11e-04 (6.00e-04)	Tok/s 89857 (95596)	Loss/tok 2.9513 (3.1652)	LR 1.400e-03
0: TRAIN [3][590/1885]	Time 0.146 (0.143)	Data 2.14e-04 (5.94e-04)	Tok/s 98875 (95607)	Loss/tok 3.0785 (3.1655)	LR 1.400e-03
0: TRAIN [3][600/1885]	Time 0.102 (0.143)	Data 2.11e-04 (5.87e-04)	Tok/s 89283 (95572)	Loss/tok 2.9146 (3.1645)	LR 1.400e-03
0: TRAIN [3][610/1885]	Time 0.104 (0.143)	Data 2.12e-04 (5.81e-04)	Tok/s 89547 (95571)	Loss/tok 2.8654 (3.1647)	LR 1.400e-03
0: TRAIN [3][620/1885]	Time 0.102 (0.143)	Data 2.10e-04 (5.75e-04)	Tok/s 89221 (95650)	Loss/tok 2.8606 (3.1654)	LR 1.400e-03
0: TRAIN [3][630/1885]	Time 0.102 (0.143)	Data 2.13e-04 (5.69e-04)	Tok/s 88472 (95714)	Loss/tok 2.9741 (3.1660)	LR 1.400e-03
0: TRAIN [3][640/1885]	Time 0.061 (0.143)	Data 2.12e-04 (5.64e-04)	Tok/s 75113 (95702)	Loss/tok 2.3958 (3.1662)	LR 1.400e-03
0: TRAIN [3][650/1885]	Time 0.147 (0.143)	Data 2.13e-04 (5.58e-04)	Tok/s 100320 (95713)	Loss/tok 3.0811 (3.1657)	LR 1.400e-03
0: TRAIN [3][660/1885]	Time 0.101 (0.143)	Data 2.16e-04 (5.53e-04)	Tok/s 89477 (95715)	Loss/tok 2.8901 (3.1655)	LR 1.400e-03
0: TRAIN [3][670/1885]	Time 0.245 (0.144)	Data 2.25e-04 (5.48e-04)	Tok/s 107747 (95775)	Loss/tok 3.3540 (3.1677)	LR 1.400e-03
0: TRAIN [3][680/1885]	Time 0.102 (0.144)	Data 2.17e-04 (5.43e-04)	Tok/s 89853 (95820)	Loss/tok 2.8826 (3.1685)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][690/1885]	Time 0.149 (0.144)	Data 2.14e-04 (5.38e-04)	Tok/s 97477 (95857)	Loss/tok 3.1226 (3.1676)	LR 1.400e-03
0: TRAIN [3][700/1885]	Time 0.104 (0.144)	Data 2.11e-04 (5.34e-04)	Tok/s 86835 (95886)	Loss/tok 2.9473 (3.1671)	LR 1.400e-03
0: TRAIN [3][710/1885]	Time 0.102 (0.144)	Data 2.12e-04 (5.29e-04)	Tok/s 87102 (95893)	Loss/tok 2.8880 (3.1666)	LR 1.400e-03
0: TRAIN [3][720/1885]	Time 0.103 (0.144)	Data 2.28e-04 (5.25e-04)	Tok/s 87394 (95943)	Loss/tok 2.8691 (3.1666)	LR 1.400e-03
0: TRAIN [3][730/1885]	Time 0.059 (0.144)	Data 2.09e-04 (5.21e-04)	Tok/s 77659 (95912)	Loss/tok 2.4667 (3.1660)	LR 1.400e-03
0: TRAIN [3][740/1885]	Time 0.195 (0.144)	Data 2.11e-04 (5.16e-04)	Tok/s 105702 (95951)	Loss/tok 3.3939 (3.1662)	LR 1.400e-03
0: TRAIN [3][750/1885]	Time 0.101 (0.144)	Data 2.13e-04 (5.12e-04)	Tok/s 89218 (95917)	Loss/tok 2.8870 (3.1649)	LR 1.400e-03
0: TRAIN [3][760/1885]	Time 0.102 (0.144)	Data 2.12e-04 (5.08e-04)	Tok/s 90186 (95882)	Loss/tok 2.8701 (3.1635)	LR 1.400e-03
0: TRAIN [3][770/1885]	Time 0.059 (0.144)	Data 2.11e-04 (5.04e-04)	Tok/s 77131 (95863)	Loss/tok 2.4888 (3.1633)	LR 1.400e-03
0: TRAIN [3][780/1885]	Time 0.148 (0.144)	Data 2.09e-04 (5.01e-04)	Tok/s 97963 (95909)	Loss/tok 3.1627 (3.1640)	LR 1.400e-03
0: TRAIN [3][790/1885]	Time 0.196 (0.144)	Data 2.19e-04 (4.97e-04)	Tok/s 104529 (95952)	Loss/tok 3.2688 (3.1652)	LR 1.400e-03
0: TRAIN [3][800/1885]	Time 0.146 (0.145)	Data 2.14e-04 (4.93e-04)	Tok/s 98899 (96017)	Loss/tok 3.1184 (3.1660)	LR 1.400e-03
0: TRAIN [3][810/1885]	Time 0.145 (0.145)	Data 2.12e-04 (4.90e-04)	Tok/s 100342 (96008)	Loss/tok 3.2039 (3.1650)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][820/1885]	Time 0.146 (0.145)	Data 2.09e-04 (4.87e-04)	Tok/s 100630 (96052)	Loss/tok 3.1009 (3.1652)	LR 1.400e-03
0: TRAIN [3][830/1885]	Time 0.195 (0.145)	Data 2.12e-04 (4.83e-04)	Tok/s 105774 (96031)	Loss/tok 3.2535 (3.1648)	LR 1.400e-03
0: TRAIN [3][840/1885]	Time 0.148 (0.145)	Data 2.13e-04 (4.80e-04)	Tok/s 99363 (96030)	Loss/tok 3.1085 (3.1645)	LR 1.400e-03
0: TRAIN [3][850/1885]	Time 0.102 (0.145)	Data 2.14e-04 (4.77e-04)	Tok/s 86796 (96003)	Loss/tok 2.9027 (3.1634)	LR 1.400e-03
0: TRAIN [3][860/1885]	Time 0.103 (0.145)	Data 2.08e-04 (4.74e-04)	Tok/s 88272 (96016)	Loss/tok 2.8993 (3.1639)	LR 1.400e-03
0: TRAIN [3][870/1885]	Time 0.149 (0.145)	Data 2.18e-04 (4.71e-04)	Tok/s 98120 (96016)	Loss/tok 3.0932 (3.1635)	LR 1.400e-03
0: TRAIN [3][880/1885]	Time 0.195 (0.144)	Data 2.16e-04 (4.68e-04)	Tok/s 103972 (96010)	Loss/tok 3.2107 (3.1627)	LR 1.400e-03
0: TRAIN [3][890/1885]	Time 0.102 (0.145)	Data 2.12e-04 (4.65e-04)	Tok/s 89340 (96019)	Loss/tok 2.9574 (3.1625)	LR 1.400e-03
0: TRAIN [3][900/1885]	Time 0.247 (0.145)	Data 2.15e-04 (4.62e-04)	Tok/s 105181 (96049)	Loss/tok 3.5085 (3.1624)	LR 1.400e-03
0: TRAIN [3][910/1885]	Time 0.147 (0.144)	Data 2.11e-04 (4.59e-04)	Tok/s 100026 (96023)	Loss/tok 3.0306 (3.1615)	LR 1.400e-03
0: TRAIN [3][920/1885]	Time 0.147 (0.145)	Data 2.14e-04 (4.56e-04)	Tok/s 98821 (96069)	Loss/tok 3.0723 (3.1619)	LR 1.400e-03
0: TRAIN [3][930/1885]	Time 0.148 (0.145)	Data 2.13e-04 (4.54e-04)	Tok/s 99436 (96106)	Loss/tok 3.1411 (3.1621)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][940/1885]	Time 0.148 (0.145)	Data 2.14e-04 (4.51e-04)	Tok/s 98689 (96126)	Loss/tok 3.1308 (3.1619)	LR 1.400e-03
0: TRAIN [3][950/1885]	Time 0.102 (0.145)	Data 2.13e-04 (4.49e-04)	Tok/s 90283 (96112)	Loss/tok 2.7631 (3.1613)	LR 1.400e-03
0: TRAIN [3][960/1885]	Time 0.102 (0.145)	Data 2.10e-04 (4.46e-04)	Tok/s 89940 (96120)	Loss/tok 2.8241 (3.1608)	LR 1.400e-03
0: TRAIN [3][970/1885]	Time 0.148 (0.145)	Data 2.13e-04 (4.44e-04)	Tok/s 98217 (96128)	Loss/tok 3.1244 (3.1607)	LR 1.400e-03
0: TRAIN [3][980/1885]	Time 0.102 (0.145)	Data 2.16e-04 (4.42e-04)	Tok/s 88970 (96176)	Loss/tok 2.7780 (3.1615)	LR 1.400e-03
0: TRAIN [3][990/1885]	Time 0.147 (0.145)	Data 2.14e-04 (4.39e-04)	Tok/s 100370 (96153)	Loss/tok 3.1198 (3.1606)	LR 1.400e-03
0: TRAIN [3][1000/1885]	Time 0.148 (0.145)	Data 2.14e-04 (4.37e-04)	Tok/s 98960 (96150)	Loss/tok 3.0464 (3.1597)	LR 1.400e-03
0: TRAIN [3][1010/1885]	Time 0.196 (0.145)	Data 2.01e-04 (4.35e-04)	Tok/s 105920 (96148)	Loss/tok 3.1322 (3.1593)	LR 1.400e-03
0: TRAIN [3][1020/1885]	Time 0.062 (0.145)	Data 2.00e-04 (4.32e-04)	Tok/s 75290 (96159)	Loss/tok 2.3547 (3.1595)	LR 1.400e-03
0: TRAIN [3][1030/1885]	Time 0.194 (0.145)	Data 2.11e-04 (4.30e-04)	Tok/s 105647 (96166)	Loss/tok 3.2150 (3.1596)	LR 1.400e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1040/1885]	Time 0.147 (0.145)	Data 2.11e-04 (4.28e-04)	Tok/s 101504 (96194)	Loss/tok 3.1179 (3.1605)	LR 1.400e-03
0: TRAIN [3][1050/1885]	Time 0.148 (0.145)	Data 1.88e-04 (4.26e-04)	Tok/s 98336 (96237)	Loss/tok 3.1214 (3.1616)	LR 1.400e-03
0: TRAIN [3][1060/1885]	Time 0.148 (0.145)	Data 2.12e-04 (4.24e-04)	Tok/s 99412 (96219)	Loss/tok 3.1011 (3.1611)	LR 1.400e-03
0: TRAIN [3][1070/1885]	Time 0.147 (0.145)	Data 2.14e-04 (4.22e-04)	Tok/s 100122 (96223)	Loss/tok 3.0609 (3.1601)	LR 1.400e-03
0: TRAIN [3][1080/1885]	Time 0.147 (0.145)	Data 2.13e-04 (4.20e-04)	Tok/s 100105 (96220)	Loss/tok 3.0218 (3.1596)	LR 1.400e-03
0: TRAIN [3][1090/1885]	Time 0.060 (0.145)	Data 2.15e-04 (4.18e-04)	Tok/s 76247 (96165)	Loss/tok 2.5144 (3.1585)	LR 1.400e-03
0: TRAIN [3][1100/1885]	Time 0.100 (0.145)	Data 2.10e-04 (4.16e-04)	Tok/s 90705 (96151)	Loss/tok 2.8635 (3.1584)	LR 1.400e-03
0: TRAIN [3][1110/1885]	Time 0.060 (0.145)	Data 2.11e-04 (4.14e-04)	Tok/s 76302 (96132)	Loss/tok 2.4587 (3.1579)	LR 1.400e-03
0: TRAIN [3][1120/1885]	Time 0.196 (0.145)	Data 2.10e-04 (4.13e-04)	Tok/s 103382 (96125)	Loss/tok 3.1637 (3.1567)	LR 1.400e-03
0: TRAIN [3][1130/1885]	Time 0.102 (0.145)	Data 2.17e-04 (4.11e-04)	Tok/s 88845 (96109)	Loss/tok 2.8970 (3.1563)	LR 1.400e-03
0: TRAIN [3][1140/1885]	Time 0.102 (0.144)	Data 2.13e-04 (4.09e-04)	Tok/s 90269 (96093)	Loss/tok 2.8687 (3.1554)	LR 1.400e-03
0: TRAIN [3][1150/1885]	Time 0.147 (0.144)	Data 2.12e-04 (4.07e-04)	Tok/s 100637 (96077)	Loss/tok 3.1566 (3.1551)	LR 1.400e-03
0: TRAIN [3][1160/1885]	Time 0.103 (0.144)	Data 2.16e-04 (4.06e-04)	Tok/s 91874 (96064)	Loss/tok 2.8582 (3.1541)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1170/1885]	Time 0.100 (0.144)	Data 2.12e-04 (4.04e-04)	Tok/s 90433 (96048)	Loss/tok 2.9951 (3.1540)	LR 1.400e-03
0: TRAIN [3][1180/1885]	Time 0.148 (0.144)	Data 1.96e-04 (4.02e-04)	Tok/s 99274 (96078)	Loss/tok 3.1418 (3.1545)	LR 1.400e-03
0: TRAIN [3][1190/1885]	Time 0.102 (0.144)	Data 2.12e-04 (4.01e-04)	Tok/s 88528 (96073)	Loss/tok 2.8358 (3.1543)	LR 1.400e-03
0: TRAIN [3][1200/1885]	Time 0.149 (0.144)	Data 2.15e-04 (3.99e-04)	Tok/s 99573 (96082)	Loss/tok 3.0407 (3.1536)	LR 1.400e-03
0: TRAIN [3][1210/1885]	Time 0.101 (0.144)	Data 2.17e-04 (3.97e-04)	Tok/s 91605 (96082)	Loss/tok 2.8532 (3.1535)	LR 1.400e-03
0: TRAIN [3][1220/1885]	Time 0.060 (0.144)	Data 2.11e-04 (3.96e-04)	Tok/s 74627 (96080)	Loss/tok 2.3892 (3.1539)	LR 1.400e-03
0: TRAIN [3][1230/1885]	Time 0.060 (0.144)	Data 2.26e-04 (3.94e-04)	Tok/s 75681 (96039)	Loss/tok 2.4470 (3.1528)	LR 1.400e-03
0: TRAIN [3][1240/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.93e-04)	Tok/s 89091 (96057)	Loss/tok 2.9011 (3.1532)	LR 1.400e-03
0: TRAIN [3][1250/1885]	Time 0.194 (0.144)	Data 2.13e-04 (3.91e-04)	Tok/s 106245 (96065)	Loss/tok 3.2686 (3.1531)	LR 1.400e-03
0: TRAIN [3][1260/1885]	Time 0.101 (0.144)	Data 2.13e-04 (3.90e-04)	Tok/s 89962 (96045)	Loss/tok 2.8630 (3.1523)	LR 1.400e-03
0: TRAIN [3][1270/1885]	Time 0.245 (0.144)	Data 2.11e-04 (3.89e-04)	Tok/s 106617 (96029)	Loss/tok 3.3611 (3.1522)	LR 1.400e-03
0: TRAIN [3][1280/1885]	Time 0.149 (0.144)	Data 2.21e-04 (3.87e-04)	Tok/s 99400 (96059)	Loss/tok 2.9780 (3.1525)	LR 1.400e-03
0: TRAIN [3][1290/1885]	Time 0.245 (0.144)	Data 2.17e-04 (3.86e-04)	Tok/s 106851 (96044)	Loss/tok 3.3277 (3.1519)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1300/1885]	Time 0.147 (0.144)	Data 2.16e-04 (3.85e-04)	Tok/s 99767 (96017)	Loss/tok 3.1598 (3.1512)	LR 7.000e-04
0: TRAIN [3][1310/1885]	Time 0.147 (0.144)	Data 2.14e-04 (3.83e-04)	Tok/s 100673 (96056)	Loss/tok 3.1331 (3.1515)	LR 7.000e-04
0: TRAIN [3][1320/1885]	Time 0.101 (0.144)	Data 2.11e-04 (3.82e-04)	Tok/s 89789 (96022)	Loss/tok 3.0695 (3.1508)	LR 7.000e-04
0: TRAIN [3][1330/1885]	Time 0.195 (0.144)	Data 2.26e-04 (3.81e-04)	Tok/s 105544 (96012)	Loss/tok 3.2514 (3.1502)	LR 7.000e-04
0: TRAIN [3][1340/1885]	Time 0.146 (0.144)	Data 2.11e-04 (3.79e-04)	Tok/s 100967 (95977)	Loss/tok 3.0881 (3.1491)	LR 7.000e-04
0: TRAIN [3][1350/1885]	Time 0.146 (0.144)	Data 2.12e-04 (3.78e-04)	Tok/s 97238 (95991)	Loss/tok 3.0189 (3.1487)	LR 7.000e-04
0: TRAIN [3][1360/1885]	Time 0.102 (0.143)	Data 2.13e-04 (3.77e-04)	Tok/s 87508 (95967)	Loss/tok 2.8692 (3.1481)	LR 7.000e-04
0: TRAIN [3][1370/1885]	Time 0.103 (0.143)	Data 2.12e-04 (3.76e-04)	Tok/s 88616 (95956)	Loss/tok 2.8805 (3.1474)	LR 7.000e-04
0: TRAIN [3][1380/1885]	Time 0.245 (0.143)	Data 2.13e-04 (3.75e-04)	Tok/s 108655 (95962)	Loss/tok 3.3168 (3.1478)	LR 7.000e-04
0: TRAIN [3][1390/1885]	Time 0.102 (0.143)	Data 2.15e-04 (3.73e-04)	Tok/s 88262 (95951)	Loss/tok 2.9874 (3.1474)	LR 7.000e-04
0: TRAIN [3][1400/1885]	Time 0.194 (0.144)	Data 2.14e-04 (3.72e-04)	Tok/s 105012 (95965)	Loss/tok 3.2484 (3.1474)	LR 7.000e-04
0: TRAIN [3][1410/1885]	Time 0.246 (0.144)	Data 2.14e-04 (3.71e-04)	Tok/s 106534 (95962)	Loss/tok 3.3774 (3.1476)	LR 7.000e-04
0: TRAIN [3][1420/1885]	Time 0.147 (0.144)	Data 2.14e-04 (3.70e-04)	Tok/s 101212 (95973)	Loss/tok 3.0678 (3.1475)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1430/1885]	Time 0.146 (0.144)	Data 2.14e-04 (3.69e-04)	Tok/s 100657 (95976)	Loss/tok 3.0704 (3.1473)	LR 7.000e-04
0: TRAIN [3][1440/1885]	Time 0.103 (0.144)	Data 2.11e-04 (3.68e-04)	Tok/s 87874 (95988)	Loss/tok 2.8481 (3.1472)	LR 7.000e-04
0: TRAIN [3][1450/1885]	Time 0.150 (0.144)	Data 2.23e-04 (3.67e-04)	Tok/s 96953 (95976)	Loss/tok 3.0904 (3.1467)	LR 7.000e-04
0: TRAIN [3][1460/1885]	Time 0.103 (0.144)	Data 2.00e-04 (3.66e-04)	Tok/s 87994 (95982)	Loss/tok 2.8426 (3.1469)	LR 7.000e-04
0: TRAIN [3][1470/1885]	Time 0.198 (0.144)	Data 2.15e-04 (3.65e-04)	Tok/s 103490 (95996)	Loss/tok 3.2320 (3.1469)	LR 7.000e-04
0: TRAIN [3][1480/1885]	Time 0.149 (0.144)	Data 2.06e-04 (3.64e-04)	Tok/s 97828 (96019)	Loss/tok 3.1450 (3.1474)	LR 7.000e-04
0: TRAIN [3][1490/1885]	Time 0.059 (0.144)	Data 2.13e-04 (3.63e-04)	Tok/s 77567 (96010)	Loss/tok 2.4941 (3.1467)	LR 7.000e-04
0: TRAIN [3][1500/1885]	Time 0.248 (0.144)	Data 1.99e-04 (3.62e-04)	Tok/s 105784 (96023)	Loss/tok 3.4057 (3.1472)	LR 7.000e-04
0: TRAIN [3][1510/1885]	Time 0.197 (0.144)	Data 2.13e-04 (3.61e-04)	Tok/s 105599 (96050)	Loss/tok 3.1221 (3.1475)	LR 7.000e-04
0: TRAIN [3][1520/1885]	Time 0.061 (0.144)	Data 1.99e-04 (3.60e-04)	Tok/s 75425 (96019)	Loss/tok 2.4050 (3.1471)	LR 7.000e-04
0: TRAIN [3][1530/1885]	Time 0.148 (0.144)	Data 2.09e-04 (3.59e-04)	Tok/s 98935 (96019)	Loss/tok 3.1802 (3.1469)	LR 7.000e-04
0: TRAIN [3][1540/1885]	Time 0.247 (0.144)	Data 1.89e-04 (3.58e-04)	Tok/s 104546 (96010)	Loss/tok 3.4763 (3.1467)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1550/1885]	Time 0.101 (0.144)	Data 2.14e-04 (3.57e-04)	Tok/s 89529 (96005)	Loss/tok 2.8668 (3.1466)	LR 7.000e-04
0: TRAIN [3][1560/1885]	Time 0.148 (0.144)	Data 1.76e-04 (3.56e-04)	Tok/s 98307 (96016)	Loss/tok 3.0858 (3.1464)	LR 7.000e-04
0: TRAIN [3][1570/1885]	Time 0.148 (0.144)	Data 2.11e-04 (3.55e-04)	Tok/s 99307 (96011)	Loss/tok 3.0957 (3.1464)	LR 7.000e-04
0: TRAIN [3][1580/1885]	Time 0.105 (0.144)	Data 2.14e-04 (3.54e-04)	Tok/s 87731 (96014)	Loss/tok 2.8772 (3.1466)	LR 7.000e-04
0: TRAIN [3][1590/1885]	Time 0.149 (0.144)	Data 1.75e-04 (3.53e-04)	Tok/s 98378 (96016)	Loss/tok 2.9967 (3.1463)	LR 7.000e-04
0: TRAIN [3][1600/1885]	Time 0.149 (0.144)	Data 2.11e-04 (3.52e-04)	Tok/s 96126 (96026)	Loss/tok 3.1437 (3.1460)	LR 7.000e-04
0: TRAIN [3][1610/1885]	Time 0.105 (0.144)	Data 1.47e-04 (3.51e-04)	Tok/s 86029 (96003)	Loss/tok 2.7770 (3.1452)	LR 7.000e-04
0: TRAIN [3][1620/1885]	Time 0.149 (0.144)	Data 1.47e-04 (3.50e-04)	Tok/s 97427 (96008)	Loss/tok 2.9802 (3.1448)	LR 7.000e-04
0: TRAIN [3][1630/1885]	Time 0.105 (0.144)	Data 2.13e-04 (3.49e-04)	Tok/s 85014 (95996)	Loss/tok 2.8190 (3.1444)	LR 7.000e-04
0: TRAIN [3][1640/1885]	Time 0.102 (0.144)	Data 2.13e-04 (3.48e-04)	Tok/s 89527 (95998)	Loss/tok 2.7524 (3.1442)	LR 7.000e-04
0: TRAIN [3][1650/1885]	Time 0.104 (0.144)	Data 2.15e-04 (3.47e-04)	Tok/s 85906 (95999)	Loss/tok 2.9165 (3.1438)	LR 7.000e-04
0: TRAIN [3][1660/1885]	Time 0.103 (0.144)	Data 2.14e-04 (3.46e-04)	Tok/s 87634 (95982)	Loss/tok 2.8733 (3.1431)	LR 7.000e-04
0: TRAIN [3][1670/1885]	Time 0.194 (0.144)	Data 2.11e-04 (3.46e-04)	Tok/s 105134 (95984)	Loss/tok 3.2367 (3.1430)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1680/1885]	Time 0.104 (0.144)	Data 1.96e-04 (3.45e-04)	Tok/s 87528 (95994)	Loss/tok 2.8520 (3.1433)	LR 7.000e-04
0: TRAIN [3][1690/1885]	Time 0.150 (0.144)	Data 1.83e-04 (3.44e-04)	Tok/s 96210 (95999)	Loss/tok 3.0510 (3.1430)	LR 7.000e-04
0: TRAIN [3][1700/1885]	Time 0.103 (0.144)	Data 2.12e-04 (3.43e-04)	Tok/s 86871 (95973)	Loss/tok 2.9566 (3.1424)	LR 7.000e-04
0: TRAIN [3][1710/1885]	Time 0.195 (0.144)	Data 1.48e-04 (3.42e-04)	Tok/s 104891 (95946)	Loss/tok 3.2598 (3.1419)	LR 7.000e-04
0: TRAIN [3][1720/1885]	Time 0.104 (0.144)	Data 2.14e-04 (3.41e-04)	Tok/s 85393 (95954)	Loss/tok 2.7864 (3.1424)	LR 7.000e-04
0: TRAIN [3][1730/1885]	Time 0.106 (0.144)	Data 2.11e-04 (3.40e-04)	Tok/s 86868 (95932)	Loss/tok 2.9013 (3.1419)	LR 7.000e-04
0: TRAIN [3][1740/1885]	Time 0.105 (0.144)	Data 2.15e-04 (3.40e-04)	Tok/s 85425 (95908)	Loss/tok 2.8840 (3.1412)	LR 7.000e-04
0: TRAIN [3][1750/1885]	Time 0.148 (0.144)	Data 2.11e-04 (3.39e-04)	Tok/s 97928 (95912)	Loss/tok 3.1417 (3.1409)	LR 7.000e-04
0: TRAIN [3][1760/1885]	Time 0.104 (0.144)	Data 2.12e-04 (3.38e-04)	Tok/s 87381 (95907)	Loss/tok 2.8718 (3.1404)	LR 7.000e-04
0: TRAIN [3][1770/1885]	Time 0.106 (0.144)	Data 2.11e-04 (3.37e-04)	Tok/s 85116 (95892)	Loss/tok 2.8155 (3.1399)	LR 7.000e-04
0: TRAIN [3][1780/1885]	Time 0.150 (0.144)	Data 2.11e-04 (3.37e-04)	Tok/s 99020 (95885)	Loss/tok 3.0163 (3.1394)	LR 7.000e-04
0: TRAIN [3][1790/1885]	Time 0.148 (0.144)	Data 2.13e-04 (3.36e-04)	Tok/s 98967 (95873)	Loss/tok 3.0067 (3.1387)	LR 7.000e-04
0: TRAIN [3][1800/1885]	Time 0.245 (0.144)	Data 2.15e-04 (3.35e-04)	Tok/s 106597 (95873)	Loss/tok 3.4090 (3.1390)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1810/1885]	Time 0.149 (0.144)	Data 2.12e-04 (3.34e-04)	Tok/s 100109 (95851)	Loss/tok 3.0037 (3.1384)	LR 7.000e-04
0: TRAIN [3][1820/1885]	Time 0.106 (0.144)	Data 2.12e-04 (3.34e-04)	Tok/s 85126 (95819)	Loss/tok 2.9324 (3.1379)	LR 7.000e-04
0: TRAIN [3][1830/1885]	Time 0.105 (0.144)	Data 2.11e-04 (3.33e-04)	Tok/s 87452 (95820)	Loss/tok 2.9911 (3.1379)	LR 7.000e-04
0: TRAIN [3][1840/1885]	Time 0.149 (0.144)	Data 2.26e-04 (3.32e-04)	Tok/s 100372 (95801)	Loss/tok 3.0308 (3.1372)	LR 7.000e-04
0: TRAIN [3][1850/1885]	Time 0.103 (0.144)	Data 2.11e-04 (3.31e-04)	Tok/s 88540 (95795)	Loss/tok 2.8482 (3.1371)	LR 7.000e-04
0: TRAIN [3][1860/1885]	Time 0.148 (0.144)	Data 2.09e-04 (3.31e-04)	Tok/s 97265 (95784)	Loss/tok 2.9543 (3.1365)	LR 7.000e-04
0: TRAIN [3][1870/1885]	Time 0.196 (0.144)	Data 2.14e-04 (3.30e-04)	Tok/s 104132 (95775)	Loss/tok 3.1546 (3.1362)	LR 7.000e-04
0: TRAIN [3][1880/1885]	Time 0.105 (0.144)	Data 2.11e-04 (3.29e-04)	Tok/s 85021 (95756)	Loss/tok 2.8247 (3.1357)	LR 7.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593835384645, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593835384646, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.557 (0.557)	Decoder iters 99.0 (99.0)	Tok/s 29223 (29223)
0: Running moses detokenizer
0: BLEU(score=24.107153274077987, counts=[36881, 18410, 10486, 6180], totals=[64659, 61656, 58653, 55657], precisions=[57.03923661052599, 29.859218891916438, 17.878028404344192, 11.10372459888244], bp=0.9997371167912463, sys_len=64659, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593835386171, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24109999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593835386172, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1351	Test BLEU: 24.11
0: Performance: Epoch: 3	Training: 765929 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593835386172, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593835386172, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-07-04 04:03:12 AM
RESULT,RNN_TRANSLATOR,,1114,Fujitsu,2020-07-04 03:44:38 AM
