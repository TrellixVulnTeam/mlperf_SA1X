Beginning trial 1 of 1
Gathering sys log on gx2570-03
:::MLLOG {"namespace": "", "time_ms": 1593840126812, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593840126849, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Fujitsu", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593840126849, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593840126849, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593840126849, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xGX2570M5", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
Clearing caches
:::MLLOG {"namespace": "", "time_ms": 1593840130082, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/opt/conda/lib/python3.6/site-packages/mlperf_logging/mllog/mllog.py", "lineno": 261}}
Launching on node gx2570-03
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e PGSYSTEM=PG -e 'MULTI_NODE= --master_port=4260' -e LR=2.8e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6053 -e DECAY_INTERVAL=859 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=65 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200704114252288086675 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200704114252288086675 ./run_and_time.sh
STARTING TIMING RUN AT 2020-07-04 05:22:11 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.8e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6053
+ DECAY_INTERVAL=859
+ TARGET=24.0
+ MAX_SEQ_LEN=65
+ NUMEPOCHS=8
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ PGSYSTEM=PG
+ [[ -f config_PG.sh ]]
+ source config_PG.sh
++ export PGNNODES=1
++ PGNNODES=1
+++ sed 's/^config_//'
+++ sed 's/\.sh$//'
++++ readlink -f config_PG.sh
+++ basename /workspace/rnn_translator/config_PG.sh
++ export PGSYSTEM=PG
++ PGSYSTEM=PG
++ export WALLTIME=00:30:00
++ WALLTIME=00:30:00
++ export LR=2.8e-3
++ LR=2.8e-3
++ export TRAIN_BATCH_SIZE=256
++ TRAIN_BATCH_SIZE=256
++ export TEST_BATCH_SIZE=128
++ TEST_BATCH_SIZE=128
++ export WARMUP_STEPS=200
++ WARMUP_STEPS=200
++ export REMAIN_STEPS=6053
++ REMAIN_STEPS=6053
++ export DECAY_INTERVAL=859
++ DECAY_INTERVAL=859
++ export TARGET=24.0
++ TARGET=24.0
++ export MAX_SEQ_LEN=65
++ MAX_SEQ_LEN=65
++ export NUMEPOCHS=8
++ NUMEPOCHS=8
++ export MATH=fp16
++ MATH=fp16
++ export DIST_OPTS=
++ DIST_OPTS=
++ export 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ export PGNGPU=8
++ PGNGPU=8
++ export PGSOCKETCORES=24
++ PGSOCKETCORES=24
++ export PGHT=2
++ PGHT=2
++ export PGNSOCKET=2
++ PGNSOCKET=2
+ declare -a CMD
+ '[' -n '' ']'
+ CMD=('python' '-u' '-m' 'bind_launch' "--nsockets_per_node=${PGNSOCKET}" "--ncores_per_socket=${PGSOCKETCORES}" "--nproc_per_node=${PGNGPU}")
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ PG == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ python -u -m bind_launch --nsockets_per_node=2 --ncores_per_socket=24 --nproc_per_node=8 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 65 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.8e-3 --warmup-steps 200 --remain-steps 6053 --decay-interval 859 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593840133078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593840133078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593840133078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593840133083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593840133109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593840133110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593840133110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593840133115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=859, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.0028, math='fp16', max_length_test=150, max_length_train=65, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6053, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2952644717
:::MLLOG {"namespace": "", "time_ms": 1593840141115, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2952644717, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 27250554
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.0028}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.0028
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593840143920, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593840143921, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0028, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593840143921, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593840143921, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593840143922, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593840146394, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593840146408, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593840146408, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 65, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3866375 min length: 0 max length: 65 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593840146644, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593840146645, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3860480, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593840146645, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6053, 'decay_interval': 859, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6053
0: Scheduler decay interval: 859
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593840146646, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593840146646, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593840146646, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 859, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593840146647, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593840146647, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593840146647, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6053, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593840146647, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593840146647, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593840146647, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2399617384
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1885]	Time 0.494 (0.494)	Data 2.88e-01 (2.88e-01)	Tok/s 41350 (41350)	Loss/tok 10.7950 (10.7950)	LR 2.865e-05
0: TRAIN [0][10/1885]	Time 0.063 (0.171)	Data 2.03e-04 (2.64e-02)	Tok/s 73984 (87573)	Loss/tok 9.3264 (10.1454)	LR 3.607e-05
0: TRAIN [0][20/1885]	Time 0.243 (0.171)	Data 1.64e-04 (1.39e-02)	Tok/s 107553 (94030)	Loss/tok 9.2672 (9.7455)	LR 4.541e-05
0: TRAIN [0][30/1885]	Time 0.104 (0.167)	Data 1.33e-04 (9.48e-03)	Tok/s 88590 (95413)	Loss/tok 8.7479 (9.5113)	LR 5.717e-05
0: TRAIN [0][40/1885]	Time 0.102 (0.157)	Data 2.20e-04 (7.21e-03)	Tok/s 88168 (95112)	Loss/tok 8.5015 (9.3516)	LR 7.197e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][50/1885]	Time 0.149 (0.151)	Data 2.21e-04 (5.84e-03)	Tok/s 99997 (95034)	Loss/tok 8.4294 (9.2238)	LR 8.854e-05
0: TRAIN [0][60/1885]	Time 0.149 (0.153)	Data 1.02e-04 (4.91e-03)	Tok/s 98821 (95778)	Loss/tok 8.2772 (9.0683)	LR 1.115e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][70/1885]	Time 0.147 (0.153)	Data 2.20e-04 (4.25e-03)	Tok/s 99066 (95968)	Loss/tok 8.1006 (8.9466)	LR 1.371e-04
0: TRAIN [0][80/1885]	Time 0.149 (0.150)	Data 1.51e-04 (3.75e-03)	Tok/s 98835 (95528)	Loss/tok 8.1000 (8.8518)	LR 1.726e-04
0: TRAIN [0][90/1885]	Time 0.148 (0.151)	Data 1.94e-04 (3.36e-03)	Tok/s 98318 (95811)	Loss/tok 7.9775 (8.7537)	LR 2.173e-04
0: TRAIN [0][100/1885]	Time 0.148 (0.150)	Data 2.19e-04 (3.04e-03)	Tok/s 97998 (95660)	Loss/tok 7.9479 (8.6751)	LR 2.736e-04
0: TRAIN [0][110/1885]	Time 0.147 (0.149)	Data 2.21e-04 (2.79e-03)	Tok/s 100812 (95567)	Loss/tok 7.8389 (8.6102)	LR 3.445e-04
0: TRAIN [0][120/1885]	Time 0.148 (0.148)	Data 1.47e-04 (2.57e-03)	Tok/s 98021 (95444)	Loss/tok 7.7731 (8.5483)	LR 4.337e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][130/1885]	Time 0.194 (0.149)	Data 1.22e-04 (2.39e-03)	Tok/s 105007 (95806)	Loss/tok 7.8321 (8.4896)	LR 5.335e-04
0: TRAIN [0][140/1885]	Time 0.193 (0.148)	Data 2.28e-04 (2.24e-03)	Tok/s 105108 (95573)	Loss/tok 8.4022 (8.4453)	LR 6.717e-04
0: TRAIN [0][150/1885]	Time 0.148 (0.148)	Data 1.42e-04 (2.10e-03)	Tok/s 98979 (95728)	Loss/tok 7.7011 (8.4013)	LR 8.456e-04
0: TRAIN [0][160/1885]	Time 0.062 (0.146)	Data 1.75e-04 (1.98e-03)	Tok/s 72557 (95499)	Loss/tok 6.3194 (8.3546)	LR 1.065e-03
0: TRAIN [0][170/1885]	Time 0.060 (0.145)	Data 2.13e-04 (1.88e-03)	Tok/s 75681 (95174)	Loss/tok 6.3693 (8.3076)	LR 1.340e-03
0: TRAIN [0][180/1885]	Time 0.061 (0.144)	Data 2.19e-04 (1.78e-03)	Tok/s 75368 (94857)	Loss/tok 5.8955 (8.2599)	LR 1.687e-03
0: TRAIN [0][190/1885]	Time 0.149 (0.143)	Data 2.29e-04 (1.70e-03)	Tok/s 98093 (94812)	Loss/tok 7.2564 (8.2078)	LR 2.124e-03
0: TRAIN [0][200/1885]	Time 0.102 (0.143)	Data 2.17e-04 (1.63e-03)	Tok/s 87077 (94801)	Loss/tok 6.5138 (8.1504)	LR 2.674e-03
0: TRAIN [0][210/1885]	Time 0.106 (0.143)	Data 1.32e-04 (1.56e-03)	Tok/s 86051 (94798)	Loss/tok 6.5114 (8.0906)	LR 2.800e-03
0: TRAIN [0][220/1885]	Time 0.104 (0.144)	Data 2.14e-04 (1.49e-03)	Tok/s 88496 (94918)	Loss/tok 6.2303 (8.0205)	LR 2.800e-03
0: TRAIN [0][230/1885]	Time 0.104 (0.144)	Data 2.13e-04 (1.44e-03)	Tok/s 84867 (94861)	Loss/tok 6.2030 (7.9570)	LR 2.800e-03
0: TRAIN [0][240/1885]	Time 0.104 (0.145)	Data 1.76e-04 (1.39e-03)	Tok/s 89229 (95011)	Loss/tok 6.0756 (7.8870)	LR 2.800e-03
0: TRAIN [0][250/1885]	Time 0.103 (0.144)	Data 1.87e-04 (1.34e-03)	Tok/s 88237 (95000)	Loss/tok 5.7973 (7.8263)	LR 2.800e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][260/1885]	Time 0.194 (0.145)	Data 1.79e-04 (1.29e-03)	Tok/s 104529 (95075)	Loss/tok 6.1992 (7.7592)	LR 2.800e-03
0: TRAIN [0][270/1885]	Time 0.149 (0.145)	Data 2.19e-04 (1.25e-03)	Tok/s 97850 (95124)	Loss/tok 5.9996 (7.6925)	LR 2.800e-03
0: TRAIN [0][280/1885]	Time 0.103 (0.145)	Data 2.17e-04 (1.22e-03)	Tok/s 89145 (95193)	Loss/tok 5.7196 (7.6265)	LR 2.800e-03
0: TRAIN [0][290/1885]	Time 0.148 (0.145)	Data 2.19e-04 (1.18e-03)	Tok/s 101243 (95202)	Loss/tok 5.7235 (7.5673)	LR 2.800e-03
0: TRAIN [0][300/1885]	Time 0.194 (0.146)	Data 1.49e-04 (1.15e-03)	Tok/s 104977 (95388)	Loss/tok 5.7750 (7.4934)	LR 2.800e-03
0: TRAIN [0][310/1885]	Time 0.148 (0.146)	Data 2.05e-04 (1.12e-03)	Tok/s 98464 (95321)	Loss/tok 5.5111 (7.4332)	LR 2.800e-03
0: TRAIN [0][320/1885]	Time 0.246 (0.145)	Data 2.18e-04 (1.09e-03)	Tok/s 105809 (95199)	Loss/tok 5.8655 (7.3778)	LR 2.800e-03
0: TRAIN [0][330/1885]	Time 0.105 (0.145)	Data 1.86e-04 (1.06e-03)	Tok/s 86536 (95092)	Loss/tok 4.8969 (7.3239)	LR 2.800e-03
0: TRAIN [0][340/1885]	Time 0.105 (0.144)	Data 2.17e-04 (1.04e-03)	Tok/s 87875 (95080)	Loss/tok 4.9543 (7.2663)	LR 2.800e-03
0: TRAIN [0][350/1885]	Time 0.103 (0.145)	Data 1.69e-04 (1.01e-03)	Tok/s 88712 (95185)	Loss/tok 4.7348 (7.2005)	LR 2.800e-03
0: TRAIN [0][360/1885]	Time 0.062 (0.145)	Data 2.18e-04 (9.88e-04)	Tok/s 73185 (95207)	Loss/tok 3.7599 (7.1407)	LR 2.800e-03
0: TRAIN [0][370/1885]	Time 0.196 (0.145)	Data 2.15e-04 (9.67e-04)	Tok/s 103594 (95270)	Loss/tok 5.2150 (7.0796)	LR 2.800e-03
0: TRAIN [0][380/1885]	Time 0.104 (0.145)	Data 2.16e-04 (9.47e-04)	Tok/s 87830 (95316)	Loss/tok 4.4457 (7.0204)	LR 2.800e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][390/1885]	Time 0.149 (0.146)	Data 2.15e-04 (9.28e-04)	Tok/s 99579 (95435)	Loss/tok 4.7798 (6.9555)	LR 2.800e-03
0: TRAIN [0][400/1885]	Time 0.103 (0.146)	Data 1.66e-04 (9.09e-04)	Tok/s 89826 (95373)	Loss/tok 4.4344 (6.9063)	LR 2.800e-03
0: TRAIN [0][410/1885]	Time 0.149 (0.145)	Data 2.28e-04 (8.92e-04)	Tok/s 95288 (95296)	Loss/tok 4.7308 (6.8567)	LR 2.800e-03
0: TRAIN [0][420/1885]	Time 0.194 (0.145)	Data 2.01e-04 (8.75e-04)	Tok/s 104793 (95368)	Loss/tok 4.7896 (6.8008)	LR 2.800e-03
0: TRAIN [0][430/1885]	Time 0.150 (0.146)	Data 2.20e-04 (8.59e-04)	Tok/s 99307 (95345)	Loss/tok 4.4676 (6.7496)	LR 2.800e-03
0: TRAIN [0][440/1885]	Time 0.148 (0.145)	Data 2.17e-04 (8.44e-04)	Tok/s 98699 (95334)	Loss/tok 4.5655 (6.7024)	LR 2.800e-03
0: TRAIN [0][450/1885]	Time 0.148 (0.145)	Data 1.74e-04 (8.30e-04)	Tok/s 98550 (95348)	Loss/tok 4.3639 (6.6522)	LR 2.800e-03
0: TRAIN [0][460/1885]	Time 0.196 (0.145)	Data 1.67e-04 (8.16e-04)	Tok/s 104616 (95369)	Loss/tok 4.5605 (6.6029)	LR 2.800e-03
0: TRAIN [0][470/1885]	Time 0.148 (0.145)	Data 2.21e-04 (8.03e-04)	Tok/s 99323 (95344)	Loss/tok 4.3066 (6.5581)	LR 2.800e-03
0: TRAIN [0][480/1885]	Time 0.105 (0.145)	Data 1.77e-04 (7.90e-04)	Tok/s 86947 (95339)	Loss/tok 4.0351 (6.5139)	LR 2.800e-03
0: TRAIN [0][490/1885]	Time 0.196 (0.145)	Data 1.87e-04 (7.78e-04)	Tok/s 104444 (95355)	Loss/tok 4.5058 (6.4692)	LR 2.800e-03
0: TRAIN [0][500/1885]	Time 0.149 (0.145)	Data 1.53e-04 (7.66e-04)	Tok/s 97220 (95292)	Loss/tok 4.2583 (6.4319)	LR 2.800e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][510/1885]	Time 0.102 (0.145)	Data 1.22e-04 (7.55e-04)	Tok/s 88960 (95234)	Loss/tok 3.9236 (6.3944)	LR 2.800e-03
0: TRAIN [0][520/1885]	Time 0.149 (0.145)	Data 1.58e-04 (7.44e-04)	Tok/s 99023 (95274)	Loss/tok 4.1700 (6.3496)	LR 2.800e-03
0: TRAIN [0][530/1885]	Time 0.105 (0.145)	Data 2.14e-04 (7.34e-04)	Tok/s 85971 (95261)	Loss/tok 3.8781 (6.3108)	LR 2.800e-03
0: TRAIN [0][540/1885]	Time 0.103 (0.145)	Data 2.18e-04 (7.24e-04)	Tok/s 86945 (95249)	Loss/tok 3.7571 (6.2702)	LR 2.800e-03
0: TRAIN [0][550/1885]	Time 0.196 (0.145)	Data 2.15e-04 (7.14e-04)	Tok/s 105677 (95318)	Loss/tok 4.1988 (6.2281)	LR 2.800e-03
0: TRAIN [0][560/1885]	Time 0.104 (0.145)	Data 2.14e-04 (7.05e-04)	Tok/s 88015 (95309)	Loss/tok 3.6894 (6.1919)	LR 2.800e-03
0: TRAIN [0][570/1885]	Time 0.147 (0.145)	Data 1.50e-04 (6.96e-04)	Tok/s 100626 (95335)	Loss/tok 4.0215 (6.1546)	LR 2.800e-03
0: TRAIN [0][580/1885]	Time 0.148 (0.145)	Data 1.49e-04 (6.87e-04)	Tok/s 98890 (95344)	Loss/tok 3.9354 (6.1203)	LR 2.800e-03
0: TRAIN [0][590/1885]	Time 0.064 (0.145)	Data 1.44e-04 (6.79e-04)	Tok/s 71697 (95378)	Loss/tok 3.1413 (6.0840)	LR 2.800e-03
0: TRAIN [0][600/1885]	Time 0.148 (0.146)	Data 2.15e-04 (6.70e-04)	Tok/s 99374 (95479)	Loss/tok 3.9933 (6.0441)	LR 2.800e-03
0: TRAIN [0][610/1885]	Time 0.194 (0.146)	Data 2.17e-04 (6.63e-04)	Tok/s 105988 (95395)	Loss/tok 4.2615 (6.0167)	LR 2.800e-03
0: TRAIN [0][620/1885]	Time 0.195 (0.146)	Data 1.88e-04 (6.55e-04)	Tok/s 104550 (95372)	Loss/tok 4.2054 (5.9859)	LR 2.800e-03
0: TRAIN [0][630/1885]	Time 0.149 (0.145)	Data 2.32e-04 (6.47e-04)	Tok/s 98775 (95306)	Loss/tok 4.0121 (5.9596)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][640/1885]	Time 0.148 (0.145)	Data 2.15e-04 (6.40e-04)	Tok/s 98770 (95301)	Loss/tok 3.9075 (5.9299)	LR 2.800e-03
0: TRAIN [0][650/1885]	Time 0.105 (0.145)	Data 2.14e-04 (6.33e-04)	Tok/s 86240 (95274)	Loss/tok 3.6433 (5.9031)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][660/1885]	Time 0.245 (0.145)	Data 2.19e-04 (6.26e-04)	Tok/s 107284 (95262)	Loss/tok 4.2901 (5.8736)	LR 2.800e-03
0: TRAIN [0][670/1885]	Time 0.195 (0.145)	Data 2.30e-04 (6.20e-04)	Tok/s 103484 (95273)	Loss/tok 4.1938 (5.8455)	LR 2.800e-03
0: TRAIN [0][680/1885]	Time 0.102 (0.145)	Data 2.21e-04 (6.13e-04)	Tok/s 90800 (95266)	Loss/tok 3.5816 (5.8188)	LR 2.800e-03
0: TRAIN [0][690/1885]	Time 0.105 (0.145)	Data 2.26e-04 (6.07e-04)	Tok/s 87527 (95271)	Loss/tok 3.6597 (5.7918)	LR 2.800e-03
0: TRAIN [0][700/1885]	Time 0.105 (0.145)	Data 2.22e-04 (6.01e-04)	Tok/s 87722 (95271)	Loss/tok 3.6619 (5.7648)	LR 2.800e-03
0: TRAIN [0][710/1885]	Time 0.106 (0.145)	Data 1.43e-04 (5.96e-04)	Tok/s 86079 (95193)	Loss/tok 3.6173 (5.7439)	LR 2.800e-03
0: TRAIN [0][720/1885]	Time 0.194 (0.145)	Data 2.20e-04 (5.90e-04)	Tok/s 104359 (95171)	Loss/tok 4.0460 (5.7197)	LR 2.800e-03
0: TRAIN [0][730/1885]	Time 0.247 (0.145)	Data 2.17e-04 (5.85e-04)	Tok/s 105021 (95209)	Loss/tok 4.3754 (5.6928)	LR 2.800e-03
0: TRAIN [0][740/1885]	Time 0.062 (0.145)	Data 2.18e-04 (5.80e-04)	Tok/s 74985 (95144)	Loss/tok 2.8593 (5.6720)	LR 2.800e-03
0: TRAIN [0][750/1885]	Time 0.246 (0.145)	Data 1.50e-04 (5.74e-04)	Tok/s 106224 (95119)	Loss/tok 4.2543 (5.6500)	LR 2.800e-03
0: TRAIN [0][760/1885]	Time 0.149 (0.145)	Data 2.16e-04 (5.69e-04)	Tok/s 98481 (95152)	Loss/tok 3.8759 (5.6248)	LR 2.800e-03
0: TRAIN [0][770/1885]	Time 0.150 (0.145)	Data 2.19e-04 (5.64e-04)	Tok/s 98336 (95163)	Loss/tok 3.9630 (5.6019)	LR 2.800e-03
0: TRAIN [0][780/1885]	Time 0.247 (0.145)	Data 1.06e-04 (5.59e-04)	Tok/s 105277 (95147)	Loss/tok 4.2619 (5.5806)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][790/1885]	Time 0.149 (0.145)	Data 1.76e-04 (5.53e-04)	Tok/s 97095 (95168)	Loss/tok 3.8885 (5.5584)	LR 2.800e-03
0: TRAIN [0][800/1885]	Time 0.148 (0.145)	Data 2.29e-04 (5.49e-04)	Tok/s 98395 (95174)	Loss/tok 3.9387 (5.5368)	LR 2.800e-03
0: TRAIN [0][810/1885]	Time 0.195 (0.145)	Data 2.19e-04 (5.45e-04)	Tok/s 104028 (95138)	Loss/tok 4.0775 (5.5173)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][820/1885]	Time 0.105 (0.145)	Data 2.18e-04 (5.40e-04)	Tok/s 85114 (95178)	Loss/tok 3.5526 (5.4947)	LR 2.800e-03
0: TRAIN [0][830/1885]	Time 0.104 (0.145)	Data 1.64e-04 (5.36e-04)	Tok/s 86760 (95183)	Loss/tok 3.4854 (5.4745)	LR 2.800e-03
0: TRAIN [0][840/1885]	Time 0.102 (0.145)	Data 2.25e-04 (5.32e-04)	Tok/s 90177 (95165)	Loss/tok 3.6722 (5.4563)	LR 2.800e-03
0: TRAIN [0][850/1885]	Time 0.102 (0.145)	Data 2.19e-04 (5.28e-04)	Tok/s 88765 (95208)	Loss/tok 3.4750 (5.4350)	LR 2.800e-03
0: TRAIN [0][860/1885]	Time 0.104 (0.145)	Data 2.17e-04 (5.24e-04)	Tok/s 85935 (95182)	Loss/tok 3.6064 (5.4178)	LR 2.800e-03
0: TRAIN [0][870/1885]	Time 0.148 (0.145)	Data 2.20e-04 (5.21e-04)	Tok/s 98667 (95150)	Loss/tok 3.7929 (5.4015)	LR 2.800e-03
0: TRAIN [0][880/1885]	Time 0.247 (0.145)	Data 1.60e-04 (5.17e-04)	Tok/s 106308 (95198)	Loss/tok 4.2053 (5.3809)	LR 2.800e-03
0: TRAIN [0][890/1885]	Time 0.195 (0.145)	Data 1.76e-04 (5.13e-04)	Tok/s 103498 (95219)	Loss/tok 3.9341 (5.3624)	LR 2.800e-03
0: TRAIN [0][900/1885]	Time 0.149 (0.146)	Data 2.30e-04 (5.10e-04)	Tok/s 97811 (95284)	Loss/tok 3.7872 (5.3419)	LR 2.800e-03
0: TRAIN [0][910/1885]	Time 0.148 (0.146)	Data 1.78e-04 (5.06e-04)	Tok/s 100074 (95254)	Loss/tok 3.7374 (5.3270)	LR 2.800e-03
0: TRAIN [0][920/1885]	Time 0.247 (0.145)	Data 2.20e-04 (5.03e-04)	Tok/s 106179 (95227)	Loss/tok 4.1394 (5.3109)	LR 2.800e-03
0: TRAIN [0][930/1885]	Time 0.106 (0.146)	Data 2.19e-04 (5.00e-04)	Tok/s 84813 (95260)	Loss/tok 3.5265 (5.2933)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][940/1885]	Time 0.102 (0.146)	Data 2.14e-04 (4.97e-04)	Tok/s 87333 (95271)	Loss/tok 3.5018 (5.2766)	LR 2.800e-03
0: TRAIN [0][950/1885]	Time 0.196 (0.146)	Data 2.19e-04 (4.94e-04)	Tok/s 103851 (95318)	Loss/tok 4.0842 (5.2593)	LR 2.800e-03
0: TRAIN [0][960/1885]	Time 0.105 (0.146)	Data 1.61e-04 (4.90e-04)	Tok/s 86384 (95317)	Loss/tok 3.3916 (5.2434)	LR 2.800e-03
0: TRAIN [0][970/1885]	Time 0.195 (0.146)	Data 1.23e-04 (4.87e-04)	Tok/s 103872 (95350)	Loss/tok 3.9368 (5.2271)	LR 2.800e-03
0: TRAIN [0][980/1885]	Time 0.062 (0.146)	Data 2.17e-04 (4.84e-04)	Tok/s 74078 (95309)	Loss/tok 2.8232 (5.2139)	LR 2.800e-03
0: TRAIN [0][990/1885]	Time 0.105 (0.146)	Data 1.43e-04 (4.81e-04)	Tok/s 85754 (95316)	Loss/tok 3.5740 (5.1990)	LR 2.800e-03
0: TRAIN [0][1000/1885]	Time 0.104 (0.146)	Data 1.70e-04 (4.78e-04)	Tok/s 88371 (95303)	Loss/tok 3.4241 (5.1855)	LR 2.800e-03
0: TRAIN [0][1010/1885]	Time 0.196 (0.146)	Data 2.01e-04 (4.75e-04)	Tok/s 103851 (95297)	Loss/tok 3.9449 (5.1705)	LR 2.800e-03
0: TRAIN [0][1020/1885]	Time 0.196 (0.146)	Data 2.20e-04 (4.72e-04)	Tok/s 103115 (95243)	Loss/tok 3.9823 (5.1587)	LR 2.800e-03
0: TRAIN [0][1030/1885]	Time 0.150 (0.146)	Data 2.18e-04 (4.70e-04)	Tok/s 97170 (95249)	Loss/tok 3.6575 (5.1443)	LR 2.800e-03
0: TRAIN [0][1040/1885]	Time 0.153 (0.146)	Data 2.33e-04 (4.67e-04)	Tok/s 95931 (95194)	Loss/tok 3.6059 (5.1322)	LR 2.800e-03
0: TRAIN [0][1050/1885]	Time 0.195 (0.146)	Data 2.17e-04 (4.65e-04)	Tok/s 104937 (95223)	Loss/tok 3.9924 (5.1177)	LR 2.800e-03
0: TRAIN [0][1060/1885]	Time 0.104 (0.146)	Data 1.60e-04 (4.62e-04)	Tok/s 87173 (95239)	Loss/tok 3.4464 (5.1038)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1070/1885]	Time 0.103 (0.146)	Data 1.92e-04 (4.59e-04)	Tok/s 89029 (95244)	Loss/tok 3.4646 (5.0908)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1080/1885]	Time 0.150 (0.146)	Data 2.19e-04 (4.57e-04)	Tok/s 98652 (95286)	Loss/tok 3.5348 (5.0765)	LR 2.800e-03
0: TRAIN [0][1090/1885]	Time 0.104 (0.146)	Data 2.16e-04 (4.54e-04)	Tok/s 86571 (95241)	Loss/tok 3.3187 (5.0653)	LR 2.800e-03
0: TRAIN [0][1100/1885]	Time 0.247 (0.146)	Data 2.07e-04 (4.52e-04)	Tok/s 106730 (95266)	Loss/tok 4.0133 (5.0516)	LR 2.800e-03
0: TRAIN [0][1110/1885]	Time 0.194 (0.146)	Data 2.20e-04 (4.50e-04)	Tok/s 106989 (95281)	Loss/tok 3.8295 (5.0384)	LR 2.800e-03
0: TRAIN [0][1120/1885]	Time 0.105 (0.146)	Data 2.15e-04 (4.48e-04)	Tok/s 85241 (95257)	Loss/tok 3.4099 (5.0277)	LR 2.800e-03
0: TRAIN [0][1130/1885]	Time 0.149 (0.146)	Data 2.16e-04 (4.45e-04)	Tok/s 97180 (95258)	Loss/tok 3.6767 (5.0162)	LR 2.800e-03
0: TRAIN [0][1140/1885]	Time 0.149 (0.146)	Data 1.52e-04 (4.43e-04)	Tok/s 98140 (95270)	Loss/tok 3.6762 (5.0038)	LR 2.800e-03
0: TRAIN [0][1150/1885]	Time 0.102 (0.146)	Data 2.18e-04 (4.41e-04)	Tok/s 89675 (95272)	Loss/tok 3.3829 (4.9925)	LR 2.800e-03
0: TRAIN [0][1160/1885]	Time 0.246 (0.146)	Data 1.43e-04 (4.39e-04)	Tok/s 105924 (95252)	Loss/tok 4.0152 (4.9820)	LR 2.800e-03
0: TRAIN [0][1170/1885]	Time 0.148 (0.146)	Data 2.22e-04 (4.37e-04)	Tok/s 98888 (95271)	Loss/tok 3.7078 (4.9705)	LR 2.800e-03
0: TRAIN [0][1180/1885]	Time 0.148 (0.146)	Data 2.29e-04 (4.35e-04)	Tok/s 100261 (95254)	Loss/tok 3.4934 (4.9595)	LR 2.800e-03
0: TRAIN [0][1190/1885]	Time 0.148 (0.146)	Data 1.11e-04 (4.33e-04)	Tok/s 98384 (95212)	Loss/tok 3.6180 (4.9498)	LR 2.800e-03
0: TRAIN [0][1200/1885]	Time 0.105 (0.146)	Data 2.20e-04 (4.31e-04)	Tok/s 85857 (95191)	Loss/tok 3.2950 (4.9395)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1210/1885]	Time 0.105 (0.146)	Data 2.20e-04 (4.29e-04)	Tok/s 87226 (95181)	Loss/tok 3.3251 (4.9295)	LR 2.800e-03
0: TRAIN [0][1220/1885]	Time 0.062 (0.146)	Data 2.17e-04 (4.27e-04)	Tok/s 73439 (95227)	Loss/tok 2.7699 (4.9174)	LR 2.800e-03
0: TRAIN [0][1230/1885]	Time 0.148 (0.146)	Data 2.18e-04 (4.25e-04)	Tok/s 98759 (95211)	Loss/tok 3.6657 (4.9077)	LR 2.800e-03
0: TRAIN [0][1240/1885]	Time 0.105 (0.146)	Data 2.17e-04 (4.23e-04)	Tok/s 87045 (95186)	Loss/tok 3.3064 (4.8983)	LR 2.800e-03
0: TRAIN [0][1250/1885]	Time 0.149 (0.146)	Data 2.18e-04 (4.22e-04)	Tok/s 98091 (95195)	Loss/tok 3.6227 (4.8882)	LR 2.800e-03
0: TRAIN [0][1260/1885]	Time 0.104 (0.146)	Data 2.15e-04 (4.20e-04)	Tok/s 89481 (95221)	Loss/tok 3.3937 (4.8773)	LR 2.800e-03
0: TRAIN [0][1270/1885]	Time 0.063 (0.146)	Data 2.16e-04 (4.18e-04)	Tok/s 71248 (95210)	Loss/tok 2.7149 (4.8678)	LR 2.800e-03
0: TRAIN [0][1280/1885]	Time 0.195 (0.146)	Data 2.20e-04 (4.17e-04)	Tok/s 105252 (95215)	Loss/tok 3.7210 (4.8581)	LR 2.800e-03
0: TRAIN [0][1290/1885]	Time 0.150 (0.146)	Data 2.20e-04 (4.15e-04)	Tok/s 96631 (95223)	Loss/tok 3.5784 (4.8485)	LR 2.800e-03
0: TRAIN [0][1300/1885]	Time 0.105 (0.146)	Data 2.34e-04 (4.14e-04)	Tok/s 86671 (95177)	Loss/tok 3.3139 (4.8403)	LR 2.800e-03
0: TRAIN [0][1310/1885]	Time 0.194 (0.146)	Data 2.17e-04 (4.12e-04)	Tok/s 104258 (95175)	Loss/tok 3.9245 (4.8314)	LR 2.800e-03
0: TRAIN [0][1320/1885]	Time 0.149 (0.146)	Data 1.94e-04 (4.10e-04)	Tok/s 98740 (95199)	Loss/tok 3.5961 (4.8213)	LR 2.800e-03
0: TRAIN [0][1330/1885]	Time 0.103 (0.146)	Data 2.19e-04 (4.09e-04)	Tok/s 87451 (95203)	Loss/tok 3.2625 (4.8124)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1340/1885]	Time 0.104 (0.146)	Data 1.90e-04 (4.07e-04)	Tok/s 87577 (95166)	Loss/tok 3.4090 (4.8049)	LR 2.800e-03
0: TRAIN [0][1350/1885]	Time 0.195 (0.146)	Data 2.18e-04 (4.06e-04)	Tok/s 104594 (95180)	Loss/tok 3.7135 (4.7958)	LR 2.800e-03
0: TRAIN [0][1360/1885]	Time 0.150 (0.146)	Data 1.44e-04 (4.04e-04)	Tok/s 99159 (95194)	Loss/tok 3.5449 (4.7870)	LR 2.800e-03
0: TRAIN [0][1370/1885]	Time 0.106 (0.146)	Data 2.17e-04 (4.03e-04)	Tok/s 85733 (95203)	Loss/tok 3.3018 (4.7781)	LR 2.800e-03
0: TRAIN [0][1380/1885]	Time 0.103 (0.146)	Data 2.21e-04 (4.01e-04)	Tok/s 87572 (95195)	Loss/tok 3.3202 (4.7702)	LR 2.800e-03
0: TRAIN [0][1390/1885]	Time 0.104 (0.146)	Data 2.20e-04 (4.00e-04)	Tok/s 86533 (95166)	Loss/tok 3.3597 (4.7631)	LR 2.800e-03
0: TRAIN [0][1400/1885]	Time 0.064 (0.146)	Data 2.17e-04 (3.99e-04)	Tok/s 71932 (95134)	Loss/tok 2.8403 (4.7561)	LR 2.800e-03
0: TRAIN [0][1410/1885]	Time 0.196 (0.146)	Data 2.17e-04 (3.97e-04)	Tok/s 104669 (95094)	Loss/tok 3.7008 (4.7488)	LR 2.800e-03
0: TRAIN [0][1420/1885]	Time 0.149 (0.146)	Data 1.69e-04 (3.96e-04)	Tok/s 99009 (95080)	Loss/tok 3.5985 (4.7408)	LR 2.800e-03
0: TRAIN [0][1430/1885]	Time 0.194 (0.146)	Data 2.01e-04 (3.94e-04)	Tok/s 105343 (95090)	Loss/tok 3.6030 (4.7323)	LR 2.800e-03
0: TRAIN [0][1440/1885]	Time 0.105 (0.146)	Data 2.22e-04 (3.93e-04)	Tok/s 87590 (95090)	Loss/tok 3.2806 (4.7245)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1450/1885]	Time 0.245 (0.146)	Data 1.44e-04 (3.92e-04)	Tok/s 107376 (95093)	Loss/tok 3.8519 (4.7162)	LR 2.800e-03
0: TRAIN [0][1460/1885]	Time 0.194 (0.146)	Data 2.30e-04 (3.90e-04)	Tok/s 104400 (95089)	Loss/tok 3.7552 (4.7086)	LR 2.800e-03
0: TRAIN [0][1470/1885]	Time 0.103 (0.146)	Data 1.69e-04 (3.89e-04)	Tok/s 88173 (95085)	Loss/tok 3.3371 (4.7012)	LR 2.800e-03
0: TRAIN [0][1480/1885]	Time 0.147 (0.146)	Data 2.15e-04 (3.88e-04)	Tok/s 100796 (95101)	Loss/tok 3.4597 (4.6932)	LR 2.800e-03
0: TRAIN [0][1490/1885]	Time 0.105 (0.146)	Data 2.17e-04 (3.87e-04)	Tok/s 86193 (95079)	Loss/tok 3.2707 (4.6863)	LR 2.800e-03
0: TRAIN [0][1500/1885]	Time 0.146 (0.146)	Data 2.20e-04 (3.85e-04)	Tok/s 100272 (95104)	Loss/tok 3.4301 (4.6781)	LR 2.800e-03
0: TRAIN [0][1510/1885]	Time 0.104 (0.145)	Data 1.45e-04 (3.84e-04)	Tok/s 89249 (95068)	Loss/tok 3.3194 (4.6720)	LR 2.800e-03
0: TRAIN [0][1520/1885]	Time 0.248 (0.145)	Data 1.66e-04 (3.83e-04)	Tok/s 105999 (95063)	Loss/tok 3.8677 (4.6649)	LR 2.800e-03
0: TRAIN [0][1530/1885]	Time 0.196 (0.146)	Data 2.05e-04 (3.82e-04)	Tok/s 103000 (95086)	Loss/tok 3.7097 (4.6571)	LR 2.800e-03
0: TRAIN [0][1540/1885]	Time 0.149 (0.145)	Data 1.50e-04 (3.80e-04)	Tok/s 98402 (95045)	Loss/tok 3.5233 (4.6513)	LR 2.800e-03
0: TRAIN [0][1550/1885]	Time 0.105 (0.145)	Data 1.43e-04 (3.79e-04)	Tok/s 87040 (95030)	Loss/tok 3.1816 (4.6448)	LR 2.800e-03
0: TRAIN [0][1560/1885]	Time 0.150 (0.145)	Data 1.70e-04 (3.78e-04)	Tok/s 97555 (95023)	Loss/tok 3.4520 (4.6379)	LR 2.800e-03
0: TRAIN [0][1570/1885]	Time 0.148 (0.145)	Data 2.34e-04 (3.77e-04)	Tok/s 99378 (95050)	Loss/tok 3.4263 (4.6303)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1580/1885]	Time 0.104 (0.145)	Data 2.18e-04 (3.76e-04)	Tok/s 85513 (95067)	Loss/tok 3.1580 (4.6230)	LR 2.800e-03
0: TRAIN [0][1590/1885]	Time 0.064 (0.145)	Data 1.58e-04 (3.75e-04)	Tok/s 71538 (95057)	Loss/tok 2.7464 (4.6166)	LR 2.800e-03
0: TRAIN [0][1600/1885]	Time 0.246 (0.145)	Data 2.05e-04 (3.74e-04)	Tok/s 105625 (95053)	Loss/tok 3.8055 (4.6099)	LR 2.800e-03
0: TRAIN [0][1610/1885]	Time 0.105 (0.145)	Data 2.19e-04 (3.73e-04)	Tok/s 85836 (95036)	Loss/tok 3.1885 (4.6040)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1620/1885]	Time 0.196 (0.145)	Data 1.66e-04 (3.72e-04)	Tok/s 105101 (95008)	Loss/tok 3.7357 (4.5983)	LR 2.800e-03
0: TRAIN [0][1630/1885]	Time 0.102 (0.145)	Data 2.20e-04 (3.71e-04)	Tok/s 89250 (95004)	Loss/tok 3.2408 (4.5921)	LR 2.800e-03
0: TRAIN [0][1640/1885]	Time 0.105 (0.145)	Data 1.67e-04 (3.69e-04)	Tok/s 87653 (94995)	Loss/tok 3.2952 (4.5861)	LR 2.800e-03
0: TRAIN [0][1650/1885]	Time 0.147 (0.145)	Data 2.19e-04 (3.69e-04)	Tok/s 98200 (94975)	Loss/tok 3.4552 (4.5803)	LR 2.800e-03
0: TRAIN [0][1660/1885]	Time 0.103 (0.145)	Data 2.21e-04 (3.67e-04)	Tok/s 86654 (94966)	Loss/tok 3.2029 (4.5745)	LR 2.800e-03
0: TRAIN [0][1670/1885]	Time 0.197 (0.145)	Data 2.18e-04 (3.67e-04)	Tok/s 104614 (94990)	Loss/tok 3.6415 (4.5680)	LR 2.800e-03
0: TRAIN [0][1680/1885]	Time 0.104 (0.145)	Data 2.18e-04 (3.65e-04)	Tok/s 88238 (94981)	Loss/tok 3.1527 (4.5620)	LR 2.800e-03
0: TRAIN [0][1690/1885]	Time 0.105 (0.145)	Data 1.40e-04 (3.64e-04)	Tok/s 84753 (94975)	Loss/tok 3.2721 (4.5562)	LR 2.800e-03
0: TRAIN [0][1700/1885]	Time 0.151 (0.145)	Data 2.19e-04 (3.63e-04)	Tok/s 97472 (94988)	Loss/tok 3.5598 (4.5498)	LR 2.800e-03
0: TRAIN [0][1710/1885]	Time 0.148 (0.145)	Data 1.23e-04 (3.62e-04)	Tok/s 98743 (94968)	Loss/tok 3.6098 (4.5444)	LR 2.800e-03
0: TRAIN [0][1720/1885]	Time 0.149 (0.145)	Data 1.91e-04 (3.61e-04)	Tok/s 97633 (94990)	Loss/tok 3.4208 (4.5380)	LR 2.800e-03
0: TRAIN [0][1730/1885]	Time 0.060 (0.145)	Data 2.23e-04 (3.60e-04)	Tok/s 76629 (94982)	Loss/tok 2.7307 (4.5323)	LR 2.800e-03
0: TRAIN [0][1740/1885]	Time 0.147 (0.145)	Data 2.18e-04 (3.59e-04)	Tok/s 99585 (94952)	Loss/tok 3.5597 (4.5272)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1750/1885]	Time 0.196 (0.145)	Data 2.17e-04 (3.59e-04)	Tok/s 104778 (94965)	Loss/tok 3.6526 (4.5215)	LR 2.800e-03
0: TRAIN [0][1760/1885]	Time 0.147 (0.145)	Data 2.18e-04 (3.58e-04)	Tok/s 99048 (94941)	Loss/tok 3.6295 (4.5163)	LR 2.800e-03
0: TRAIN [0][1770/1885]	Time 0.147 (0.145)	Data 1.06e-04 (3.57e-04)	Tok/s 98235 (94948)	Loss/tok 3.3944 (4.5104)	LR 2.800e-03
0: TRAIN [0][1780/1885]	Time 0.104 (0.144)	Data 1.31e-04 (3.56e-04)	Tok/s 87319 (94912)	Loss/tok 3.2744 (4.5054)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1790/1885]	Time 0.246 (0.145)	Data 2.20e-04 (3.55e-04)	Tok/s 104914 (94935)	Loss/tok 3.9278 (4.4993)	LR 2.800e-03
0: TRAIN [0][1800/1885]	Time 0.104 (0.145)	Data 2.20e-04 (3.54e-04)	Tok/s 86995 (94954)	Loss/tok 3.3078 (4.4933)	LR 2.800e-03
0: TRAIN [0][1810/1885]	Time 0.195 (0.145)	Data 1.41e-04 (3.53e-04)	Tok/s 103840 (94975)	Loss/tok 3.6112 (4.4876)	LR 2.800e-03
0: TRAIN [0][1820/1885]	Time 0.103 (0.145)	Data 1.40e-04 (3.52e-04)	Tok/s 89948 (94982)	Loss/tok 3.3752 (4.4820)	LR 2.800e-03
0: TRAIN [0][1830/1885]	Time 0.194 (0.145)	Data 1.51e-04 (3.51e-04)	Tok/s 104313 (94991)	Loss/tok 3.8074 (4.4764)	LR 2.800e-03
0: TRAIN [0][1840/1885]	Time 0.105 (0.145)	Data 2.21e-04 (3.51e-04)	Tok/s 88261 (94978)	Loss/tok 3.3445 (4.4719)	LR 2.800e-03
0: TRAIN [0][1850/1885]	Time 0.149 (0.145)	Data 2.17e-04 (3.50e-04)	Tok/s 99011 (94965)	Loss/tok 3.4647 (4.4672)	LR 2.800e-03
0: TRAIN [0][1860/1885]	Time 0.196 (0.145)	Data 2.21e-04 (3.49e-04)	Tok/s 104439 (94978)	Loss/tok 3.6911 (4.4619)	LR 2.800e-03
0: TRAIN [0][1870/1885]	Time 0.150 (0.145)	Data 2.19e-04 (3.48e-04)	Tok/s 98990 (94973)	Loss/tok 3.4437 (4.4570)	LR 2.800e-03
0: TRAIN [0][1880/1885]	Time 0.197 (0.145)	Data 2.18e-04 (3.47e-04)	Tok/s 103076 (94978)	Loss/tok 3.6993 (4.4519)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593840420239, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593840420240, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.670 (0.670)	Decoder iters 149.0 (149.0)	Tok/s 24049 (24049)
0: Running moses detokenizer
0: BLEU(score=20.299011127986283, counts=[34754, 16037, 8537, 4713], totals=[64882, 61879, 58876, 55876], precisions=[53.564933263462905, 25.91670841480955, 14.499966030300971, 8.4347483713938], bp=1.0, sys_len=64882, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593840421957, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.203, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593840421957, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.4499	Test BLEU: 20.30
0: Performance: Epoch: 0	Training: 759603 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593840421957, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593840421957, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593840421957, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2268448802
0: TRAIN [1][0/1885]	Time 0.338 (0.338)	Data 2.21e-01 (2.21e-01)	Tok/s 27322 (27322)	Loss/tok 3.2527 (3.2527)	LR 2.800e-03
0: TRAIN [1][10/1885]	Time 0.245 (0.179)	Data 1.92e-04 (2.03e-02)	Tok/s 106841 (91822)	Loss/tok 3.7295 (3.4860)	LR 2.800e-03
0: TRAIN [1][20/1885]	Time 0.061 (0.157)	Data 2.20e-04 (1.07e-02)	Tok/s 75713 (91614)	Loss/tok 2.6178 (3.4581)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][30/1885]	Time 0.197 (0.155)	Data 1.70e-04 (7.33e-03)	Tok/s 104500 (93284)	Loss/tok 3.5388 (3.4525)	LR 2.800e-03
0: TRAIN [1][40/1885]	Time 0.150 (0.158)	Data 2.17e-04 (5.59e-03)	Tok/s 97385 (94482)	Loss/tok 3.4021 (3.4689)	LR 2.800e-03
0: TRAIN [1][50/1885]	Time 0.149 (0.156)	Data 2.34e-04 (4.54e-03)	Tok/s 99184 (94755)	Loss/tok 3.4075 (3.4630)	LR 2.800e-03
0: TRAIN [1][60/1885]	Time 0.246 (0.152)	Data 1.96e-04 (3.82e-03)	Tok/s 104952 (94436)	Loss/tok 3.8333 (3.4538)	LR 2.800e-03
0: TRAIN [1][70/1885]	Time 0.102 (0.148)	Data 2.21e-04 (3.32e-03)	Tok/s 89832 (94191)	Loss/tok 3.0827 (3.4404)	LR 2.800e-03
0: TRAIN [1][80/1885]	Time 0.151 (0.152)	Data 2.19e-04 (2.93e-03)	Tok/s 96961 (95008)	Loss/tok 3.3215 (3.4523)	LR 2.800e-03
0: TRAIN [1][90/1885]	Time 0.064 (0.150)	Data 2.22e-04 (2.63e-03)	Tok/s 72373 (94792)	Loss/tok 2.6071 (3.4389)	LR 2.800e-03
0: TRAIN [1][100/1885]	Time 0.104 (0.148)	Data 2.22e-04 (2.39e-03)	Tok/s 88049 (94589)	Loss/tok 3.0995 (3.4286)	LR 2.800e-03
0: TRAIN [1][110/1885]	Time 0.147 (0.148)	Data 2.22e-04 (2.19e-03)	Tok/s 101087 (94803)	Loss/tok 3.5045 (3.4329)	LR 2.800e-03
0: TRAIN [1][120/1885]	Time 0.106 (0.147)	Data 1.42e-04 (2.03e-03)	Tok/s 87228 (94623)	Loss/tok 3.1334 (3.4263)	LR 2.800e-03
0: TRAIN [1][130/1885]	Time 0.247 (0.148)	Data 2.04e-04 (1.89e-03)	Tok/s 106868 (94845)	Loss/tok 3.6863 (3.4307)	LR 2.800e-03
0: TRAIN [1][140/1885]	Time 0.106 (0.147)	Data 1.03e-04 (1.77e-03)	Tok/s 86733 (94856)	Loss/tok 3.1819 (3.4299)	LR 2.800e-03
0: TRAIN [1][150/1885]	Time 0.105 (0.147)	Data 1.78e-04 (1.66e-03)	Tok/s 86113 (94837)	Loss/tok 3.1538 (3.4311)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][160/1885]	Time 0.195 (0.147)	Data 2.24e-04 (1.57e-03)	Tok/s 104086 (94875)	Loss/tok 3.5332 (3.4341)	LR 2.800e-03
0: TRAIN [1][170/1885]	Time 0.104 (0.147)	Data 2.01e-04 (1.49e-03)	Tok/s 87233 (94875)	Loss/tok 3.1030 (3.4348)	LR 2.800e-03
0: TRAIN [1][180/1885]	Time 0.246 (0.149)	Data 2.20e-04 (1.42e-03)	Tok/s 105824 (95119)	Loss/tok 3.8208 (3.4416)	LR 2.800e-03
0: TRAIN [1][190/1885]	Time 0.197 (0.149)	Data 2.19e-04 (1.36e-03)	Tok/s 102999 (95032)	Loss/tok 3.6036 (3.4424)	LR 2.800e-03
0: TRAIN [1][200/1885]	Time 0.149 (0.149)	Data 1.45e-04 (1.30e-03)	Tok/s 98293 (95153)	Loss/tok 3.3747 (3.4413)	LR 2.800e-03
0: TRAIN [1][210/1885]	Time 0.147 (0.149)	Data 1.76e-04 (1.25e-03)	Tok/s 100471 (95138)	Loss/tok 3.3760 (3.4403)	LR 2.800e-03
0: TRAIN [1][220/1885]	Time 0.104 (0.150)	Data 1.94e-04 (1.20e-03)	Tok/s 85947 (95198)	Loss/tok 3.0685 (3.4475)	LR 2.800e-03
0: TRAIN [1][230/1885]	Time 0.195 (0.150)	Data 1.80e-04 (1.16e-03)	Tok/s 104373 (95369)	Loss/tok 3.6305 (3.4509)	LR 2.800e-03
0: TRAIN [1][240/1885]	Time 0.148 (0.150)	Data 1.48e-04 (1.12e-03)	Tok/s 99462 (95307)	Loss/tok 3.4112 (3.4491)	LR 2.800e-03
0: TRAIN [1][250/1885]	Time 0.197 (0.150)	Data 1.80e-04 (1.08e-03)	Tok/s 102990 (95296)	Loss/tok 3.7141 (3.4501)	LR 2.800e-03
0: TRAIN [1][260/1885]	Time 0.064 (0.149)	Data 1.47e-04 (1.04e-03)	Tok/s 72269 (95227)	Loss/tok 2.6380 (3.4489)	LR 2.800e-03
0: TRAIN [1][270/1885]	Time 0.148 (0.149)	Data 1.51e-04 (1.01e-03)	Tok/s 98980 (95288)	Loss/tok 3.5239 (3.4489)	LR 2.800e-03
0: TRAIN [1][280/1885]	Time 0.105 (0.149)	Data 1.79e-04 (9.85e-04)	Tok/s 86079 (95342)	Loss/tok 3.1186 (3.4489)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][290/1885]	Time 0.196 (0.148)	Data 2.16e-04 (9.57e-04)	Tok/s 102537 (95091)	Loss/tok 3.5781 (3.4443)	LR 2.800e-03
0: TRAIN [1][300/1885]	Time 0.104 (0.148)	Data 1.96e-04 (9.32e-04)	Tok/s 88336 (94967)	Loss/tok 3.1441 (3.4427)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][310/1885]	Time 0.103 (0.147)	Data 2.19e-04 (9.08e-04)	Tok/s 88475 (94909)	Loss/tok 3.2062 (3.4435)	LR 2.800e-03
0: TRAIN [1][320/1885]	Time 0.147 (0.147)	Data 2.30e-04 (8.86e-04)	Tok/s 98659 (94889)	Loss/tok 3.4902 (3.4415)	LR 2.800e-03
0: TRAIN [1][330/1885]	Time 0.148 (0.147)	Data 1.82e-04 (8.65e-04)	Tok/s 100541 (94958)	Loss/tok 3.4041 (3.4432)	LR 2.800e-03
0: TRAIN [1][340/1885]	Time 0.149 (0.147)	Data 2.17e-04 (8.46e-04)	Tok/s 97824 (94937)	Loss/tok 3.3630 (3.4414)	LR 2.800e-03
0: TRAIN [1][350/1885]	Time 0.195 (0.147)	Data 2.18e-04 (8.28e-04)	Tok/s 103679 (95006)	Loss/tok 3.7260 (3.4428)	LR 2.800e-03
0: TRAIN [1][360/1885]	Time 0.103 (0.147)	Data 2.23e-04 (8.11e-04)	Tok/s 88248 (94871)	Loss/tok 3.1096 (3.4393)	LR 2.800e-03
0: TRAIN [1][370/1885]	Time 0.103 (0.147)	Data 1.82e-04 (7.94e-04)	Tok/s 88827 (94885)	Loss/tok 3.0622 (3.4396)	LR 2.800e-03
0: TRAIN [1][380/1885]	Time 0.199 (0.148)	Data 2.24e-04 (7.79e-04)	Tok/s 102139 (95038)	Loss/tok 3.5558 (3.4425)	LR 2.800e-03
0: TRAIN [1][390/1885]	Time 0.247 (0.148)	Data 2.20e-04 (7.64e-04)	Tok/s 106811 (95048)	Loss/tok 3.6683 (3.4435)	LR 2.800e-03
0: TRAIN [1][400/1885]	Time 0.060 (0.147)	Data 2.22e-04 (7.50e-04)	Tok/s 75894 (94859)	Loss/tok 2.5620 (3.4391)	LR 2.800e-03
0: TRAIN [1][410/1885]	Time 0.063 (0.147)	Data 2.22e-04 (7.37e-04)	Tok/s 72028 (94884)	Loss/tok 2.6625 (3.4404)	LR 2.800e-03
0: TRAIN [1][420/1885]	Time 0.105 (0.147)	Data 1.81e-04 (7.24e-04)	Tok/s 86618 (94871)	Loss/tok 3.2027 (3.4407)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][430/1885]	Time 0.103 (0.147)	Data 2.26e-04 (7.12e-04)	Tok/s 88509 (94889)	Loss/tok 3.1654 (3.4410)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][440/1885]	Time 0.061 (0.147)	Data 1.80e-04 (7.00e-04)	Tok/s 75675 (94881)	Loss/tok 2.6775 (3.4396)	LR 2.800e-03
0: TRAIN [1][450/1885]	Time 0.246 (0.147)	Data 2.19e-04 (6.89e-04)	Tok/s 106478 (94909)	Loss/tok 3.8390 (3.4399)	LR 2.800e-03
0: TRAIN [1][460/1885]	Time 0.149 (0.147)	Data 2.21e-04 (6.78e-04)	Tok/s 99233 (94849)	Loss/tok 3.3347 (3.4381)	LR 2.800e-03
0: TRAIN [1][470/1885]	Time 0.105 (0.147)	Data 2.16e-04 (6.68e-04)	Tok/s 88050 (94885)	Loss/tok 3.1998 (3.4375)	LR 2.800e-03
0: TRAIN [1][480/1885]	Time 0.104 (0.147)	Data 2.21e-04 (6.59e-04)	Tok/s 87816 (94852)	Loss/tok 3.1550 (3.4359)	LR 2.800e-03
0: TRAIN [1][490/1885]	Time 0.148 (0.147)	Data 2.17e-04 (6.50e-04)	Tok/s 99229 (94872)	Loss/tok 3.4360 (3.4352)	LR 2.800e-03
0: TRAIN [1][500/1885]	Time 0.102 (0.146)	Data 2.22e-04 (6.41e-04)	Tok/s 89004 (94888)	Loss/tok 3.0258 (3.4343)	LR 2.800e-03
0: TRAIN [1][510/1885]	Time 0.149 (0.146)	Data 2.22e-04 (6.33e-04)	Tok/s 97508 (94865)	Loss/tok 3.4830 (3.4330)	LR 2.800e-03
0: TRAIN [1][520/1885]	Time 0.106 (0.146)	Data 2.23e-04 (6.25e-04)	Tok/s 85130 (94840)	Loss/tok 3.1470 (3.4321)	LR 2.800e-03
0: TRAIN [1][530/1885]	Time 0.148 (0.146)	Data 2.21e-04 (6.17e-04)	Tok/s 99628 (94808)	Loss/tok 3.3988 (3.4322)	LR 2.800e-03
0: TRAIN [1][540/1885]	Time 0.198 (0.146)	Data 2.33e-04 (6.10e-04)	Tok/s 102808 (94868)	Loss/tok 3.6197 (3.4326)	LR 2.800e-03
0: TRAIN [1][550/1885]	Time 0.103 (0.146)	Data 2.18e-04 (6.03e-04)	Tok/s 90423 (94892)	Loss/tok 3.0696 (3.4318)	LR 2.800e-03
0: TRAIN [1][560/1885]	Time 0.193 (0.146)	Data 2.25e-04 (5.96e-04)	Tok/s 107446 (94800)	Loss/tok 3.3796 (3.4290)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][570/1885]	Time 0.151 (0.145)	Data 2.19e-04 (5.89e-04)	Tok/s 96874 (94770)	Loss/tok 3.3740 (3.4271)	LR 2.800e-03
0: TRAIN [1][580/1885]	Time 0.147 (0.145)	Data 2.25e-04 (5.82e-04)	Tok/s 99128 (94695)	Loss/tok 3.3264 (3.4246)	LR 2.800e-03
0: TRAIN [1][590/1885]	Time 0.195 (0.145)	Data 2.23e-04 (5.76e-04)	Tok/s 105397 (94686)	Loss/tok 3.6416 (3.4239)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][600/1885]	Time 0.151 (0.145)	Data 2.24e-04 (5.70e-04)	Tok/s 98110 (94670)	Loss/tok 3.2953 (3.4217)	LR 2.800e-03
0: TRAIN [1][610/1885]	Time 0.063 (0.144)	Data 2.21e-04 (5.64e-04)	Tok/s 73632 (94580)	Loss/tok 2.6022 (3.4200)	LR 2.800e-03
0: TRAIN [1][620/1885]	Time 0.104 (0.144)	Data 1.68e-04 (5.58e-04)	Tok/s 89967 (94545)	Loss/tok 3.0616 (3.4185)	LR 2.800e-03
0: TRAIN [1][630/1885]	Time 0.103 (0.144)	Data 2.22e-04 (5.52e-04)	Tok/s 89038 (94570)	Loss/tok 3.1540 (3.4185)	LR 2.800e-03
0: TRAIN [1][640/1885]	Time 0.149 (0.144)	Data 2.47e-04 (5.47e-04)	Tok/s 97960 (94636)	Loss/tok 3.2911 (3.4185)	LR 2.800e-03
0: TRAIN [1][650/1885]	Time 0.197 (0.144)	Data 2.19e-04 (5.42e-04)	Tok/s 102727 (94600)	Loss/tok 3.4692 (3.4165)	LR 2.800e-03
0: TRAIN [1][660/1885]	Time 0.104 (0.143)	Data 1.53e-04 (5.37e-04)	Tok/s 88934 (94528)	Loss/tok 3.1844 (3.4142)	LR 2.800e-03
0: TRAIN [1][670/1885]	Time 0.148 (0.144)	Data 1.83e-04 (5.32e-04)	Tok/s 99231 (94584)	Loss/tok 3.4512 (3.4150)	LR 2.800e-03
0: TRAIN [1][680/1885]	Time 0.150 (0.143)	Data 2.21e-04 (5.28e-04)	Tok/s 98842 (94559)	Loss/tok 3.2867 (3.4141)	LR 2.800e-03
0: TRAIN [1][690/1885]	Time 0.149 (0.143)	Data 2.22e-04 (5.23e-04)	Tok/s 100071 (94552)	Loss/tok 3.2813 (3.4129)	LR 2.800e-03
0: TRAIN [1][700/1885]	Time 0.148 (0.143)	Data 1.91e-04 (5.18e-04)	Tok/s 99972 (94507)	Loss/tok 3.3475 (3.4114)	LR 2.800e-03
0: TRAIN [1][710/1885]	Time 0.105 (0.143)	Data 2.25e-04 (5.14e-04)	Tok/s 85243 (94506)	Loss/tok 3.1336 (3.4102)	LR 2.800e-03
0: TRAIN [1][720/1885]	Time 0.148 (0.143)	Data 3.04e-04 (5.10e-04)	Tok/s 99176 (94507)	Loss/tok 3.3616 (3.4097)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][730/1885]	Time 0.103 (0.143)	Data 2.05e-04 (5.06e-04)	Tok/s 86923 (94492)	Loss/tok 3.1290 (3.4093)	LR 2.800e-03
0: TRAIN [1][740/1885]	Time 0.197 (0.143)	Data 2.07e-04 (5.02e-04)	Tok/s 103491 (94456)	Loss/tok 3.5430 (3.4081)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][750/1885]	Time 0.104 (0.143)	Data 2.39e-04 (4.98e-04)	Tok/s 87174 (94428)	Loss/tok 3.0006 (3.4083)	LR 2.800e-03
0: TRAIN [1][760/1885]	Time 0.196 (0.143)	Data 2.23e-04 (4.94e-04)	Tok/s 103427 (94441)	Loss/tok 3.5959 (3.4081)	LR 2.800e-03
0: TRAIN [1][770/1885]	Time 0.148 (0.143)	Data 2.21e-04 (4.91e-04)	Tok/s 99180 (94411)	Loss/tok 3.3234 (3.4065)	LR 2.800e-03
0: TRAIN [1][780/1885]	Time 0.101 (0.143)	Data 2.07e-04 (4.87e-04)	Tok/s 87853 (94443)	Loss/tok 3.0194 (3.4080)	LR 2.800e-03
0: TRAIN [1][790/1885]	Time 0.197 (0.143)	Data 2.18e-04 (4.84e-04)	Tok/s 104739 (94462)	Loss/tok 3.5833 (3.4073)	LR 2.800e-03
0: TRAIN [1][800/1885]	Time 0.150 (0.143)	Data 2.07e-04 (4.81e-04)	Tok/s 98234 (94501)	Loss/tok 3.2596 (3.4074)	LR 2.800e-03
0: TRAIN [1][810/1885]	Time 0.063 (0.143)	Data 2.17e-04 (4.77e-04)	Tok/s 71607 (94466)	Loss/tok 2.5621 (3.4061)	LR 2.800e-03
0: TRAIN [1][820/1885]	Time 0.101 (0.142)	Data 2.22e-04 (4.74e-04)	Tok/s 90570 (94396)	Loss/tok 3.1654 (3.4046)	LR 2.800e-03
0: TRAIN [1][830/1885]	Time 0.150 (0.143)	Data 2.27e-04 (4.71e-04)	Tok/s 97656 (94432)	Loss/tok 3.4188 (3.4053)	LR 2.800e-03
0: TRAIN [1][840/1885]	Time 0.248 (0.143)	Data 2.22e-04 (4.68e-04)	Tok/s 104156 (94432)	Loss/tok 3.7136 (3.4048)	LR 2.800e-03
0: TRAIN [1][850/1885]	Time 0.103 (0.143)	Data 2.20e-04 (4.65e-04)	Tok/s 87782 (94477)	Loss/tok 3.1383 (3.4062)	LR 2.800e-03
0: TRAIN [1][860/1885]	Time 0.104 (0.143)	Data 2.20e-04 (4.62e-04)	Tok/s 86518 (94467)	Loss/tok 3.0155 (3.4049)	LR 2.800e-03
0: TRAIN [1][870/1885]	Time 0.149 (0.143)	Data 2.21e-04 (4.59e-04)	Tok/s 98568 (94423)	Loss/tok 3.4909 (3.4038)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][880/1885]	Time 0.148 (0.142)	Data 2.23e-04 (4.56e-04)	Tok/s 99247 (94419)	Loss/tok 3.3117 (3.4028)	LR 2.800e-03
0: TRAIN [1][890/1885]	Time 0.106 (0.142)	Data 2.28e-04 (4.53e-04)	Tok/s 86956 (94392)	Loss/tok 3.2511 (3.4015)	LR 2.800e-03
0: TRAIN [1][900/1885]	Time 0.150 (0.142)	Data 2.23e-04 (4.50e-04)	Tok/s 97485 (94392)	Loss/tok 3.2541 (3.4011)	LR 2.800e-03
0: TRAIN [1][910/1885]	Time 0.150 (0.142)	Data 2.27e-04 (4.48e-04)	Tok/s 99488 (94383)	Loss/tok 3.3352 (3.4000)	LR 2.800e-03
0: TRAIN [1][920/1885]	Time 0.105 (0.142)	Data 1.44e-04 (4.45e-04)	Tok/s 86956 (94411)	Loss/tok 3.0429 (3.4009)	LR 2.800e-03
0: Gradient norm: nan
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][930/1885]	Time 0.195 (0.142)	Data 2.26e-04 (4.43e-04)	Tok/s 105774 (94393)	Loss/tok 3.6606 (3.4013)	LR 2.800e-03
0: TRAIN [1][940/1885]	Time 0.103 (0.142)	Data 2.20e-04 (4.40e-04)	Tok/s 88154 (94393)	Loss/tok 3.1331 (3.4014)	LR 2.800e-03
0: TRAIN [1][950/1885]	Time 0.195 (0.142)	Data 2.17e-04 (4.38e-04)	Tok/s 104959 (94375)	Loss/tok 3.4252 (3.4005)	LR 2.800e-03
0: TRAIN [1][960/1885]	Time 0.246 (0.142)	Data 2.19e-04 (4.35e-04)	Tok/s 105574 (94385)	Loss/tok 3.7059 (3.4013)	LR 2.800e-03
0: TRAIN [1][970/1885]	Time 0.197 (0.143)	Data 1.69e-04 (4.33e-04)	Tok/s 105531 (94408)	Loss/tok 3.3683 (3.4006)	LR 2.800e-03
0: TRAIN [1][980/1885]	Time 0.103 (0.143)	Data 2.21e-04 (4.31e-04)	Tok/s 87853 (94430)	Loss/tok 3.1145 (3.4010)	LR 2.800e-03
0: TRAIN [1][990/1885]	Time 0.246 (0.143)	Data 2.05e-04 (4.28e-04)	Tok/s 107461 (94435)	Loss/tok 3.7604 (3.4012)	LR 2.800e-03
0: TRAIN [1][1000/1885]	Time 0.104 (0.143)	Data 2.24e-04 (4.26e-04)	Tok/s 88903 (94390)	Loss/tok 3.1252 (3.4008)	LR 2.800e-03
0: TRAIN [1][1010/1885]	Time 0.104 (0.143)	Data 2.21e-04 (4.24e-04)	Tok/s 86411 (94383)	Loss/tok 3.1953 (3.4015)	LR 2.800e-03
0: TRAIN [1][1020/1885]	Time 0.149 (0.143)	Data 1.81e-04 (4.22e-04)	Tok/s 98092 (94392)	Loss/tok 3.4057 (3.4019)	LR 2.800e-03
0: TRAIN [1][1030/1885]	Time 0.148 (0.143)	Data 2.24e-04 (4.20e-04)	Tok/s 100357 (94415)	Loss/tok 3.3113 (3.4018)	LR 2.800e-03
0: TRAIN [1][1040/1885]	Time 0.147 (0.143)	Data 2.17e-04 (4.18e-04)	Tok/s 100701 (94445)	Loss/tok 3.2563 (3.4021)	LR 2.800e-03
0: TRAIN [1][1050/1885]	Time 0.105 (0.143)	Data 1.47e-04 (4.16e-04)	Tok/s 87508 (94415)	Loss/tok 3.0421 (3.4015)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1060/1885]	Time 0.064 (0.143)	Data 2.08e-04 (4.14e-04)	Tok/s 71872 (94414)	Loss/tok 2.6490 (3.4019)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1070/1885]	Time 0.148 (0.143)	Data 2.32e-04 (4.12e-04)	Tok/s 98760 (94446)	Loss/tok 3.5198 (3.4031)	LR 2.800e-03
0: TRAIN [1][1080/1885]	Time 0.194 (0.143)	Data 2.61e-04 (4.11e-04)	Tok/s 103571 (94472)	Loss/tok 3.5380 (3.4035)	LR 2.800e-03
0: TRAIN [1][1090/1885]	Time 0.105 (0.143)	Data 1.73e-04 (4.09e-04)	Tok/s 87156 (94478)	Loss/tok 3.1283 (3.4032)	LR 2.800e-03
0: TRAIN [1][1100/1885]	Time 0.246 (0.143)	Data 2.20e-04 (4.07e-04)	Tok/s 105350 (94501)	Loss/tok 3.7199 (3.4041)	LR 2.800e-03
0: TRAIN [1][1110/1885]	Time 0.149 (0.144)	Data 2.20e-04 (4.05e-04)	Tok/s 97530 (94530)	Loss/tok 3.2985 (3.4046)	LR 2.800e-03
0: TRAIN [1][1120/1885]	Time 0.196 (0.144)	Data 2.24e-04 (4.04e-04)	Tok/s 103848 (94548)	Loss/tok 3.5521 (3.4046)	LR 2.800e-03
0: TRAIN [1][1130/1885]	Time 0.149 (0.144)	Data 2.23e-04 (4.02e-04)	Tok/s 98295 (94584)	Loss/tok 3.3414 (3.4057)	LR 2.800e-03
0: TRAIN [1][1140/1885]	Time 0.105 (0.144)	Data 2.24e-04 (4.00e-04)	Tok/s 86029 (94609)	Loss/tok 3.2299 (3.4060)	LR 2.800e-03
0: TRAIN [1][1150/1885]	Time 0.196 (0.144)	Data 2.25e-04 (3.99e-04)	Tok/s 104900 (94611)	Loss/tok 3.4341 (3.4054)	LR 2.800e-03
0: TRAIN [1][1160/1885]	Time 0.149 (0.144)	Data 2.23e-04 (3.97e-04)	Tok/s 99944 (94619)	Loss/tok 3.2220 (3.4047)	LR 2.800e-03
0: TRAIN [1][1170/1885]	Time 0.105 (0.144)	Data 2.23e-04 (3.96e-04)	Tok/s 85933 (94610)	Loss/tok 3.2392 (3.4041)	LR 2.800e-03
0: TRAIN [1][1180/1885]	Time 0.147 (0.144)	Data 2.22e-04 (3.94e-04)	Tok/s 98568 (94599)	Loss/tok 3.3968 (3.4034)	LR 2.800e-03
0: TRAIN [1][1190/1885]	Time 0.196 (0.144)	Data 2.20e-04 (3.92e-04)	Tok/s 102843 (94607)	Loss/tok 3.4930 (3.4030)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1200/1885]	Time 0.105 (0.144)	Data 1.95e-04 (3.91e-04)	Tok/s 86388 (94594)	Loss/tok 3.0529 (3.4025)	LR 2.800e-03
0: TRAIN [1][1210/1885]	Time 0.148 (0.144)	Data 1.94e-04 (3.90e-04)	Tok/s 99523 (94633)	Loss/tok 3.3869 (3.4025)	LR 2.800e-03
0: TRAIN [1][1220/1885]	Time 0.193 (0.144)	Data 2.33e-04 (3.88e-04)	Tok/s 105390 (94635)	Loss/tok 3.5126 (3.4022)	LR 2.800e-03
0: TRAIN [1][1230/1885]	Time 0.151 (0.144)	Data 2.26e-04 (3.87e-04)	Tok/s 96774 (94632)	Loss/tok 3.4821 (3.4021)	LR 2.800e-03
0: TRAIN [1][1240/1885]	Time 0.149 (0.144)	Data 2.22e-04 (3.86e-04)	Tok/s 97895 (94658)	Loss/tok 3.2940 (3.4014)	LR 2.800e-03
0: TRAIN [1][1250/1885]	Time 0.195 (0.144)	Data 2.25e-04 (3.84e-04)	Tok/s 104083 (94685)	Loss/tok 3.4630 (3.4016)	LR 2.800e-03
0: TRAIN [1][1260/1885]	Time 0.064 (0.144)	Data 2.22e-04 (3.83e-04)	Tok/s 72467 (94642)	Loss/tok 2.4792 (3.4007)	LR 2.800e-03
0: TRAIN [1][1270/1885]	Time 0.198 (0.144)	Data 2.51e-04 (3.81e-04)	Tok/s 102966 (94604)	Loss/tok 3.6250 (3.4001)	LR 2.800e-03
0: TRAIN [1][1280/1885]	Time 0.104 (0.144)	Data 2.22e-04 (3.80e-04)	Tok/s 87646 (94602)	Loss/tok 3.0827 (3.4001)	LR 2.800e-03
0: TRAIN [1][1290/1885]	Time 0.149 (0.144)	Data 2.23e-04 (3.79e-04)	Tok/s 97554 (94641)	Loss/tok 3.4055 (3.4009)	LR 2.800e-03
0: TRAIN [1][1300/1885]	Time 0.197 (0.144)	Data 3.03e-04 (3.77e-04)	Tok/s 102819 (94670)	Loss/tok 3.4834 (3.4010)	LR 2.800e-03
0: TRAIN [1][1310/1885]	Time 0.104 (0.144)	Data 2.21e-04 (3.76e-04)	Tok/s 87901 (94653)	Loss/tok 2.9461 (3.4001)	LR 2.800e-03
0: TRAIN [1][1320/1885]	Time 0.148 (0.144)	Data 2.19e-04 (3.75e-04)	Tok/s 99066 (94648)	Loss/tok 3.3311 (3.3998)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1330/1885]	Time 0.246 (0.144)	Data 2.19e-04 (3.74e-04)	Tok/s 106284 (94673)	Loss/tok 3.6925 (3.4004)	LR 2.800e-03
0: TRAIN [1][1340/1885]	Time 0.246 (0.144)	Data 2.69e-04 (3.73e-04)	Tok/s 106007 (94690)	Loss/tok 3.6766 (3.4001)	LR 2.800e-03
0: TRAIN [1][1350/1885]	Time 0.197 (0.145)	Data 1.55e-04 (3.71e-04)	Tok/s 104272 (94747)	Loss/tok 3.3770 (3.4008)	LR 2.800e-03
0: TRAIN [1][1360/1885]	Time 0.105 (0.145)	Data 2.20e-04 (3.70e-04)	Tok/s 86268 (94743)	Loss/tok 3.1172 (3.4007)	LR 2.800e-03
0: TRAIN [1][1370/1885]	Time 0.103 (0.145)	Data 2.17e-04 (3.69e-04)	Tok/s 88674 (94729)	Loss/tok 3.1634 (3.3999)	LR 2.800e-03
0: TRAIN [1][1380/1885]	Time 0.196 (0.145)	Data 2.24e-04 (3.68e-04)	Tok/s 104792 (94746)	Loss/tok 3.5658 (3.3994)	LR 2.800e-03
0: TRAIN [1][1390/1885]	Time 0.148 (0.145)	Data 2.19e-04 (3.67e-04)	Tok/s 99448 (94769)	Loss/tok 3.3136 (3.3996)	LR 2.800e-03
0: TRAIN [1][1400/1885]	Time 0.149 (0.145)	Data 2.73e-04 (3.66e-04)	Tok/s 99104 (94768)	Loss/tok 3.3228 (3.3991)	LR 2.800e-03
0: TRAIN [1][1410/1885]	Time 0.104 (0.145)	Data 2.19e-04 (3.65e-04)	Tok/s 87374 (94759)	Loss/tok 2.9711 (3.3991)	LR 2.800e-03
0: TRAIN [1][1420/1885]	Time 0.062 (0.145)	Data 2.19e-04 (3.64e-04)	Tok/s 73255 (94722)	Loss/tok 2.6945 (3.3985)	LR 2.800e-03
0: TRAIN [1][1430/1885]	Time 0.105 (0.145)	Data 2.13e-04 (3.62e-04)	Tok/s 86672 (94707)	Loss/tok 2.9969 (3.3978)	LR 2.800e-03
0: TRAIN [1][1440/1885]	Time 0.105 (0.144)	Data 1.25e-04 (3.61e-04)	Tok/s 87342 (94702)	Loss/tok 3.1268 (3.3972)	LR 2.800e-03
0: TRAIN [1][1450/1885]	Time 0.104 (0.144)	Data 2.04e-04 (3.60e-04)	Tok/s 87369 (94682)	Loss/tok 3.1967 (3.3964)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1460/1885]	Time 0.149 (0.144)	Data 2.06e-04 (3.59e-04)	Tok/s 98531 (94702)	Loss/tok 3.2888 (3.3963)	LR 2.800e-03
0: TRAIN [1][1470/1885]	Time 0.149 (0.144)	Data 2.23e-04 (3.58e-04)	Tok/s 98578 (94688)	Loss/tok 3.3241 (3.3956)	LR 2.800e-03
0: TRAIN [1][1480/1885]	Time 0.104 (0.144)	Data 2.20e-04 (3.57e-04)	Tok/s 88526 (94690)	Loss/tok 3.2660 (3.3956)	LR 2.800e-03
0: TRAIN [1][1490/1885]	Time 0.148 (0.144)	Data 1.10e-04 (3.56e-04)	Tok/s 98922 (94717)	Loss/tok 3.3699 (3.3958)	LR 2.800e-03
0: TRAIN [1][1500/1885]	Time 0.150 (0.144)	Data 2.18e-04 (3.55e-04)	Tok/s 97906 (94712)	Loss/tok 3.2131 (3.3952)	LR 2.800e-03
0: TRAIN [1][1510/1885]	Time 0.247 (0.144)	Data 2.22e-04 (3.54e-04)	Tok/s 106164 (94711)	Loss/tok 3.7548 (3.3950)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1520/1885]	Time 0.246 (0.145)	Data 2.19e-04 (3.53e-04)	Tok/s 104997 (94714)	Loss/tok 3.7960 (3.3958)	LR 2.800e-03
0: TRAIN [1][1530/1885]	Time 0.104 (0.145)	Data 2.18e-04 (3.52e-04)	Tok/s 86103 (94727)	Loss/tok 3.1872 (3.3958)	LR 2.800e-03
0: TRAIN [1][1540/1885]	Time 0.104 (0.145)	Data 1.12e-04 (3.51e-04)	Tok/s 87630 (94736)	Loss/tok 3.1124 (3.3954)	LR 2.800e-03
0: TRAIN [1][1550/1885]	Time 0.105 (0.145)	Data 2.21e-04 (3.50e-04)	Tok/s 86075 (94738)	Loss/tok 3.0878 (3.3950)	LR 2.800e-03
0: TRAIN [1][1560/1885]	Time 0.150 (0.145)	Data 2.22e-04 (3.49e-04)	Tok/s 98300 (94746)	Loss/tok 3.3975 (3.3946)	LR 2.800e-03
0: TRAIN [1][1570/1885]	Time 0.149 (0.145)	Data 2.21e-04 (3.48e-04)	Tok/s 99400 (94760)	Loss/tok 3.3023 (3.3945)	LR 2.800e-03
0: TRAIN [1][1580/1885]	Time 0.104 (0.145)	Data 2.33e-04 (3.47e-04)	Tok/s 87177 (94761)	Loss/tok 3.0266 (3.3942)	LR 2.800e-03
0: TRAIN [1][1590/1885]	Time 0.149 (0.145)	Data 2.19e-04 (3.46e-04)	Tok/s 98304 (94772)	Loss/tok 3.3421 (3.3938)	LR 2.800e-03
0: TRAIN [1][1600/1885]	Time 0.246 (0.145)	Data 2.18e-04 (3.45e-04)	Tok/s 106430 (94776)	Loss/tok 3.7037 (3.3936)	LR 2.800e-03
0: TRAIN [1][1610/1885]	Time 0.062 (0.145)	Data 2.17e-04 (3.44e-04)	Tok/s 73795 (94779)	Loss/tok 2.4495 (3.3933)	LR 2.800e-03
0: TRAIN [1][1620/1885]	Time 0.102 (0.145)	Data 2.21e-04 (3.43e-04)	Tok/s 89022 (94792)	Loss/tok 3.0672 (3.3932)	LR 2.800e-03
0: TRAIN [1][1630/1885]	Time 0.248 (0.145)	Data 2.23e-04 (3.42e-04)	Tok/s 104141 (94790)	Loss/tok 3.8225 (3.3932)	LR 2.800e-03
0: TRAIN [1][1640/1885]	Time 0.247 (0.145)	Data 2.23e-04 (3.41e-04)	Tok/s 107113 (94822)	Loss/tok 3.5936 (3.3938)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1650/1885]	Time 0.148 (0.145)	Data 2.20e-04 (3.41e-04)	Tok/s 99095 (94849)	Loss/tok 3.3954 (3.3941)	LR 2.800e-03
0: TRAIN [1][1660/1885]	Time 0.195 (0.145)	Data 1.80e-04 (3.40e-04)	Tok/s 103934 (94882)	Loss/tok 3.4918 (3.3945)	LR 2.800e-03
0: TRAIN [1][1670/1885]	Time 0.199 (0.145)	Data 2.28e-04 (3.39e-04)	Tok/s 102765 (94890)	Loss/tok 3.4996 (3.3938)	LR 2.800e-03
0: TRAIN [1][1680/1885]	Time 0.149 (0.145)	Data 2.18e-04 (3.38e-04)	Tok/s 99709 (94877)	Loss/tok 3.3301 (3.3930)	LR 2.800e-03
0: TRAIN [1][1690/1885]	Time 0.150 (0.145)	Data 2.16e-04 (3.37e-04)	Tok/s 97214 (94873)	Loss/tok 3.4206 (3.3927)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1700/1885]	Time 0.149 (0.145)	Data 2.19e-04 (3.37e-04)	Tok/s 97849 (94882)	Loss/tok 3.3419 (3.3927)	LR 2.800e-03
0: TRAIN [1][1710/1885]	Time 0.148 (0.145)	Data 2.21e-04 (3.36e-04)	Tok/s 98618 (94896)	Loss/tok 3.2831 (3.3926)	LR 2.800e-03
0: TRAIN [1][1720/1885]	Time 0.147 (0.145)	Data 1.81e-04 (3.35e-04)	Tok/s 99455 (94872)	Loss/tok 3.3067 (3.3921)	LR 2.800e-03
0: TRAIN [1][1730/1885]	Time 0.194 (0.145)	Data 1.79e-04 (3.34e-04)	Tok/s 105724 (94879)	Loss/tok 3.3982 (3.3915)	LR 2.800e-03
0: TRAIN [1][1740/1885]	Time 0.062 (0.145)	Data 2.19e-04 (3.34e-04)	Tok/s 73606 (94873)	Loss/tok 2.4548 (3.3909)	LR 2.800e-03
0: TRAIN [1][1750/1885]	Time 0.150 (0.145)	Data 2.20e-04 (3.33e-04)	Tok/s 98342 (94883)	Loss/tok 3.2799 (3.3906)	LR 2.800e-03
0: TRAIN [1][1760/1885]	Time 0.197 (0.145)	Data 2.22e-04 (3.32e-04)	Tok/s 103031 (94882)	Loss/tok 3.4884 (3.3900)	LR 2.800e-03
0: TRAIN [1][1770/1885]	Time 0.105 (0.145)	Data 2.17e-04 (3.31e-04)	Tok/s 87020 (94885)	Loss/tok 3.1068 (3.3899)	LR 2.800e-03
0: TRAIN [1][1780/1885]	Time 0.149 (0.145)	Data 2.19e-04 (3.31e-04)	Tok/s 98237 (94898)	Loss/tok 3.2393 (3.3896)	LR 2.800e-03
0: TRAIN [1][1790/1885]	Time 0.247 (0.145)	Data 2.20e-04 (3.30e-04)	Tok/s 106297 (94923)	Loss/tok 3.6243 (3.3903)	LR 2.800e-03
0: TRAIN [1][1800/1885]	Time 0.103 (0.145)	Data 2.21e-04 (3.29e-04)	Tok/s 87386 (94936)	Loss/tok 3.1058 (3.3904)	LR 2.800e-03
0: TRAIN [1][1810/1885]	Time 0.246 (0.145)	Data 1.80e-04 (3.29e-04)	Tok/s 106173 (94941)	Loss/tok 3.6097 (3.3904)	LR 2.800e-03
0: TRAIN [1][1820/1885]	Time 0.103 (0.146)	Data 1.41e-04 (3.28e-04)	Tok/s 88343 (94954)	Loss/tok 2.9957 (3.3905)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1830/1885]	Time 0.246 (0.146)	Data 2.18e-04 (3.27e-04)	Tok/s 106919 (94965)	Loss/tok 3.7060 (3.3905)	LR 2.800e-03
0: TRAIN [1][1840/1885]	Time 0.104 (0.145)	Data 2.21e-04 (3.26e-04)	Tok/s 87740 (94945)	Loss/tok 3.0239 (3.3896)	LR 2.800e-03
0: TRAIN [1][1850/1885]	Time 0.061 (0.145)	Data 2.18e-04 (3.26e-04)	Tok/s 73532 (94915)	Loss/tok 2.5727 (3.3887)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1860/1885]	Time 0.104 (0.145)	Data 2.02e-04 (3.25e-04)	Tok/s 87264 (94930)	Loss/tok 3.1243 (3.3890)	LR 2.800e-03
0: TRAIN [1][1870/1885]	Time 0.102 (0.145)	Data 1.18e-04 (3.24e-04)	Tok/s 89711 (94906)	Loss/tok 3.1675 (3.3883)	LR 2.800e-03
0: TRAIN [1][1880/1885]	Time 0.060 (0.145)	Data 2.17e-04 (3.24e-04)	Tok/s 73945 (94875)	Loss/tok 2.5239 (3.3874)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593840695934, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593840695934, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.671 (0.671)	Decoder iters 149.0 (149.0)	Tok/s 24282 (24282)
0: Running moses detokenizer
0: BLEU(score=22.185919104384848, counts=[35850, 17261, 9518, 5483], totals=[65020, 62017, 59014, 56014], precisions=[55.136880959704705, 27.83269103632875, 16.128376317483987, 9.788624272503302], bp=1.0, sys_len=65020, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593840697601, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22190000000000001, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593840697601, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3870	Test BLEU: 22.19
0: Performance: Epoch: 1	Training: 758528 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593840697601, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593840697601, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593840697601, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2395601770
0: TRAIN [2][0/1885]	Time 0.493 (0.493)	Data 2.00e-01 (2.00e-01)	Tok/s 52769 (52769)	Loss/tok 3.6125 (3.6125)	LR 2.800e-03
0: TRAIN [2][10/1885]	Time 0.146 (0.167)	Data 2.18e-04 (1.83e-02)	Tok/s 99664 (90587)	Loss/tok 3.2257 (3.2571)	LR 2.800e-03
0: TRAIN [2][20/1885]	Time 0.146 (0.155)	Data 2.16e-04 (9.71e-03)	Tok/s 100417 (93820)	Loss/tok 3.2149 (3.2338)	LR 2.800e-03
0: TRAIN [2][30/1885]	Time 0.102 (0.151)	Data 1.07e-04 (6.64e-03)	Tok/s 88177 (95161)	Loss/tok 3.0437 (3.2301)	LR 2.800e-03
0: TRAIN [2][40/1885]	Time 0.195 (0.152)	Data 1.06e-04 (5.04e-03)	Tok/s 103871 (95706)	Loss/tok 3.3983 (3.2530)	LR 2.800e-03
0: TRAIN [2][50/1885]	Time 0.101 (0.151)	Data 1.36e-04 (4.08e-03)	Tok/s 89050 (96009)	Loss/tok 2.9579 (3.2489)	LR 2.800e-03
0: TRAIN [2][60/1885]	Time 0.060 (0.145)	Data 1.08e-04 (3.43e-03)	Tok/s 76620 (95340)	Loss/tok 2.4237 (3.2373)	LR 2.800e-03
0: TRAIN [2][70/1885]	Time 0.147 (0.145)	Data 1.06e-04 (2.97e-03)	Tok/s 99376 (95509)	Loss/tok 3.2207 (3.2381)	LR 2.800e-03
0: TRAIN [2][80/1885]	Time 0.061 (0.145)	Data 8.70e-05 (2.61e-03)	Tok/s 73187 (95574)	Loss/tok 2.4088 (3.2405)	LR 2.800e-03
0: TRAIN [2][90/1885]	Time 0.101 (0.148)	Data 2.18e-04 (2.35e-03)	Tok/s 90516 (96120)	Loss/tok 3.0391 (3.2536)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][100/1885]	Time 0.194 (0.147)	Data 2.20e-04 (2.14e-03)	Tok/s 104563 (96243)	Loss/tok 3.3550 (3.2475)	LR 2.800e-03
0: TRAIN [2][110/1885]	Time 0.102 (0.144)	Data 2.19e-04 (1.96e-03)	Tok/s 89877 (95911)	Loss/tok 2.9951 (3.2358)	LR 2.800e-03
0: TRAIN [2][120/1885]	Time 0.102 (0.144)	Data 2.12e-04 (1.82e-03)	Tok/s 88523 (95900)	Loss/tok 2.8732 (3.2348)	LR 2.800e-03
0: TRAIN [2][130/1885]	Time 0.195 (0.144)	Data 2.14e-04 (1.70e-03)	Tok/s 103812 (95888)	Loss/tok 3.3447 (3.2369)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][140/1885]	Time 0.102 (0.143)	Data 2.27e-04 (1.59e-03)	Tok/s 89542 (95868)	Loss/tok 3.0085 (3.2377)	LR 2.800e-03
0: TRAIN [2][150/1885]	Time 0.102 (0.143)	Data 2.12e-04 (1.50e-03)	Tok/s 91053 (96088)	Loss/tok 2.8384 (3.2355)	LR 2.800e-03
0: TRAIN [2][160/1885]	Time 0.194 (0.144)	Data 2.18e-04 (1.42e-03)	Tok/s 104628 (96028)	Loss/tok 3.4596 (3.2410)	LR 2.800e-03
0: TRAIN [2][170/1885]	Time 0.061 (0.142)	Data 2.15e-04 (1.35e-03)	Tok/s 76080 (95702)	Loss/tok 2.5986 (3.2354)	LR 2.800e-03
0: TRAIN [2][180/1885]	Time 0.101 (0.141)	Data 2.17e-04 (1.29e-03)	Tok/s 90649 (95617)	Loss/tok 2.9771 (3.2320)	LR 2.800e-03
0: TRAIN [2][190/1885]	Time 0.102 (0.140)	Data 2.20e-04 (1.23e-03)	Tok/s 88780 (95614)	Loss/tok 2.9669 (3.2278)	LR 2.800e-03
0: TRAIN [2][200/1885]	Time 0.245 (0.140)	Data 2.20e-04 (1.18e-03)	Tok/s 108153 (95645)	Loss/tok 3.4822 (3.2276)	LR 2.800e-03
0: TRAIN [2][210/1885]	Time 0.146 (0.139)	Data 2.19e-04 (1.14e-03)	Tok/s 100881 (95578)	Loss/tok 3.2947 (3.2241)	LR 2.800e-03
0: TRAIN [2][220/1885]	Time 0.147 (0.140)	Data 1.90e-04 (1.09e-03)	Tok/s 98577 (95684)	Loss/tok 3.2605 (3.2284)	LR 2.800e-03
0: TRAIN [2][230/1885]	Time 0.102 (0.139)	Data 2.17e-04 (1.06e-03)	Tok/s 88853 (95553)	Loss/tok 2.9954 (3.2260)	LR 2.800e-03
0: TRAIN [2][240/1885]	Time 0.147 (0.139)	Data 2.16e-04 (1.02e-03)	Tok/s 100378 (95592)	Loss/tok 3.1918 (3.2255)	LR 2.800e-03
0: TRAIN [2][250/1885]	Time 0.147 (0.140)	Data 2.16e-04 (9.89e-04)	Tok/s 101297 (95748)	Loss/tok 3.1426 (3.2350)	LR 2.800e-03
0: TRAIN [2][260/1885]	Time 0.147 (0.140)	Data 2.19e-04 (9.59e-04)	Tok/s 98573 (95725)	Loss/tok 3.3325 (3.2334)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][270/1885]	Time 0.102 (0.140)	Data 2.19e-04 (9.32e-04)	Tok/s 88355 (95668)	Loss/tok 2.9989 (3.2312)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][280/1885]	Time 0.104 (0.140)	Data 2.18e-04 (9.06e-04)	Tok/s 87587 (95621)	Loss/tok 2.9305 (3.2309)	LR 2.800e-03
0: TRAIN [2][290/1885]	Time 0.245 (0.140)	Data 1.77e-04 (8.82e-04)	Tok/s 106530 (95507)	Loss/tok 3.5521 (3.2362)	LR 2.800e-03
0: TRAIN [2][300/1885]	Time 0.105 (0.140)	Data 2.18e-04 (8.60e-04)	Tok/s 86487 (95545)	Loss/tok 3.0429 (3.2406)	LR 2.800e-03
0: TRAIN [2][310/1885]	Time 0.103 (0.141)	Data 2.20e-04 (8.39e-04)	Tok/s 87882 (95548)	Loss/tok 3.0806 (3.2410)	LR 2.800e-03
0: TRAIN [2][320/1885]	Time 0.193 (0.142)	Data 2.18e-04 (8.19e-04)	Tok/s 104523 (95721)	Loss/tok 3.3603 (3.2495)	LR 2.800e-03
0: TRAIN [2][330/1885]	Time 0.150 (0.143)	Data 2.19e-04 (8.01e-04)	Tok/s 98690 (95728)	Loss/tok 3.1564 (3.2499)	LR 2.800e-03
0: TRAIN [2][340/1885]	Time 0.104 (0.143)	Data 2.19e-04 (7.83e-04)	Tok/s 87247 (95835)	Loss/tok 2.9522 (3.2511)	LR 2.800e-03
0: TRAIN [2][350/1885]	Time 0.149 (0.144)	Data 2.25e-04 (7.67e-04)	Tok/s 100209 (95917)	Loss/tok 3.3028 (3.2541)	LR 2.800e-03
0: TRAIN [2][360/1885]	Time 0.247 (0.144)	Data 1.08e-04 (7.52e-04)	Tok/s 106710 (95986)	Loss/tok 3.4638 (3.2548)	LR 2.800e-03
0: TRAIN [2][370/1885]	Time 0.103 (0.144)	Data 2.20e-04 (7.37e-04)	Tok/s 89096 (96006)	Loss/tok 3.0290 (3.2557)	LR 2.800e-03
0: TRAIN [2][380/1885]	Time 0.146 (0.144)	Data 1.37e-04 (7.23e-04)	Tok/s 100260 (95986)	Loss/tok 3.1961 (3.2575)	LR 2.800e-03
0: TRAIN [2][390/1885]	Time 0.149 (0.144)	Data 2.23e-04 (7.10e-04)	Tok/s 98362 (95943)	Loss/tok 3.2309 (3.2569)	LR 2.800e-03
0: TRAIN [2][400/1885]	Time 0.149 (0.145)	Data 2.19e-04 (6.97e-04)	Tok/s 97175 (96049)	Loss/tok 3.2865 (3.2580)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][410/1885]	Time 0.148 (0.144)	Data 1.80e-04 (6.85e-04)	Tok/s 99150 (95911)	Loss/tok 3.2969 (3.2557)	LR 2.800e-03
0: TRAIN [2][420/1885]	Time 0.148 (0.144)	Data 1.72e-04 (6.74e-04)	Tok/s 98665 (95770)	Loss/tok 3.2447 (3.2544)	LR 2.800e-03
0: TRAIN [2][430/1885]	Time 0.104 (0.143)	Data 2.35e-04 (6.63e-04)	Tok/s 86196 (95674)	Loss/tok 3.0541 (3.2524)	LR 2.800e-03
0: TRAIN [2][440/1885]	Time 0.105 (0.143)	Data 2.31e-04 (6.53e-04)	Tok/s 86389 (95634)	Loss/tok 3.0165 (3.2533)	LR 2.800e-03
0: TRAIN [2][450/1885]	Time 0.148 (0.143)	Data 2.17e-04 (6.44e-04)	Tok/s 98842 (95606)	Loss/tok 3.2888 (3.2515)	LR 2.800e-03
0: TRAIN [2][460/1885]	Time 0.195 (0.143)	Data 2.34e-04 (6.34e-04)	Tok/s 103075 (95546)	Loss/tok 3.4304 (3.2498)	LR 2.800e-03
0: TRAIN [2][470/1885]	Time 0.149 (0.143)	Data 2.33e-04 (6.26e-04)	Tok/s 97588 (95548)	Loss/tok 3.2506 (3.2506)	LR 2.800e-03
0: TRAIN [2][480/1885]	Time 0.148 (0.142)	Data 2.20e-04 (6.17e-04)	Tok/s 97923 (95449)	Loss/tok 3.3350 (3.2505)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][490/1885]	Time 0.147 (0.143)	Data 2.22e-04 (6.09e-04)	Tok/s 98444 (95572)	Loss/tok 3.2319 (3.2532)	LR 2.800e-03
0: TRAIN [2][500/1885]	Time 0.245 (0.143)	Data 1.79e-04 (6.01e-04)	Tok/s 106079 (95609)	Loss/tok 3.5632 (3.2537)	LR 2.800e-03
0: TRAIN [2][510/1885]	Time 0.146 (0.143)	Data 1.82e-04 (5.93e-04)	Tok/s 98806 (95610)	Loss/tok 3.3505 (3.2551)	LR 2.800e-03
0: TRAIN [2][520/1885]	Time 0.149 (0.143)	Data 2.20e-04 (5.86e-04)	Tok/s 98794 (95566)	Loss/tok 3.1717 (3.2538)	LR 2.800e-03
0: TRAIN [2][530/1885]	Time 0.103 (0.143)	Data 2.16e-04 (5.79e-04)	Tok/s 88285 (95533)	Loss/tok 2.9370 (3.2526)	LR 2.800e-03
0: TRAIN [2][540/1885]	Time 0.196 (0.143)	Data 2.18e-04 (5.72e-04)	Tok/s 105104 (95528)	Loss/tok 3.3460 (3.2522)	LR 2.800e-03
0: TRAIN [2][550/1885]	Time 0.103 (0.143)	Data 2.19e-04 (5.65e-04)	Tok/s 88871 (95522)	Loss/tok 3.0114 (3.2512)	LR 2.800e-03
0: TRAIN [2][560/1885]	Time 0.196 (0.143)	Data 2.20e-04 (5.59e-04)	Tok/s 104565 (95557)	Loss/tok 3.4445 (3.2536)	LR 2.800e-03
0: TRAIN [2][570/1885]	Time 0.148 (0.143)	Data 2.20e-04 (5.53e-04)	Tok/s 98025 (95551)	Loss/tok 3.2390 (3.2532)	LR 2.800e-03
0: TRAIN [2][580/1885]	Time 0.149 (0.143)	Data 2.21e-04 (5.47e-04)	Tok/s 99361 (95534)	Loss/tok 3.2126 (3.2524)	LR 2.800e-03
0: TRAIN [2][590/1885]	Time 0.196 (0.143)	Data 2.20e-04 (5.41e-04)	Tok/s 104208 (95601)	Loss/tok 3.3352 (3.2543)	LR 2.800e-03
0: TRAIN [2][600/1885]	Time 0.149 (0.144)	Data 2.19e-04 (5.36e-04)	Tok/s 98413 (95605)	Loss/tok 3.2328 (3.2551)	LR 2.800e-03
0: TRAIN [2][610/1885]	Time 0.104 (0.143)	Data 2.20e-04 (5.30e-04)	Tok/s 88558 (95499)	Loss/tok 3.0262 (3.2530)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][620/1885]	Time 0.105 (0.143)	Data 2.17e-04 (5.25e-04)	Tok/s 85391 (95476)	Loss/tok 3.0755 (3.2528)	LR 2.800e-03
0: TRAIN [2][630/1885]	Time 0.247 (0.143)	Data 2.20e-04 (5.20e-04)	Tok/s 107383 (95427)	Loss/tok 3.4668 (3.2522)	LR 2.800e-03
0: TRAIN [2][640/1885]	Time 0.106 (0.143)	Data 1.13e-04 (5.15e-04)	Tok/s 84147 (95435)	Loss/tok 2.9842 (3.2515)	LR 2.800e-03
0: TRAIN [2][650/1885]	Time 0.148 (0.143)	Data 1.42e-04 (5.10e-04)	Tok/s 99000 (95468)	Loss/tok 3.2274 (3.2535)	LR 2.800e-03
0: TRAIN [2][660/1885]	Time 0.150 (0.143)	Data 1.38e-04 (5.05e-04)	Tok/s 98253 (95412)	Loss/tok 3.3401 (3.2521)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][670/1885]	Time 0.194 (0.143)	Data 2.20e-04 (5.00e-04)	Tok/s 106678 (95448)	Loss/tok 3.3845 (3.2524)	LR 2.800e-03
0: TRAIN [2][680/1885]	Time 0.195 (0.143)	Data 1.34e-04 (4.96e-04)	Tok/s 104217 (95447)	Loss/tok 3.3869 (3.2531)	LR 2.800e-03
0: TRAIN [2][690/1885]	Time 0.195 (0.143)	Data 1.44e-04 (4.91e-04)	Tok/s 103642 (95491)	Loss/tok 3.3947 (3.2537)	LR 2.800e-03
0: TRAIN [2][700/1885]	Time 0.196 (0.143)	Data 2.01e-04 (4.87e-04)	Tok/s 104620 (95479)	Loss/tok 3.3872 (3.2535)	LR 2.800e-03
0: TRAIN [2][710/1885]	Time 0.149 (0.143)	Data 2.19e-04 (4.83e-04)	Tok/s 99296 (95478)	Loss/tok 3.2809 (3.2540)	LR 2.800e-03
0: TRAIN [2][720/1885]	Time 0.195 (0.143)	Data 1.53e-04 (4.79e-04)	Tok/s 103751 (95464)	Loss/tok 3.5193 (3.2537)	LR 2.800e-03
0: TRAIN [2][730/1885]	Time 0.104 (0.143)	Data 2.20e-04 (4.76e-04)	Tok/s 85577 (95474)	Loss/tok 2.9299 (3.2538)	LR 2.800e-03
0: TRAIN [2][740/1885]	Time 0.104 (0.143)	Data 2.28e-04 (4.72e-04)	Tok/s 87093 (95468)	Loss/tok 3.0556 (3.2540)	LR 2.800e-03
0: TRAIN [2][750/1885]	Time 0.150 (0.143)	Data 2.22e-04 (4.69e-04)	Tok/s 96409 (95405)	Loss/tok 3.2579 (3.2522)	LR 2.800e-03
0: TRAIN [2][760/1885]	Time 0.103 (0.143)	Data 2.36e-04 (4.65e-04)	Tok/s 88206 (95352)	Loss/tok 2.9255 (3.2512)	LR 2.800e-03
0: TRAIN [2][770/1885]	Time 0.103 (0.143)	Data 2.20e-04 (4.62e-04)	Tok/s 89738 (95331)	Loss/tok 2.9673 (3.2507)	LR 2.800e-03
0: TRAIN [2][780/1885]	Time 0.106 (0.143)	Data 1.79e-04 (4.59e-04)	Tok/s 85793 (95318)	Loss/tok 2.9991 (3.2517)	LR 2.800e-03
0: TRAIN [2][790/1885]	Time 0.149 (0.143)	Data 2.17e-04 (4.56e-04)	Tok/s 99522 (95332)	Loss/tok 3.2650 (3.2526)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][800/1885]	Time 0.195 (0.143)	Data 1.79e-04 (4.53e-04)	Tok/s 102528 (95391)	Loss/tok 3.4095 (3.2536)	LR 2.800e-03
0: TRAIN [2][810/1885]	Time 0.148 (0.143)	Data 2.19e-04 (4.50e-04)	Tok/s 97822 (95420)	Loss/tok 3.3207 (3.2535)	LR 2.800e-03
0: TRAIN [2][820/1885]	Time 0.196 (0.143)	Data 2.21e-04 (4.47e-04)	Tok/s 104244 (95386)	Loss/tok 3.4599 (3.2531)	LR 2.800e-03
0: TRAIN [2][830/1885]	Time 0.103 (0.143)	Data 1.42e-04 (4.44e-04)	Tok/s 88342 (95380)	Loss/tok 3.0941 (3.2538)	LR 2.800e-03
0: TRAIN [2][840/1885]	Time 0.247 (0.144)	Data 2.21e-04 (4.41e-04)	Tok/s 105070 (95444)	Loss/tok 3.4656 (3.2556)	LR 2.800e-03
0: TRAIN [2][850/1885]	Time 0.198 (0.144)	Data 2.17e-04 (4.38e-04)	Tok/s 102870 (95414)	Loss/tok 3.3909 (3.2556)	LR 2.800e-03
0: TRAIN [2][860/1885]	Time 0.106 (0.144)	Data 2.21e-04 (4.36e-04)	Tok/s 84680 (95394)	Loss/tok 3.0280 (3.2555)	LR 2.800e-03
0: TRAIN [2][870/1885]	Time 0.148 (0.144)	Data 2.04e-04 (4.33e-04)	Tok/s 98771 (95399)	Loss/tok 3.0616 (3.2550)	LR 2.800e-03
0: TRAIN [2][880/1885]	Time 0.149 (0.144)	Data 2.01e-04 (4.31e-04)	Tok/s 98891 (95365)	Loss/tok 3.2788 (3.2544)	LR 2.800e-03
0: TRAIN [2][890/1885]	Time 0.104 (0.143)	Data 2.21e-04 (4.28e-04)	Tok/s 85325 (95294)	Loss/tok 3.0383 (3.2533)	LR 2.800e-03
0: TRAIN [2][900/1885]	Time 0.149 (0.143)	Data 2.23e-04 (4.25e-04)	Tok/s 97605 (95244)	Loss/tok 3.1524 (3.2524)	LR 2.800e-03
0: TRAIN [2][910/1885]	Time 0.103 (0.143)	Data 2.21e-04 (4.23e-04)	Tok/s 87490 (95200)	Loss/tok 3.0290 (3.2519)	LR 2.800e-03
0: TRAIN [2][920/1885]	Time 0.066 (0.143)	Data 2.21e-04 (4.21e-04)	Tok/s 70684 (95164)	Loss/tok 2.5713 (3.2525)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][930/1885]	Time 0.105 (0.143)	Data 2.18e-04 (4.19e-04)	Tok/s 86288 (95155)	Loss/tok 3.0883 (3.2538)	LR 2.800e-03
0: TRAIN [2][940/1885]	Time 0.149 (0.143)	Data 2.19e-04 (4.16e-04)	Tok/s 99585 (95138)	Loss/tok 3.3365 (3.2543)	LR 2.800e-03
0: TRAIN [2][950/1885]	Time 0.149 (0.143)	Data 2.17e-04 (4.14e-04)	Tok/s 97905 (95092)	Loss/tok 3.2132 (3.2532)	LR 2.800e-03
0: TRAIN [2][960/1885]	Time 0.104 (0.143)	Data 2.18e-04 (4.12e-04)	Tok/s 87342 (95074)	Loss/tok 2.9533 (3.2523)	LR 2.800e-03
0: TRAIN [2][970/1885]	Time 0.104 (0.143)	Data 2.19e-04 (4.10e-04)	Tok/s 87234 (95101)	Loss/tok 3.0891 (3.2532)	LR 2.800e-03
0: TRAIN [2][980/1885]	Time 0.148 (0.143)	Data 2.17e-04 (4.08e-04)	Tok/s 98868 (95054)	Loss/tok 3.2095 (3.2525)	LR 2.800e-03
0: TRAIN [2][990/1885]	Time 0.150 (0.143)	Data 2.19e-04 (4.06e-04)	Tok/s 98931 (95050)	Loss/tok 3.1853 (3.2517)	LR 2.800e-03
0: TRAIN [2][1000/1885]	Time 0.197 (0.143)	Data 2.21e-04 (4.04e-04)	Tok/s 103916 (95063)	Loss/tok 3.3693 (3.2528)	LR 2.800e-03
0: TRAIN [2][1010/1885]	Time 0.197 (0.143)	Data 1.79e-04 (4.02e-04)	Tok/s 103047 (95067)	Loss/tok 3.3939 (3.2529)	LR 2.800e-03
0: TRAIN [2][1020/1885]	Time 0.248 (0.143)	Data 2.23e-04 (4.00e-04)	Tok/s 105882 (95126)	Loss/tok 3.4893 (3.2541)	LR 2.800e-03
0: TRAIN [2][1030/1885]	Time 0.063 (0.143)	Data 2.18e-04 (3.99e-04)	Tok/s 71874 (95080)	Loss/tok 2.4442 (3.2533)	LR 2.800e-03
0: TRAIN [2][1040/1885]	Time 0.105 (0.143)	Data 2.18e-04 (3.97e-04)	Tok/s 86631 (95057)	Loss/tok 3.1031 (3.2528)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1050/1885]	Time 0.195 (0.143)	Data 2.19e-04 (3.95e-04)	Tok/s 103939 (95066)	Loss/tok 3.3987 (3.2528)	LR 2.800e-03
0: TRAIN [2][1060/1885]	Time 0.148 (0.143)	Data 1.80e-04 (3.93e-04)	Tok/s 99066 (95016)	Loss/tok 3.2174 (3.2519)	LR 2.800e-03
0: TRAIN [2][1070/1885]	Time 0.149 (0.143)	Data 2.17e-04 (3.92e-04)	Tok/s 98712 (95015)	Loss/tok 3.2138 (3.2515)	LR 2.800e-03
0: TRAIN [2][1080/1885]	Time 0.103 (0.143)	Data 1.23e-04 (3.90e-04)	Tok/s 88868 (94986)	Loss/tok 3.1257 (3.2512)	LR 2.800e-03
0: TRAIN [2][1090/1885]	Time 0.104 (0.143)	Data 2.16e-04 (3.88e-04)	Tok/s 88006 (94991)	Loss/tok 3.0184 (3.2511)	LR 2.800e-03
0: TRAIN [2][1100/1885]	Time 0.148 (0.143)	Data 1.53e-04 (3.87e-04)	Tok/s 98648 (94967)	Loss/tok 3.2976 (3.2509)	LR 2.800e-03
0: TRAIN [2][1110/1885]	Time 0.103 (0.143)	Data 2.31e-04 (3.85e-04)	Tok/s 88677 (94972)	Loss/tok 2.9119 (3.2508)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1120/1885]	Time 0.147 (0.143)	Data 2.17e-04 (3.83e-04)	Tok/s 99540 (94970)	Loss/tok 3.2665 (3.2514)	LR 2.800e-03
0: TRAIN [2][1130/1885]	Time 0.246 (0.143)	Data 2.22e-04 (3.82e-04)	Tok/s 105869 (94947)	Loss/tok 3.5887 (3.2518)	LR 2.800e-03
0: TRAIN [2][1140/1885]	Time 0.104 (0.143)	Data 2.30e-04 (3.80e-04)	Tok/s 86371 (94968)	Loss/tok 3.0815 (3.2529)	LR 2.800e-03
0: TRAIN [2][1150/1885]	Time 0.103 (0.143)	Data 1.82e-04 (3.79e-04)	Tok/s 88609 (94960)	Loss/tok 2.9951 (3.2527)	LR 2.800e-03
0: TRAIN [2][1160/1885]	Time 0.248 (0.143)	Data 2.18e-04 (3.78e-04)	Tok/s 105575 (95006)	Loss/tok 3.4923 (3.2539)	LR 2.800e-03
0: TRAIN [2][1170/1885]	Time 0.104 (0.143)	Data 2.19e-04 (3.76e-04)	Tok/s 84439 (94989)	Loss/tok 3.0087 (3.2538)	LR 2.800e-03
0: TRAIN [2][1180/1885]	Time 0.106 (0.143)	Data 2.20e-04 (3.75e-04)	Tok/s 85954 (95002)	Loss/tok 2.9725 (3.2545)	LR 2.800e-03
0: TRAIN [2][1190/1885]	Time 0.150 (0.143)	Data 2.19e-04 (3.73e-04)	Tok/s 97931 (95020)	Loss/tok 3.2166 (3.2548)	LR 2.800e-03
0: TRAIN [2][1200/1885]	Time 0.105 (0.143)	Data 2.18e-04 (3.72e-04)	Tok/s 86258 (95051)	Loss/tok 2.9757 (3.2555)	LR 2.800e-03
0: TRAIN [2][1210/1885]	Time 0.247 (0.144)	Data 1.37e-04 (3.71e-04)	Tok/s 105580 (95060)	Loss/tok 3.6008 (3.2565)	LR 2.800e-03
0: TRAIN [2][1220/1885]	Time 0.104 (0.143)	Data 2.21e-04 (3.70e-04)	Tok/s 87317 (95054)	Loss/tok 3.0143 (3.2561)	LR 2.800e-03
0: TRAIN [2][1230/1885]	Time 0.196 (0.144)	Data 2.20e-04 (3.68e-04)	Tok/s 104360 (95077)	Loss/tok 3.2639 (3.2563)	LR 2.800e-03
0: TRAIN [2][1240/1885]	Time 0.104 (0.144)	Data 2.19e-04 (3.67e-04)	Tok/s 88621 (95077)	Loss/tok 3.0677 (3.2561)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1250/1885]	Time 0.197 (0.144)	Data 2.18e-04 (3.66e-04)	Tok/s 104500 (95090)	Loss/tok 3.3794 (3.2562)	LR 2.800e-03
0: TRAIN [2][1260/1885]	Time 0.105 (0.144)	Data 2.20e-04 (3.65e-04)	Tok/s 86255 (95101)	Loss/tok 2.9569 (3.2564)	LR 2.800e-03
0: TRAIN [2][1270/1885]	Time 0.065 (0.144)	Data 2.16e-04 (3.63e-04)	Tok/s 70088 (95082)	Loss/tok 2.4235 (3.2562)	LR 2.800e-03
0: TRAIN [2][1280/1885]	Time 0.149 (0.144)	Data 2.17e-04 (3.62e-04)	Tok/s 99169 (95042)	Loss/tok 3.2581 (3.2554)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1290/1885]	Time 0.246 (0.144)	Data 1.78e-04 (3.61e-04)	Tok/s 107082 (95028)	Loss/tok 3.6223 (3.2560)	LR 2.800e-03
0: TRAIN [2][1300/1885]	Time 0.148 (0.144)	Data 2.18e-04 (3.60e-04)	Tok/s 99751 (95050)	Loss/tok 3.3016 (3.2561)	LR 2.800e-03
0: TRAIN [2][1310/1885]	Time 0.103 (0.144)	Data 2.20e-04 (3.59e-04)	Tok/s 89290 (95062)	Loss/tok 2.9635 (3.2564)	LR 2.800e-03
0: TRAIN [2][1320/1885]	Time 0.104 (0.144)	Data 2.20e-04 (3.57e-04)	Tok/s 87048 (95048)	Loss/tok 2.9494 (3.2560)	LR 2.800e-03
0: TRAIN [2][1330/1885]	Time 0.104 (0.143)	Data 2.19e-04 (3.56e-04)	Tok/s 86613 (94995)	Loss/tok 3.0929 (3.2558)	LR 2.800e-03
0: TRAIN [2][1340/1885]	Time 0.105 (0.144)	Data 2.29e-04 (3.55e-04)	Tok/s 85674 (94996)	Loss/tok 2.9764 (3.2559)	LR 2.800e-03
0: TRAIN [2][1350/1885]	Time 0.247 (0.144)	Data 2.18e-04 (3.54e-04)	Tok/s 105938 (94994)	Loss/tok 3.6196 (3.2562)	LR 2.800e-03
0: TRAIN [2][1360/1885]	Time 0.104 (0.143)	Data 2.19e-04 (3.53e-04)	Tok/s 86706 (94972)	Loss/tok 3.1509 (3.2556)	LR 2.800e-03
0: TRAIN [2][1370/1885]	Time 0.148 (0.144)	Data 2.16e-04 (3.52e-04)	Tok/s 99150 (94989)	Loss/tok 3.2372 (3.2562)	LR 2.800e-03
0: TRAIN [2][1380/1885]	Time 0.105 (0.143)	Data 2.23e-04 (3.51e-04)	Tok/s 84892 (94940)	Loss/tok 2.9549 (3.2556)	LR 2.800e-03
0: TRAIN [2][1390/1885]	Time 0.246 (0.144)	Data 1.80e-04 (3.50e-04)	Tok/s 105461 (94950)	Loss/tok 3.6221 (3.2564)	LR 2.800e-03
0: TRAIN [2][1400/1885]	Time 0.103 (0.143)	Data 2.23e-04 (3.49e-04)	Tok/s 88761 (94945)	Loss/tok 2.9980 (3.2559)	LR 2.800e-03
0: TRAIN [2][1410/1885]	Time 0.194 (0.144)	Data 1.76e-04 (3.48e-04)	Tok/s 105813 (94997)	Loss/tok 3.3752 (3.2570)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1420/1885]	Time 0.059 (0.144)	Data 1.77e-04 (3.47e-04)	Tok/s 77115 (94986)	Loss/tok 2.4885 (3.2565)	LR 2.800e-03
0: TRAIN [2][1430/1885]	Time 0.104 (0.144)	Data 2.19e-04 (3.46e-04)	Tok/s 88143 (94980)	Loss/tok 3.0390 (3.2561)	LR 2.800e-03
0: TRAIN [2][1440/1885]	Time 0.105 (0.144)	Data 2.20e-04 (3.45e-04)	Tok/s 86839 (94997)	Loss/tok 2.9538 (3.2558)	LR 2.800e-03
0: TRAIN [2][1450/1885]	Time 0.103 (0.143)	Data 2.17e-04 (3.44e-04)	Tok/s 86442 (94949)	Loss/tok 2.9984 (3.2550)	LR 2.800e-03
0: TRAIN [2][1460/1885]	Time 0.149 (0.143)	Data 2.17e-04 (3.43e-04)	Tok/s 98539 (94952)	Loss/tok 3.1627 (3.2545)	LR 2.800e-03
0: TRAIN [2][1470/1885]	Time 0.062 (0.143)	Data 2.19e-04 (3.42e-04)	Tok/s 73048 (94941)	Loss/tok 2.5540 (3.2542)	LR 2.800e-03
0: TRAIN [2][1480/1885]	Time 0.104 (0.143)	Data 2.17e-04 (3.41e-04)	Tok/s 86520 (94943)	Loss/tok 2.9059 (3.2538)	LR 2.800e-03
0: TRAIN [2][1490/1885]	Time 0.246 (0.143)	Data 2.17e-04 (3.40e-04)	Tok/s 106645 (94935)	Loss/tok 3.5763 (3.2536)	LR 2.800e-03
0: TRAIN [2][1500/1885]	Time 0.196 (0.143)	Data 2.29e-04 (3.40e-04)	Tok/s 103009 (94950)	Loss/tok 3.4582 (3.2543)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1510/1885]	Time 0.104 (0.143)	Data 2.17e-04 (3.39e-04)	Tok/s 86139 (94944)	Loss/tok 3.0604 (3.2542)	LR 2.800e-03
0: TRAIN [2][1520/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.38e-04)	Tok/s 104326 (94937)	Loss/tok 3.5318 (3.2544)	LR 2.800e-03
0: TRAIN [2][1530/1885]	Time 0.061 (0.143)	Data 2.17e-04 (3.37e-04)	Tok/s 72778 (94944)	Loss/tok 2.4152 (3.2547)	LR 2.800e-03
0: TRAIN [2][1540/1885]	Time 0.246 (0.143)	Data 2.20e-04 (3.36e-04)	Tok/s 106007 (94938)	Loss/tok 3.5987 (3.2546)	LR 2.800e-03
0: TRAIN [2][1550/1885]	Time 0.197 (0.143)	Data 2.22e-04 (3.35e-04)	Tok/s 103761 (94949)	Loss/tok 3.3355 (3.2549)	LR 2.800e-03
0: TRAIN [2][1560/1885]	Time 0.246 (0.144)	Data 2.16e-04 (3.35e-04)	Tok/s 107034 (94983)	Loss/tok 3.5776 (3.2555)	LR 2.800e-03
0: TRAIN [2][1570/1885]	Time 0.149 (0.143)	Data 2.30e-04 (3.34e-04)	Tok/s 96830 (94961)	Loss/tok 3.3067 (3.2551)	LR 2.800e-03
0: TRAIN [2][1580/1885]	Time 0.104 (0.143)	Data 2.19e-04 (3.33e-04)	Tok/s 86161 (94950)	Loss/tok 2.9892 (3.2546)	LR 2.800e-03
0: TRAIN [2][1590/1885]	Time 0.247 (0.143)	Data 2.17e-04 (3.33e-04)	Tok/s 106310 (94962)	Loss/tok 3.4843 (3.2546)	LR 2.800e-03
0: TRAIN [2][1600/1885]	Time 0.249 (0.144)	Data 2.18e-04 (3.32e-04)	Tok/s 104999 (94983)	Loss/tok 3.4601 (3.2549)	LR 2.800e-03
0: TRAIN [2][1610/1885]	Time 0.149 (0.144)	Data 2.19e-04 (3.31e-04)	Tok/s 98221 (94997)	Loss/tok 3.2371 (3.2552)	LR 2.800e-03
0: TRAIN [2][1620/1885]	Time 0.104 (0.144)	Data 2.18e-04 (3.30e-04)	Tok/s 87278 (95003)	Loss/tok 3.0028 (3.2553)	LR 2.800e-03
0: TRAIN [2][1630/1885]	Time 0.195 (0.144)	Data 2.19e-04 (3.30e-04)	Tok/s 103090 (95032)	Loss/tok 3.4225 (3.2564)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1640/1885]	Time 0.102 (0.144)	Data 1.80e-04 (3.29e-04)	Tok/s 89331 (95010)	Loss/tok 3.0119 (3.2561)	LR 2.800e-03
0: TRAIN [2][1650/1885]	Time 0.148 (0.144)	Data 2.15e-04 (3.28e-04)	Tok/s 98539 (95023)	Loss/tok 3.2163 (3.2560)	LR 2.800e-03
0: TRAIN [2][1660/1885]	Time 0.104 (0.144)	Data 2.18e-04 (3.28e-04)	Tok/s 87012 (95024)	Loss/tok 3.0184 (3.2554)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1670/1885]	Time 0.103 (0.144)	Data 2.17e-04 (3.27e-04)	Tok/s 88004 (95034)	Loss/tok 3.0752 (3.2560)	LR 2.800e-03
0: TRAIN [2][1680/1885]	Time 0.195 (0.144)	Data 2.29e-04 (3.26e-04)	Tok/s 105181 (95051)	Loss/tok 3.4766 (3.2567)	LR 2.800e-03
0: TRAIN [2][1690/1885]	Time 0.105 (0.144)	Data 2.14e-04 (3.26e-04)	Tok/s 86383 (95036)	Loss/tok 3.0355 (3.2564)	LR 2.800e-03
0: TRAIN [2][1700/1885]	Time 0.148 (0.144)	Data 2.18e-04 (3.25e-04)	Tok/s 99701 (95030)	Loss/tok 3.2879 (3.2562)	LR 2.800e-03
0: TRAIN [2][1710/1885]	Time 0.103 (0.144)	Data 2.18e-04 (3.24e-04)	Tok/s 87296 (95043)	Loss/tok 3.0412 (3.2567)	LR 2.800e-03
0: TRAIN [2][1720/1885]	Time 0.150 (0.144)	Data 2.18e-04 (3.24e-04)	Tok/s 98252 (95068)	Loss/tok 3.0793 (3.2573)	LR 2.800e-03
0: TRAIN [2][1730/1885]	Time 0.247 (0.144)	Data 2.31e-04 (3.23e-04)	Tok/s 105472 (95100)	Loss/tok 3.6182 (3.2584)	LR 2.800e-03
0: TRAIN [2][1740/1885]	Time 0.147 (0.145)	Data 2.18e-04 (3.22e-04)	Tok/s 99709 (95138)	Loss/tok 3.1932 (3.2588)	LR 2.800e-03
0: TRAIN [2][1750/1885]	Time 0.060 (0.144)	Data 2.17e-04 (3.22e-04)	Tok/s 76538 (95130)	Loss/tok 2.5073 (3.2586)	LR 2.800e-03
0: TRAIN [2][1760/1885]	Time 0.101 (0.145)	Data 2.19e-04 (3.21e-04)	Tok/s 91176 (95140)	Loss/tok 2.9921 (3.2588)	LR 2.800e-03
0: TRAIN [2][1770/1885]	Time 0.100 (0.145)	Data 2.18e-04 (3.21e-04)	Tok/s 90325 (95171)	Loss/tok 2.9551 (3.2595)	LR 2.800e-03
0: TRAIN [2][1780/1885]	Time 0.146 (0.145)	Data 2.17e-04 (3.20e-04)	Tok/s 100487 (95187)	Loss/tok 3.1827 (3.2593)	LR 2.800e-03
0: TRAIN [2][1790/1885]	Time 0.102 (0.145)	Data 2.21e-04 (3.19e-04)	Tok/s 87915 (95185)	Loss/tok 2.9645 (3.2589)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1800/1885]	Time 0.194 (0.145)	Data 2.20e-04 (3.19e-04)	Tok/s 105618 (95193)	Loss/tok 3.3524 (3.2589)	LR 2.800e-03
0: TRAIN [2][1810/1885]	Time 0.146 (0.145)	Data 2.22e-04 (3.18e-04)	Tok/s 99210 (95195)	Loss/tok 3.3126 (3.2589)	LR 2.800e-03
0: TRAIN [2][1820/1885]	Time 0.102 (0.145)	Data 2.31e-04 (3.18e-04)	Tok/s 89703 (95178)	Loss/tok 2.9802 (3.2585)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1830/1885]	Time 0.147 (0.145)	Data 2.16e-04 (3.17e-04)	Tok/s 99719 (95186)	Loss/tok 3.3305 (3.2589)	LR 2.800e-03
0: TRAIN [2][1840/1885]	Time 0.148 (0.145)	Data 2.04e-04 (3.17e-04)	Tok/s 100374 (95194)	Loss/tok 3.2087 (3.2587)	LR 2.800e-03
0: TRAIN [2][1850/1885]	Time 0.102 (0.145)	Data 1.90e-04 (3.16e-04)	Tok/s 89015 (95200)	Loss/tok 2.9879 (3.2586)	LR 2.800e-03
0: TRAIN [2][1860/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.16e-04)	Tok/s 89865 (95189)	Loss/tok 3.0237 (3.2580)	LR 2.800e-03
0: TRAIN [2][1870/1885]	Time 0.060 (0.145)	Data 2.19e-04 (3.15e-04)	Tok/s 76482 (95214)	Loss/tok 2.4848 (3.2584)	LR 2.800e-03
0: TRAIN [2][1880/1885]	Time 0.101 (0.144)	Data 2.19e-04 (3.15e-04)	Tok/s 89888 (95207)	Loss/tok 2.9450 (3.2579)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593840970780, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593840970781, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.588 (0.588)	Decoder iters 105.0 (105.0)	Tok/s 28323 (28323)
0: Running moses detokenizer
0: BLEU(score=22.47628441865688, counts=[36609, 17861, 9936, 5760], totals=[66475, 63472, 60469, 57472], precisions=[55.07183151560737, 28.13996722964457, 16.431559972878667, 10.022271714922049], bp=1.0, sys_len=66475, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593840972400, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2248, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593840972400, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2592	Test BLEU: 22.48
0: Performance: Epoch: 2	Training: 761521 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593840972400, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593840972400, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593840972400, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1965592172
0: TRAIN [3][0/1885]	Time 0.416 (0.416)	Data 2.11e-01 (2.11e-01)	Tok/s 35431 (35431)	Loss/tok 3.0898 (3.0898)	LR 2.800e-03
0: TRAIN [3][10/1885]	Time 0.102 (0.160)	Data 2.29e-04 (1.93e-02)	Tok/s 87624 (88630)	Loss/tok 2.9649 (3.1149)	LR 2.800e-03
0: TRAIN [3][20/1885]	Time 0.148 (0.157)	Data 2.17e-04 (1.02e-02)	Tok/s 98527 (93273)	Loss/tok 3.1317 (3.1492)	LR 2.800e-03
0: TRAIN [3][30/1885]	Time 0.147 (0.151)	Data 2.18e-04 (7.00e-03)	Tok/s 99627 (94418)	Loss/tok 3.1818 (3.1478)	LR 2.800e-03
0: TRAIN [3][40/1885]	Time 0.102 (0.145)	Data 2.19e-04 (5.34e-03)	Tok/s 90253 (94734)	Loss/tok 2.8448 (3.1298)	LR 2.800e-03
0: TRAIN [3][50/1885]	Time 0.102 (0.144)	Data 2.04e-04 (4.34e-03)	Tok/s 90228 (94919)	Loss/tok 2.9632 (3.1389)	LR 2.800e-03
0: TRAIN [3][60/1885]	Time 0.194 (0.146)	Data 2.19e-04 (3.66e-03)	Tok/s 105049 (95468)	Loss/tok 3.3990 (3.1472)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][70/1885]	Time 0.147 (0.145)	Data 2.22e-04 (3.18e-03)	Tok/s 99171 (95654)	Loss/tok 3.2098 (3.1471)	LR 2.800e-03
0: TRAIN [3][80/1885]	Time 0.193 (0.144)	Data 2.19e-04 (2.81e-03)	Tok/s 104926 (95392)	Loss/tok 3.2668 (3.1482)	LR 2.800e-03
0: TRAIN [3][90/1885]	Time 0.147 (0.146)	Data 2.15e-04 (2.52e-03)	Tok/s 99968 (95919)	Loss/tok 3.2265 (3.1598)	LR 2.800e-03
0: TRAIN [3][100/1885]	Time 0.195 (0.147)	Data 2.19e-04 (2.30e-03)	Tok/s 104906 (96144)	Loss/tok 3.2180 (3.1607)	LR 2.800e-03
0: TRAIN [3][110/1885]	Time 0.102 (0.146)	Data 2.04e-04 (2.11e-03)	Tok/s 86694 (96001)	Loss/tok 2.8404 (3.1610)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][120/1885]	Time 0.059 (0.148)	Data 2.30e-04 (1.95e-03)	Tok/s 79333 (96223)	Loss/tok 2.5242 (3.1704)	LR 2.800e-03
0: TRAIN [3][130/1885]	Time 0.146 (0.147)	Data 2.18e-04 (1.82e-03)	Tok/s 102107 (96375)	Loss/tok 3.0792 (3.1684)	LR 2.800e-03
0: TRAIN [3][140/1885]	Time 0.147 (0.148)	Data 2.31e-04 (1.71e-03)	Tok/s 99773 (96412)	Loss/tok 3.1464 (3.1727)	LR 2.800e-03
0: TRAIN [3][150/1885]	Time 0.102 (0.147)	Data 2.15e-04 (1.61e-03)	Tok/s 89381 (96424)	Loss/tok 2.9439 (3.1698)	LR 2.800e-03
0: TRAIN [3][160/1885]	Time 0.246 (0.146)	Data 2.21e-04 (1.52e-03)	Tok/s 106301 (96245)	Loss/tok 3.4537 (3.1713)	LR 2.800e-03
0: TRAIN [3][170/1885]	Time 0.148 (0.146)	Data 2.19e-04 (1.45e-03)	Tok/s 99245 (96260)	Loss/tok 3.0876 (3.1692)	LR 2.800e-03
0: TRAIN [3][180/1885]	Time 0.147 (0.148)	Data 2.21e-04 (1.38e-03)	Tok/s 100349 (96524)	Loss/tok 3.1921 (3.1820)	LR 2.800e-03
0: TRAIN [3][190/1885]	Time 0.146 (0.147)	Data 2.25e-04 (1.32e-03)	Tok/s 100103 (96372)	Loss/tok 3.1041 (3.1804)	LR 2.800e-03
0: TRAIN [3][200/1885]	Time 0.195 (0.148)	Data 2.22e-04 (1.26e-03)	Tok/s 104481 (96533)	Loss/tok 3.2893 (3.1870)	LR 2.800e-03
0: TRAIN [3][210/1885]	Time 0.101 (0.147)	Data 2.16e-04 (1.21e-03)	Tok/s 91312 (96256)	Loss/tok 2.8877 (3.1801)	LR 2.800e-03
0: TRAIN [3][220/1885]	Time 0.246 (0.148)	Data 2.16e-04 (1.17e-03)	Tok/s 104978 (96456)	Loss/tok 3.4902 (3.1860)	LR 2.800e-03
0: TRAIN [3][230/1885]	Time 0.059 (0.147)	Data 2.18e-04 (1.13e-03)	Tok/s 79296 (96381)	Loss/tok 2.4868 (3.1858)	LR 2.800e-03
0: TRAIN [3][240/1885]	Time 0.147 (0.147)	Data 2.20e-04 (1.09e-03)	Tok/s 98937 (96366)	Loss/tok 3.0740 (3.1843)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][250/1885]	Time 0.102 (0.146)	Data 2.18e-04 (1.05e-03)	Tok/s 89942 (96352)	Loss/tok 3.1422 (3.1832)	LR 2.800e-03
0: TRAIN [3][260/1885]	Time 0.101 (0.146)	Data 2.18e-04 (1.02e-03)	Tok/s 91278 (96439)	Loss/tok 2.9091 (3.1816)	LR 2.800e-03
0: TRAIN [3][270/1885]	Time 0.148 (0.146)	Data 2.33e-04 (9.93e-04)	Tok/s 99099 (96398)	Loss/tok 3.1277 (3.1824)	LR 2.800e-03
0: TRAIN [3][280/1885]	Time 0.102 (0.145)	Data 2.19e-04 (9.65e-04)	Tok/s 87723 (96314)	Loss/tok 2.8826 (3.1796)	LR 2.800e-03
0: TRAIN [3][290/1885]	Time 0.148 (0.145)	Data 2.18e-04 (9.40e-04)	Tok/s 100576 (96312)	Loss/tok 3.1079 (3.1798)	LR 2.800e-03
0: TRAIN [3][300/1885]	Time 0.101 (0.145)	Data 2.19e-04 (9.16e-04)	Tok/s 91197 (96306)	Loss/tok 2.8345 (3.1788)	LR 2.800e-03
0: TRAIN [3][310/1885]	Time 0.147 (0.145)	Data 2.03e-04 (8.93e-04)	Tok/s 100108 (96270)	Loss/tok 3.1547 (3.1770)	LR 2.800e-03
0: TRAIN [3][320/1885]	Time 0.147 (0.144)	Data 2.20e-04 (8.72e-04)	Tok/s 98873 (96279)	Loss/tok 3.1947 (3.1755)	LR 2.800e-03
0: TRAIN [3][330/1885]	Time 0.247 (0.145)	Data 2.16e-04 (8.52e-04)	Tok/s 105884 (96359)	Loss/tok 3.4899 (3.1783)	LR 2.800e-03
0: TRAIN [3][340/1885]	Time 0.101 (0.145)	Data 2.18e-04 (8.33e-04)	Tok/s 87841 (96444)	Loss/tok 2.9826 (3.1791)	LR 2.800e-03
0: TRAIN [3][350/1885]	Time 0.147 (0.145)	Data 2.21e-04 (8.16e-04)	Tok/s 100723 (96495)	Loss/tok 3.2038 (3.1788)	LR 2.800e-03
0: TRAIN [3][360/1885]	Time 0.146 (0.145)	Data 2.22e-04 (7.99e-04)	Tok/s 100778 (96450)	Loss/tok 3.2152 (3.1783)	LR 2.800e-03
0: TRAIN [3][370/1885]	Time 0.060 (0.145)	Data 2.18e-04 (7.83e-04)	Tok/s 77258 (96387)	Loss/tok 2.4709 (3.1767)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][380/1885]	Time 0.101 (0.144)	Data 2.15e-04 (7.68e-04)	Tok/s 88210 (96294)	Loss/tok 2.8801 (3.1762)	LR 2.800e-03
0: TRAIN [3][390/1885]	Time 0.246 (0.144)	Data 2.18e-04 (7.54e-04)	Tok/s 106485 (96230)	Loss/tok 3.5107 (3.1785)	LR 2.800e-03
0: TRAIN [3][400/1885]	Time 0.148 (0.144)	Data 2.16e-04 (7.41e-04)	Tok/s 99293 (96258)	Loss/tok 3.0185 (3.1784)	LR 2.800e-03
0: TRAIN [3][410/1885]	Time 0.146 (0.145)	Data 2.20e-04 (7.28e-04)	Tok/s 99881 (96284)	Loss/tok 3.1458 (3.1787)	LR 2.800e-03
0: TRAIN [3][420/1885]	Time 0.101 (0.144)	Data 2.31e-04 (7.16e-04)	Tok/s 89972 (96291)	Loss/tok 2.9480 (3.1773)	LR 2.800e-03
0: TRAIN [3][430/1885]	Time 0.146 (0.144)	Data 2.35e-04 (7.04e-04)	Tok/s 99534 (96265)	Loss/tok 3.1485 (3.1784)	LR 1.400e-03
0: TRAIN [3][440/1885]	Time 0.246 (0.144)	Data 2.18e-04 (6.93e-04)	Tok/s 107147 (96213)	Loss/tok 3.3315 (3.1775)	LR 1.400e-03
0: TRAIN [3][450/1885]	Time 0.146 (0.144)	Data 2.29e-04 (6.83e-04)	Tok/s 101406 (96163)	Loss/tok 3.1471 (3.1754)	LR 1.400e-03
0: TRAIN [3][460/1885]	Time 0.193 (0.144)	Data 2.44e-04 (6.73e-04)	Tok/s 106700 (96279)	Loss/tok 3.2447 (3.1767)	LR 1.400e-03
0: TRAIN [3][470/1885]	Time 0.195 (0.145)	Data 2.18e-04 (6.63e-04)	Tok/s 104299 (96373)	Loss/tok 3.3214 (3.1785)	LR 1.400e-03
0: TRAIN [3][480/1885]	Time 0.195 (0.145)	Data 2.20e-04 (6.54e-04)	Tok/s 103685 (96416)	Loss/tok 3.2878 (3.1787)	LR 1.400e-03
0: TRAIN [3][490/1885]	Time 0.146 (0.145)	Data 2.18e-04 (6.45e-04)	Tok/s 98862 (96496)	Loss/tok 3.2481 (3.1789)	LR 1.400e-03
0: TRAIN [3][500/1885]	Time 0.195 (0.146)	Data 2.21e-04 (6.37e-04)	Tok/s 104100 (96524)	Loss/tok 3.3952 (3.1793)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][510/1885]	Time 0.060 (0.146)	Data 2.17e-04 (6.28e-04)	Tok/s 75521 (96547)	Loss/tok 2.5397 (3.1804)	LR 1.400e-03
0: TRAIN [3][520/1885]	Time 0.102 (0.146)	Data 2.18e-04 (6.21e-04)	Tok/s 90247 (96592)	Loss/tok 2.8422 (3.1791)	LR 1.400e-03
0: TRAIN [3][530/1885]	Time 0.147 (0.145)	Data 2.20e-04 (6.13e-04)	Tok/s 101056 (96534)	Loss/tok 2.9852 (3.1777)	LR 1.400e-03
0: TRAIN [3][540/1885]	Time 0.245 (0.146)	Data 2.19e-04 (6.06e-04)	Tok/s 108847 (96541)	Loss/tok 3.3746 (3.1794)	LR 1.400e-03
0: TRAIN [3][550/1885]	Time 0.147 (0.146)	Data 2.21e-04 (5.99e-04)	Tok/s 102057 (96615)	Loss/tok 3.0738 (3.1796)	LR 1.400e-03
0: TRAIN [3][560/1885]	Time 0.102 (0.146)	Data 2.19e-04 (5.92e-04)	Tok/s 88092 (96598)	Loss/tok 2.9513 (3.1782)	LR 1.400e-03
0: TRAIN [3][570/1885]	Time 0.102 (0.146)	Data 1.89e-04 (5.85e-04)	Tok/s 89429 (96622)	Loss/tok 2.8675 (3.1782)	LR 1.400e-03
0: TRAIN [3][580/1885]	Time 0.102 (0.146)	Data 2.20e-04 (5.79e-04)	Tok/s 89772 (96597)	Loss/tok 2.9049 (3.1778)	LR 1.400e-03
0: TRAIN [3][590/1885]	Time 0.102 (0.146)	Data 1.93e-04 (5.73e-04)	Tok/s 89602 (96599)	Loss/tok 2.8053 (3.1772)	LR 1.400e-03
0: TRAIN [3][600/1885]	Time 0.147 (0.146)	Data 2.26e-04 (5.67e-04)	Tok/s 99491 (96570)	Loss/tok 3.1399 (3.1754)	LR 1.400e-03
0: TRAIN [3][610/1885]	Time 0.147 (0.145)	Data 2.17e-04 (5.61e-04)	Tok/s 100648 (96577)	Loss/tok 3.0058 (3.1738)	LR 1.400e-03
0: TRAIN [3][620/1885]	Time 0.102 (0.145)	Data 2.17e-04 (5.56e-04)	Tok/s 91271 (96573)	Loss/tok 2.8846 (3.1724)	LR 1.400e-03
0: TRAIN [3][630/1885]	Time 0.194 (0.145)	Data 2.18e-04 (5.50e-04)	Tok/s 105307 (96601)	Loss/tok 3.2600 (3.1719)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][640/1885]	Time 0.102 (0.145)	Data 2.16e-04 (5.45e-04)	Tok/s 88703 (96614)	Loss/tok 2.8774 (3.1724)	LR 1.400e-03
0: TRAIN [3][650/1885]	Time 0.148 (0.145)	Data 2.19e-04 (5.40e-04)	Tok/s 99683 (96582)	Loss/tok 3.0456 (3.1713)	LR 1.400e-03
0: TRAIN [3][660/1885]	Time 0.102 (0.145)	Data 2.20e-04 (5.35e-04)	Tok/s 88969 (96556)	Loss/tok 2.8800 (3.1706)	LR 1.400e-03
0: TRAIN [3][670/1885]	Time 0.101 (0.145)	Data 2.20e-04 (5.31e-04)	Tok/s 89003 (96506)	Loss/tok 2.9741 (3.1705)	LR 1.400e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][680/1885]	Time 0.195 (0.145)	Data 2.19e-04 (5.26e-04)	Tok/s 103328 (96563)	Loss/tok 3.2862 (3.1720)	LR 1.400e-03
0: TRAIN [3][690/1885]	Time 0.101 (0.145)	Data 2.15e-04 (5.22e-04)	Tok/s 88594 (96554)	Loss/tok 2.9272 (3.1712)	LR 1.400e-03
0: TRAIN [3][700/1885]	Time 0.146 (0.145)	Data 2.19e-04 (5.17e-04)	Tok/s 101441 (96526)	Loss/tok 2.9407 (3.1699)	LR 1.400e-03
0: TRAIN [3][710/1885]	Time 0.147 (0.145)	Data 2.18e-04 (5.13e-04)	Tok/s 100457 (96507)	Loss/tok 3.1813 (3.1697)	LR 1.400e-03
0: TRAIN [3][720/1885]	Time 0.147 (0.145)	Data 2.19e-04 (5.09e-04)	Tok/s 99846 (96480)	Loss/tok 3.0796 (3.1691)	LR 1.400e-03
0: TRAIN [3][730/1885]	Time 0.246 (0.145)	Data 2.17e-04 (5.05e-04)	Tok/s 106439 (96528)	Loss/tok 3.5057 (3.1697)	LR 1.400e-03
0: TRAIN [3][740/1885]	Time 0.102 (0.145)	Data 2.23e-04 (5.01e-04)	Tok/s 89918 (96538)	Loss/tok 2.9455 (3.1704)	LR 1.400e-03
0: TRAIN [3][750/1885]	Time 0.194 (0.146)	Data 2.21e-04 (4.97e-04)	Tok/s 106035 (96606)	Loss/tok 3.2024 (3.1711)	LR 1.400e-03
0: TRAIN [3][760/1885]	Time 0.101 (0.145)	Data 2.21e-04 (4.94e-04)	Tok/s 89895 (96546)	Loss/tok 2.9174 (3.1701)	LR 1.400e-03
0: TRAIN [3][770/1885]	Time 0.246 (0.146)	Data 2.17e-04 (4.90e-04)	Tok/s 106219 (96592)	Loss/tok 3.4014 (3.1718)	LR 1.400e-03
0: TRAIN [3][780/1885]	Time 0.102 (0.146)	Data 2.18e-04 (4.87e-04)	Tok/s 91017 (96622)	Loss/tok 2.9095 (3.1719)	LR 1.400e-03
0: TRAIN [3][790/1885]	Time 0.101 (0.146)	Data 2.18e-04 (4.83e-04)	Tok/s 90453 (96630)	Loss/tok 2.9065 (3.1715)	LR 1.400e-03
0: TRAIN [3][800/1885]	Time 0.148 (0.146)	Data 2.20e-04 (4.80e-04)	Tok/s 99739 (96616)	Loss/tok 3.2040 (3.1702)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][810/1885]	Time 0.247 (0.146)	Data 2.18e-04 (4.77e-04)	Tok/s 104991 (96607)	Loss/tok 3.3802 (3.1691)	LR 1.400e-03
0: TRAIN [3][820/1885]	Time 0.245 (0.146)	Data 2.21e-04 (4.74e-04)	Tok/s 106170 (96677)	Loss/tok 3.4166 (3.1707)	LR 1.400e-03
0: TRAIN [3][830/1885]	Time 0.149 (0.146)	Data 2.20e-04 (4.70e-04)	Tok/s 97094 (96678)	Loss/tok 3.1299 (3.1699)	LR 1.400e-03
0: TRAIN [3][840/1885]	Time 0.102 (0.146)	Data 2.21e-04 (4.67e-04)	Tok/s 90400 (96644)	Loss/tok 2.8825 (3.1689)	LR 1.400e-03
0: TRAIN [3][850/1885]	Time 0.147 (0.146)	Data 2.32e-04 (4.64e-04)	Tok/s 100476 (96613)	Loss/tok 3.0675 (3.1679)	LR 1.400e-03
0: TRAIN [3][860/1885]	Time 0.102 (0.145)	Data 2.19e-04 (4.62e-04)	Tok/s 89234 (96558)	Loss/tok 2.8978 (3.1664)	LR 1.400e-03
0: TRAIN [3][870/1885]	Time 0.060 (0.145)	Data 2.19e-04 (4.59e-04)	Tok/s 74892 (96492)	Loss/tok 2.3692 (3.1655)	LR 1.400e-03
0: TRAIN [3][880/1885]	Time 0.103 (0.145)	Data 2.19e-04 (4.56e-04)	Tok/s 87399 (96526)	Loss/tok 2.9662 (3.1658)	LR 1.400e-03
0: TRAIN [3][890/1885]	Time 0.060 (0.145)	Data 2.27e-04 (4.53e-04)	Tok/s 76501 (96508)	Loss/tok 2.5031 (3.1654)	LR 1.400e-03
0: TRAIN [3][900/1885]	Time 0.148 (0.145)	Data 2.21e-04 (4.51e-04)	Tok/s 98683 (96535)	Loss/tok 3.0995 (3.1654)	LR 1.400e-03
0: TRAIN [3][910/1885]	Time 0.101 (0.145)	Data 2.21e-04 (4.48e-04)	Tok/s 89899 (96507)	Loss/tok 2.8553 (3.1645)	LR 1.400e-03
0: TRAIN [3][920/1885]	Time 0.194 (0.145)	Data 2.20e-04 (4.46e-04)	Tok/s 106519 (96532)	Loss/tok 3.3218 (3.1649)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][930/1885]	Time 0.146 (0.145)	Data 2.20e-04 (4.43e-04)	Tok/s 99486 (96581)	Loss/tok 3.1414 (3.1654)	LR 1.400e-03
0: TRAIN [3][940/1885]	Time 0.146 (0.145)	Data 2.18e-04 (4.41e-04)	Tok/s 100174 (96600)	Loss/tok 3.0947 (3.1646)	LR 1.400e-03
0: TRAIN [3][950/1885]	Time 0.101 (0.145)	Data 2.17e-04 (4.39e-04)	Tok/s 89749 (96575)	Loss/tok 2.8901 (3.1637)	LR 1.400e-03
0: TRAIN [3][960/1885]	Time 0.194 (0.145)	Data 2.22e-04 (4.36e-04)	Tok/s 105286 (96570)	Loss/tok 3.2277 (3.1633)	LR 1.400e-03
0: TRAIN [3][970/1885]	Time 0.195 (0.145)	Data 2.22e-04 (4.34e-04)	Tok/s 105416 (96608)	Loss/tok 3.2975 (3.1630)	LR 1.400e-03
0: TRAIN [3][980/1885]	Time 0.060 (0.145)	Data 2.18e-04 (4.32e-04)	Tok/s 77940 (96568)	Loss/tok 2.3924 (3.1621)	LR 1.400e-03
0: TRAIN [3][990/1885]	Time 0.059 (0.145)	Data 2.19e-04 (4.30e-04)	Tok/s 78153 (96526)	Loss/tok 2.4712 (3.1610)	LR 1.400e-03
0: TRAIN [3][1000/1885]	Time 0.102 (0.145)	Data 2.21e-04 (4.28e-04)	Tok/s 88254 (96543)	Loss/tok 2.9105 (3.1611)	LR 1.400e-03
0: TRAIN [3][1010/1885]	Time 0.246 (0.145)	Data 2.18e-04 (4.26e-04)	Tok/s 107489 (96553)	Loss/tok 3.2830 (3.1613)	LR 1.400e-03
0: TRAIN [3][1020/1885]	Time 0.102 (0.145)	Data 2.18e-04 (4.24e-04)	Tok/s 90707 (96518)	Loss/tok 2.8846 (3.1603)	LR 1.400e-03
0: TRAIN [3][1030/1885]	Time 0.102 (0.145)	Data 2.17e-04 (4.22e-04)	Tok/s 88389 (96507)	Loss/tok 2.9697 (3.1612)	LR 1.400e-03
0: TRAIN [3][1040/1885]	Time 0.147 (0.145)	Data 2.16e-04 (4.20e-04)	Tok/s 100978 (96504)	Loss/tok 3.1099 (3.1608)	LR 1.400e-03
0: TRAIN [3][1050/1885]	Time 0.194 (0.145)	Data 2.17e-04 (4.18e-04)	Tok/s 104915 (96490)	Loss/tok 3.2154 (3.1598)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1060/1885]	Time 0.101 (0.145)	Data 2.17e-04 (4.16e-04)	Tok/s 89634 (96485)	Loss/tok 2.8426 (3.1594)	LR 1.400e-03
0: TRAIN [3][1070/1885]	Time 0.148 (0.144)	Data 1.80e-04 (4.14e-04)	Tok/s 100330 (96490)	Loss/tok 3.1899 (3.1590)	LR 1.400e-03
0: TRAIN [3][1080/1885]	Time 0.102 (0.145)	Data 2.19e-04 (4.12e-04)	Tok/s 90566 (96524)	Loss/tok 2.9513 (3.1596)	LR 1.400e-03
0: TRAIN [3][1090/1885]	Time 0.145 (0.145)	Data 2.30e-04 (4.10e-04)	Tok/s 100494 (96541)	Loss/tok 3.1098 (3.1592)	LR 1.400e-03
0: TRAIN [3][1100/1885]	Time 0.060 (0.145)	Data 2.16e-04 (4.09e-04)	Tok/s 77357 (96516)	Loss/tok 2.5287 (3.1585)	LR 1.400e-03
0: TRAIN [3][1110/1885]	Time 0.147 (0.144)	Data 2.13e-04 (4.07e-04)	Tok/s 99641 (96519)	Loss/tok 3.1041 (3.1578)	LR 1.400e-03
0: TRAIN [3][1120/1885]	Time 0.146 (0.145)	Data 2.17e-04 (4.05e-04)	Tok/s 100678 (96522)	Loss/tok 3.0476 (3.1579)	LR 1.400e-03
0: TRAIN [3][1130/1885]	Time 0.102 (0.144)	Data 2.19e-04 (4.03e-04)	Tok/s 89884 (96500)	Loss/tok 3.0212 (3.1571)	LR 1.400e-03
0: TRAIN [3][1140/1885]	Time 0.196 (0.144)	Data 2.18e-04 (4.02e-04)	Tok/s 103600 (96494)	Loss/tok 3.3333 (3.1568)	LR 1.400e-03
0: TRAIN [3][1150/1885]	Time 0.060 (0.144)	Data 2.22e-04 (4.00e-04)	Tok/s 77246 (96468)	Loss/tok 2.4787 (3.1564)	LR 1.400e-03
0: TRAIN [3][1160/1885]	Time 0.101 (0.144)	Data 2.17e-04 (3.99e-04)	Tok/s 90634 (96483)	Loss/tok 2.7974 (3.1564)	LR 1.400e-03
0: TRAIN [3][1170/1885]	Time 0.102 (0.144)	Data 2.32e-04 (3.97e-04)	Tok/s 91352 (96478)	Loss/tok 2.8284 (3.1560)	LR 1.400e-03
0: TRAIN [3][1180/1885]	Time 0.146 (0.144)	Data 2.22e-04 (3.95e-04)	Tok/s 99618 (96440)	Loss/tok 3.0232 (3.1551)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1190/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.94e-04)	Tok/s 100933 (96446)	Loss/tok 3.0692 (3.1552)	LR 1.400e-03
0: TRAIN [3][1200/1885]	Time 0.244 (0.144)	Data 2.18e-04 (3.92e-04)	Tok/s 105541 (96403)	Loss/tok 3.4123 (3.1546)	LR 1.400e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1210/1885]	Time 0.194 (0.144)	Data 1.56e-04 (3.91e-04)	Tok/s 104234 (96401)	Loss/tok 3.2679 (3.1556)	LR 1.400e-03
0: TRAIN [3][1220/1885]	Time 0.148 (0.144)	Data 2.23e-04 (3.89e-04)	Tok/s 99624 (96415)	Loss/tok 3.0674 (3.1554)	LR 1.400e-03
0: TRAIN [3][1230/1885]	Time 0.102 (0.144)	Data 2.21e-04 (3.88e-04)	Tok/s 90127 (96398)	Loss/tok 2.7315 (3.1547)	LR 1.400e-03
0: TRAIN [3][1240/1885]	Time 0.060 (0.144)	Data 2.21e-04 (3.87e-04)	Tok/s 76016 (96368)	Loss/tok 2.4422 (3.1544)	LR 1.400e-03
0: TRAIN [3][1250/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.85e-04)	Tok/s 100875 (96378)	Loss/tok 3.1802 (3.1544)	LR 1.400e-03
0: TRAIN [3][1260/1885]	Time 0.245 (0.144)	Data 2.19e-04 (3.84e-04)	Tok/s 106844 (96418)	Loss/tok 3.4252 (3.1556)	LR 1.400e-03
0: TRAIN [3][1270/1885]	Time 0.101 (0.144)	Data 2.19e-04 (3.83e-04)	Tok/s 88895 (96409)	Loss/tok 2.8116 (3.1558)	LR 1.400e-03
0: TRAIN [3][1280/1885]	Time 0.195 (0.144)	Data 2.34e-04 (3.81e-04)	Tok/s 103692 (96418)	Loss/tok 3.2759 (3.1557)	LR 1.400e-03
0: TRAIN [3][1290/1885]	Time 0.147 (0.144)	Data 2.21e-04 (3.80e-04)	Tok/s 99384 (96413)	Loss/tok 3.0629 (3.1555)	LR 1.400e-03
0: TRAIN [3][1300/1885]	Time 0.148 (0.144)	Data 2.21e-04 (3.79e-04)	Tok/s 101248 (96402)	Loss/tok 3.2849 (3.1549)	LR 7.000e-04
0: TRAIN [3][1310/1885]	Time 0.148 (0.144)	Data 2.23e-04 (3.77e-04)	Tok/s 100319 (96413)	Loss/tok 3.1001 (3.1546)	LR 7.000e-04
0: TRAIN [3][1320/1885]	Time 0.148 (0.144)	Data 2.22e-04 (3.76e-04)	Tok/s 100182 (96400)	Loss/tok 3.0768 (3.1540)	LR 7.000e-04
0: TRAIN [3][1330/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.75e-04)	Tok/s 101446 (96395)	Loss/tok 3.0624 (3.1532)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1340/1885]	Time 0.148 (0.144)	Data 2.18e-04 (3.74e-04)	Tok/s 98634 (96405)	Loss/tok 3.0920 (3.1528)	LR 7.000e-04
0: TRAIN [3][1350/1885]	Time 0.102 (0.144)	Data 1.81e-04 (3.73e-04)	Tok/s 89593 (96408)	Loss/tok 2.9219 (3.1526)	LR 7.000e-04
0: TRAIN [3][1360/1885]	Time 0.245 (0.144)	Data 2.17e-04 (3.71e-04)	Tok/s 107078 (96448)	Loss/tok 3.3464 (3.1528)	LR 7.000e-04
0: TRAIN [3][1370/1885]	Time 0.101 (0.144)	Data 2.20e-04 (3.70e-04)	Tok/s 91540 (96429)	Loss/tok 2.8656 (3.1526)	LR 7.000e-04
0: TRAIN [3][1380/1885]	Time 0.102 (0.144)	Data 2.20e-04 (3.69e-04)	Tok/s 88842 (96415)	Loss/tok 2.8541 (3.1517)	LR 7.000e-04
0: TRAIN [3][1390/1885]	Time 0.147 (0.144)	Data 2.23e-04 (3.68e-04)	Tok/s 100021 (96413)	Loss/tok 3.1162 (3.1516)	LR 7.000e-04
0: TRAIN [3][1400/1885]	Time 0.146 (0.144)	Data 2.16e-04 (3.67e-04)	Tok/s 100878 (96427)	Loss/tok 2.9847 (3.1508)	LR 7.000e-04
0: TRAIN [3][1410/1885]	Time 0.244 (0.144)	Data 2.20e-04 (3.66e-04)	Tok/s 107203 (96409)	Loss/tok 3.4040 (3.1504)	LR 7.000e-04
0: TRAIN [3][1420/1885]	Time 0.146 (0.144)	Data 2.20e-04 (3.65e-04)	Tok/s 99746 (96400)	Loss/tok 3.0915 (3.1501)	LR 7.000e-04
0: TRAIN [3][1430/1885]	Time 0.147 (0.144)	Data 2.20e-04 (3.64e-04)	Tok/s 99778 (96394)	Loss/tok 3.1044 (3.1496)	LR 7.000e-04
0: TRAIN [3][1440/1885]	Time 0.246 (0.144)	Data 2.17e-04 (3.63e-04)	Tok/s 106499 (96373)	Loss/tok 3.3836 (3.1493)	LR 7.000e-04
0: TRAIN [3][1450/1885]	Time 0.101 (0.144)	Data 2.19e-04 (3.62e-04)	Tok/s 88929 (96374)	Loss/tok 2.8923 (3.1487)	LR 7.000e-04
0: TRAIN [3][1460/1885]	Time 0.101 (0.144)	Data 2.19e-04 (3.61e-04)	Tok/s 90181 (96395)	Loss/tok 2.8927 (3.1491)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1470/1885]	Time 0.194 (0.144)	Data 2.21e-04 (3.60e-04)	Tok/s 104569 (96396)	Loss/tok 3.2493 (3.1493)	LR 7.000e-04
0: TRAIN [3][1480/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.59e-04)	Tok/s 90106 (96407)	Loss/tok 2.8748 (3.1498)	LR 7.000e-04
0: TRAIN [3][1490/1885]	Time 0.193 (0.144)	Data 2.18e-04 (3.58e-04)	Tok/s 104842 (96416)	Loss/tok 3.3533 (3.1498)	LR 7.000e-04
0: TRAIN [3][1500/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.57e-04)	Tok/s 100078 (96434)	Loss/tok 3.1955 (3.1498)	LR 7.000e-04
0: TRAIN [3][1510/1885]	Time 0.147 (0.144)	Data 2.32e-04 (3.56e-04)	Tok/s 99950 (96424)	Loss/tok 3.0539 (3.1494)	LR 7.000e-04
0: TRAIN [3][1520/1885]	Time 0.245 (0.144)	Data 2.17e-04 (3.55e-04)	Tok/s 107002 (96429)	Loss/tok 3.2882 (3.1493)	LR 7.000e-04
0: TRAIN [3][1530/1885]	Time 0.101 (0.144)	Data 2.17e-04 (3.54e-04)	Tok/s 90076 (96406)	Loss/tok 2.9167 (3.1485)	LR 7.000e-04
0: TRAIN [3][1540/1885]	Time 0.101 (0.144)	Data 2.25e-04 (3.53e-04)	Tok/s 89271 (96404)	Loss/tok 2.8736 (3.1485)	LR 7.000e-04
0: TRAIN [3][1550/1885]	Time 0.103 (0.144)	Data 2.18e-04 (3.53e-04)	Tok/s 88661 (96401)	Loss/tok 2.8202 (3.1480)	LR 7.000e-04
0: TRAIN [3][1560/1885]	Time 0.148 (0.144)	Data 2.24e-04 (3.52e-04)	Tok/s 98418 (96403)	Loss/tok 3.1146 (3.1480)	LR 7.000e-04
0: TRAIN [3][1570/1885]	Time 0.245 (0.144)	Data 2.23e-04 (3.51e-04)	Tok/s 105944 (96411)	Loss/tok 3.5013 (3.1483)	LR 7.000e-04
0: TRAIN [3][1580/1885]	Time 0.148 (0.144)	Data 2.16e-04 (3.50e-04)	Tok/s 99046 (96419)	Loss/tok 3.0932 (3.1481)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1590/1885]	Time 0.147 (0.144)	Data 2.16e-04 (3.49e-04)	Tok/s 100078 (96420)	Loss/tok 3.0169 (3.1475)	LR 7.000e-04
0: TRAIN [3][1600/1885]	Time 0.147 (0.144)	Data 2.32e-04 (3.48e-04)	Tok/s 100484 (96412)	Loss/tok 3.0207 (3.1468)	LR 7.000e-04
0: TRAIN [3][1610/1885]	Time 0.194 (0.144)	Data 2.17e-04 (3.48e-04)	Tok/s 105423 (96421)	Loss/tok 3.0780 (3.1465)	LR 7.000e-04
0: TRAIN [3][1620/1885]	Time 0.147 (0.144)	Data 2.20e-04 (3.47e-04)	Tok/s 99353 (96431)	Loss/tok 3.1087 (3.1460)	LR 7.000e-04
0: TRAIN [3][1630/1885]	Time 0.147 (0.144)	Data 2.22e-04 (3.46e-04)	Tok/s 99462 (96438)	Loss/tok 3.1726 (3.1454)	LR 7.000e-04
0: TRAIN [3][1640/1885]	Time 0.101 (0.144)	Data 2.19e-04 (3.45e-04)	Tok/s 90411 (96405)	Loss/tok 2.9386 (3.1445)	LR 7.000e-04
0: TRAIN [3][1650/1885]	Time 0.103 (0.144)	Data 1.90e-04 (3.44e-04)	Tok/s 87201 (96411)	Loss/tok 2.8480 (3.1444)	LR 7.000e-04
0: TRAIN [3][1660/1885]	Time 0.147 (0.144)	Data 2.16e-04 (3.44e-04)	Tok/s 99825 (96394)	Loss/tok 2.9573 (3.1435)	LR 7.000e-04
0: TRAIN [3][1670/1885]	Time 0.148 (0.144)	Data 2.17e-04 (3.43e-04)	Tok/s 99092 (96396)	Loss/tok 3.1412 (3.1432)	LR 7.000e-04
0: TRAIN [3][1680/1885]	Time 0.101 (0.144)	Data 2.18e-04 (3.42e-04)	Tok/s 90314 (96374)	Loss/tok 2.8083 (3.1426)	LR 7.000e-04
0: TRAIN [3][1690/1885]	Time 0.245 (0.144)	Data 2.18e-04 (3.41e-04)	Tok/s 104706 (96385)	Loss/tok 3.3885 (3.1426)	LR 7.000e-04
0: TRAIN [3][1700/1885]	Time 0.102 (0.144)	Data 2.24e-04 (3.41e-04)	Tok/s 88869 (96392)	Loss/tok 2.7914 (3.1421)	LR 7.000e-04
0: TRAIN [3][1710/1885]	Time 0.061 (0.144)	Data 2.33e-04 (3.40e-04)	Tok/s 75414 (96384)	Loss/tok 2.3354 (3.1417)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1720/1885]	Time 0.146 (0.144)	Data 2.34e-04 (3.39e-04)	Tok/s 100744 (96376)	Loss/tok 3.1714 (3.1414)	LR 7.000e-04
0: TRAIN [3][1730/1885]	Time 0.146 (0.144)	Data 2.29e-04 (3.39e-04)	Tok/s 99582 (96393)	Loss/tok 3.0579 (3.1417)	LR 7.000e-04
0: TRAIN [3][1740/1885]	Time 0.102 (0.144)	Data 2.21e-04 (3.38e-04)	Tok/s 87326 (96368)	Loss/tok 2.8145 (3.1411)	LR 7.000e-04
0: TRAIN [3][1750/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.37e-04)	Tok/s 88567 (96331)	Loss/tok 2.7904 (3.1402)	LR 7.000e-04
0: TRAIN [3][1760/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.36e-04)	Tok/s 99876 (96344)	Loss/tok 3.0433 (3.1405)	LR 7.000e-04
0: TRAIN [3][1770/1885]	Time 0.194 (0.144)	Data 2.16e-04 (3.36e-04)	Tok/s 104492 (96372)	Loss/tok 3.2498 (3.1407)	LR 7.000e-04
0: TRAIN [3][1780/1885]	Time 0.060 (0.144)	Data 2.19e-04 (3.35e-04)	Tok/s 77906 (96371)	Loss/tok 2.4403 (3.1411)	LR 7.000e-04
0: TRAIN [3][1790/1885]	Time 0.148 (0.144)	Data 2.18e-04 (3.34e-04)	Tok/s 98455 (96344)	Loss/tok 3.0840 (3.1406)	LR 7.000e-04
0: TRAIN [3][1800/1885]	Time 0.147 (0.144)	Data 2.22e-04 (3.34e-04)	Tok/s 99609 (96304)	Loss/tok 3.0801 (3.1399)	LR 7.000e-04
0: TRAIN [3][1810/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.33e-04)	Tok/s 100403 (96278)	Loss/tok 2.9914 (3.1391)	LR 7.000e-04
0: TRAIN [3][1820/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.32e-04)	Tok/s 92539 (96275)	Loss/tok 2.8753 (3.1390)	LR 7.000e-04
0: TRAIN [3][1830/1885]	Time 0.102 (0.143)	Data 2.19e-04 (3.32e-04)	Tok/s 89190 (96261)	Loss/tok 2.9806 (3.1387)	LR 7.000e-04
0: TRAIN [3][1840/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.31e-04)	Tok/s 88174 (96273)	Loss/tok 2.8458 (3.1387)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1850/1885]	Time 0.060 (0.143)	Data 2.20e-04 (3.31e-04)	Tok/s 76259 (96239)	Loss/tok 2.4938 (3.1379)	LR 7.000e-04
0: TRAIN [3][1860/1885]	Time 0.146 (0.143)	Data 2.22e-04 (3.30e-04)	Tok/s 99313 (96249)	Loss/tok 3.1105 (3.1375)	LR 7.000e-04
0: TRAIN [3][1870/1885]	Time 0.196 (0.143)	Data 2.07e-04 (3.29e-04)	Tok/s 102820 (96229)	Loss/tok 3.2114 (3.1371)	LR 7.000e-04
0: TRAIN [3][1880/1885]	Time 0.146 (0.143)	Data 2.17e-04 (3.29e-04)	Tok/s 100923 (96239)	Loss/tok 3.0322 (3.1370)	LR 7.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593841242982, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593841242982, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.562 (0.562)	Decoder iters 102.0 (102.0)	Tok/s 28648 (28648)
0: Running moses detokenizer
0: BLEU(score=24.033444173534274, counts=[36659, 18349, 10433, 6182], totals=[64270, 61267, 58264, 55266], precisions=[57.039053990975574, 29.949238578680202, 17.90642592338322, 11.185900915571962], bp=0.9937028111905462, sys_len=64270, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593841244514, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2403, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593841244514, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1389	Test BLEU: 24.03
0: Performance: Epoch: 3	Training: 769726 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593841244514, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593841244514, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-07-04 05:40:50 AM
RESULT,RNN_TRANSLATOR,,1119,Fujitsu,2020-07-04 05:22:11 AM
