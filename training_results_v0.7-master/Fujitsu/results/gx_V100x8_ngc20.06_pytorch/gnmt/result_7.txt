Beginning trial 1 of 1
Gathering sys log on gx2570-03
:::MLLOG {"namespace": "", "time_ms": 1593837896296, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593837896332, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Fujitsu", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593837896332, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593837896332, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593837896332, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xGX2570M5", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
Clearing caches
:::MLLOG {"namespace": "", "time_ms": 1593837899627, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/opt/conda/lib/python3.6/site-packages/mlperf_logging/mllog/mllog.py", "lineno": 261}}
Launching on node gx2570-03
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e PGSYSTEM=PG -e 'MULTI_NODE= --master_port=4260' -e LR=2.8e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6053 -e DECAY_INTERVAL=859 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=65 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200704114252288086675 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200704114252288086675 ./run_and_time.sh
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.8e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6053
+ DECAY_INTERVAL=859
+ TARGET=24.0
STARTING TIMING RUN AT 2020-07-04 04:45:00 AM
+ MAX_SEQ_LEN=65
+ NUMEPOCHS=8
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ PGSYSTEM=PG
+ [[ -f config_PG.sh ]]
+ source config_PG.sh
++ export PGNNODES=1
++ PGNNODES=1
+++ sed 's/^config_//'
+++ sed 's/\.sh$//'
++++ readlink -f config_PG.sh
+++ basename /workspace/rnn_translator/config_PG.sh
++ export PGSYSTEM=PG
++ PGSYSTEM=PG
++ export WALLTIME=00:30:00
++ WALLTIME=00:30:00
++ export LR=2.8e-3
++ LR=2.8e-3
++ export TRAIN_BATCH_SIZE=256
++ TRAIN_BATCH_SIZE=256
++ export TEST_BATCH_SIZE=128
++ TEST_BATCH_SIZE=128
++ export WARMUP_STEPS=200
++ WARMUP_STEPS=200
++ export REMAIN_STEPS=6053
++ REMAIN_STEPS=6053
++ export DECAY_INTERVAL=859
++ DECAY_INTERVAL=859
++ export TARGET=24.0
++ TARGET=24.0
++ export MAX_SEQ_LEN=65
++ MAX_SEQ_LEN=65
++ export NUMEPOCHS=8
++ NUMEPOCHS=8
++ export MATH=fp16
++ MATH=fp16
++ export DIST_OPTS=
++ DIST_OPTS=
++ export 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ export PGNGPU=8
++ PGNGPU=8
++ export PGSOCKETCORES=24
++ PGSOCKETCORES=24
++ export PGHT=2
++ PGHT=2
++ export PGNSOCKET=2
++ PGNSOCKET=2
+ declare -a CMD
+ '[' -n '' ']'
+ CMD=('python' '-u' '-m' 'bind_launch' "--nsockets_per_node=${PGNSOCKET}" "--ncores_per_socket=${PGSOCKETCORES}" "--nproc_per_node=${PGNGPU}")
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ PG == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ python -u -m bind_launch --nsockets_per_node=2 --ncores_per_socket=24 --nproc_per_node=8 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 65 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.8e-3 --warmup-steps 200 --remain-steps 6053 --decay-interval 859 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593837902637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593837902637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593837902637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593837902658, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593837902659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593837902659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593837902662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593837902690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=859, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.0028, math='fp16', max_length_test=150, max_length_train=65, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6053, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1256079986
:::MLLOG {"namespace": "", "time_ms": 1593837911601, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1256079986, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 413696758
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.0028}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.0028
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593837914470, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593837914470, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0028, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593837914470, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593837914471, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593837914471, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593837917000, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593837917028, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593837917028, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 65, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3866375 min length: 0 max length: 65 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593837917309, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593837917310, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3860480, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593837917310, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6053, 'decay_interval': 859, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6053
0: Scheduler decay interval: 859
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593837917310, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593837917311, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593837917311, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 859, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593837917311, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593837917311, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593837917311, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6053, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593837917311, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593837917312, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593837917312, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 4191212735
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1885]	Time 0.374 (0.374)	Data 2.68e-01 (2.68e-01)	Tok/s 24496 (24496)	Loss/tok 10.6290 (10.6290)	LR 2.865e-05
0: TRAIN [0][10/1885]	Time 0.191 (0.193)	Data 2.20e-04 (2.46e-02)	Tok/s 107243 (95187)	Loss/tok 9.6086 (9.9878)	LR 3.607e-05
0: TRAIN [0][20/1885]	Time 0.100 (0.158)	Data 2.23e-04 (1.30e-02)	Tok/s 89076 (94796)	Loss/tok 9.0735 (9.7420)	LR 4.541e-05
0: TRAIN [0][30/1885]	Time 0.146 (0.149)	Data 2.19e-04 (8.87e-03)	Tok/s 101447 (95474)	Loss/tok 8.8392 (9.5274)	LR 5.717e-05
0: TRAIN [0][40/1885]	Time 0.144 (0.149)	Data 2.22e-04 (6.76e-03)	Tok/s 102075 (96277)	Loss/tok 8.5925 (9.3245)	LR 7.197e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][50/1885]	Time 0.100 (0.141)	Data 2.19e-04 (5.48e-03)	Tok/s 91774 (95510)	Loss/tok 8.1903 (9.1844)	LR 8.854e-05
0: TRAIN [0][60/1885]	Time 0.192 (0.143)	Data 2.21e-04 (4.61e-03)	Tok/s 106623 (96075)	Loss/tok 8.3144 (9.0287)	LR 1.115e-04
0: TRAIN [0][70/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.99e-03)	Tok/s 90812 (96277)	Loss/tok 7.8352 (8.8966)	LR 1.403e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][80/1885]	Time 0.146 (0.143)	Data 2.16e-04 (3.53e-03)	Tok/s 100878 (96437)	Loss/tok 7.9596 (8.7876)	LR 1.726e-04
0: TRAIN [0][90/1885]	Time 0.190 (0.142)	Data 2.19e-04 (3.17e-03)	Tok/s 107641 (96583)	Loss/tok 8.0707 (8.6923)	LR 2.173e-04
0: TRAIN [0][100/1885]	Time 0.100 (0.142)	Data 2.19e-04 (2.87e-03)	Tok/s 90716 (96723)	Loss/tok 7.5521 (8.6137)	LR 2.736e-04
0: TRAIN [0][110/1885]	Time 0.101 (0.141)	Data 2.17e-04 (2.63e-03)	Tok/s 89237 (96712)	Loss/tok 7.6618 (8.5493)	LR 3.445e-04
0: TRAIN [0][120/1885]	Time 0.193 (0.143)	Data 2.19e-04 (2.43e-03)	Tok/s 106178 (97169)	Loss/tok 7.8995 (8.4809)	LR 4.337e-04
0: TRAIN [0][130/1885]	Time 0.191 (0.142)	Data 2.21e-04 (2.26e-03)	Tok/s 107690 (97175)	Loss/tok 8.0048 (8.4300)	LR 5.460e-04
0: TRAIN [0][140/1885]	Time 0.243 (0.144)	Data 2.16e-04 (2.12e-03)	Tok/s 106664 (97505)	Loss/tok 7.9136 (8.3745)	LR 6.873e-04
0: TRAIN [0][150/1885]	Time 0.100 (0.143)	Data 2.17e-04 (1.99e-03)	Tok/s 91775 (97455)	Loss/tok 7.1980 (8.3250)	LR 8.653e-04
0: TRAIN [0][160/1885]	Time 0.147 (0.143)	Data 2.16e-04 (1.88e-03)	Tok/s 99123 (97499)	Loss/tok 7.4142 (8.2750)	LR 1.089e-03
0: TRAIN [0][170/1885]	Time 0.145 (0.143)	Data 2.15e-04 (1.79e-03)	Tok/s 101582 (97471)	Loss/tok 7.3248 (8.2236)	LR 1.371e-03
0: TRAIN [0][180/1885]	Time 0.102 (0.142)	Data 2.17e-04 (1.70e-03)	Tok/s 90668 (97359)	Loss/tok 6.8925 (8.1695)	LR 1.726e-03
0: TRAIN [0][190/1885]	Time 0.101 (0.141)	Data 2.16e-04 (1.62e-03)	Tok/s 90296 (97254)	Loss/tok 6.8708 (8.1156)	LR 2.173e-03
0: TRAIN [0][200/1885]	Time 0.193 (0.141)	Data 2.17e-04 (1.55e-03)	Tok/s 104779 (97187)	Loss/tok 7.0495 (8.0586)	LR 2.736e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][210/1885]	Time 0.101 (0.140)	Data 2.30e-04 (1.49e-03)	Tok/s 92073 (96936)	Loss/tok 6.4521 (8.0046)	LR 2.800e-03
0: TRAIN [0][220/1885]	Time 0.102 (0.141)	Data 2.19e-04 (1.43e-03)	Tok/s 88487 (96970)	Loss/tok 6.3654 (7.9444)	LR 2.800e-03
0: TRAIN [0][230/1885]	Time 0.145 (0.140)	Data 2.17e-04 (1.38e-03)	Tok/s 101204 (96873)	Loss/tok 6.6118 (7.8880)	LR 2.800e-03
0: TRAIN [0][240/1885]	Time 0.145 (0.139)	Data 2.18e-04 (1.33e-03)	Tok/s 100062 (96617)	Loss/tok 6.3480 (7.8361)	LR 2.800e-03
0: TRAIN [0][250/1885]	Time 0.146 (0.139)	Data 2.18e-04 (1.28e-03)	Tok/s 100700 (96688)	Loss/tok 6.3097 (7.7705)	LR 2.800e-03
0: TRAIN [0][260/1885]	Time 0.146 (0.139)	Data 2.17e-04 (1.24e-03)	Tok/s 99685 (96721)	Loss/tok 6.1078 (7.7081)	LR 2.800e-03
0: TRAIN [0][270/1885]	Time 0.145 (0.140)	Data 2.17e-04 (1.21e-03)	Tok/s 101800 (96849)	Loss/tok 5.9716 (7.6365)	LR 2.800e-03
0: TRAIN [0][280/1885]	Time 0.244 (0.140)	Data 2.18e-04 (1.17e-03)	Tok/s 106209 (96875)	Loss/tok 6.1351 (7.5730)	LR 2.800e-03
0: TRAIN [0][290/1885]	Time 0.101 (0.140)	Data 2.20e-04 (1.14e-03)	Tok/s 90301 (96889)	Loss/tok 5.3701 (7.5114)	LR 2.800e-03
0: TRAIN [0][300/1885]	Time 0.101 (0.140)	Data 2.18e-04 (1.11e-03)	Tok/s 87446 (96905)	Loss/tok 5.3648 (7.4493)	LR 2.800e-03
0: TRAIN [0][310/1885]	Time 0.192 (0.141)	Data 2.24e-04 (1.08e-03)	Tok/s 105457 (96945)	Loss/tok 5.8193 (7.3843)	LR 2.800e-03
0: TRAIN [0][320/1885]	Time 0.059 (0.141)	Data 2.20e-04 (1.05e-03)	Tok/s 77736 (96897)	Loss/tok 3.9633 (7.3251)	LR 2.800e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][330/1885]	Time 0.243 (0.141)	Data 2.17e-04 (1.03e-03)	Tok/s 106345 (96908)	Loss/tok 5.7911 (7.2655)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][340/1885]	Time 0.101 (0.141)	Data 2.19e-04 (1.00e-03)	Tok/s 90423 (96953)	Loss/tok 4.7516 (7.2010)	LR 2.800e-03
0: TRAIN [0][350/1885]	Time 0.145 (0.141)	Data 8.54e-05 (9.77e-04)	Tok/s 100623 (96937)	Loss/tok 5.0855 (7.1457)	LR 2.800e-03
0: TRAIN [0][360/1885]	Time 0.102 (0.141)	Data 8.56e-05 (9.52e-04)	Tok/s 89778 (97010)	Loss/tok 4.7759 (7.0846)	LR 2.800e-03
0: TRAIN [0][370/1885]	Time 0.101 (0.140)	Data 8.51e-05 (9.29e-04)	Tok/s 88686 (96881)	Loss/tok 4.6098 (7.0367)	LR 2.800e-03
0: TRAIN [0][380/1885]	Time 0.191 (0.141)	Data 8.63e-05 (9.07e-04)	Tok/s 105931 (97039)	Loss/tok 5.0794 (6.9691)	LR 2.800e-03
0: TRAIN [0][390/1885]	Time 0.194 (0.141)	Data 8.51e-05 (8.86e-04)	Tok/s 106386 (97040)	Loss/tok 4.9909 (6.9150)	LR 2.800e-03
0: TRAIN [0][400/1885]	Time 0.146 (0.141)	Data 8.49e-05 (8.66e-04)	Tok/s 100104 (97067)	Loss/tok 4.7289 (6.8608)	LR 2.800e-03
0: TRAIN [0][410/1885]	Time 0.191 (0.142)	Data 8.61e-05 (8.47e-04)	Tok/s 105939 (97145)	Loss/tok 4.8900 (6.8033)	LR 2.800e-03
0: TRAIN [0][420/1885]	Time 0.101 (0.142)	Data 8.51e-05 (8.29e-04)	Tok/s 89180 (97162)	Loss/tok 4.3242 (6.7514)	LR 2.800e-03
0: TRAIN [0][430/1885]	Time 0.146 (0.142)	Data 8.80e-05 (8.12e-04)	Tok/s 99893 (97187)	Loss/tok 4.5758 (6.6965)	LR 2.800e-03
0: TRAIN [0][440/1885]	Time 0.146 (0.143)	Data 8.75e-05 (7.95e-04)	Tok/s 100416 (97245)	Loss/tok 4.4316 (6.6388)	LR 2.800e-03
0: TRAIN [0][450/1885]	Time 0.246 (0.143)	Data 8.61e-05 (7.79e-04)	Tok/s 107382 (97277)	Loss/tok 4.8046 (6.5882)	LR 2.800e-03
0: TRAIN [0][460/1885]	Time 0.146 (0.143)	Data 8.54e-05 (7.64e-04)	Tok/s 100589 (97291)	Loss/tok 4.4404 (6.5404)	LR 2.800e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][470/1885]	Time 0.059 (0.143)	Data 8.51e-05 (7.50e-04)	Tok/s 77960 (97310)	Loss/tok 3.2717 (6.4922)	LR 2.800e-03
0: TRAIN [0][480/1885]	Time 0.194 (0.142)	Data 8.46e-05 (7.36e-04)	Tok/s 105796 (97221)	Loss/tok 4.4999 (6.4540)	LR 2.800e-03
0: TRAIN [0][490/1885]	Time 0.147 (0.142)	Data 8.63e-05 (7.23e-04)	Tok/s 99697 (97197)	Loss/tok 4.1525 (6.4121)	LR 2.800e-03
0: TRAIN [0][500/1885]	Time 0.101 (0.142)	Data 8.46e-05 (7.10e-04)	Tok/s 90454 (97176)	Loss/tok 3.9385 (6.3714)	LR 2.800e-03
0: TRAIN [0][510/1885]	Time 0.146 (0.142)	Data 8.73e-05 (6.98e-04)	Tok/s 100680 (97199)	Loss/tok 4.2600 (6.3275)	LR 2.800e-03
0: TRAIN [0][520/1885]	Time 0.246 (0.143)	Data 8.63e-05 (6.86e-04)	Tok/s 107953 (97211)	Loss/tok 4.4464 (6.2849)	LR 2.800e-03
0: TRAIN [0][530/1885]	Time 0.194 (0.143)	Data 8.46e-05 (6.75e-04)	Tok/s 104762 (97265)	Loss/tok 4.2933 (6.2424)	LR 2.800e-03
0: TRAIN [0][540/1885]	Time 0.102 (0.143)	Data 8.51e-05 (6.64e-04)	Tok/s 88137 (97227)	Loss/tok 3.8648 (6.2061)	LR 2.800e-03
0: TRAIN [0][550/1885]	Time 0.145 (0.143)	Data 8.46e-05 (6.53e-04)	Tok/s 101967 (97205)	Loss/tok 4.2273 (6.1697)	LR 2.800e-03
0: TRAIN [0][560/1885]	Time 0.146 (0.143)	Data 8.44e-05 (6.43e-04)	Tok/s 100128 (97222)	Loss/tok 4.0761 (6.1324)	LR 2.800e-03
0: TRAIN [0][570/1885]	Time 0.101 (0.143)	Data 8.51e-05 (6.34e-04)	Tok/s 90027 (97211)	Loss/tok 3.7512 (6.0988)	LR 2.800e-03
0: TRAIN [0][580/1885]	Time 0.245 (0.143)	Data 8.63e-05 (6.24e-04)	Tok/s 107095 (97229)	Loss/tok 4.4089 (6.0627)	LR 2.800e-03
0: TRAIN [0][590/1885]	Time 0.146 (0.143)	Data 8.44e-05 (6.15e-04)	Tok/s 101262 (97217)	Loss/tok 3.9681 (6.0311)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][600/1885]	Time 0.101 (0.142)	Data 8.56e-05 (6.06e-04)	Tok/s 90946 (97162)	Loss/tok 3.5932 (6.0028)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][610/1885]	Time 0.146 (0.143)	Data 8.58e-05 (5.98e-04)	Tok/s 100616 (97187)	Loss/tok 3.9869 (5.9681)	LR 2.800e-03
0: TRAIN [0][620/1885]	Time 0.148 (0.143)	Data 8.51e-05 (5.89e-04)	Tok/s 100125 (97193)	Loss/tok 4.0503 (5.9366)	LR 2.800e-03
0: TRAIN [0][630/1885]	Time 0.145 (0.143)	Data 8.65e-05 (5.81e-04)	Tok/s 101740 (97233)	Loss/tok 4.0452 (5.9038)	LR 2.800e-03
0: TRAIN [0][640/1885]	Time 0.102 (0.143)	Data 8.65e-05 (5.74e-04)	Tok/s 87203 (97227)	Loss/tok 3.7161 (5.8753)	LR 2.800e-03
0: TRAIN [0][650/1885]	Time 0.102 (0.143)	Data 8.46e-05 (5.66e-04)	Tok/s 88519 (97251)	Loss/tok 3.6647 (5.8443)	LR 2.800e-03
0: TRAIN [0][660/1885]	Time 0.194 (0.143)	Data 8.46e-05 (5.59e-04)	Tok/s 106095 (97269)	Loss/tok 4.0183 (5.8139)	LR 2.800e-03
0: TRAIN [0][670/1885]	Time 0.102 (0.143)	Data 8.65e-05 (5.52e-04)	Tok/s 89638 (97205)	Loss/tok 3.7877 (5.7897)	LR 2.800e-03
0: TRAIN [0][680/1885]	Time 0.101 (0.143)	Data 8.51e-05 (5.45e-04)	Tok/s 88371 (97185)	Loss/tok 3.6360 (5.7639)	LR 2.800e-03
0: TRAIN [0][690/1885]	Time 0.146 (0.142)	Data 8.63e-05 (5.38e-04)	Tok/s 100973 (97160)	Loss/tok 3.8991 (5.7401)	LR 2.800e-03
0: TRAIN [0][700/1885]	Time 0.246 (0.143)	Data 9.04e-05 (5.32e-04)	Tok/s 105082 (97199)	Loss/tok 4.3000 (5.7127)	LR 2.800e-03
0: TRAIN [0][710/1885]	Time 0.101 (0.143)	Data 8.39e-05 (5.26e-04)	Tok/s 89692 (97203)	Loss/tok 3.5607 (5.6882)	LR 2.800e-03
0: TRAIN [0][720/1885]	Time 0.101 (0.142)	Data 8.51e-05 (5.19e-04)	Tok/s 90049 (97145)	Loss/tok 3.7019 (5.6668)	LR 2.800e-03
0: TRAIN [0][730/1885]	Time 0.101 (0.142)	Data 8.70e-05 (5.14e-04)	Tok/s 88303 (97096)	Loss/tok 3.5034 (5.6445)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][740/1885]	Time 0.101 (0.142)	Data 8.44e-05 (5.08e-04)	Tok/s 88683 (96992)	Loss/tok 3.6185 (5.6267)	LR 2.800e-03
0: TRAIN [0][750/1885]	Time 0.195 (0.142)	Data 8.44e-05 (5.02e-04)	Tok/s 106281 (96983)	Loss/tok 3.9961 (5.6048)	LR 2.800e-03
0: TRAIN [0][760/1885]	Time 0.059 (0.141)	Data 8.42e-05 (4.97e-04)	Tok/s 76566 (96934)	Loss/tok 2.8283 (5.5850)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][770/1885]	Time 0.059 (0.141)	Data 8.63e-05 (4.91e-04)	Tok/s 77563 (96913)	Loss/tok 2.8070 (5.5641)	LR 2.800e-03
0: TRAIN [0][780/1885]	Time 0.101 (0.141)	Data 8.58e-05 (4.86e-04)	Tok/s 90793 (96905)	Loss/tok 3.5661 (5.5431)	LR 2.800e-03
0: TRAIN [0][790/1885]	Time 0.147 (0.142)	Data 8.54e-05 (4.81e-04)	Tok/s 98441 (96963)	Loss/tok 3.9120 (5.5185)	LR 2.800e-03
0: TRAIN [0][800/1885]	Time 0.194 (0.141)	Data 8.73e-05 (4.76e-04)	Tok/s 105999 (96971)	Loss/tok 3.9492 (5.4979)	LR 2.800e-03
0: TRAIN [0][810/1885]	Time 0.193 (0.142)	Data 8.54e-05 (4.71e-04)	Tok/s 106643 (96984)	Loss/tok 4.0302 (5.4769)	LR 2.800e-03
0: TRAIN [0][820/1885]	Time 0.102 (0.141)	Data 8.56e-05 (4.67e-04)	Tok/s 88758 (96958)	Loss/tok 3.4525 (5.4581)	LR 2.800e-03
0: TRAIN [0][830/1885]	Time 0.193 (0.141)	Data 8.37e-05 (4.62e-04)	Tok/s 107210 (96959)	Loss/tok 4.0175 (5.4379)	LR 2.800e-03
0: TRAIN [0][840/1885]	Time 0.101 (0.141)	Data 8.56e-05 (4.58e-04)	Tok/s 91531 (96961)	Loss/tok 3.4816 (5.4186)	LR 2.800e-03
0: TRAIN [0][850/1885]	Time 0.147 (0.141)	Data 8.37e-05 (4.53e-04)	Tok/s 99394 (96942)	Loss/tok 3.6746 (5.4007)	LR 2.800e-03
0: TRAIN [0][860/1885]	Time 0.146 (0.141)	Data 8.42e-05 (4.49e-04)	Tok/s 98805 (96877)	Loss/tok 3.8152 (5.3855)	LR 2.800e-03
0: TRAIN [0][870/1885]	Time 0.147 (0.141)	Data 8.37e-05 (4.45e-04)	Tok/s 101082 (96885)	Loss/tok 3.7860 (5.3675)	LR 2.800e-03
0: TRAIN [0][880/1885]	Time 0.146 (0.141)	Data 8.73e-05 (4.41e-04)	Tok/s 99334 (96920)	Loss/tok 3.8142 (5.3476)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][890/1885]	Time 0.195 (0.141)	Data 8.44e-05 (4.37e-04)	Tok/s 104618 (96867)	Loss/tok 3.9745 (5.3328)	LR 2.800e-03
0: TRAIN [0][900/1885]	Time 0.101 (0.141)	Data 8.63e-05 (4.33e-04)	Tok/s 91401 (96877)	Loss/tok 3.4898 (5.3151)	LR 2.800e-03
0: TRAIN [0][910/1885]	Time 0.101 (0.141)	Data 8.51e-05 (4.29e-04)	Tok/s 91212 (96853)	Loss/tok 3.5204 (5.2993)	LR 2.800e-03
0: TRAIN [0][920/1885]	Time 0.060 (0.141)	Data 8.51e-05 (4.25e-04)	Tok/s 74955 (96851)	Loss/tok 2.7477 (5.2823)	LR 2.800e-03
0: TRAIN [0][930/1885]	Time 0.246 (0.141)	Data 8.58e-05 (4.22e-04)	Tok/s 105647 (96898)	Loss/tok 4.1218 (5.2638)	LR 2.800e-03
0: TRAIN [0][940/1885]	Time 0.147 (0.142)	Data 8.51e-05 (4.18e-04)	Tok/s 98913 (96932)	Loss/tok 3.7657 (5.2459)	LR 2.800e-03
0: TRAIN [0][950/1885]	Time 0.246 (0.142)	Data 8.70e-05 (4.14e-04)	Tok/s 106593 (96932)	Loss/tok 4.1287 (5.2304)	LR 2.800e-03
0: TRAIN [0][960/1885]	Time 0.147 (0.142)	Data 8.37e-05 (4.11e-04)	Tok/s 99664 (96919)	Loss/tok 3.6519 (5.2150)	LR 2.800e-03
0: TRAIN [0][970/1885]	Time 0.146 (0.141)	Data 8.39e-05 (4.08e-04)	Tok/s 99387 (96901)	Loss/tok 3.7985 (5.2008)	LR 2.800e-03
0: TRAIN [0][980/1885]	Time 0.194 (0.142)	Data 8.56e-05 (4.04e-04)	Tok/s 104739 (96897)	Loss/tok 4.0442 (5.1856)	LR 2.800e-03
0: TRAIN [0][990/1885]	Time 0.245 (0.141)	Data 8.32e-05 (4.01e-04)	Tok/s 107391 (96821)	Loss/tok 4.0683 (5.1740)	LR 2.800e-03
0: TRAIN [0][1000/1885]	Time 0.101 (0.141)	Data 8.32e-05 (3.98e-04)	Tok/s 89399 (96820)	Loss/tok 3.4198 (5.1597)	LR 2.800e-03
0: TRAIN [0][1010/1885]	Time 0.101 (0.141)	Data 8.68e-05 (3.95e-04)	Tok/s 90102 (96785)	Loss/tok 3.4654 (5.1467)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1020/1885]	Time 0.193 (0.141)	Data 8.58e-05 (3.92e-04)	Tok/s 106821 (96807)	Loss/tok 3.8461 (5.1316)	LR 2.800e-03
0: TRAIN [0][1030/1885]	Time 0.102 (0.141)	Data 8.49e-05 (3.89e-04)	Tok/s 89774 (96803)	Loss/tok 3.4688 (5.1181)	LR 2.800e-03
0: TRAIN [0][1040/1885]	Time 0.102 (0.141)	Data 1.03e-04 (3.86e-04)	Tok/s 90187 (96777)	Loss/tok 3.5163 (5.1062)	LR 2.800e-03
0: TRAIN [0][1050/1885]	Time 0.245 (0.141)	Data 8.85e-05 (3.83e-04)	Tok/s 107654 (96823)	Loss/tok 4.0830 (5.0910)	LR 2.800e-03
0: TRAIN [0][1060/1885]	Time 0.102 (0.141)	Data 8.49e-05 (3.80e-04)	Tok/s 87932 (96849)	Loss/tok 3.3953 (5.0767)	LR 2.800e-03
0: TRAIN [0][1070/1885]	Time 0.147 (0.141)	Data 8.39e-05 (3.78e-04)	Tok/s 98503 (96834)	Loss/tok 3.6859 (5.0642)	LR 2.800e-03
0: TRAIN [0][1080/1885]	Time 0.148 (0.141)	Data 8.51e-05 (3.75e-04)	Tok/s 99214 (96823)	Loss/tok 3.5416 (5.0516)	LR 2.800e-03
0: TRAIN [0][1090/1885]	Time 0.102 (0.142)	Data 8.68e-05 (3.72e-04)	Tok/s 89804 (96852)	Loss/tok 3.3821 (5.0382)	LR 2.800e-03
0: TRAIN [0][1100/1885]	Time 0.102 (0.142)	Data 8.42e-05 (3.70e-04)	Tok/s 88947 (96844)	Loss/tok 3.4096 (5.0258)	LR 2.800e-03
0: TRAIN [0][1110/1885]	Time 0.102 (0.142)	Data 8.56e-05 (3.67e-04)	Tok/s 89854 (96822)	Loss/tok 3.3979 (5.0145)	LR 2.800e-03
0: TRAIN [0][1120/1885]	Time 0.101 (0.142)	Data 8.44e-05 (3.65e-04)	Tok/s 88711 (96816)	Loss/tok 3.2886 (5.0030)	LR 2.800e-03
0: TRAIN [0][1130/1885]	Time 0.102 (0.142)	Data 8.56e-05 (3.62e-04)	Tok/s 88571 (96802)	Loss/tok 3.3073 (4.9919)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1140/1885]	Time 0.102 (0.142)	Data 8.85e-05 (3.60e-04)	Tok/s 87758 (96783)	Loss/tok 3.4642 (4.9807)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1150/1885]	Time 0.102 (0.141)	Data 8.63e-05 (3.57e-04)	Tok/s 89384 (96756)	Loss/tok 3.3225 (4.9701)	LR 2.800e-03
0: TRAIN [0][1160/1885]	Time 0.245 (0.142)	Data 8.54e-05 (3.55e-04)	Tok/s 106766 (96796)	Loss/tok 4.0123 (4.9570)	LR 2.800e-03
0: TRAIN [0][1170/1885]	Time 0.102 (0.142)	Data 8.51e-05 (3.53e-04)	Tok/s 89676 (96786)	Loss/tok 3.4041 (4.9465)	LR 2.800e-03
0: TRAIN [0][1180/1885]	Time 0.102 (0.142)	Data 8.65e-05 (3.50e-04)	Tok/s 89441 (96805)	Loss/tok 3.2929 (4.9351)	LR 2.800e-03
0: TRAIN [0][1190/1885]	Time 0.101 (0.142)	Data 8.51e-05 (3.48e-04)	Tok/s 90953 (96831)	Loss/tok 3.3233 (4.9238)	LR 2.800e-03
0: TRAIN [0][1200/1885]	Time 0.101 (0.142)	Data 8.23e-05 (3.46e-04)	Tok/s 90492 (96777)	Loss/tok 3.4132 (4.9151)	LR 2.800e-03
0: TRAIN [0][1210/1885]	Time 0.193 (0.142)	Data 8.42e-05 (3.44e-04)	Tok/s 106153 (96756)	Loss/tok 3.7540 (4.9054)	LR 2.800e-03
0: TRAIN [0][1220/1885]	Time 0.194 (0.142)	Data 8.46e-05 (3.42e-04)	Tok/s 105111 (96757)	Loss/tok 3.8101 (4.8947)	LR 2.800e-03
0: TRAIN [0][1230/1885]	Time 0.193 (0.142)	Data 8.34e-05 (3.40e-04)	Tok/s 104692 (96766)	Loss/tok 3.8526 (4.8845)	LR 2.800e-03
0: TRAIN [0][1240/1885]	Time 0.060 (0.142)	Data 8.73e-05 (3.38e-04)	Tok/s 75133 (96724)	Loss/tok 2.7629 (4.8760)	LR 2.800e-03
0: TRAIN [0][1250/1885]	Time 0.194 (0.141)	Data 8.49e-05 (3.36e-04)	Tok/s 104240 (96690)	Loss/tok 3.7968 (4.8672)	LR 2.800e-03
0: TRAIN [0][1260/1885]	Time 0.247 (0.142)	Data 2.32e-04 (3.34e-04)	Tok/s 105513 (96700)	Loss/tok 4.0083 (4.8569)	LR 2.800e-03
0: TRAIN [0][1270/1885]	Time 0.146 (0.142)	Data 1.51e-04 (3.33e-04)	Tok/s 100605 (96686)	Loss/tok 3.6085 (4.8479)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1280/1885]	Time 0.146 (0.141)	Data 1.29e-04 (3.32e-04)	Tok/s 99395 (96672)	Loss/tok 3.5662 (4.8385)	LR 2.800e-03
0: TRAIN [0][1290/1885]	Time 0.192 (0.142)	Data 1.38e-04 (3.31e-04)	Tok/s 105961 (96683)	Loss/tok 3.7584 (4.8281)	LR 2.800e-03
0: TRAIN [0][1300/1885]	Time 0.060 (0.142)	Data 2.19e-04 (3.30e-04)	Tok/s 76180 (96672)	Loss/tok 2.8011 (4.8187)	LR 2.800e-03
0: TRAIN [0][1310/1885]	Time 0.146 (0.142)	Data 1.79e-04 (3.29e-04)	Tok/s 100812 (96650)	Loss/tok 3.4768 (4.8101)	LR 2.800e-03
0: TRAIN [0][1320/1885]	Time 0.148 (0.142)	Data 2.22e-04 (3.28e-04)	Tok/s 99913 (96648)	Loss/tok 3.5957 (4.8010)	LR 2.800e-03
0: TRAIN [0][1330/1885]	Time 0.149 (0.142)	Data 2.17e-04 (3.27e-04)	Tok/s 97961 (96642)	Loss/tok 3.5359 (4.7919)	LR 2.800e-03
0: TRAIN [0][1340/1885]	Time 0.148 (0.142)	Data 2.17e-04 (3.26e-04)	Tok/s 99585 (96673)	Loss/tok 3.4001 (4.7814)	LR 2.800e-03
0: TRAIN [0][1350/1885]	Time 0.194 (0.142)	Data 1.39e-04 (3.25e-04)	Tok/s 104093 (96676)	Loss/tok 3.7735 (4.7726)	LR 2.800e-03
0: TRAIN [0][1360/1885]	Time 0.103 (0.142)	Data 2.17e-04 (3.24e-04)	Tok/s 91098 (96662)	Loss/tok 3.3403 (4.7641)	LR 2.800e-03
0: TRAIN [0][1370/1885]	Time 0.099 (0.142)	Data 1.07e-04 (3.23e-04)	Tok/s 90643 (96656)	Loss/tok 3.3416 (4.7553)	LR 2.800e-03
0: TRAIN [0][1380/1885]	Time 0.148 (0.142)	Data 2.19e-04 (3.22e-04)	Tok/s 98328 (96642)	Loss/tok 3.5970 (4.7470)	LR 2.800e-03
0: TRAIN [0][1390/1885]	Time 0.147 (0.142)	Data 2.18e-04 (3.21e-04)	Tok/s 100245 (96631)	Loss/tok 3.5872 (4.7386)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1400/1885]	Time 0.103 (0.142)	Data 2.16e-04 (3.20e-04)	Tok/s 86996 (96576)	Loss/tok 3.2445 (4.7320)	LR 2.800e-03
0: TRAIN [0][1410/1885]	Time 0.148 (0.142)	Data 2.21e-04 (3.19e-04)	Tok/s 98838 (96576)	Loss/tok 3.5454 (4.7236)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1420/1885]	Time 0.105 (0.142)	Data 2.42e-04 (3.19e-04)	Tok/s 87691 (96594)	Loss/tok 3.3303 (4.7145)	LR 2.800e-03
0: TRAIN [0][1430/1885]	Time 0.103 (0.142)	Data 2.16e-04 (3.18e-04)	Tok/s 86471 (96583)	Loss/tok 3.4225 (4.7066)	LR 2.800e-03
0: TRAIN [0][1440/1885]	Time 0.147 (0.142)	Data 2.19e-04 (3.17e-04)	Tok/s 99935 (96596)	Loss/tok 3.6600 (4.6985)	LR 2.800e-03
0: TRAIN [0][1450/1885]	Time 0.147 (0.142)	Data 1.09e-04 (3.15e-04)	Tok/s 98752 (96582)	Loss/tok 3.5857 (4.6914)	LR 2.800e-03
0: TRAIN [0][1460/1885]	Time 0.147 (0.142)	Data 1.32e-04 (3.14e-04)	Tok/s 99346 (96603)	Loss/tok 3.4668 (4.6830)	LR 2.800e-03
0: TRAIN [0][1470/1885]	Time 0.197 (0.142)	Data 2.19e-04 (3.13e-04)	Tok/s 103865 (96609)	Loss/tok 3.7504 (4.6753)	LR 2.800e-03
0: TRAIN [0][1480/1885]	Time 0.102 (0.142)	Data 1.13e-04 (3.12e-04)	Tok/s 90879 (96611)	Loss/tok 3.2149 (4.6678)	LR 2.800e-03
0: TRAIN [0][1490/1885]	Time 0.146 (0.142)	Data 1.65e-04 (3.11e-04)	Tok/s 101961 (96587)	Loss/tok 3.4967 (4.6608)	LR 2.800e-03
0: TRAIN [0][1500/1885]	Time 0.060 (0.142)	Data 1.08e-04 (3.10e-04)	Tok/s 76719 (96572)	Loss/tok 2.7938 (4.6534)	LR 2.800e-03
0: TRAIN [0][1510/1885]	Time 0.193 (0.142)	Data 1.60e-04 (3.09e-04)	Tok/s 106236 (96586)	Loss/tok 3.7117 (4.6455)	LR 2.800e-03
0: TRAIN [0][1520/1885]	Time 0.244 (0.142)	Data 1.78e-04 (3.08e-04)	Tok/s 107251 (96601)	Loss/tok 3.8875 (4.6380)	LR 2.800e-03
0: TRAIN [0][1530/1885]	Time 0.101 (0.142)	Data 1.38e-04 (3.07e-04)	Tok/s 89981 (96591)	Loss/tok 3.3535 (4.6313)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1540/1885]	Time 0.102 (0.142)	Data 1.05e-04 (3.06e-04)	Tok/s 89032 (96590)	Loss/tok 3.3939 (4.6243)	LR 2.800e-03
0: TRAIN [0][1550/1885]	Time 0.149 (0.142)	Data 8.54e-05 (3.05e-04)	Tok/s 99472 (96596)	Loss/tok 3.5222 (4.6172)	LR 2.800e-03
0: TRAIN [0][1560/1885]	Time 0.103 (0.142)	Data 1.07e-04 (3.04e-04)	Tok/s 89695 (96595)	Loss/tok 3.3564 (4.6103)	LR 2.800e-03
0: TRAIN [0][1570/1885]	Time 0.101 (0.142)	Data 2.18e-04 (3.04e-04)	Tok/s 88520 (96600)	Loss/tok 3.2777 (4.6036)	LR 2.800e-03
0: TRAIN [0][1580/1885]	Time 0.194 (0.142)	Data 1.09e-04 (3.03e-04)	Tok/s 104397 (96610)	Loss/tok 3.7571 (4.5968)	LR 2.800e-03
0: TRAIN [0][1590/1885]	Time 0.148 (0.142)	Data 2.33e-04 (3.02e-04)	Tok/s 99674 (96592)	Loss/tok 3.4550 (4.5905)	LR 2.800e-03
0: TRAIN [0][1600/1885]	Time 0.146 (0.142)	Data 2.26e-04 (3.01e-04)	Tok/s 99872 (96603)	Loss/tok 3.3683 (4.5833)	LR 2.800e-03
0: TRAIN [0][1610/1885]	Time 0.059 (0.142)	Data 1.36e-04 (3.00e-04)	Tok/s 76966 (96608)	Loss/tok 2.7675 (4.5763)	LR 2.800e-03
0: TRAIN [0][1620/1885]	Time 0.103 (0.142)	Data 2.18e-04 (2.99e-04)	Tok/s 86744 (96624)	Loss/tok 3.1668 (4.5693)	LR 2.800e-03
0: TRAIN [0][1630/1885]	Time 0.101 (0.142)	Data 1.40e-04 (2.98e-04)	Tok/s 89189 (96625)	Loss/tok 3.2312 (4.5628)	LR 2.800e-03
0: TRAIN [0][1640/1885]	Time 0.147 (0.142)	Data 1.37e-04 (2.98e-04)	Tok/s 98814 (96622)	Loss/tok 3.5731 (4.5566)	LR 2.800e-03
0: TRAIN [0][1650/1885]	Time 0.102 (0.142)	Data 1.37e-04 (2.97e-04)	Tok/s 90989 (96607)	Loss/tok 3.2647 (4.5508)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1660/1885]	Time 0.245 (0.142)	Data 1.40e-04 (2.96e-04)	Tok/s 106599 (96600)	Loss/tok 3.8424 (4.5447)	LR 2.800e-03
0: TRAIN [0][1670/1885]	Time 0.103 (0.142)	Data 2.18e-04 (2.95e-04)	Tok/s 89410 (96598)	Loss/tok 3.3237 (4.5386)	LR 2.800e-03
0: TRAIN [0][1680/1885]	Time 0.244 (0.142)	Data 1.38e-04 (2.94e-04)	Tok/s 106604 (96607)	Loss/tok 3.8647 (4.5323)	LR 2.800e-03
0: TRAIN [0][1690/1885]	Time 0.101 (0.142)	Data 1.07e-04 (2.94e-04)	Tok/s 92336 (96595)	Loss/tok 3.3086 (4.5267)	LR 2.800e-03
0: TRAIN [0][1700/1885]	Time 0.196 (0.142)	Data 2.16e-04 (2.93e-04)	Tok/s 103440 (96605)	Loss/tok 3.6769 (4.5205)	LR 2.800e-03
0: TRAIN [0][1710/1885]	Time 0.059 (0.142)	Data 1.75e-04 (2.92e-04)	Tok/s 77539 (96577)	Loss/tok 2.7169 (4.5152)	LR 2.800e-03
0: TRAIN [0][1720/1885]	Time 0.101 (0.142)	Data 1.13e-04 (2.91e-04)	Tok/s 89736 (96573)	Loss/tok 3.3225 (4.5097)	LR 2.800e-03
0: TRAIN [0][1730/1885]	Time 0.145 (0.142)	Data 1.77e-04 (2.91e-04)	Tok/s 102294 (96553)	Loss/tok 3.4796 (4.5043)	LR 2.800e-03
0: TRAIN [0][1740/1885]	Time 0.102 (0.142)	Data 1.71e-04 (2.90e-04)	Tok/s 87427 (96536)	Loss/tok 3.2390 (4.4992)	LR 2.800e-03
0: TRAIN [0][1750/1885]	Time 0.193 (0.142)	Data 1.50e-04 (2.89e-04)	Tok/s 105709 (96569)	Loss/tok 3.7801 (4.4926)	LR 2.800e-03
0: TRAIN [0][1760/1885]	Time 0.194 (0.142)	Data 1.09e-04 (2.88e-04)	Tok/s 105100 (96582)	Loss/tok 3.7136 (4.4866)	LR 2.800e-03
0: TRAIN [0][1770/1885]	Time 0.194 (0.142)	Data 1.68e-04 (2.88e-04)	Tok/s 104612 (96601)	Loss/tok 3.6925 (4.4805)	LR 2.800e-03
0: TRAIN [0][1780/1885]	Time 0.245 (0.142)	Data 2.17e-04 (2.87e-04)	Tok/s 106070 (96597)	Loss/tok 3.9116 (4.4753)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1790/1885]	Time 0.244 (0.142)	Data 1.40e-04 (2.86e-04)	Tok/s 107278 (96613)	Loss/tok 3.8116 (4.4698)	LR 2.800e-03
0: TRAIN [0][1800/1885]	Time 0.145 (0.142)	Data 1.07e-04 (2.86e-04)	Tok/s 101447 (96608)	Loss/tok 3.4491 (4.4646)	LR 2.800e-03
0: TRAIN [0][1810/1885]	Time 0.148 (0.143)	Data 2.16e-04 (2.85e-04)	Tok/s 100140 (96614)	Loss/tok 3.4224 (4.4592)	LR 2.800e-03
0: TRAIN [0][1820/1885]	Time 0.193 (0.143)	Data 1.80e-04 (2.84e-04)	Tok/s 104983 (96611)	Loss/tok 3.6649 (4.4539)	LR 2.800e-03
0: TRAIN [0][1830/1885]	Time 0.102 (0.142)	Data 1.39e-04 (2.83e-04)	Tok/s 89078 (96593)	Loss/tok 3.2463 (4.4490)	LR 2.800e-03
0: TRAIN [0][1840/1885]	Time 0.147 (0.143)	Data 2.18e-04 (2.83e-04)	Tok/s 100176 (96605)	Loss/tok 3.4648 (4.4435)	LR 2.800e-03
0: TRAIN [0][1850/1885]	Time 0.102 (0.143)	Data 1.16e-04 (2.82e-04)	Tok/s 88677 (96608)	Loss/tok 3.2059 (4.4383)	LR 2.800e-03
0: TRAIN [0][1860/1885]	Time 0.101 (0.143)	Data 1.40e-04 (2.81e-04)	Tok/s 90812 (96607)	Loss/tok 3.2835 (4.4331)	LR 2.800e-03
0: TRAIN [0][1870/1885]	Time 0.148 (0.143)	Data 1.92e-04 (2.81e-04)	Tok/s 98900 (96610)	Loss/tok 3.5323 (4.4283)	LR 2.800e-03
0: TRAIN [0][1880/1885]	Time 0.193 (0.143)	Data 1.39e-04 (2.80e-04)	Tok/s 105785 (96600)	Loss/tok 3.6683 (4.4235)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593838186904, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593838186904, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.672 (0.672)	Decoder iters 149.0 (149.0)	Tok/s 24028 (24028)
0: Running moses detokenizer
0: BLEU(score=19.876153269650427, counts=[34890, 16040, 8591, 4779], totals=[66537, 63534, 60531, 57532], precisions=[52.43698994544389, 25.24632480246797, 14.192727693248088, 8.306681498991866], bp=1.0, sys_len=66537, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593838188670, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.19879999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593838188670, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.4239	Test BLEU: 19.88
0: Performance: Epoch: 0	Training: 772709 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593838188671, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593838188671, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593838188671, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2730140799
0: TRAIN [1][0/1885]	Time 0.355 (0.355)	Data 2.04e-01 (2.04e-01)	Tok/s 25555 (25555)	Loss/tok 3.1394 (3.1394)	LR 2.800e-03
0: TRAIN [1][10/1885]	Time 0.101 (0.154)	Data 2.19e-04 (1.87e-02)	Tok/s 89495 (88598)	Loss/tok 3.3008 (3.3815)	LR 2.800e-03
0: TRAIN [1][20/1885]	Time 0.102 (0.147)	Data 2.19e-04 (9.90e-03)	Tok/s 89561 (91665)	Loss/tok 3.2633 (3.4026)	LR 2.800e-03
0: TRAIN [1][30/1885]	Time 0.147 (0.144)	Data 2.18e-04 (6.77e-03)	Tok/s 100326 (93508)	Loss/tok 3.4334 (3.3934)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][40/1885]	Time 0.245 (0.142)	Data 2.19e-04 (5.18e-03)	Tok/s 106803 (93609)	Loss/tok 3.6411 (3.3927)	LR 2.800e-03
0: TRAIN [1][50/1885]	Time 0.102 (0.139)	Data 2.34e-04 (4.20e-03)	Tok/s 89976 (93518)	Loss/tok 3.1525 (3.3838)	LR 2.800e-03
0: TRAIN [1][60/1885]	Time 0.147 (0.140)	Data 2.21e-04 (3.55e-03)	Tok/s 99762 (93952)	Loss/tok 3.3983 (3.3868)	LR 2.800e-03
0: TRAIN [1][70/1885]	Time 0.194 (0.140)	Data 2.20e-04 (3.08e-03)	Tok/s 105026 (94293)	Loss/tok 3.6354 (3.3921)	LR 2.800e-03
0: TRAIN [1][80/1885]	Time 0.103 (0.137)	Data 2.00e-04 (2.72e-03)	Tok/s 88151 (93688)	Loss/tok 3.0834 (3.3804)	LR 2.800e-03
0: TRAIN [1][90/1885]	Time 0.147 (0.137)	Data 2.18e-04 (2.45e-03)	Tok/s 100153 (93841)	Loss/tok 3.3005 (3.3861)	LR 2.800e-03
0: TRAIN [1][100/1885]	Time 0.146 (0.138)	Data 2.19e-04 (2.23e-03)	Tok/s 100229 (94225)	Loss/tok 3.4137 (3.3888)	LR 2.800e-03
0: TRAIN [1][110/1885]	Time 0.100 (0.135)	Data 2.19e-04 (2.04e-03)	Tok/s 90982 (93853)	Loss/tok 3.0853 (3.3768)	LR 2.800e-03
0: TRAIN [1][120/1885]	Time 0.244 (0.136)	Data 1.77e-04 (1.89e-03)	Tok/s 107671 (94036)	Loss/tok 3.6758 (3.3840)	LR 2.800e-03
0: TRAIN [1][130/1885]	Time 0.146 (0.136)	Data 2.04e-04 (1.76e-03)	Tok/s 101068 (94044)	Loss/tok 3.3504 (3.3859)	LR 2.800e-03
0: TRAIN [1][140/1885]	Time 0.101 (0.137)	Data 2.21e-04 (1.65e-03)	Tok/s 90730 (94223)	Loss/tok 3.1777 (3.3901)	LR 2.800e-03
0: TRAIN [1][150/1885]	Time 0.194 (0.139)	Data 2.16e-04 (1.56e-03)	Tok/s 104421 (94624)	Loss/tok 3.6935 (3.3999)	LR 2.800e-03
0: TRAIN [1][160/1885]	Time 0.146 (0.139)	Data 2.20e-04 (1.47e-03)	Tok/s 102213 (94667)	Loss/tok 3.4093 (3.3993)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][170/1885]	Time 0.148 (0.139)	Data 2.20e-04 (1.40e-03)	Tok/s 100725 (94866)	Loss/tok 3.4088 (3.3997)	LR 2.800e-03
0: TRAIN [1][180/1885]	Time 0.102 (0.141)	Data 1.93e-04 (1.33e-03)	Tok/s 89437 (95095)	Loss/tok 3.1258 (3.4093)	LR 2.800e-03
0: TRAIN [1][190/1885]	Time 0.146 (0.141)	Data 2.20e-04 (1.27e-03)	Tok/s 101379 (95157)	Loss/tok 3.3325 (3.4108)	LR 2.800e-03
0: TRAIN [1][200/1885]	Time 0.102 (0.143)	Data 2.21e-04 (1.22e-03)	Tok/s 88530 (95368)	Loss/tok 3.0952 (3.4194)	LR 2.800e-03
0: TRAIN [1][210/1885]	Time 0.148 (0.144)	Data 2.32e-04 (1.17e-03)	Tok/s 99397 (95464)	Loss/tok 3.3510 (3.4248)	LR 2.800e-03
0: TRAIN [1][220/1885]	Time 0.148 (0.143)	Data 2.19e-04 (1.13e-03)	Tok/s 100227 (95451)	Loss/tok 3.4105 (3.4238)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][230/1885]	Time 0.148 (0.143)	Data 2.29e-04 (1.09e-03)	Tok/s 99533 (95354)	Loss/tok 3.3822 (3.4222)	LR 2.800e-03
0: TRAIN [1][240/1885]	Time 0.195 (0.142)	Data 2.27e-04 (1.05e-03)	Tok/s 105722 (95358)	Loss/tok 3.5231 (3.4197)	LR 2.800e-03
0: TRAIN [1][250/1885]	Time 0.148 (0.143)	Data 2.22e-04 (1.02e-03)	Tok/s 97808 (95412)	Loss/tok 3.5046 (3.4254)	LR 2.800e-03
0: TRAIN [1][260/1885]	Time 0.193 (0.143)	Data 2.24e-04 (9.89e-04)	Tok/s 105264 (95411)	Loss/tok 3.5494 (3.4235)	LR 2.800e-03
0: TRAIN [1][270/1885]	Time 0.104 (0.143)	Data 1.93e-04 (9.60e-04)	Tok/s 84585 (95407)	Loss/tok 3.1800 (3.4266)	LR 2.800e-03
0: TRAIN [1][280/1885]	Time 0.149 (0.143)	Data 1.92e-04 (9.33e-04)	Tok/s 98751 (95405)	Loss/tok 3.2961 (3.4277)	LR 2.800e-03
0: TRAIN [1][290/1885]	Time 0.103 (0.144)	Data 2.19e-04 (9.08e-04)	Tok/s 87769 (95397)	Loss/tok 3.1307 (3.4297)	LR 2.800e-03
0: TRAIN [1][300/1885]	Time 0.147 (0.142)	Data 2.19e-04 (8.84e-04)	Tok/s 98566 (95231)	Loss/tok 3.4694 (3.4249)	LR 2.800e-03
0: TRAIN [1][310/1885]	Time 0.102 (0.142)	Data 2.19e-04 (8.63e-04)	Tok/s 87443 (95258)	Loss/tok 3.0046 (3.4228)	LR 2.800e-03
0: TRAIN [1][320/1885]	Time 0.146 (0.142)	Data 2.21e-04 (8.42e-04)	Tok/s 99297 (95271)	Loss/tok 3.4575 (3.4223)	LR 2.800e-03
0: TRAIN [1][330/1885]	Time 0.195 (0.142)	Data 2.18e-04 (8.23e-04)	Tok/s 104577 (95216)	Loss/tok 3.5025 (3.4202)	LR 2.800e-03
0: TRAIN [1][340/1885]	Time 0.193 (0.142)	Data 2.21e-04 (8.06e-04)	Tok/s 106263 (95271)	Loss/tok 3.4994 (3.4225)	LR 2.800e-03
0: TRAIN [1][350/1885]	Time 0.193 (0.143)	Data 2.22e-04 (7.89e-04)	Tok/s 106679 (95374)	Loss/tok 3.4736 (3.4235)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][360/1885]	Time 0.245 (0.143)	Data 2.20e-04 (7.72e-04)	Tok/s 107273 (95375)	Loss/tok 3.9037 (3.4264)	LR 2.800e-03
0: TRAIN [1][370/1885]	Time 0.146 (0.143)	Data 2.16e-04 (7.57e-04)	Tok/s 100959 (95382)	Loss/tok 3.3526 (3.4266)	LR 2.800e-03
0: TRAIN [1][380/1885]	Time 0.147 (0.143)	Data 2.19e-04 (7.42e-04)	Tok/s 99749 (95492)	Loss/tok 3.3537 (3.4294)	LR 2.800e-03
0: TRAIN [1][390/1885]	Time 0.146 (0.144)	Data 2.21e-04 (7.29e-04)	Tok/s 99730 (95602)	Loss/tok 3.3918 (3.4299)	LR 2.800e-03
0: TRAIN [1][400/1885]	Time 0.147 (0.144)	Data 2.18e-04 (7.16e-04)	Tok/s 99710 (95646)	Loss/tok 3.3182 (3.4283)	LR 2.800e-03
0: TRAIN [1][410/1885]	Time 0.146 (0.144)	Data 2.20e-04 (7.03e-04)	Tok/s 100982 (95714)	Loss/tok 3.4355 (3.4284)	LR 2.800e-03
0: TRAIN [1][420/1885]	Time 0.104 (0.144)	Data 2.21e-04 (6.92e-04)	Tok/s 90652 (95721)	Loss/tok 3.1439 (3.4286)	LR 2.800e-03
0: TRAIN [1][430/1885]	Time 0.103 (0.144)	Data 2.20e-04 (6.81e-04)	Tok/s 88202 (95694)	Loss/tok 3.2391 (3.4282)	LR 2.800e-03
0: TRAIN [1][440/1885]	Time 0.194 (0.144)	Data 2.17e-04 (6.70e-04)	Tok/s 105618 (95750)	Loss/tok 3.4105 (3.4262)	LR 2.800e-03
0: TRAIN [1][450/1885]	Time 0.101 (0.143)	Data 2.21e-04 (6.60e-04)	Tok/s 90670 (95713)	Loss/tok 3.1980 (3.4245)	LR 2.800e-03
0: TRAIN [1][460/1885]	Time 0.102 (0.143)	Data 2.19e-04 (6.50e-04)	Tok/s 87871 (95740)	Loss/tok 3.1210 (3.4222)	LR 2.800e-03
0: TRAIN [1][470/1885]	Time 0.193 (0.144)	Data 1.63e-04 (6.41e-04)	Tok/s 104767 (95836)	Loss/tok 3.5108 (3.4243)	LR 2.800e-03
0: TRAIN [1][480/1885]	Time 0.147 (0.144)	Data 2.21e-04 (6.32e-04)	Tok/s 98668 (95884)	Loss/tok 3.3980 (3.4240)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][490/1885]	Time 0.195 (0.144)	Data 2.21e-04 (6.23e-04)	Tok/s 102816 (95814)	Loss/tok 3.5405 (3.4225)	LR 2.800e-03
0: TRAIN [1][500/1885]	Time 0.103 (0.143)	Data 2.22e-04 (6.15e-04)	Tok/s 88911 (95665)	Loss/tok 3.1274 (3.4194)	LR 2.800e-03
0: TRAIN [1][510/1885]	Time 0.147 (0.143)	Data 2.21e-04 (6.07e-04)	Tok/s 100203 (95613)	Loss/tok 3.3475 (3.4182)	LR 2.800e-03
0: TRAIN [1][520/1885]	Time 0.194 (0.142)	Data 2.19e-04 (6.00e-04)	Tok/s 106424 (95583)	Loss/tok 3.5710 (3.4169)	LR 2.800e-03
0: TRAIN [1][530/1885]	Time 0.103 (0.142)	Data 2.16e-04 (5.92e-04)	Tok/s 88122 (95519)	Loss/tok 3.1219 (3.4152)	LR 2.800e-03
0: TRAIN [1][540/1885]	Time 0.103 (0.142)	Data 2.04e-04 (5.85e-04)	Tok/s 86202 (95550)	Loss/tok 3.2628 (3.4159)	LR 2.800e-03
0: TRAIN [1][550/1885]	Time 0.103 (0.142)	Data 2.20e-04 (5.78e-04)	Tok/s 87466 (95590)	Loss/tok 3.0973 (3.4171)	LR 2.800e-03
0: TRAIN [1][560/1885]	Time 0.103 (0.142)	Data 1.57e-04 (5.72e-04)	Tok/s 85117 (95546)	Loss/tok 3.2257 (3.4162)	LR 2.800e-03
0: TRAIN [1][570/1885]	Time 0.193 (0.142)	Data 2.31e-04 (5.65e-04)	Tok/s 106592 (95575)	Loss/tok 3.5758 (3.4170)	LR 2.800e-03
0: TRAIN [1][580/1885]	Time 0.146 (0.142)	Data 2.19e-04 (5.59e-04)	Tok/s 99555 (95576)	Loss/tok 3.2685 (3.4158)	LR 2.800e-03
0: TRAIN [1][590/1885]	Time 0.147 (0.142)	Data 1.92e-04 (5.53e-04)	Tok/s 100631 (95596)	Loss/tok 3.4628 (3.4154)	LR 2.800e-03
0: TRAIN [1][600/1885]	Time 0.147 (0.142)	Data 2.17e-04 (5.48e-04)	Tok/s 99851 (95640)	Loss/tok 3.3915 (3.4156)	LR 2.800e-03
0: TRAIN [1][610/1885]	Time 0.196 (0.143)	Data 1.41e-04 (5.42e-04)	Tok/s 103809 (95690)	Loss/tok 3.4437 (3.4150)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][620/1885]	Time 0.102 (0.143)	Data 1.53e-04 (5.37e-04)	Tok/s 87652 (95757)	Loss/tok 3.0925 (3.4148)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][630/1885]	Time 0.245 (0.143)	Data 2.24e-04 (5.32e-04)	Tok/s 107834 (95784)	Loss/tok 3.8157 (3.4185)	LR 2.800e-03
0: TRAIN [1][640/1885]	Time 0.102 (0.143)	Data 2.20e-04 (5.26e-04)	Tok/s 89712 (95820)	Loss/tok 3.0735 (3.4182)	LR 2.800e-03
0: TRAIN [1][650/1885]	Time 0.195 (0.143)	Data 2.20e-04 (5.22e-04)	Tok/s 103799 (95804)	Loss/tok 3.6593 (3.4176)	LR 2.800e-03
0: TRAIN [1][660/1885]	Time 0.102 (0.143)	Data 2.20e-04 (5.17e-04)	Tok/s 91077 (95719)	Loss/tok 3.1621 (3.4149)	LR 2.800e-03
0: TRAIN [1][670/1885]	Time 0.104 (0.142)	Data 2.18e-04 (5.12e-04)	Tok/s 87356 (95695)	Loss/tok 3.0756 (3.4135)	LR 2.800e-03
0: TRAIN [1][680/1885]	Time 0.196 (0.142)	Data 2.20e-04 (5.08e-04)	Tok/s 103366 (95656)	Loss/tok 3.5151 (3.4124)	LR 2.800e-03
0: TRAIN [1][690/1885]	Time 0.148 (0.142)	Data 1.60e-04 (5.03e-04)	Tok/s 100297 (95658)	Loss/tok 3.2562 (3.4109)	LR 2.800e-03
0: TRAIN [1][700/1885]	Time 0.102 (0.142)	Data 2.21e-04 (4.99e-04)	Tok/s 88198 (95693)	Loss/tok 2.9989 (3.4118)	LR 2.800e-03
0: TRAIN [1][710/1885]	Time 0.103 (0.142)	Data 2.20e-04 (4.95e-04)	Tok/s 86664 (95704)	Loss/tok 3.0532 (3.4120)	LR 2.800e-03
0: TRAIN [1][720/1885]	Time 0.194 (0.142)	Data 2.19e-04 (4.91e-04)	Tok/s 105110 (95692)	Loss/tok 3.4826 (3.4106)	LR 2.800e-03
0: TRAIN [1][730/1885]	Time 0.146 (0.142)	Data 2.21e-04 (4.88e-04)	Tok/s 98788 (95649)	Loss/tok 3.4205 (3.4107)	LR 2.800e-03
0: TRAIN [1][740/1885]	Time 0.193 (0.142)	Data 1.61e-04 (4.84e-04)	Tok/s 106932 (95702)	Loss/tok 3.5443 (3.4113)	LR 2.800e-03
0: TRAIN [1][750/1885]	Time 0.147 (0.142)	Data 1.88e-04 (4.80e-04)	Tok/s 99468 (95718)	Loss/tok 3.4012 (3.4107)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][760/1885]	Time 0.060 (0.143)	Data 2.20e-04 (4.77e-04)	Tok/s 75698 (95733)	Loss/tok 2.6393 (3.4112)	LR 2.800e-03
0: TRAIN [1][770/1885]	Time 0.244 (0.143)	Data 2.19e-04 (4.73e-04)	Tok/s 106772 (95750)	Loss/tok 3.7174 (3.4121)	LR 2.800e-03
0: TRAIN [1][780/1885]	Time 0.103 (0.143)	Data 2.04e-04 (4.70e-04)	Tok/s 87120 (95726)	Loss/tok 3.1304 (3.4112)	LR 2.800e-03
0: TRAIN [1][790/1885]	Time 0.194 (0.143)	Data 2.17e-04 (4.67e-04)	Tok/s 104497 (95746)	Loss/tok 3.5485 (3.4110)	LR 2.800e-03
0: TRAIN [1][800/1885]	Time 0.244 (0.143)	Data 2.20e-04 (4.63e-04)	Tok/s 106167 (95756)	Loss/tok 3.6440 (3.4115)	LR 2.800e-03
0: TRAIN [1][810/1885]	Time 0.195 (0.143)	Data 2.05e-04 (4.60e-04)	Tok/s 104082 (95810)	Loss/tok 3.4391 (3.4117)	LR 2.800e-03
0: TRAIN [1][820/1885]	Time 0.103 (0.143)	Data 1.77e-04 (4.57e-04)	Tok/s 87701 (95798)	Loss/tok 3.1729 (3.4111)	LR 2.800e-03
0: TRAIN [1][830/1885]	Time 0.193 (0.143)	Data 2.19e-04 (4.54e-04)	Tok/s 105829 (95853)	Loss/tok 3.5146 (3.4121)	LR 2.800e-03
0: TRAIN [1][840/1885]	Time 0.147 (0.144)	Data 2.21e-04 (4.51e-04)	Tok/s 100268 (95900)	Loss/tok 3.3377 (3.4118)	LR 2.800e-03
0: TRAIN [1][850/1885]	Time 0.102 (0.143)	Data 2.19e-04 (4.49e-04)	Tok/s 89731 (95892)	Loss/tok 3.0575 (3.4111)	LR 2.800e-03
0: TRAIN [1][860/1885]	Time 0.196 (0.144)	Data 2.19e-04 (4.46e-04)	Tok/s 103171 (95923)	Loss/tok 3.5648 (3.4109)	LR 2.800e-03
0: TRAIN [1][870/1885]	Time 0.102 (0.143)	Data 1.90e-04 (4.43e-04)	Tok/s 88014 (95912)	Loss/tok 3.2416 (3.4110)	LR 2.800e-03
0: TRAIN [1][880/1885]	Time 0.061 (0.143)	Data 1.09e-04 (4.40e-04)	Tok/s 75829 (95872)	Loss/tok 2.6759 (3.4097)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][890/1885]	Time 0.196 (0.143)	Data 1.07e-04 (4.37e-04)	Tok/s 104986 (95901)	Loss/tok 3.5202 (3.4099)	LR 2.800e-03
0: TRAIN [1][900/1885]	Time 0.101 (0.143)	Data 2.17e-04 (4.33e-04)	Tok/s 90477 (95863)	Loss/tok 3.0839 (3.4089)	LR 2.800e-03
0: TRAIN [1][910/1885]	Time 0.060 (0.143)	Data 2.20e-04 (4.31e-04)	Tok/s 76770 (95827)	Loss/tok 2.6159 (3.4078)	LR 2.800e-03
0: TRAIN [1][920/1885]	Time 0.148 (0.143)	Data 2.23e-04 (4.28e-04)	Tok/s 99983 (95798)	Loss/tok 3.3515 (3.4062)	LR 2.800e-03
0: TRAIN [1][930/1885]	Time 0.148 (0.143)	Data 2.18e-04 (4.26e-04)	Tok/s 98442 (95809)	Loss/tok 3.4336 (3.4056)	LR 2.800e-03
0: TRAIN [1][940/1885]	Time 0.146 (0.143)	Data 2.18e-04 (4.24e-04)	Tok/s 99173 (95788)	Loss/tok 3.3017 (3.4047)	LR 2.800e-03
0: TRAIN [1][950/1885]	Time 0.103 (0.143)	Data 2.19e-04 (4.21e-04)	Tok/s 88936 (95821)	Loss/tok 3.1068 (3.4047)	LR 2.800e-03
0: TRAIN [1][960/1885]	Time 0.147 (0.143)	Data 2.20e-04 (4.19e-04)	Tok/s 99239 (95837)	Loss/tok 3.3712 (3.4046)	LR 2.800e-03
0: TRAIN [1][970/1885]	Time 0.245 (0.143)	Data 2.17e-04 (4.17e-04)	Tok/s 105744 (95902)	Loss/tok 3.7996 (3.4062)	LR 2.800e-03
0: TRAIN [1][980/1885]	Time 0.104 (0.143)	Data 2.20e-04 (4.15e-04)	Tok/s 88007 (95930)	Loss/tok 3.1988 (3.4056)	LR 2.800e-03
0: TRAIN [1][990/1885]	Time 0.148 (0.143)	Data 1.66e-04 (4.13e-04)	Tok/s 98878 (95925)	Loss/tok 3.3624 (3.4049)	LR 2.800e-03
0: TRAIN [1][1000/1885]	Time 0.103 (0.143)	Data 1.88e-04 (4.11e-04)	Tok/s 88681 (95909)	Loss/tok 3.2731 (3.4041)	LR 2.800e-03
0: TRAIN [1][1010/1885]	Time 0.101 (0.143)	Data 2.20e-04 (4.09e-04)	Tok/s 90839 (95839)	Loss/tok 3.1031 (3.4023)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1020/1885]	Time 0.102 (0.143)	Data 2.05e-04 (4.07e-04)	Tok/s 90127 (95833)	Loss/tok 3.2160 (3.4015)	LR 2.800e-03
0: TRAIN [1][1030/1885]	Time 0.195 (0.143)	Data 1.81e-04 (4.05e-04)	Tok/s 105352 (95832)	Loss/tok 3.5072 (3.4012)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1040/1885]	Time 0.101 (0.143)	Data 2.19e-04 (4.03e-04)	Tok/s 89180 (95816)	Loss/tok 3.0067 (3.4000)	LR 2.800e-03
0: TRAIN [1][1050/1885]	Time 0.102 (0.143)	Data 2.19e-04 (4.01e-04)	Tok/s 89626 (95842)	Loss/tok 3.0643 (3.3999)	LR 2.800e-03
0: TRAIN [1][1060/1885]	Time 0.146 (0.143)	Data 2.21e-04 (4.00e-04)	Tok/s 99867 (95819)	Loss/tok 3.2939 (3.3985)	LR 2.800e-03
0: TRAIN [1][1070/1885]	Time 0.100 (0.142)	Data 2.20e-04 (3.98e-04)	Tok/s 89647 (95803)	Loss/tok 3.1071 (3.3976)	LR 2.800e-03
0: TRAIN [1][1080/1885]	Time 0.195 (0.142)	Data 2.36e-04 (3.96e-04)	Tok/s 103969 (95803)	Loss/tok 3.6065 (3.3974)	LR 2.800e-03
0: TRAIN [1][1090/1885]	Time 0.102 (0.142)	Data 2.20e-04 (3.94e-04)	Tok/s 88581 (95806)	Loss/tok 3.0839 (3.3971)	LR 2.800e-03
0: TRAIN [1][1100/1885]	Time 0.147 (0.142)	Data 2.21e-04 (3.93e-04)	Tok/s 99828 (95810)	Loss/tok 3.4260 (3.3967)	LR 2.800e-03
0: TRAIN [1][1110/1885]	Time 0.244 (0.142)	Data 2.19e-04 (3.91e-04)	Tok/s 107941 (95821)	Loss/tok 3.6126 (3.3972)	LR 2.800e-03
0: TRAIN [1][1120/1885]	Time 0.103 (0.142)	Data 2.18e-04 (3.90e-04)	Tok/s 88650 (95773)	Loss/tok 3.0861 (3.3965)	LR 2.800e-03
0: TRAIN [1][1130/1885]	Time 0.146 (0.142)	Data 2.21e-04 (3.88e-04)	Tok/s 101925 (95781)	Loss/tok 3.3185 (3.3956)	LR 2.800e-03
0: TRAIN [1][1140/1885]	Time 0.103 (0.142)	Data 2.20e-04 (3.87e-04)	Tok/s 87607 (95774)	Loss/tok 3.0758 (3.3951)	LR 2.800e-03
0: TRAIN [1][1150/1885]	Time 0.103 (0.142)	Data 2.20e-04 (3.85e-04)	Tok/s 87224 (95758)	Loss/tok 3.1595 (3.3943)	LR 2.800e-03
0: TRAIN [1][1160/1885]	Time 0.102 (0.142)	Data 2.20e-04 (3.84e-04)	Tok/s 90765 (95734)	Loss/tok 3.0584 (3.3929)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1170/1885]	Time 0.103 (0.142)	Data 2.21e-04 (3.82e-04)	Tok/s 89316 (95767)	Loss/tok 3.0633 (3.3931)	LR 2.800e-03
0: TRAIN [1][1180/1885]	Time 0.102 (0.142)	Data 2.21e-04 (3.81e-04)	Tok/s 89277 (95781)	Loss/tok 3.2492 (3.3931)	LR 2.800e-03
0: TRAIN [1][1190/1885]	Time 0.147 (0.143)	Data 2.04e-04 (3.79e-04)	Tok/s 98799 (95829)	Loss/tok 3.3942 (3.3943)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1200/1885]	Time 0.195 (0.143)	Data 2.22e-04 (3.78e-04)	Tok/s 104614 (95845)	Loss/tok 3.5274 (3.3946)	LR 2.800e-03
0: TRAIN [1][1210/1885]	Time 0.244 (0.143)	Data 2.26e-04 (3.77e-04)	Tok/s 105295 (95868)	Loss/tok 3.7569 (3.3950)	LR 2.800e-03
0: TRAIN [1][1220/1885]	Time 0.103 (0.143)	Data 2.23e-04 (3.76e-04)	Tok/s 86549 (95894)	Loss/tok 3.1792 (3.3952)	LR 2.800e-03
0: TRAIN [1][1230/1885]	Time 0.103 (0.143)	Data 2.21e-04 (3.74e-04)	Tok/s 87610 (95902)	Loss/tok 3.1642 (3.3954)	LR 2.800e-03
0: TRAIN [1][1240/1885]	Time 0.148 (0.143)	Data 2.21e-04 (3.73e-04)	Tok/s 99479 (95906)	Loss/tok 3.3017 (3.3950)	LR 2.800e-03
0: TRAIN [1][1250/1885]	Time 0.103 (0.143)	Data 2.28e-04 (3.72e-04)	Tok/s 89193 (95903)	Loss/tok 3.1385 (3.3949)	LR 2.800e-03
0: TRAIN [1][1260/1885]	Time 0.195 (0.143)	Data 2.21e-04 (3.71e-04)	Tok/s 105527 (95914)	Loss/tok 3.4136 (3.3947)	LR 2.800e-03
0: TRAIN [1][1270/1885]	Time 0.103 (0.143)	Data 2.19e-04 (3.69e-04)	Tok/s 87895 (95903)	Loss/tok 3.0619 (3.3945)	LR 2.800e-03
0: TRAIN [1][1280/1885]	Time 0.192 (0.143)	Data 2.20e-04 (3.68e-04)	Tok/s 106628 (95900)	Loss/tok 3.5330 (3.3942)	LR 2.800e-03
0: TRAIN [1][1290/1885]	Time 0.195 (0.143)	Data 2.03e-04 (3.67e-04)	Tok/s 106300 (95899)	Loss/tok 3.3608 (3.3938)	LR 2.800e-03
0: TRAIN [1][1300/1885]	Time 0.060 (0.143)	Data 2.22e-04 (3.66e-04)	Tok/s 75462 (95889)	Loss/tok 2.6319 (3.3933)	LR 2.800e-03
0: TRAIN [1][1310/1885]	Time 0.060 (0.143)	Data 2.38e-04 (3.65e-04)	Tok/s 77502 (95863)	Loss/tok 2.7341 (3.3929)	LR 2.800e-03
0: TRAIN [1][1320/1885]	Time 0.246 (0.143)	Data 2.23e-04 (3.63e-04)	Tok/s 107331 (95883)	Loss/tok 3.6754 (3.3937)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1330/1885]	Time 0.194 (0.143)	Data 2.22e-04 (3.62e-04)	Tok/s 105473 (95930)	Loss/tok 3.4297 (3.3944)	LR 2.800e-03
0: TRAIN [1][1340/1885]	Time 0.148 (0.143)	Data 1.68e-04 (3.61e-04)	Tok/s 98778 (95925)	Loss/tok 3.4171 (3.3939)	LR 2.800e-03
0: TRAIN [1][1350/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.60e-04)	Tok/s 90693 (95926)	Loss/tok 3.0954 (3.3935)	LR 2.800e-03
0: TRAIN [1][1360/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.59e-04)	Tok/s 100410 (95925)	Loss/tok 3.2887 (3.3930)	LR 2.800e-03
0: TRAIN [1][1370/1885]	Time 0.195 (0.143)	Data 2.19e-04 (3.58e-04)	Tok/s 103948 (95954)	Loss/tok 3.5591 (3.3934)	LR 2.800e-03
0: TRAIN [1][1380/1885]	Time 0.245 (0.143)	Data 1.82e-04 (3.57e-04)	Tok/s 107531 (95929)	Loss/tok 3.5746 (3.3926)	LR 2.800e-03
0: TRAIN [1][1390/1885]	Time 0.103 (0.143)	Data 2.20e-04 (3.56e-04)	Tok/s 87352 (95936)	Loss/tok 3.1835 (3.3920)	LR 2.800e-03
0: TRAIN [1][1400/1885]	Time 0.103 (0.143)	Data 2.21e-04 (3.55e-04)	Tok/s 88495 (95921)	Loss/tok 3.0200 (3.3916)	LR 2.800e-03
0: TRAIN [1][1410/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.54e-04)	Tok/s 105146 (95940)	Loss/tok 3.4382 (3.3918)	LR 2.800e-03
0: TRAIN [1][1420/1885]	Time 0.148 (0.143)	Data 2.20e-04 (3.53e-04)	Tok/s 98526 (95943)	Loss/tok 3.3049 (3.3913)	LR 2.800e-03
0: TRAIN [1][1430/1885]	Time 0.195 (0.143)	Data 2.21e-04 (3.52e-04)	Tok/s 104643 (95959)	Loss/tok 3.5352 (3.3914)	LR 2.800e-03
0: TRAIN [1][1440/1885]	Time 0.059 (0.143)	Data 2.22e-04 (3.51e-04)	Tok/s 78930 (95933)	Loss/tok 2.6046 (3.3911)	LR 2.800e-03
0: TRAIN [1][1450/1885]	Time 0.148 (0.144)	Data 2.31e-04 (3.50e-04)	Tok/s 99489 (95970)	Loss/tok 3.2439 (3.3917)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1460/1885]	Time 0.103 (0.144)	Data 2.23e-04 (3.49e-04)	Tok/s 87110 (95992)	Loss/tok 3.1098 (3.3918)	LR 2.800e-03
0: TRAIN [1][1470/1885]	Time 0.194 (0.144)	Data 2.24e-04 (3.48e-04)	Tok/s 104158 (96017)	Loss/tok 3.5465 (3.3920)	LR 2.800e-03
0: TRAIN [1][1480/1885]	Time 0.195 (0.144)	Data 2.21e-04 (3.48e-04)	Tok/s 104884 (96029)	Loss/tok 3.4939 (3.3918)	LR 2.800e-03
0: TRAIN [1][1490/1885]	Time 0.147 (0.144)	Data 2.20e-04 (3.47e-04)	Tok/s 99304 (96031)	Loss/tok 3.2679 (3.3917)	LR 2.800e-03
0: TRAIN [1][1500/1885]	Time 0.148 (0.144)	Data 2.23e-04 (3.46e-04)	Tok/s 99606 (96026)	Loss/tok 3.2054 (3.3911)	LR 2.800e-03
0: TRAIN [1][1510/1885]	Time 0.060 (0.144)	Data 2.21e-04 (3.45e-04)	Tok/s 77113 (95994)	Loss/tok 2.5354 (3.3904)	LR 2.800e-03
0: TRAIN [1][1520/1885]	Time 0.149 (0.144)	Data 2.19e-04 (3.44e-04)	Tok/s 97327 (95986)	Loss/tok 3.2867 (3.3898)	LR 2.800e-03
0: TRAIN [1][1530/1885]	Time 0.147 (0.143)	Data 2.05e-04 (3.43e-04)	Tok/s 99698 (95974)	Loss/tok 3.2631 (3.3889)	LR 2.800e-03
0: TRAIN [1][1540/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.42e-04)	Tok/s 89081 (95935)	Loss/tok 3.1775 (3.3880)	LR 2.800e-03
0: TRAIN [1][1550/1885]	Time 0.148 (0.143)	Data 1.08e-04 (3.42e-04)	Tok/s 98266 (95943)	Loss/tok 3.4251 (3.3875)	LR 2.800e-03
0: TRAIN [1][1560/1885]	Time 0.147 (0.143)	Data 1.08e-04 (3.40e-04)	Tok/s 99003 (95937)	Loss/tok 3.3988 (3.3872)	LR 2.800e-03
0: TRAIN [1][1570/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.39e-04)	Tok/s 100976 (95957)	Loss/tok 3.3172 (3.3872)	LR 2.800e-03
0: TRAIN [1][1580/1885]	Time 0.149 (0.143)	Data 2.40e-04 (3.38e-04)	Tok/s 98822 (95966)	Loss/tok 3.2419 (3.3869)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1590/1885]	Time 0.246 (0.143)	Data 2.06e-04 (3.37e-04)	Tok/s 107293 (95976)	Loss/tok 3.7630 (3.3869)	LR 2.800e-03
0: TRAIN [1][1600/1885]	Time 0.246 (0.143)	Data 2.33e-04 (3.36e-04)	Tok/s 106540 (95976)	Loss/tok 3.6335 (3.3868)	LR 2.800e-03
0: TRAIN [1][1610/1885]	Time 0.103 (0.144)	Data 2.20e-04 (3.36e-04)	Tok/s 88678 (96009)	Loss/tok 3.0389 (3.3868)	LR 2.800e-03
0: TRAIN [1][1620/1885]	Time 0.104 (0.144)	Data 2.17e-04 (3.35e-04)	Tok/s 86284 (95991)	Loss/tok 3.2234 (3.3863)	LR 2.800e-03
0: TRAIN [1][1630/1885]	Time 0.100 (0.144)	Data 2.25e-04 (3.34e-04)	Tok/s 91086 (96002)	Loss/tok 3.0202 (3.3859)	LR 2.800e-03
0: TRAIN [1][1640/1885]	Time 0.102 (0.143)	Data 1.53e-04 (3.33e-04)	Tok/s 90095 (95982)	Loss/tok 3.0459 (3.3850)	LR 2.800e-03
0: TRAIN [1][1650/1885]	Time 0.195 (0.144)	Data 1.91e-04 (3.33e-04)	Tok/s 103330 (96011)	Loss/tok 3.6075 (3.3852)	LR 2.800e-03
0: TRAIN [1][1660/1885]	Time 0.102 (0.144)	Data 2.22e-04 (3.32e-04)	Tok/s 88945 (96014)	Loss/tok 3.0935 (3.3849)	LR 2.800e-03
0: TRAIN [1][1670/1885]	Time 0.192 (0.144)	Data 2.22e-04 (3.31e-04)	Tok/s 107170 (96012)	Loss/tok 3.4114 (3.3844)	LR 2.800e-03
0: TRAIN [1][1680/1885]	Time 0.103 (0.144)	Data 8.54e-05 (3.31e-04)	Tok/s 88081 (96007)	Loss/tok 3.0002 (3.3838)	LR 2.800e-03
0: TRAIN [1][1690/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.30e-04)	Tok/s 88645 (95988)	Loss/tok 3.0353 (3.3833)	LR 2.800e-03
0: TRAIN [1][1700/1885]	Time 0.104 (0.144)	Data 2.19e-04 (3.29e-04)	Tok/s 87223 (95996)	Loss/tok 3.1178 (3.3837)	LR 2.800e-03
0: TRAIN [1][1710/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.28e-04)	Tok/s 87059 (95984)	Loss/tok 3.1011 (3.3832)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1720/1885]	Time 0.148 (0.143)	Data 2.20e-04 (3.28e-04)	Tok/s 100378 (95996)	Loss/tok 3.3897 (3.3829)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1730/1885]	Time 0.145 (0.144)	Data 2.21e-04 (3.27e-04)	Tok/s 101442 (96008)	Loss/tok 3.3468 (3.3832)	LR 2.800e-03
0: TRAIN [1][1740/1885]	Time 0.145 (0.143)	Data 2.19e-04 (3.26e-04)	Tok/s 99452 (95994)	Loss/tok 3.2751 (3.3824)	LR 2.800e-03
0: TRAIN [1][1750/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.26e-04)	Tok/s 105148 (95996)	Loss/tok 3.4624 (3.3819)	LR 2.800e-03
0: TRAIN [1][1760/1885]	Time 0.146 (0.144)	Data 2.24e-04 (3.25e-04)	Tok/s 98969 (96020)	Loss/tok 3.3223 (3.3822)	LR 2.800e-03
0: TRAIN [1][1770/1885]	Time 0.148 (0.144)	Data 2.05e-04 (3.25e-04)	Tok/s 99060 (96019)	Loss/tok 3.2032 (3.3817)	LR 2.800e-03
0: TRAIN [1][1780/1885]	Time 0.148 (0.143)	Data 2.20e-04 (3.24e-04)	Tok/s 98456 (95986)	Loss/tok 3.3199 (3.3809)	LR 2.800e-03
0: TRAIN [1][1790/1885]	Time 0.103 (0.143)	Data 2.19e-04 (3.23e-04)	Tok/s 90726 (95989)	Loss/tok 3.0466 (3.3806)	LR 2.800e-03
0: TRAIN [1][1800/1885]	Time 0.103 (0.143)	Data 2.25e-04 (3.23e-04)	Tok/s 88563 (95980)	Loss/tok 3.0982 (3.3800)	LR 2.800e-03
0: TRAIN [1][1810/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.22e-04)	Tok/s 89711 (95968)	Loss/tok 3.0377 (3.3795)	LR 2.800e-03
0: TRAIN [1][1820/1885]	Time 0.149 (0.143)	Data 2.19e-04 (3.22e-04)	Tok/s 97395 (95995)	Loss/tok 3.2790 (3.3799)	LR 2.800e-03
0: TRAIN [1][1830/1885]	Time 0.245 (0.144)	Data 2.20e-04 (3.21e-04)	Tok/s 107427 (96017)	Loss/tok 3.6390 (3.3798)	LR 2.800e-03
0: TRAIN [1][1840/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.20e-04)	Tok/s 99733 (96016)	Loss/tok 3.2430 (3.3792)	LR 2.800e-03
0: TRAIN [1][1850/1885]	Time 0.149 (0.143)	Data 1.88e-04 (3.20e-04)	Tok/s 98760 (96006)	Loss/tok 3.4309 (3.3787)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1860/1885]	Time 0.244 (0.144)	Data 2.24e-04 (3.19e-04)	Tok/s 108773 (96016)	Loss/tok 3.4937 (3.3786)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1870/1885]	Time 0.103 (0.143)	Data 2.23e-04 (3.19e-04)	Tok/s 86412 (95999)	Loss/tok 3.0308 (3.3781)	LR 2.800e-03
0: TRAIN [1][1880/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.18e-04)	Tok/s 89652 (96006)	Loss/tok 3.0640 (3.3778)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593838459643, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593838459644, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.677 (0.677)	Decoder iters 149.0 (149.0)	Tok/s 24133 (24133)
0: Running moses detokenizer
0: BLEU(score=22.05529667812641, counts=[35670, 17061, 9410, 5408], totals=[64744, 61741, 58739, 55742], precisions=[55.09390831582849, 27.63317730519428, 16.02002076984627, 9.70184062286965], bp=1.0, sys_len=64744, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593838461295, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2206, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593838461296, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3792	Test BLEU: 22.06
0: Performance: Epoch: 1	Training: 767909 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593838461296, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593838461296, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593838461296, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1746317358
0: TRAIN [2][0/1885]	Time 0.447 (0.447)	Data 2.08e-01 (2.08e-01)	Tok/s 45416 (45416)	Loss/tok 3.4589 (3.4589)	LR 2.800e-03
0: TRAIN [2][10/1885]	Time 0.247 (0.184)	Data 2.20e-04 (1.91e-02)	Tok/s 106346 (93168)	Loss/tok 3.3942 (3.2942)	LR 2.800e-03
0: TRAIN [2][20/1885]	Time 0.147 (0.164)	Data 2.25e-04 (1.01e-02)	Tok/s 100395 (95424)	Loss/tok 3.2642 (3.2683)	LR 2.800e-03
0: TRAIN [2][30/1885]	Time 0.245 (0.155)	Data 2.40e-04 (6.93e-03)	Tok/s 106893 (95006)	Loss/tok 3.5681 (3.2663)	LR 2.800e-03
0: TRAIN [2][40/1885]	Time 0.195 (0.149)	Data 2.21e-04 (5.29e-03)	Tok/s 104657 (94976)	Loss/tok 3.3767 (3.2514)	LR 2.800e-03
0: TRAIN [2][50/1885]	Time 0.101 (0.146)	Data 2.21e-04 (4.30e-03)	Tok/s 90121 (95445)	Loss/tok 3.0190 (3.2420)	LR 2.800e-03
0: TRAIN [2][60/1885]	Time 0.245 (0.148)	Data 2.22e-04 (3.63e-03)	Tok/s 107794 (95718)	Loss/tok 3.4826 (3.2577)	LR 2.800e-03
0: TRAIN [2][70/1885]	Time 0.147 (0.148)	Data 2.23e-04 (3.15e-03)	Tok/s 100943 (95905)	Loss/tok 3.1512 (3.2522)	LR 2.800e-03
0: TRAIN [2][80/1885]	Time 0.195 (0.146)	Data 1.94e-04 (2.79e-03)	Tok/s 104134 (95860)	Loss/tok 3.4027 (3.2482)	LR 2.800e-03
0: TRAIN [2][90/1885]	Time 0.102 (0.145)	Data 2.16e-04 (2.51e-03)	Tok/s 89494 (95883)	Loss/tok 3.1056 (3.2493)	LR 2.800e-03
0: TRAIN [2][100/1885]	Time 0.195 (0.146)	Data 2.05e-04 (2.28e-03)	Tok/s 104592 (96208)	Loss/tok 3.4659 (3.2567)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][110/1885]	Time 0.195 (0.149)	Data 2.23e-04 (2.09e-03)	Tok/s 104520 (96660)	Loss/tok 3.4695 (3.2651)	LR 2.800e-03
0: TRAIN [2][120/1885]	Time 0.102 (0.146)	Data 2.18e-04 (1.94e-03)	Tok/s 88804 (96268)	Loss/tok 2.9015 (3.2573)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][130/1885]	Time 0.101 (0.144)	Data 2.22e-04 (1.81e-03)	Tok/s 90056 (95814)	Loss/tok 3.1667 (3.2501)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][140/1885]	Time 0.061 (0.142)	Data 2.19e-04 (1.70e-03)	Tok/s 75539 (95568)	Loss/tok 2.6563 (3.2485)	LR 2.800e-03
0: TRAIN [2][150/1885]	Time 0.147 (0.146)	Data 2.24e-04 (1.60e-03)	Tok/s 101414 (96122)	Loss/tok 3.2990 (3.2639)	LR 2.800e-03
0: TRAIN [2][160/1885]	Time 0.102 (0.147)	Data 2.19e-04 (1.51e-03)	Tok/s 87986 (96288)	Loss/tok 2.9800 (3.2681)	LR 2.800e-03
0: TRAIN [2][170/1885]	Time 0.147 (0.147)	Data 2.22e-04 (1.44e-03)	Tok/s 100243 (96344)	Loss/tok 3.3153 (3.2668)	LR 2.800e-03
0: TRAIN [2][180/1885]	Time 0.146 (0.147)	Data 2.20e-04 (1.37e-03)	Tok/s 98458 (96491)	Loss/tok 3.3172 (3.2682)	LR 2.800e-03
0: TRAIN [2][190/1885]	Time 0.146 (0.148)	Data 2.22e-04 (1.31e-03)	Tok/s 100325 (96631)	Loss/tok 3.1792 (3.2703)	LR 2.800e-03
0: TRAIN [2][200/1885]	Time 0.148 (0.147)	Data 2.19e-04 (1.26e-03)	Tok/s 100995 (96554)	Loss/tok 3.1287 (3.2675)	LR 2.800e-03
0: TRAIN [2][210/1885]	Time 0.146 (0.146)	Data 2.19e-04 (1.21e-03)	Tok/s 101062 (96581)	Loss/tok 3.2421 (3.2645)	LR 2.800e-03
0: TRAIN [2][220/1885]	Time 0.102 (0.147)	Data 2.15e-04 (1.16e-03)	Tok/s 89199 (96768)	Loss/tok 2.9845 (3.2665)	LR 2.800e-03
0: TRAIN [2][230/1885]	Time 0.194 (0.147)	Data 2.19e-04 (1.12e-03)	Tok/s 105256 (96742)	Loss/tok 3.3335 (3.2668)	LR 2.800e-03
0: TRAIN [2][240/1885]	Time 0.102 (0.147)	Data 2.16e-04 (1.09e-03)	Tok/s 87089 (96772)	Loss/tok 2.9024 (3.2657)	LR 2.800e-03
0: TRAIN [2][250/1885]	Time 0.194 (0.146)	Data 2.21e-04 (1.05e-03)	Tok/s 105161 (96784)	Loss/tok 3.3421 (3.2629)	LR 2.800e-03
0: TRAIN [2][260/1885]	Time 0.194 (0.147)	Data 2.21e-04 (1.02e-03)	Tok/s 104392 (96813)	Loss/tok 3.3408 (3.2658)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][270/1885]	Time 0.101 (0.148)	Data 2.20e-04 (9.91e-04)	Tok/s 89880 (96943)	Loss/tok 3.0168 (3.2698)	LR 2.800e-03
0: TRAIN [2][280/1885]	Time 0.147 (0.147)	Data 2.20e-04 (9.64e-04)	Tok/s 99864 (96847)	Loss/tok 3.2098 (3.2671)	LR 2.800e-03
0: TRAIN [2][290/1885]	Time 0.060 (0.147)	Data 2.19e-04 (9.38e-04)	Tok/s 76389 (96967)	Loss/tok 2.4910 (3.2707)	LR 2.800e-03
0: TRAIN [2][300/1885]	Time 0.147 (0.147)	Data 2.21e-04 (9.14e-04)	Tok/s 100078 (96903)	Loss/tok 3.1068 (3.2695)	LR 2.800e-03
0: TRAIN [2][310/1885]	Time 0.147 (0.147)	Data 3.00e-04 (8.92e-04)	Tok/s 97619 (96933)	Loss/tok 3.2154 (3.2694)	LR 2.800e-03
0: TRAIN [2][320/1885]	Time 0.146 (0.148)	Data 2.20e-04 (8.71e-04)	Tok/s 99248 (97022)	Loss/tok 3.2698 (3.2718)	LR 2.800e-03
0: TRAIN [2][330/1885]	Time 0.146 (0.148)	Data 2.74e-04 (8.52e-04)	Tok/s 100248 (97103)	Loss/tok 3.1856 (3.2723)	LR 2.800e-03
0: TRAIN [2][340/1885]	Time 0.147 (0.147)	Data 2.20e-04 (8.34e-04)	Tok/s 100273 (97047)	Loss/tok 3.2654 (3.2716)	LR 2.800e-03
0: TRAIN [2][350/1885]	Time 0.147 (0.147)	Data 2.17e-04 (8.16e-04)	Tok/s 100920 (97056)	Loss/tok 3.1576 (3.2702)	LR 2.800e-03
0: TRAIN [2][360/1885]	Time 0.100 (0.147)	Data 2.19e-04 (8.00e-04)	Tok/s 89802 (97125)	Loss/tok 3.0415 (3.2723)	LR 2.800e-03
0: TRAIN [2][370/1885]	Time 0.194 (0.148)	Data 2.19e-04 (7.84e-04)	Tok/s 105032 (97156)	Loss/tok 3.3644 (3.2725)	LR 2.800e-03
0: TRAIN [2][380/1885]	Time 0.194 (0.148)	Data 2.25e-04 (7.70e-04)	Tok/s 104878 (97190)	Loss/tok 3.4936 (3.2736)	LR 2.800e-03
0: TRAIN [2][390/1885]	Time 0.147 (0.148)	Data 2.23e-04 (7.56e-04)	Tok/s 100130 (97213)	Loss/tok 3.1860 (3.2724)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][400/1885]	Time 0.147 (0.148)	Data 2.24e-04 (7.43e-04)	Tok/s 99083 (97197)	Loss/tok 3.1441 (3.2701)	LR 2.800e-03
0: TRAIN [2][410/1885]	Time 0.195 (0.148)	Data 2.20e-04 (7.30e-04)	Tok/s 104805 (97232)	Loss/tok 3.3426 (3.2696)	LR 2.800e-03
0: TRAIN [2][420/1885]	Time 0.195 (0.148)	Data 2.20e-04 (7.18e-04)	Tok/s 104777 (97258)	Loss/tok 3.2922 (3.2683)	LR 2.800e-03
0: TRAIN [2][430/1885]	Time 0.147 (0.148)	Data 2.28e-04 (7.07e-04)	Tok/s 99607 (97282)	Loss/tok 3.0812 (3.2683)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][440/1885]	Time 0.101 (0.148)	Data 2.19e-04 (6.96e-04)	Tok/s 90657 (97313)	Loss/tok 2.9944 (3.2702)	LR 2.800e-03
0: TRAIN [2][450/1885]	Time 0.101 (0.148)	Data 2.21e-04 (6.86e-04)	Tok/s 87576 (97311)	Loss/tok 2.9143 (3.2700)	LR 2.800e-03
0: TRAIN [2][460/1885]	Time 0.147 (0.148)	Data 2.37e-04 (6.75e-04)	Tok/s 100986 (97245)	Loss/tok 3.2961 (3.2687)	LR 2.800e-03
0: TRAIN [2][470/1885]	Time 0.147 (0.148)	Data 2.24e-04 (6.66e-04)	Tok/s 99339 (97316)	Loss/tok 3.2863 (3.2685)	LR 2.800e-03
0: TRAIN [2][480/1885]	Time 0.102 (0.148)	Data 2.20e-04 (6.57e-04)	Tok/s 89749 (97295)	Loss/tok 3.0192 (3.2665)	LR 2.800e-03
0: TRAIN [2][490/1885]	Time 0.146 (0.148)	Data 2.20e-04 (6.48e-04)	Tok/s 99506 (97313)	Loss/tok 3.2454 (3.2672)	LR 2.800e-03
0: TRAIN [2][500/1885]	Time 0.101 (0.148)	Data 2.21e-04 (6.39e-04)	Tok/s 92264 (97315)	Loss/tok 3.0633 (3.2670)	LR 2.800e-03
0: TRAIN [2][510/1885]	Time 0.193 (0.148)	Data 2.22e-04 (6.31e-04)	Tok/s 104602 (97335)	Loss/tok 3.3561 (3.2661)	LR 2.800e-03
0: TRAIN [2][520/1885]	Time 0.147 (0.148)	Data 2.22e-04 (6.23e-04)	Tok/s 99283 (97401)	Loss/tok 3.2484 (3.2681)	LR 2.800e-03
0: TRAIN [2][530/1885]	Time 0.195 (0.148)	Data 2.19e-04 (6.16e-04)	Tok/s 106080 (97422)	Loss/tok 3.3247 (3.2680)	LR 2.800e-03
0: TRAIN [2][540/1885]	Time 0.102 (0.148)	Data 2.21e-04 (6.08e-04)	Tok/s 87164 (97360)	Loss/tok 2.9180 (3.2665)	LR 2.800e-03
0: TRAIN [2][550/1885]	Time 0.147 (0.148)	Data 2.19e-04 (6.01e-04)	Tok/s 97973 (97324)	Loss/tok 3.2786 (3.2661)	LR 2.800e-03
0: TRAIN [2][560/1885]	Time 0.147 (0.147)	Data 2.20e-04 (5.95e-04)	Tok/s 99582 (97287)	Loss/tok 3.1669 (3.2648)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][570/1885]	Time 0.247 (0.147)	Data 2.23e-04 (5.88e-04)	Tok/s 106041 (97307)	Loss/tok 3.4955 (3.2647)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][580/1885]	Time 0.246 (0.147)	Data 2.25e-04 (5.82e-04)	Tok/s 106972 (97288)	Loss/tok 3.4829 (3.2645)	LR 2.800e-03
0: TRAIN [2][590/1885]	Time 0.147 (0.147)	Data 2.21e-04 (5.76e-04)	Tok/s 98870 (97259)	Loss/tok 3.1156 (3.2623)	LR 2.800e-03
0: TRAIN [2][600/1885]	Time 0.195 (0.148)	Data 2.20e-04 (5.70e-04)	Tok/s 102968 (97314)	Loss/tok 3.3515 (3.2653)	LR 2.800e-03
0: TRAIN [2][610/1885]	Time 0.148 (0.148)	Data 2.32e-04 (5.64e-04)	Tok/s 98344 (97329)	Loss/tok 3.1701 (3.2650)	LR 2.800e-03
0: TRAIN [2][620/1885]	Time 0.194 (0.148)	Data 2.16e-04 (5.58e-04)	Tok/s 105867 (97307)	Loss/tok 3.4536 (3.2656)	LR 2.800e-03
0: TRAIN [2][630/1885]	Time 0.146 (0.148)	Data 2.16e-04 (5.53e-04)	Tok/s 101746 (97313)	Loss/tok 3.2240 (3.2656)	LR 2.800e-03
0: TRAIN [2][640/1885]	Time 0.243 (0.147)	Data 2.23e-04 (5.48e-04)	Tok/s 109367 (97258)	Loss/tok 3.4680 (3.2649)	LR 2.800e-03
0: TRAIN [2][650/1885]	Time 0.102 (0.147)	Data 2.23e-04 (5.43e-04)	Tok/s 89663 (97215)	Loss/tok 3.0600 (3.2641)	LR 2.800e-03
0: TRAIN [2][660/1885]	Time 0.193 (0.147)	Data 2.20e-04 (5.38e-04)	Tok/s 104931 (97174)	Loss/tok 3.3621 (3.2635)	LR 2.800e-03
0: TRAIN [2][670/1885]	Time 0.194 (0.147)	Data 2.22e-04 (5.33e-04)	Tok/s 103901 (97155)	Loss/tok 3.3660 (3.2627)	LR 2.800e-03
0: TRAIN [2][680/1885]	Time 0.102 (0.146)	Data 2.32e-04 (5.28e-04)	Tok/s 90031 (97114)	Loss/tok 2.9652 (3.2618)	LR 2.800e-03
0: TRAIN [2][690/1885]	Time 0.147 (0.146)	Data 2.21e-04 (5.24e-04)	Tok/s 100311 (97082)	Loss/tok 3.2653 (3.2614)	LR 2.800e-03
0: TRAIN [2][700/1885]	Time 0.245 (0.146)	Data 2.16e-04 (5.20e-04)	Tok/s 107135 (97125)	Loss/tok 3.5839 (3.2616)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][710/1885]	Time 0.246 (0.146)	Data 2.20e-04 (5.15e-04)	Tok/s 105932 (97138)	Loss/tok 3.6871 (3.2619)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][720/1885]	Time 0.146 (0.146)	Data 2.21e-04 (5.11e-04)	Tok/s 101456 (97167)	Loss/tok 3.1013 (3.2624)	LR 2.800e-03
0: TRAIN [2][730/1885]	Time 0.100 (0.146)	Data 2.18e-04 (5.07e-04)	Tok/s 90794 (97124)	Loss/tok 2.9844 (3.2625)	LR 2.800e-03
0: TRAIN [2][740/1885]	Time 0.060 (0.146)	Data 2.23e-04 (5.03e-04)	Tok/s 76343 (97118)	Loss/tok 2.5632 (3.2634)	LR 2.800e-03
0: TRAIN [2][750/1885]	Time 0.148 (0.146)	Data 2.21e-04 (5.00e-04)	Tok/s 99145 (97180)	Loss/tok 3.1736 (3.2641)	LR 2.800e-03
0: TRAIN [2][760/1885]	Time 0.102 (0.146)	Data 2.17e-04 (4.96e-04)	Tok/s 90115 (97168)	Loss/tok 2.9494 (3.2635)	LR 2.800e-03
0: TRAIN [2][770/1885]	Time 0.194 (0.147)	Data 2.20e-04 (4.92e-04)	Tok/s 106507 (97191)	Loss/tok 3.2911 (3.2644)	LR 2.800e-03
0: TRAIN [2][780/1885]	Time 0.102 (0.146)	Data 2.18e-04 (4.89e-04)	Tok/s 90208 (97164)	Loss/tok 2.9326 (3.2640)	LR 2.800e-03
0: TRAIN [2][790/1885]	Time 0.101 (0.146)	Data 2.16e-04 (4.85e-04)	Tok/s 89294 (97133)	Loss/tok 2.9738 (3.2632)	LR 2.800e-03
0: TRAIN [2][800/1885]	Time 0.146 (0.146)	Data 1.92e-04 (4.82e-04)	Tok/s 101075 (97142)	Loss/tok 3.1779 (3.2637)	LR 2.800e-03
0: TRAIN [2][810/1885]	Time 0.102 (0.146)	Data 2.19e-04 (4.79e-04)	Tok/s 89372 (97143)	Loss/tok 3.0129 (3.2630)	LR 2.800e-03
0: TRAIN [2][820/1885]	Time 0.244 (0.146)	Data 2.19e-04 (4.75e-04)	Tok/s 106128 (97158)	Loss/tok 3.5944 (3.2640)	LR 2.800e-03
0: TRAIN [2][830/1885]	Time 0.146 (0.146)	Data 1.88e-04 (4.72e-04)	Tok/s 100191 (97116)	Loss/tok 3.1729 (3.2628)	LR 2.800e-03
0: TRAIN [2][840/1885]	Time 0.146 (0.146)	Data 2.20e-04 (4.69e-04)	Tok/s 100207 (97091)	Loss/tok 3.2434 (3.2623)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][850/1885]	Time 0.193 (0.146)	Data 2.20e-04 (4.66e-04)	Tok/s 105834 (97076)	Loss/tok 3.2907 (3.2621)	LR 2.800e-03
0: TRAIN [2][860/1885]	Time 0.146 (0.146)	Data 2.01e-04 (4.63e-04)	Tok/s 99794 (97094)	Loss/tok 3.2141 (3.2616)	LR 2.800e-03
0: TRAIN [2][870/1885]	Time 0.101 (0.146)	Data 2.16e-04 (4.60e-04)	Tok/s 88956 (97111)	Loss/tok 2.9693 (3.2617)	LR 2.800e-03
0: TRAIN [2][880/1885]	Time 0.246 (0.146)	Data 2.20e-04 (4.57e-04)	Tok/s 105326 (97059)	Loss/tok 3.5841 (3.2619)	LR 2.800e-03
0: TRAIN [2][890/1885]	Time 0.146 (0.145)	Data 2.19e-04 (4.54e-04)	Tok/s 100657 (97021)	Loss/tok 3.3231 (3.2605)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][900/1885]	Time 0.244 (0.145)	Data 2.20e-04 (4.52e-04)	Tok/s 107866 (97010)	Loss/tok 3.4713 (3.2605)	LR 2.800e-03
0: TRAIN [2][910/1885]	Time 0.246 (0.146)	Data 1.51e-04 (4.49e-04)	Tok/s 108030 (97037)	Loss/tok 3.4509 (3.2612)	LR 2.800e-03
0: TRAIN [2][920/1885]	Time 0.059 (0.146)	Data 2.18e-04 (4.46e-04)	Tok/s 76107 (97054)	Loss/tok 2.5020 (3.2620)	LR 2.800e-03
0: TRAIN [2][930/1885]	Time 0.102 (0.145)	Data 2.21e-04 (4.44e-04)	Tok/s 89107 (96986)	Loss/tok 3.0336 (3.2604)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][940/1885]	Time 0.059 (0.145)	Data 2.20e-04 (4.41e-04)	Tok/s 77183 (96990)	Loss/tok 2.4676 (3.2611)	LR 2.800e-03
0: TRAIN [2][950/1885]	Time 0.147 (0.145)	Data 1.08e-04 (4.39e-04)	Tok/s 98908 (96979)	Loss/tok 3.2134 (3.2606)	LR 2.800e-03
0: TRAIN [2][960/1885]	Time 0.146 (0.145)	Data 2.19e-04 (4.36e-04)	Tok/s 100019 (96977)	Loss/tok 3.2414 (3.2603)	LR 2.800e-03
0: TRAIN [2][970/1885]	Time 0.101 (0.145)	Data 2.21e-04 (4.34e-04)	Tok/s 91918 (96999)	Loss/tok 2.9573 (3.2606)	LR 2.800e-03
0: TRAIN [2][980/1885]	Time 0.101 (0.145)	Data 2.19e-04 (4.32e-04)	Tok/s 91891 (96937)	Loss/tok 2.9436 (3.2595)	LR 2.800e-03
0: TRAIN [2][990/1885]	Time 0.102 (0.145)	Data 2.17e-04 (4.30e-04)	Tok/s 91342 (96885)	Loss/tok 3.1238 (3.2584)	LR 2.800e-03
0: TRAIN [2][1000/1885]	Time 0.147 (0.145)	Data 2.19e-04 (4.27e-04)	Tok/s 99034 (96883)	Loss/tok 3.1196 (3.2585)	LR 2.800e-03
0: TRAIN [2][1010/1885]	Time 0.147 (0.145)	Data 1.90e-04 (4.25e-04)	Tok/s 100903 (96915)	Loss/tok 3.2189 (3.2599)	LR 2.800e-03
0: TRAIN [2][1020/1885]	Time 0.195 (0.145)	Data 1.08e-04 (4.23e-04)	Tok/s 106279 (96934)	Loss/tok 3.4164 (3.2609)	LR 2.800e-03
0: TRAIN [2][1030/1885]	Time 0.195 (0.145)	Data 1.05e-04 (4.20e-04)	Tok/s 106046 (96927)	Loss/tok 3.3332 (3.2604)	LR 2.800e-03
0: TRAIN [2][1040/1885]	Time 0.102 (0.145)	Data 2.18e-04 (4.18e-04)	Tok/s 88250 (96922)	Loss/tok 3.0103 (3.2601)	LR 2.800e-03
0: TRAIN [2][1050/1885]	Time 0.147 (0.145)	Data 1.06e-04 (4.16e-04)	Tok/s 100612 (96940)	Loss/tok 3.2350 (3.2604)	LR 2.800e-03
0: TRAIN [2][1060/1885]	Time 0.060 (0.145)	Data 2.30e-04 (4.14e-04)	Tok/s 76336 (96930)	Loss/tok 2.5748 (3.2598)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1070/1885]	Time 0.101 (0.145)	Data 2.17e-04 (4.12e-04)	Tok/s 88838 (96902)	Loss/tok 2.9723 (3.2591)	LR 2.800e-03
0: TRAIN [2][1080/1885]	Time 0.101 (0.145)	Data 2.22e-04 (4.10e-04)	Tok/s 89198 (96918)	Loss/tok 3.0756 (3.2598)	LR 2.800e-03
0: TRAIN [2][1090/1885]	Time 0.146 (0.145)	Data 1.76e-04 (4.08e-04)	Tok/s 100393 (96909)	Loss/tok 3.2447 (3.2594)	LR 2.800e-03
0: TRAIN [2][1100/1885]	Time 0.102 (0.145)	Data 1.23e-04 (4.06e-04)	Tok/s 89231 (96917)	Loss/tok 2.9437 (3.2595)	LR 2.800e-03
0: TRAIN [2][1110/1885]	Time 0.193 (0.145)	Data 1.08e-04 (4.04e-04)	Tok/s 105318 (96913)	Loss/tok 3.4665 (3.2590)	LR 2.800e-03
0: TRAIN [2][1120/1885]	Time 0.101 (0.145)	Data 2.16e-04 (4.02e-04)	Tok/s 90159 (96926)	Loss/tok 3.1047 (3.2595)	LR 2.800e-03
0: TRAIN [2][1130/1885]	Time 0.147 (0.145)	Data 2.25e-04 (4.01e-04)	Tok/s 100628 (96917)	Loss/tok 3.2532 (3.2591)	LR 2.800e-03
0: TRAIN [2][1140/1885]	Time 0.059 (0.145)	Data 2.17e-04 (3.99e-04)	Tok/s 78731 (96886)	Loss/tok 2.6390 (3.2586)	LR 2.800e-03
0: TRAIN [2][1150/1885]	Time 0.245 (0.145)	Data 1.94e-04 (3.97e-04)	Tok/s 107566 (96920)	Loss/tok 3.4243 (3.2594)	LR 2.800e-03
0: TRAIN [2][1160/1885]	Time 0.101 (0.145)	Data 1.90e-04 (3.96e-04)	Tok/s 88786 (96891)	Loss/tok 3.1271 (3.2586)	LR 2.800e-03
0: TRAIN [2][1170/1885]	Time 0.245 (0.145)	Data 1.89e-04 (3.94e-04)	Tok/s 106050 (96902)	Loss/tok 3.5729 (3.2589)	LR 2.800e-03
0: TRAIN [2][1180/1885]	Time 0.147 (0.145)	Data 1.60e-04 (3.92e-04)	Tok/s 99163 (96873)	Loss/tok 3.2099 (3.2579)	LR 2.800e-03
0: TRAIN [2][1190/1885]	Time 0.059 (0.144)	Data 2.33e-04 (3.91e-04)	Tok/s 75763 (96846)	Loss/tok 2.4784 (3.2572)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1200/1885]	Time 0.146 (0.145)	Data 2.16e-04 (3.89e-04)	Tok/s 100408 (96865)	Loss/tok 3.3037 (3.2578)	LR 2.800e-03
0: TRAIN [2][1210/1885]	Time 0.060 (0.144)	Data 2.16e-04 (3.88e-04)	Tok/s 75684 (96817)	Loss/tok 2.4783 (3.2575)	LR 2.800e-03
0: TRAIN [2][1220/1885]	Time 0.194 (0.144)	Data 2.22e-04 (3.86e-04)	Tok/s 105466 (96817)	Loss/tok 3.3541 (3.2574)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1230/1885]	Time 0.147 (0.144)	Data 1.58e-04 (3.85e-04)	Tok/s 98068 (96784)	Loss/tok 3.2878 (3.2568)	LR 2.800e-03
0: TRAIN [2][1240/1885]	Time 0.101 (0.144)	Data 2.19e-04 (3.83e-04)	Tok/s 89749 (96778)	Loss/tok 3.0410 (3.2565)	LR 2.800e-03
0: TRAIN [2][1250/1885]	Time 0.101 (0.144)	Data 2.17e-04 (3.82e-04)	Tok/s 91491 (96737)	Loss/tok 2.9287 (3.2560)	LR 2.800e-03
0: TRAIN [2][1260/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.81e-04)	Tok/s 100946 (96736)	Loss/tok 3.0901 (3.2560)	LR 2.800e-03
0: TRAIN [2][1270/1885]	Time 0.147 (0.144)	Data 1.89e-04 (3.79e-04)	Tok/s 100099 (96773)	Loss/tok 3.2353 (3.2563)	LR 2.800e-03
0: TRAIN [2][1280/1885]	Time 0.102 (0.144)	Data 2.16e-04 (3.78e-04)	Tok/s 88237 (96758)	Loss/tok 2.9067 (3.2556)	LR 2.800e-03
0: TRAIN [2][1290/1885]	Time 0.148 (0.144)	Data 2.01e-04 (3.77e-04)	Tok/s 99144 (96765)	Loss/tok 3.1800 (3.2550)	LR 2.800e-03
0: TRAIN [2][1300/1885]	Time 0.244 (0.144)	Data 2.25e-04 (3.75e-04)	Tok/s 106219 (96775)	Loss/tok 3.5163 (3.2553)	LR 2.800e-03
0: TRAIN [2][1310/1885]	Time 0.195 (0.144)	Data 1.81e-04 (3.74e-04)	Tok/s 104564 (96726)	Loss/tok 3.4419 (3.2544)	LR 2.800e-03
0: TRAIN [2][1320/1885]	Time 0.147 (0.144)	Data 1.91e-04 (3.73e-04)	Tok/s 98906 (96706)	Loss/tok 3.1604 (3.2535)	LR 2.800e-03
0: TRAIN [2][1330/1885]	Time 0.147 (0.144)	Data 2.19e-04 (3.72e-04)	Tok/s 99766 (96679)	Loss/tok 3.1681 (3.2537)	LR 2.800e-03
0: TRAIN [2][1340/1885]	Time 0.193 (0.144)	Data 2.20e-04 (3.70e-04)	Tok/s 105551 (96677)	Loss/tok 3.3316 (3.2535)	LR 2.800e-03
0: TRAIN [2][1350/1885]	Time 0.195 (0.143)	Data 1.77e-04 (3.69e-04)	Tok/s 105364 (96679)	Loss/tok 3.2936 (3.2529)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1360/1885]	Time 0.194 (0.144)	Data 2.16e-04 (3.68e-04)	Tok/s 104443 (96675)	Loss/tok 3.4149 (3.2531)	LR 2.800e-03
0: TRAIN [2][1370/1885]	Time 0.060 (0.143)	Data 2.29e-04 (3.67e-04)	Tok/s 76606 (96634)	Loss/tok 2.4932 (3.2522)	LR 2.800e-03
0: TRAIN [2][1380/1885]	Time 0.101 (0.143)	Data 1.92e-04 (3.66e-04)	Tok/s 89349 (96638)	Loss/tok 3.0157 (3.2528)	LR 2.800e-03
0: TRAIN [2][1390/1885]	Time 0.194 (0.143)	Data 2.20e-04 (3.65e-04)	Tok/s 105295 (96612)	Loss/tok 3.4445 (3.2526)	LR 2.800e-03
0: TRAIN [2][1400/1885]	Time 0.194 (0.143)	Data 2.20e-04 (3.64e-04)	Tok/s 104727 (96598)	Loss/tok 3.4245 (3.2522)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1410/1885]	Time 0.245 (0.143)	Data 1.08e-04 (3.62e-04)	Tok/s 107377 (96595)	Loss/tok 3.4749 (3.2523)	LR 2.800e-03
0: TRAIN [2][1420/1885]	Time 0.193 (0.143)	Data 2.21e-04 (3.61e-04)	Tok/s 105719 (96589)	Loss/tok 3.3199 (3.2522)	LR 2.800e-03
0: TRAIN [2][1430/1885]	Time 0.147 (0.143)	Data 2.06e-04 (3.60e-04)	Tok/s 99756 (96586)	Loss/tok 3.3052 (3.2518)	LR 2.800e-03
0: TRAIN [2][1440/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.59e-04)	Tok/s 88890 (96555)	Loss/tok 2.9408 (3.2517)	LR 2.800e-03
0: TRAIN [2][1450/1885]	Time 0.193 (0.143)	Data 2.21e-04 (3.58e-04)	Tok/s 104735 (96517)	Loss/tok 3.3836 (3.2509)	LR 2.800e-03
0: TRAIN [2][1460/1885]	Time 0.059 (0.143)	Data 2.25e-04 (3.57e-04)	Tok/s 77434 (96512)	Loss/tok 2.5547 (3.2513)	LR 2.800e-03
0: TRAIN [2][1470/1885]	Time 0.059 (0.143)	Data 2.18e-04 (3.56e-04)	Tok/s 77456 (96515)	Loss/tok 2.5413 (3.2509)	LR 2.800e-03
0: TRAIN [2][1480/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.55e-04)	Tok/s 90449 (96496)	Loss/tok 3.0430 (3.2509)	LR 2.800e-03
0: TRAIN [2][1490/1885]	Time 0.101 (0.143)	Data 2.31e-04 (3.54e-04)	Tok/s 89543 (96484)	Loss/tok 2.9614 (3.2515)	LR 2.800e-03
0: TRAIN [2][1500/1885]	Time 0.102 (0.143)	Data 1.88e-04 (3.53e-04)	Tok/s 88628 (96504)	Loss/tok 3.0615 (3.2519)	LR 2.800e-03
0: TRAIN [2][1510/1885]	Time 0.195 (0.143)	Data 1.92e-04 (3.52e-04)	Tok/s 104663 (96531)	Loss/tok 3.3072 (3.2520)	LR 2.800e-03
0: TRAIN [2][1520/1885]	Time 0.246 (0.143)	Data 1.88e-04 (3.51e-04)	Tok/s 105310 (96547)	Loss/tok 3.5435 (3.2526)	LR 2.800e-03
0: TRAIN [2][1530/1885]	Time 0.195 (0.143)	Data 2.18e-04 (3.50e-04)	Tok/s 104618 (96567)	Loss/tok 3.3388 (3.2525)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1540/1885]	Time 0.194 (0.143)	Data 2.24e-04 (3.49e-04)	Tok/s 105290 (96588)	Loss/tok 3.3352 (3.2536)	LR 2.800e-03
0: TRAIN [2][1550/1885]	Time 0.245 (0.143)	Data 2.19e-04 (3.48e-04)	Tok/s 107644 (96579)	Loss/tok 3.4687 (3.2536)	LR 2.800e-03
0: TRAIN [2][1560/1885]	Time 0.103 (0.143)	Data 1.80e-04 (3.48e-04)	Tok/s 89085 (96573)	Loss/tok 3.1024 (3.2538)	LR 2.800e-03
0: TRAIN [2][1570/1885]	Time 0.148 (0.143)	Data 2.23e-04 (3.47e-04)	Tok/s 99745 (96549)	Loss/tok 3.2335 (3.2533)	LR 2.800e-03
0: TRAIN [2][1580/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.46e-04)	Tok/s 88111 (96531)	Loss/tok 3.0088 (3.2526)	LR 2.800e-03
0: TRAIN [2][1590/1885]	Time 0.244 (0.143)	Data 1.80e-04 (3.45e-04)	Tok/s 107061 (96515)	Loss/tok 3.5221 (3.2523)	LR 2.800e-03
0: TRAIN [2][1600/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.44e-04)	Tok/s 105135 (96495)	Loss/tok 3.3617 (3.2516)	LR 2.800e-03
0: TRAIN [2][1610/1885]	Time 0.194 (0.143)	Data 1.88e-04 (3.43e-04)	Tok/s 103432 (96491)	Loss/tok 3.5459 (3.2521)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1620/1885]	Time 0.193 (0.143)	Data 2.17e-04 (3.43e-04)	Tok/s 107035 (96524)	Loss/tok 3.3063 (3.2526)	LR 2.800e-03
0: TRAIN [2][1630/1885]	Time 0.193 (0.143)	Data 2.22e-04 (3.42e-04)	Tok/s 106432 (96530)	Loss/tok 3.3559 (3.2527)	LR 2.800e-03
0: TRAIN [2][1640/1885]	Time 0.147 (0.143)	Data 1.07e-04 (3.41e-04)	Tok/s 100932 (96557)	Loss/tok 3.1303 (3.2530)	LR 2.800e-03
0: TRAIN [2][1650/1885]	Time 0.146 (0.143)	Data 3.12e-04 (3.40e-04)	Tok/s 101369 (96547)	Loss/tok 3.2380 (3.2527)	LR 2.800e-03
0: TRAIN [2][1660/1885]	Time 0.148 (0.143)	Data 2.20e-04 (3.39e-04)	Tok/s 99775 (96559)	Loss/tok 3.1465 (3.2533)	LR 2.800e-03
0: TRAIN [2][1670/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.39e-04)	Tok/s 101699 (96565)	Loss/tok 3.0692 (3.2530)	LR 2.800e-03
0: TRAIN [2][1680/1885]	Time 0.194 (0.143)	Data 1.05e-04 (3.38e-04)	Tok/s 105237 (96562)	Loss/tok 3.4653 (3.2528)	LR 2.800e-03
0: TRAIN [2][1690/1885]	Time 0.193 (0.143)	Data 1.79e-04 (3.37e-04)	Tok/s 106099 (96579)	Loss/tok 3.3342 (3.2533)	LR 2.800e-03
0: TRAIN [2][1700/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.36e-04)	Tok/s 90111 (96572)	Loss/tok 3.0441 (3.2530)	LR 2.800e-03
0: TRAIN [2][1710/1885]	Time 0.194 (0.143)	Data 2.24e-04 (3.35e-04)	Tok/s 105021 (96592)	Loss/tok 3.4052 (3.2529)	LR 2.800e-03
0: TRAIN [2][1720/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.35e-04)	Tok/s 99012 (96590)	Loss/tok 3.2000 (3.2527)	LR 2.800e-03
0: TRAIN [2][1730/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.34e-04)	Tok/s 100315 (96596)	Loss/tok 3.2722 (3.2527)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1740/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.33e-04)	Tok/s 89013 (96606)	Loss/tok 2.9923 (3.2529)	LR 2.800e-03
0: TRAIN [2][1750/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.32e-04)	Tok/s 99746 (96582)	Loss/tok 3.2291 (3.2523)	LR 2.800e-03
0: TRAIN [2][1760/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.31e-04)	Tok/s 89327 (96581)	Loss/tok 2.9385 (3.2519)	LR 2.800e-03
0: TRAIN [2][1770/1885]	Time 0.146 (0.143)	Data 1.91e-04 (3.31e-04)	Tok/s 100869 (96572)	Loss/tok 3.2096 (3.2517)	LR 2.800e-03
0: TRAIN [2][1780/1885]	Time 0.147 (0.143)	Data 1.88e-04 (3.30e-04)	Tok/s 100317 (96581)	Loss/tok 3.1790 (3.2516)	LR 2.800e-03
0: TRAIN [2][1790/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.29e-04)	Tok/s 98749 (96578)	Loss/tok 3.1925 (3.2516)	LR 2.800e-03
0: TRAIN [2][1800/1885]	Time 0.194 (0.143)	Data 2.21e-04 (3.29e-04)	Tok/s 106108 (96560)	Loss/tok 3.3115 (3.2513)	LR 2.800e-03
0: TRAIN [2][1810/1885]	Time 0.195 (0.143)	Data 1.60e-04 (3.28e-04)	Tok/s 105377 (96566)	Loss/tok 3.3105 (3.2512)	LR 2.800e-03
0: TRAIN [2][1820/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.27e-04)	Tok/s 87380 (96552)	Loss/tok 3.0522 (3.2508)	LR 2.800e-03
0: TRAIN [2][1830/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.27e-04)	Tok/s 88482 (96520)	Loss/tok 2.9440 (3.2503)	LR 2.800e-03
0: TRAIN [2][1840/1885]	Time 0.195 (0.143)	Data 2.22e-04 (3.26e-04)	Tok/s 103597 (96537)	Loss/tok 3.4065 (3.2512)	LR 2.800e-03
0: TRAIN [2][1850/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.26e-04)	Tok/s 101818 (96513)	Loss/tok 3.1366 (3.2509)	LR 2.800e-03
0: TRAIN [2][1860/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.25e-04)	Tok/s 99437 (96516)	Loss/tok 3.1664 (3.2507)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1870/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.24e-04)	Tok/s 88232 (96519)	Loss/tok 2.8704 (3.2507)	LR 2.800e-03
0: TRAIN [2][1880/1885]	Time 0.245 (0.143)	Data 2.19e-04 (3.24e-04)	Tok/s 105855 (96518)	Loss/tok 3.5333 (3.2503)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593838731162, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593838731163, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.678 (0.678)	Decoder iters 149.0 (149.0)	Tok/s 24458 (24458)
0: Running moses detokenizer
0: BLEU(score=22.766019893001598, counts=[36336, 17699, 9873, 5750], totals=[65314, 62311, 59308, 56308], precisions=[55.632789294791316, 28.404294586830577, 16.646995346327646, 10.211692832279605], bp=1.0, sys_len=65314, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593838732840, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22769999999999999, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593838732841, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2532	Test BLEU: 22.77
0: Performance: Epoch: 2	Training: 771980 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593838732842, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593838732842, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593838732843, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2861683293
0: TRAIN [3][0/1885]	Time 0.358 (0.358)	Data 2.14e-01 (2.14e-01)	Tok/s 40562 (40562)	Loss/tok 3.1989 (3.1989)	LR 2.800e-03
0: Gradient norm: nan
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][10/1885]	Time 0.147 (0.172)	Data 2.25e-04 (1.97e-02)	Tok/s 100419 (91248)	Loss/tok 3.2230 (3.2692)	LR 2.800e-03
0: TRAIN [3][20/1885]	Time 0.148 (0.172)	Data 2.18e-04 (1.04e-02)	Tok/s 98973 (95519)	Loss/tok 3.1468 (3.2776)	LR 2.800e-03
0: TRAIN [3][30/1885]	Time 0.194 (0.172)	Data 2.06e-04 (7.12e-03)	Tok/s 106454 (97422)	Loss/tok 3.2123 (3.2711)	LR 2.800e-03
0: TRAIN [3][40/1885]	Time 0.103 (0.167)	Data 2.18e-04 (5.43e-03)	Tok/s 89465 (97757)	Loss/tok 2.9046 (3.2526)	LR 2.800e-03
0: TRAIN [3][50/1885]	Time 0.059 (0.159)	Data 2.17e-04 (4.41e-03)	Tok/s 79015 (96787)	Loss/tok 2.4545 (3.2311)	LR 2.800e-03
0: TRAIN [3][60/1885]	Time 0.145 (0.160)	Data 2.21e-04 (3.72e-03)	Tok/s 100348 (97137)	Loss/tok 3.0253 (3.2280)	LR 2.800e-03
0: TRAIN [3][70/1885]	Time 0.061 (0.155)	Data 2.19e-04 (3.23e-03)	Tok/s 75057 (96525)	Loss/tok 2.6343 (3.2145)	LR 2.800e-03
0: TRAIN [3][80/1885]	Time 0.194 (0.150)	Data 2.31e-04 (2.85e-03)	Tok/s 105180 (95751)	Loss/tok 3.3677 (3.2003)	LR 2.800e-03
0: TRAIN [3][90/1885]	Time 0.246 (0.151)	Data 2.48e-04 (2.56e-03)	Tok/s 106082 (96136)	Loss/tok 3.4506 (3.2023)	LR 2.800e-03
0: TRAIN [3][100/1885]	Time 0.247 (0.151)	Data 2.21e-04 (2.33e-03)	Tok/s 105882 (96199)	Loss/tok 3.3724 (3.2045)	LR 2.800e-03
0: TRAIN [3][110/1885]	Time 0.147 (0.148)	Data 2.20e-04 (2.14e-03)	Tok/s 99491 (95746)	Loss/tok 3.1998 (3.1936)	LR 2.800e-03
0: TRAIN [3][120/1885]	Time 0.102 (0.146)	Data 2.06e-04 (1.98e-03)	Tok/s 89473 (95603)	Loss/tok 2.8673 (3.1837)	LR 2.800e-03
0: TRAIN [3][130/1885]	Time 0.146 (0.148)	Data 2.19e-04 (1.84e-03)	Tok/s 100027 (96011)	Loss/tok 3.1990 (3.1945)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][140/1885]	Time 0.104 (0.147)	Data 2.18e-04 (1.73e-03)	Tok/s 87197 (95844)	Loss/tok 2.8800 (3.1915)	LR 2.800e-03
0: TRAIN [3][150/1885]	Time 0.147 (0.149)	Data 2.21e-04 (1.63e-03)	Tok/s 100786 (96169)	Loss/tok 3.1500 (3.1934)	LR 2.800e-03
0: TRAIN [3][160/1885]	Time 0.147 (0.148)	Data 2.18e-04 (1.54e-03)	Tok/s 98526 (96251)	Loss/tok 3.1729 (3.1900)	LR 2.800e-03
0: TRAIN [3][170/1885]	Time 0.147 (0.148)	Data 2.19e-04 (1.46e-03)	Tok/s 99956 (96247)	Loss/tok 3.0746 (3.1896)	LR 2.800e-03
0: TRAIN [3][180/1885]	Time 0.060 (0.146)	Data 1.92e-04 (1.39e-03)	Tok/s 74495 (96047)	Loss/tok 2.4370 (3.1856)	LR 2.800e-03
0: TRAIN [3][190/1885]	Time 0.244 (0.146)	Data 2.17e-04 (1.33e-03)	Tok/s 107573 (96115)	Loss/tok 3.4403 (3.1865)	LR 2.800e-03
0: TRAIN [3][200/1885]	Time 0.194 (0.145)	Data 1.91e-04 (1.28e-03)	Tok/s 105468 (95947)	Loss/tok 3.2544 (3.1841)	LR 2.800e-03
0: TRAIN [3][210/1885]	Time 0.194 (0.146)	Data 2.20e-04 (1.23e-03)	Tok/s 104568 (96139)	Loss/tok 3.2898 (3.1899)	LR 2.800e-03
0: TRAIN [3][220/1885]	Time 0.147 (0.147)	Data 2.19e-04 (1.18e-03)	Tok/s 99157 (96247)	Loss/tok 3.2450 (3.1905)	LR 2.800e-03
0: TRAIN [3][230/1885]	Time 0.147 (0.148)	Data 1.90e-04 (1.14e-03)	Tok/s 99931 (96438)	Loss/tok 3.3121 (3.1931)	LR 2.800e-03
0: TRAIN [3][240/1885]	Time 0.101 (0.147)	Data 2.18e-04 (1.10e-03)	Tok/s 90657 (96370)	Loss/tok 2.8743 (3.1897)	LR 2.800e-03
0: TRAIN [3][250/1885]	Time 0.147 (0.146)	Data 1.59e-04 (1.07e-03)	Tok/s 100710 (96298)	Loss/tok 3.1102 (3.1862)	LR 2.800e-03
0: TRAIN [3][260/1885]	Time 0.245 (0.146)	Data 2.21e-04 (1.03e-03)	Tok/s 108095 (96368)	Loss/tok 3.4706 (3.1863)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][270/1885]	Time 0.101 (0.146)	Data 2.20e-04 (1.00e-03)	Tok/s 88634 (96342)	Loss/tok 2.8389 (3.1849)	LR 2.800e-03
0: TRAIN [3][280/1885]	Time 0.245 (0.146)	Data 2.32e-04 (9.75e-04)	Tok/s 105736 (96320)	Loss/tok 3.5025 (3.1871)	LR 2.800e-03
0: TRAIN [3][290/1885]	Time 0.102 (0.146)	Data 2.21e-04 (9.48e-04)	Tok/s 90289 (96354)	Loss/tok 3.0200 (3.1873)	LR 2.800e-03
0: TRAIN [3][300/1885]	Time 0.244 (0.147)	Data 2.20e-04 (9.24e-04)	Tok/s 108130 (96542)	Loss/tok 3.4302 (3.1906)	LR 2.800e-03
0: TRAIN [3][310/1885]	Time 0.194 (0.147)	Data 2.24e-04 (9.02e-04)	Tok/s 105332 (96521)	Loss/tok 3.2873 (3.1896)	LR 2.800e-03
0: TRAIN [3][320/1885]	Time 0.146 (0.147)	Data 2.19e-04 (8.80e-04)	Tok/s 99165 (96611)	Loss/tok 3.1206 (3.1916)	LR 2.800e-03
0: TRAIN [3][330/1885]	Time 0.147 (0.148)	Data 2.22e-04 (8.60e-04)	Tok/s 100138 (96758)	Loss/tok 3.1009 (3.1931)	LR 2.800e-03
0: TRAIN [3][340/1885]	Time 0.059 (0.147)	Data 2.22e-04 (8.41e-04)	Tok/s 76732 (96670)	Loss/tok 2.5306 (3.1907)	LR 2.800e-03
0: TRAIN [3][350/1885]	Time 0.244 (0.147)	Data 2.20e-04 (8.23e-04)	Tok/s 106552 (96706)	Loss/tok 3.4990 (3.1925)	LR 2.800e-03
0: TRAIN [3][360/1885]	Time 0.147 (0.147)	Data 1.61e-04 (8.06e-04)	Tok/s 99422 (96725)	Loss/tok 3.2796 (3.1938)	LR 2.800e-03
0: TRAIN [3][370/1885]	Time 0.101 (0.147)	Data 2.18e-04 (7.91e-04)	Tok/s 90654 (96673)	Loss/tok 2.9931 (3.1926)	LR 2.800e-03
0: TRAIN [3][380/1885]	Time 0.146 (0.146)	Data 2.17e-04 (7.76e-04)	Tok/s 100642 (96600)	Loss/tok 3.1812 (3.1900)	LR 2.800e-03
0: TRAIN [3][390/1885]	Time 0.102 (0.146)	Data 2.20e-04 (7.61e-04)	Tok/s 89868 (96654)	Loss/tok 2.9558 (3.1899)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][400/1885]	Time 0.147 (0.146)	Data 2.21e-04 (7.48e-04)	Tok/s 99834 (96620)	Loss/tok 3.0729 (3.1886)	LR 2.800e-03
0: TRAIN [3][410/1885]	Time 0.147 (0.146)	Data 2.24e-04 (7.34e-04)	Tok/s 98561 (96653)	Loss/tok 3.2115 (3.1894)	LR 2.800e-03
0: TRAIN [3][420/1885]	Time 0.146 (0.146)	Data 2.19e-04 (7.22e-04)	Tok/s 101873 (96687)	Loss/tok 3.0183 (3.1894)	LR 2.800e-03
0: TRAIN [3][430/1885]	Time 0.059 (0.146)	Data 2.18e-04 (7.10e-04)	Tok/s 75719 (96608)	Loss/tok 2.4306 (3.1891)	LR 2.800e-03
0: TRAIN [3][440/1885]	Time 0.146 (0.145)	Data 2.18e-04 (6.99e-04)	Tok/s 100729 (96560)	Loss/tok 3.1204 (3.1866)	LR 1.400e-03
0: TRAIN [3][450/1885]	Time 0.247 (0.145)	Data 2.24e-04 (6.89e-04)	Tok/s 106926 (96548)	Loss/tok 3.4487 (3.1864)	LR 1.400e-03
0: TRAIN [3][460/1885]	Time 0.147 (0.145)	Data 2.36e-04 (6.78e-04)	Tok/s 99969 (96548)	Loss/tok 3.1168 (3.1856)	LR 1.400e-03
0: TRAIN [3][470/1885]	Time 0.102 (0.145)	Data 2.25e-04 (6.69e-04)	Tok/s 89202 (96540)	Loss/tok 2.9819 (3.1849)	LR 1.400e-03
0: TRAIN [3][480/1885]	Time 0.101 (0.145)	Data 2.25e-04 (6.59e-04)	Tok/s 89977 (96516)	Loss/tok 2.8587 (3.1831)	LR 1.400e-03
0: TRAIN [3][490/1885]	Time 0.246 (0.146)	Data 1.92e-04 (6.50e-04)	Tok/s 105125 (96617)	Loss/tok 3.4126 (3.1852)	LR 1.400e-03
0: TRAIN [3][500/1885]	Time 0.101 (0.146)	Data 2.19e-04 (6.41e-04)	Tok/s 88901 (96682)	Loss/tok 2.8911 (3.1866)	LR 1.400e-03
0: TRAIN [3][510/1885]	Time 0.102 (0.145)	Data 1.53e-04 (6.33e-04)	Tok/s 88323 (96560)	Loss/tok 2.9804 (3.1839)	LR 1.400e-03
0: TRAIN [3][520/1885]	Time 0.102 (0.146)	Data 1.78e-04 (6.25e-04)	Tok/s 89507 (96595)	Loss/tok 2.8739 (3.1839)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][530/1885]	Time 0.060 (0.145)	Data 2.20e-04 (6.17e-04)	Tok/s 76167 (96560)	Loss/tok 2.4123 (3.1832)	LR 1.400e-03
0: TRAIN [3][540/1885]	Time 0.147 (0.146)	Data 2.02e-04 (6.10e-04)	Tok/s 99638 (96597)	Loss/tok 3.1854 (3.1824)	LR 1.400e-03
0: TRAIN [3][550/1885]	Time 0.148 (0.146)	Data 2.23e-04 (6.03e-04)	Tok/s 99769 (96617)	Loss/tok 3.1768 (3.1814)	LR 1.400e-03
0: TRAIN [3][560/1885]	Time 0.245 (0.146)	Data 2.21e-04 (5.96e-04)	Tok/s 107463 (96625)	Loss/tok 3.5486 (3.1815)	LR 1.400e-03
0: TRAIN [3][570/1885]	Time 0.101 (0.145)	Data 2.19e-04 (5.89e-04)	Tok/s 90723 (96524)	Loss/tok 2.8538 (3.1787)	LR 1.400e-03
0: TRAIN [3][580/1885]	Time 0.195 (0.145)	Data 2.22e-04 (5.83e-04)	Tok/s 103833 (96560)	Loss/tok 3.3287 (3.1802)	LR 1.400e-03
0: TRAIN [3][590/1885]	Time 0.195 (0.146)	Data 1.11e-04 (5.76e-04)	Tok/s 102641 (96590)	Loss/tok 3.2910 (3.1813)	LR 1.400e-03
0: TRAIN [3][600/1885]	Time 0.102 (0.145)	Data 1.07e-04 (5.68e-04)	Tok/s 89109 (96533)	Loss/tok 2.8457 (3.1795)	LR 1.400e-03
0: TRAIN [3][610/1885]	Time 0.102 (0.145)	Data 2.19e-04 (5.61e-04)	Tok/s 89565 (96491)	Loss/tok 2.9024 (3.1783)	LR 1.400e-03
0: TRAIN [3][620/1885]	Time 0.101 (0.145)	Data 2.21e-04 (5.56e-04)	Tok/s 91432 (96419)	Loss/tok 2.9296 (3.1773)	LR 1.400e-03
0: TRAIN [3][630/1885]	Time 0.101 (0.145)	Data 2.19e-04 (5.51e-04)	Tok/s 88684 (96441)	Loss/tok 2.7695 (3.1770)	LR 1.400e-03
0: TRAIN [3][640/1885]	Time 0.146 (0.145)	Data 2.23e-04 (5.45e-04)	Tok/s 100055 (96428)	Loss/tok 3.0706 (3.1760)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][650/1885]	Time 0.146 (0.145)	Data 2.20e-04 (5.40e-04)	Tok/s 101238 (96451)	Loss/tok 3.1668 (3.1760)	LR 1.400e-03
0: TRAIN [3][660/1885]	Time 0.146 (0.145)	Data 2.21e-04 (5.35e-04)	Tok/s 101257 (96483)	Loss/tok 2.9677 (3.1752)	LR 1.400e-03
0: TRAIN [3][670/1885]	Time 0.194 (0.145)	Data 1.80e-04 (5.31e-04)	Tok/s 104395 (96551)	Loss/tok 3.3223 (3.1764)	LR 1.400e-03
0: TRAIN [3][680/1885]	Time 0.060 (0.145)	Data 2.21e-04 (5.26e-04)	Tok/s 75497 (96502)	Loss/tok 2.4122 (3.1745)	LR 1.400e-03
0: TRAIN [3][690/1885]	Time 0.146 (0.145)	Data 2.18e-04 (5.22e-04)	Tok/s 99626 (96569)	Loss/tok 3.1828 (3.1755)	LR 1.400e-03
0: TRAIN [3][700/1885]	Time 0.101 (0.145)	Data 2.20e-04 (5.17e-04)	Tok/s 89844 (96548)	Loss/tok 2.8985 (3.1741)	LR 1.400e-03
0: TRAIN [3][710/1885]	Time 0.245 (0.145)	Data 1.91e-04 (5.13e-04)	Tok/s 106967 (96578)	Loss/tok 3.4476 (3.1742)	LR 1.400e-03
0: TRAIN [3][720/1885]	Time 0.102 (0.145)	Data 2.21e-04 (5.09e-04)	Tok/s 89170 (96520)	Loss/tok 2.8638 (3.1733)	LR 1.400e-03
0: TRAIN [3][730/1885]	Time 0.102 (0.145)	Data 2.23e-04 (5.05e-04)	Tok/s 89549 (96480)	Loss/tok 2.8483 (3.1717)	LR 1.400e-03
0: TRAIN [3][740/1885]	Time 0.194 (0.145)	Data 2.24e-04 (5.01e-04)	Tok/s 105902 (96448)	Loss/tok 3.2352 (3.1704)	LR 1.400e-03
0: TRAIN [3][750/1885]	Time 0.146 (0.145)	Data 2.21e-04 (4.97e-04)	Tok/s 100511 (96483)	Loss/tok 3.1050 (3.1705)	LR 1.400e-03
0: TRAIN [3][760/1885]	Time 0.102 (0.144)	Data 2.23e-04 (4.94e-04)	Tok/s 87443 (96441)	Loss/tok 2.9459 (3.1689)	LR 1.400e-03
0: TRAIN [3][770/1885]	Time 0.101 (0.144)	Data 2.21e-04 (4.90e-04)	Tok/s 90087 (96390)	Loss/tok 2.9094 (3.1675)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][780/1885]	Time 0.194 (0.144)	Data 2.21e-04 (4.87e-04)	Tok/s 105424 (96415)	Loss/tok 3.2522 (3.1677)	LR 1.400e-03
0: TRAIN [3][790/1885]	Time 0.195 (0.144)	Data 2.18e-04 (4.83e-04)	Tok/s 104078 (96408)	Loss/tok 3.3210 (3.1679)	LR 1.400e-03
0: TRAIN [3][800/1885]	Time 0.101 (0.144)	Data 2.05e-04 (4.80e-04)	Tok/s 88574 (96392)	Loss/tok 2.8893 (3.1671)	LR 1.400e-03
0: TRAIN [3][810/1885]	Time 0.148 (0.144)	Data 2.30e-04 (4.77e-04)	Tok/s 99398 (96427)	Loss/tok 3.1183 (3.1670)	LR 1.400e-03
0: TRAIN [3][820/1885]	Time 0.195 (0.144)	Data 2.20e-04 (4.73e-04)	Tok/s 106468 (96421)	Loss/tok 3.1694 (3.1662)	LR 1.400e-03
0: TRAIN [3][830/1885]	Time 0.195 (0.144)	Data 2.18e-04 (4.70e-04)	Tok/s 104810 (96452)	Loss/tok 3.2760 (3.1661)	LR 1.400e-03
0: TRAIN [3][840/1885]	Time 0.147 (0.144)	Data 2.21e-04 (4.67e-04)	Tok/s 101112 (96434)	Loss/tok 3.1002 (3.1653)	LR 1.400e-03
0: TRAIN [3][850/1885]	Time 0.102 (0.144)	Data 2.16e-04 (4.65e-04)	Tok/s 89295 (96442)	Loss/tok 2.9845 (3.1657)	LR 1.400e-03
0: TRAIN [3][860/1885]	Time 0.101 (0.144)	Data 2.19e-04 (4.62e-04)	Tok/s 90753 (96436)	Loss/tok 2.9090 (3.1651)	LR 1.400e-03
0: TRAIN [3][870/1885]	Time 0.147 (0.144)	Data 2.20e-04 (4.59e-04)	Tok/s 100414 (96496)	Loss/tok 2.9809 (3.1654)	LR 1.400e-03
0: TRAIN [3][880/1885]	Time 0.247 (0.145)	Data 2.07e-04 (4.56e-04)	Tok/s 106711 (96558)	Loss/tok 3.2840 (3.1669)	LR 1.400e-03
0: TRAIN [3][890/1885]	Time 0.102 (0.145)	Data 2.19e-04 (4.53e-04)	Tok/s 89333 (96487)	Loss/tok 2.9128 (3.1652)	LR 1.400e-03
0: TRAIN [3][900/1885]	Time 0.193 (0.144)	Data 2.18e-04 (4.51e-04)	Tok/s 106660 (96494)	Loss/tok 3.1728 (3.1641)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][910/1885]	Time 0.147 (0.145)	Data 2.23e-04 (4.48e-04)	Tok/s 100179 (96508)	Loss/tok 3.0326 (3.1637)	LR 1.400e-03
0: TRAIN [3][920/1885]	Time 0.101 (0.144)	Data 2.17e-04 (4.46e-04)	Tok/s 89331 (96494)	Loss/tok 2.8167 (3.1626)	LR 1.400e-03
0: TRAIN [3][930/1885]	Time 0.102 (0.144)	Data 2.17e-04 (4.43e-04)	Tok/s 89337 (96461)	Loss/tok 2.8550 (3.1620)	LR 1.400e-03
0: TRAIN [3][940/1885]	Time 0.101 (0.144)	Data 2.16e-04 (4.41e-04)	Tok/s 91705 (96424)	Loss/tok 2.8632 (3.1610)	LR 1.400e-03
0: TRAIN [3][950/1885]	Time 0.101 (0.144)	Data 2.18e-04 (4.38e-04)	Tok/s 89745 (96392)	Loss/tok 2.9843 (3.1600)	LR 1.400e-03
0: TRAIN [3][960/1885]	Time 0.147 (0.144)	Data 2.17e-04 (4.36e-04)	Tok/s 99773 (96383)	Loss/tok 3.0958 (3.1596)	LR 1.400e-03
0: TRAIN [3][970/1885]	Time 0.195 (0.144)	Data 2.19e-04 (4.34e-04)	Tok/s 105091 (96376)	Loss/tok 3.2576 (3.1590)	LR 1.400e-03
0: TRAIN [3][980/1885]	Time 0.246 (0.144)	Data 2.17e-04 (4.31e-04)	Tok/s 105616 (96361)	Loss/tok 3.4932 (3.1589)	LR 1.400e-03
0: TRAIN [3][990/1885]	Time 0.147 (0.144)	Data 2.06e-04 (4.29e-04)	Tok/s 100214 (96355)	Loss/tok 3.0738 (3.1581)	LR 1.400e-03
0: TRAIN [3][1000/1885]	Time 0.146 (0.144)	Data 2.18e-04 (4.27e-04)	Tok/s 100314 (96367)	Loss/tok 3.1559 (3.1580)	LR 1.400e-03
0: TRAIN [3][1010/1885]	Time 0.060 (0.144)	Data 2.19e-04 (4.25e-04)	Tok/s 75841 (96343)	Loss/tok 2.4188 (3.1576)	LR 1.400e-03
0: TRAIN [3][1020/1885]	Time 0.193 (0.144)	Data 2.17e-04 (4.23e-04)	Tok/s 104183 (96342)	Loss/tok 3.3283 (3.1576)	LR 1.400e-03
0: TRAIN [3][1030/1885]	Time 0.244 (0.144)	Data 2.22e-04 (4.21e-04)	Tok/s 107407 (96384)	Loss/tok 3.4492 (3.1584)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1040/1885]	Time 0.101 (0.144)	Data 2.19e-04 (4.19e-04)	Tok/s 88739 (96350)	Loss/tok 2.9037 (3.1572)	LR 1.400e-03
0: TRAIN [3][1050/1885]	Time 0.102 (0.143)	Data 2.06e-04 (4.17e-04)	Tok/s 88147 (96315)	Loss/tok 2.8799 (3.1563)	LR 1.400e-03
0: TRAIN [3][1060/1885]	Time 0.195 (0.143)	Data 2.19e-04 (4.15e-04)	Tok/s 106028 (96319)	Loss/tok 3.2179 (3.1558)	LR 1.400e-03
0: TRAIN [3][1070/1885]	Time 0.147 (0.143)	Data 2.18e-04 (4.13e-04)	Tok/s 100622 (96324)	Loss/tok 3.1157 (3.1561)	LR 1.400e-03
0: TRAIN [3][1080/1885]	Time 0.194 (0.143)	Data 2.20e-04 (4.11e-04)	Tok/s 105088 (96298)	Loss/tok 3.2987 (3.1556)	LR 1.400e-03
0: TRAIN [3][1090/1885]	Time 0.102 (0.143)	Data 2.17e-04 (4.10e-04)	Tok/s 88767 (96292)	Loss/tok 2.8099 (3.1549)	LR 1.400e-03
0: TRAIN [3][1100/1885]	Time 0.193 (0.143)	Data 2.21e-04 (4.08e-04)	Tok/s 106960 (96312)	Loss/tok 3.1557 (3.1552)	LR 1.400e-03
0: TRAIN [3][1110/1885]	Time 0.102 (0.144)	Data 1.60e-04 (4.06e-04)	Tok/s 88730 (96332)	Loss/tok 2.9467 (3.1553)	LR 1.400e-03
0: TRAIN [3][1120/1885]	Time 0.101 (0.144)	Data 2.21e-04 (4.04e-04)	Tok/s 90463 (96325)	Loss/tok 2.8746 (3.1551)	LR 1.400e-03
0: TRAIN [3][1130/1885]	Time 0.101 (0.143)	Data 2.21e-04 (4.03e-04)	Tok/s 89413 (96329)	Loss/tok 2.9834 (3.1546)	LR 1.400e-03
0: TRAIN [3][1140/1885]	Time 0.147 (0.143)	Data 2.21e-04 (4.01e-04)	Tok/s 99469 (96320)	Loss/tok 3.2452 (3.1539)	LR 1.400e-03
0: TRAIN [3][1150/1885]	Time 0.147 (0.144)	Data 2.21e-04 (4.00e-04)	Tok/s 100833 (96351)	Loss/tok 3.1744 (3.1547)	LR 1.400e-03
0: TRAIN [3][1160/1885]	Time 0.101 (0.143)	Data 2.39e-04 (3.98e-04)	Tok/s 90970 (96312)	Loss/tok 2.8467 (3.1540)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1170/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.97e-04)	Tok/s 100273 (96309)	Loss/tok 2.9712 (3.1540)	LR 1.400e-03
0: TRAIN [3][1180/1885]	Time 0.195 (0.143)	Data 2.19e-04 (3.95e-04)	Tok/s 103790 (96283)	Loss/tok 3.3045 (3.1536)	LR 1.400e-03
0: TRAIN [3][1190/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.94e-04)	Tok/s 89159 (96301)	Loss/tok 2.8426 (3.1536)	LR 1.400e-03
0: TRAIN [3][1200/1885]	Time 0.146 (0.144)	Data 2.18e-04 (3.92e-04)	Tok/s 101215 (96329)	Loss/tok 3.0788 (3.1537)	LR 1.400e-03
0: TRAIN [3][1210/1885]	Time 0.102 (0.143)	Data 2.33e-04 (3.91e-04)	Tok/s 87715 (96299)	Loss/tok 2.8746 (3.1526)	LR 1.400e-03
0: TRAIN [3][1220/1885]	Time 0.101 (0.143)	Data 2.31e-04 (3.89e-04)	Tok/s 90440 (96272)	Loss/tok 2.9084 (3.1516)	LR 1.400e-03
0: TRAIN [3][1230/1885]	Time 0.193 (0.143)	Data 2.20e-04 (3.88e-04)	Tok/s 104291 (96293)	Loss/tok 3.1963 (3.1517)	LR 1.400e-03
0: TRAIN [3][1240/1885]	Time 0.102 (0.143)	Data 1.93e-04 (3.86e-04)	Tok/s 89161 (96300)	Loss/tok 2.9591 (3.1518)	LR 1.400e-03
0: TRAIN [3][1250/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.85e-04)	Tok/s 90040 (96309)	Loss/tok 2.9597 (3.1518)	LR 1.400e-03
0: TRAIN [3][1260/1885]	Time 0.102 (0.143)	Data 2.32e-04 (3.84e-04)	Tok/s 90519 (96283)	Loss/tok 2.9087 (3.1508)	LR 1.400e-03
0: TRAIN [3][1270/1885]	Time 0.147 (0.143)	Data 1.08e-04 (3.82e-04)	Tok/s 99911 (96309)	Loss/tok 3.0944 (3.1509)	LR 1.400e-03
0: TRAIN [3][1280/1885]	Time 0.147 (0.143)	Data 1.21e-04 (3.80e-04)	Tok/s 99272 (96309)	Loss/tok 3.1152 (3.1505)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1290/1885]	Time 0.148 (0.143)	Data 2.21e-04 (3.78e-04)	Tok/s 97916 (96321)	Loss/tok 3.1905 (3.1503)	LR 7.000e-04
0: TRAIN [3][1300/1885]	Time 0.102 (0.143)	Data 2.24e-04 (3.77e-04)	Tok/s 89552 (96325)	Loss/tok 2.8865 (3.1505)	LR 7.000e-04
0: TRAIN [3][1310/1885]	Time 0.194 (0.143)	Data 2.22e-04 (3.76e-04)	Tok/s 104909 (96346)	Loss/tok 3.2621 (3.1503)	LR 7.000e-04
0: TRAIN [3][1320/1885]	Time 0.146 (0.143)	Data 2.25e-04 (3.75e-04)	Tok/s 99509 (96354)	Loss/tok 3.0225 (3.1500)	LR 7.000e-04
0: TRAIN [3][1330/1885]	Time 0.059 (0.143)	Data 2.19e-04 (3.73e-04)	Tok/s 76418 (96337)	Loss/tok 2.4775 (3.1495)	LR 7.000e-04
0: TRAIN [3][1340/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.72e-04)	Tok/s 90437 (96343)	Loss/tok 2.8959 (3.1491)	LR 7.000e-04
0: TRAIN [3][1350/1885]	Time 0.245 (0.143)	Data 2.19e-04 (3.71e-04)	Tok/s 104354 (96365)	Loss/tok 3.4083 (3.1496)	LR 7.000e-04
0: TRAIN [3][1360/1885]	Time 0.147 (0.144)	Data 2.19e-04 (3.70e-04)	Tok/s 100520 (96391)	Loss/tok 3.0769 (3.1495)	LR 7.000e-04
0: TRAIN [3][1370/1885]	Time 0.148 (0.144)	Data 2.17e-04 (3.69e-04)	Tok/s 99970 (96401)	Loss/tok 3.0146 (3.1492)	LR 7.000e-04
0: TRAIN [3][1380/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.68e-04)	Tok/s 90041 (96395)	Loss/tok 2.8464 (3.1491)	LR 7.000e-04
0: TRAIN [3][1390/1885]	Time 0.102 (0.144)	Data 1.96e-04 (3.67e-04)	Tok/s 89314 (96387)	Loss/tok 2.9206 (3.1487)	LR 7.000e-04
0: TRAIN [3][1400/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.65e-04)	Tok/s 88049 (96388)	Loss/tok 2.8268 (3.1481)	LR 7.000e-04
0: TRAIN [3][1410/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.64e-04)	Tok/s 87574 (96390)	Loss/tok 2.9140 (3.1478)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1420/1885]	Time 0.102 (0.143)	Data 1.81e-04 (3.63e-04)	Tok/s 89593 (96375)	Loss/tok 2.9078 (3.1472)	LR 7.000e-04
0: TRAIN [3][1430/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.62e-04)	Tok/s 100849 (96398)	Loss/tok 3.1644 (3.1474)	LR 7.000e-04
0: TRAIN [3][1440/1885]	Time 0.147 (0.143)	Data 2.17e-04 (3.61e-04)	Tok/s 100054 (96382)	Loss/tok 3.0632 (3.1465)	LR 7.000e-04
0: TRAIN [3][1450/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.60e-04)	Tok/s 99062 (96387)	Loss/tok 3.0834 (3.1463)	LR 7.000e-04
0: TRAIN [3][1460/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.59e-04)	Tok/s 90203 (96373)	Loss/tok 2.8434 (3.1459)	LR 7.000e-04
0: TRAIN [3][1470/1885]	Time 0.194 (0.144)	Data 2.21e-04 (3.58e-04)	Tok/s 105282 (96403)	Loss/tok 3.2127 (3.1465)	LR 7.000e-04
0: TRAIN [3][1480/1885]	Time 0.102 (0.144)	Data 2.40e-04 (3.57e-04)	Tok/s 89789 (96414)	Loss/tok 2.8345 (3.1468)	LR 7.000e-04
0: TRAIN [3][1490/1885]	Time 0.102 (0.144)	Data 1.71e-04 (3.56e-04)	Tok/s 91294 (96390)	Loss/tok 2.8563 (3.1461)	LR 7.000e-04
0: TRAIN [3][1500/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.55e-04)	Tok/s 100133 (96359)	Loss/tok 3.0215 (3.1454)	LR 7.000e-04
0: TRAIN [3][1510/1885]	Time 0.147 (0.144)	Data 2.20e-04 (3.54e-04)	Tok/s 98316 (96395)	Loss/tok 3.0986 (3.1453)	LR 7.000e-04
0: TRAIN [3][1520/1885]	Time 0.194 (0.144)	Data 2.21e-04 (3.54e-04)	Tok/s 104461 (96405)	Loss/tok 3.2001 (3.1453)	LR 7.000e-04
0: TRAIN [3][1530/1885]	Time 0.102 (0.144)	Data 1.92e-04 (3.52e-04)	Tok/s 89111 (96414)	Loss/tok 2.8466 (3.1454)	LR 7.000e-04
0: TRAIN [3][1540/1885]	Time 0.147 (0.144)	Data 2.19e-04 (3.52e-04)	Tok/s 101113 (96436)	Loss/tok 3.0930 (3.1454)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1550/1885]	Time 0.060 (0.144)	Data 2.22e-04 (3.51e-04)	Tok/s 75828 (96416)	Loss/tok 2.3728 (3.1447)	LR 7.000e-04
0: TRAIN [3][1560/1885]	Time 0.194 (0.144)	Data 2.27e-04 (3.50e-04)	Tok/s 103754 (96427)	Loss/tok 3.3504 (3.1443)	LR 7.000e-04
0: TRAIN [3][1570/1885]	Time 0.245 (0.144)	Data 2.22e-04 (3.49e-04)	Tok/s 108467 (96449)	Loss/tok 3.2781 (3.1442)	LR 7.000e-04
0: TRAIN [3][1580/1885]	Time 0.148 (0.144)	Data 1.51e-04 (3.48e-04)	Tok/s 99831 (96434)	Loss/tok 3.2049 (3.1436)	LR 7.000e-04
0: TRAIN [3][1590/1885]	Time 0.245 (0.144)	Data 2.19e-04 (3.47e-04)	Tok/s 107088 (96466)	Loss/tok 3.4114 (3.1439)	LR 7.000e-04
0: TRAIN [3][1600/1885]	Time 0.101 (0.144)	Data 1.16e-04 (3.46e-04)	Tok/s 89791 (96421)	Loss/tok 2.8836 (3.1429)	LR 7.000e-04
0: TRAIN [3][1610/1885]	Time 0.101 (0.144)	Data 1.08e-04 (3.45e-04)	Tok/s 90224 (96423)	Loss/tok 2.9567 (3.1426)	LR 7.000e-04
0: TRAIN [3][1620/1885]	Time 0.100 (0.143)	Data 2.20e-04 (3.43e-04)	Tok/s 91034 (96397)	Loss/tok 2.7976 (3.1416)	LR 7.000e-04
0: TRAIN [3][1630/1885]	Time 0.101 (0.143)	Data 2.04e-04 (3.42e-04)	Tok/s 90572 (96361)	Loss/tok 2.8366 (3.1410)	LR 7.000e-04
0: TRAIN [3][1640/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.42e-04)	Tok/s 89095 (96346)	Loss/tok 2.8455 (3.1404)	LR 7.000e-04
0: TRAIN [3][1650/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.41e-04)	Tok/s 88863 (96335)	Loss/tok 2.9366 (3.1400)	LR 7.000e-04
0: TRAIN [3][1660/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.40e-04)	Tok/s 100684 (96329)	Loss/tok 3.1493 (3.1393)	LR 7.000e-04
0: TRAIN [3][1670/1885]	Time 0.146 (0.143)	Data 2.23e-04 (3.39e-04)	Tok/s 101400 (96324)	Loss/tok 2.9800 (3.1386)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1680/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.38e-04)	Tok/s 100970 (96315)	Loss/tok 3.0572 (3.1385)	LR 7.000e-04
0: TRAIN [3][1690/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.38e-04)	Tok/s 100647 (96336)	Loss/tok 3.0400 (3.1387)	LR 7.000e-04
0: TRAIN [3][1700/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.37e-04)	Tok/s 88384 (96336)	Loss/tok 3.0342 (3.1387)	LR 7.000e-04
0: TRAIN [3][1710/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.36e-04)	Tok/s 89024 (96350)	Loss/tok 2.8332 (3.1387)	LR 7.000e-04
0: TRAIN [3][1720/1885]	Time 0.101 (0.143)	Data 2.38e-04 (3.36e-04)	Tok/s 89535 (96337)	Loss/tok 2.8825 (3.1382)	LR 7.000e-04
0: TRAIN [3][1730/1885]	Time 0.147 (0.143)	Data 2.23e-04 (3.35e-04)	Tok/s 99021 (96327)	Loss/tok 3.1103 (3.1376)	LR 7.000e-04
0: TRAIN [3][1740/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.34e-04)	Tok/s 102601 (96361)	Loss/tok 3.0174 (3.1380)	LR 7.000e-04
0: TRAIN [3][1750/1885]	Time 0.194 (0.143)	Data 2.20e-04 (3.34e-04)	Tok/s 106284 (96363)	Loss/tok 3.1855 (3.1378)	LR 7.000e-04
0: TRAIN [3][1760/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.33e-04)	Tok/s 90605 (96362)	Loss/tok 2.8112 (3.1378)	LR 7.000e-04
0: TRAIN [3][1770/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.32e-04)	Tok/s 100621 (96346)	Loss/tok 3.0198 (3.1371)	LR 7.000e-04
0: TRAIN [3][1780/1885]	Time 0.148 (0.143)	Data 2.20e-04 (3.32e-04)	Tok/s 101406 (96338)	Loss/tok 3.0084 (3.1368)	LR 7.000e-04
0: TRAIN [3][1790/1885]	Time 0.148 (0.143)	Data 2.22e-04 (3.31e-04)	Tok/s 98833 (96351)	Loss/tok 3.0173 (3.1368)	LR 7.000e-04
0: TRAIN [3][1800/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.30e-04)	Tok/s 104415 (96341)	Loss/tok 3.2539 (3.1364)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1810/1885]	Time 0.193 (0.143)	Data 2.20e-04 (3.30e-04)	Tok/s 106349 (96350)	Loss/tok 3.1672 (3.1361)	LR 7.000e-04
0: TRAIN [3][1820/1885]	Time 0.061 (0.143)	Data 2.21e-04 (3.29e-04)	Tok/s 74221 (96345)	Loss/tok 2.3803 (3.1359)	LR 7.000e-04
0: TRAIN [3][1830/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.28e-04)	Tok/s 89946 (96339)	Loss/tok 2.8855 (3.1356)	LR 7.000e-04
0: TRAIN [3][1840/1885]	Time 0.146 (0.143)	Data 2.22e-04 (3.28e-04)	Tok/s 100963 (96357)	Loss/tok 2.9002 (3.1360)	LR 7.000e-04
0: TRAIN [3][1850/1885]	Time 0.101 (0.143)	Data 2.22e-04 (3.27e-04)	Tok/s 89974 (96353)	Loss/tok 2.9408 (3.1355)	LR 7.000e-04
0: TRAIN [3][1860/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.27e-04)	Tok/s 100703 (96375)	Loss/tok 3.0280 (3.1355)	LR 7.000e-04
0: TRAIN [3][1870/1885]	Time 0.195 (0.143)	Data 2.21e-04 (3.26e-04)	Tok/s 104646 (96390)	Loss/tok 3.1693 (3.1352)	LR 7.000e-04
0: TRAIN [3][1880/1885]	Time 0.061 (0.143)	Data 2.21e-04 (3.26e-04)	Tok/s 75194 (96359)	Loss/tok 2.4434 (3.1346)	LR 7.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593839003097, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593839003097, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.557 (0.557)	Decoder iters 101.0 (101.0)	Tok/s 29122 (29122)
0: Running moses detokenizer
0: BLEU(score=24.209077090489675, counts=[36823, 18455, 10517, 6257], totals=[64529, 61526, 58523, 55524], precisions=[57.06426567899704, 29.995449078438384, 17.970712369495754, 11.269000792450111], bp=0.9977245472466068, sys_len=64529, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593839004629, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2421, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593839004629, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1348	Test BLEU: 24.21
0: Performance: Epoch: 3	Training: 770819 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593839004630, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593839004630, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-07-04 05:03:30 AM
RESULT,RNN_TRANSLATOR,,1110,Fujitsu,2020-07-04 04:45:00 AM
