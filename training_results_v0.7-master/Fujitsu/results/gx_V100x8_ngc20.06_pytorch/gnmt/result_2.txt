Beginning trial 1 of 1
Gathering sys log on gx2570-03
:::MLLOG {"namespace": "", "time_ms": 1593831770142, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593831770179, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Fujitsu", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593831770179, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593831770179, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593831770179, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xGX2570M5", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
Clearing caches
:::MLLOG {"namespace": "", "time_ms": 1593831773400, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/opt/conda/lib/python3.6/site-packages/mlperf_logging/mllog/mllog.py", "lineno": 261}}
Launching on node gx2570-03
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e PGSYSTEM=PG -e 'MULTI_NODE= --master_port=4260' -e LR=2.8e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6053 -e DECAY_INTERVAL=859 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=65 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200704114252288086675 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200704114252288086675 ./run_and_time.sh
STARTING TIMING RUN AT 2020-07-04 03:02:54 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.8e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6053
+ DECAY_INTERVAL=859
+ TARGET=24.0
+ MAX_SEQ_LEN=65
+ NUMEPOCHS=8
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ PGSYSTEM=PG
+ [[ -f config_PG.sh ]]
+ source config_PG.sh
++ export PGNNODES=1
++ PGNNODES=1
+++ sed 's/^config_//'
+++ sed 's/\.sh$//'
++++ readlink -f config_PG.sh
+++ basename /workspace/rnn_translator/config_PG.sh
++ export PGSYSTEM=PG
++ PGSYSTEM=PG
++ export WALLTIME=00:30:00
++ WALLTIME=00:30:00
++ export LR=2.8e-3
++ LR=2.8e-3
++ export TRAIN_BATCH_SIZE=256
++ TRAIN_BATCH_SIZE=256
++ export TEST_BATCH_SIZE=128
++ TEST_BATCH_SIZE=128
++ export WARMUP_STEPS=200
++ WARMUP_STEPS=200
++ export REMAIN_STEPS=6053
++ REMAIN_STEPS=6053
++ export DECAY_INTERVAL=859
++ DECAY_INTERVAL=859
++ export TARGET=24.0
++ TARGET=24.0
++ export MAX_SEQ_LEN=65
++ MAX_SEQ_LEN=65
++ export NUMEPOCHS=8
++ NUMEPOCHS=8
++ export MATH=fp16
++ MATH=fp16
++ export DIST_OPTS=
++ DIST_OPTS=
++ export 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ export PGNGPU=8
++ PGNGPU=8
++ export PGSOCKETCORES=24
++ PGSOCKETCORES=24
++ export PGHT=2
++ PGHT=2
++ export PGNSOCKET=2
++ PGNSOCKET=2
+ declare -a CMD
+ '[' -n '' ']'
+ CMD=('python' '-u' '-m' 'bind_launch' "--nsockets_per_node=${PGNSOCKET}" "--ncores_per_socket=${PGSOCKETCORES}" "--nproc_per_node=${PGNGPU}")
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ PG == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ python -u -m bind_launch --nsockets_per_node=2 --ncores_per_socket=24 --nproc_per_node=8 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 65 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.8e-3 --warmup-steps 200 --remain-steps 6053 --decay-interval 859 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593831776370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593831776370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593831776370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593831776388, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593831776410, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593831776413, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593831776416, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593831776455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=859, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.0028, math='fp16', max_length_test=150, max_length_train=65, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6053, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2989730904
:::MLLOG {"namespace": "", "time_ms": 1593831785060, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2989730904, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3927658202
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.0028}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.0028
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593831787940, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593831787940, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0028, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593831787940, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593831787940, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593831787940, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593831790507, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593831790520, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593831790520, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 65, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3866375 min length: 0 max length: 65 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593831790790, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593831790791, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3860480, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593831790791, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6053, 'decay_interval': 859, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6053
0: Scheduler decay interval: 859
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593831790792, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593831790792, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593831790792, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 859, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593831790792, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593831790792, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593831790792, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6053, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593831790792, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593831790793, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593831790793, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1008096805
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1885]	Time 0.434 (0.434)	Data 2.84e-01 (2.84e-01)	Tok/s 21075 (21075)	Loss/tok 10.6146 (10.6146)	LR 2.865e-05
0: TRAIN [0][10/1885]	Time 0.191 (0.177)	Data 2.17e-04 (2.60e-02)	Tok/s 105895 (91976)	Loss/tok 9.5763 (9.9547)	LR 3.607e-05
0: TRAIN [0][20/1885]	Time 0.100 (0.164)	Data 2.17e-04 (1.37e-02)	Tok/s 89493 (95785)	Loss/tok 8.9944 (9.6248)	LR 4.541e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][30/1885]	Time 0.191 (0.160)	Data 2.20e-04 (9.36e-03)	Tok/s 107142 (96720)	Loss/tok 8.9720 (9.4261)	LR 5.587e-05
0: TRAIN [0][40/1885]	Time 0.101 (0.157)	Data 2.21e-04 (7.13e-03)	Tok/s 90480 (96981)	Loss/tok 8.4893 (9.2685)	LR 7.033e-05
0: TRAIN [0][50/1885]	Time 0.100 (0.148)	Data 2.15e-04 (5.78e-03)	Tok/s 90990 (96041)	Loss/tok 8.2719 (9.1504)	LR 8.854e-05
0: TRAIN [0][60/1885]	Time 0.145 (0.147)	Data 2.15e-04 (4.86e-03)	Tok/s 100070 (96531)	Loss/tok 8.2069 (9.0109)	LR 1.115e-04
0: TRAIN [0][70/1885]	Time 0.065 (0.146)	Data 2.17e-04 (4.21e-03)	Tok/s 71248 (96696)	Loss/tok 7.6554 (8.8835)	LR 1.403e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][80/1885]	Time 0.195 (0.147)	Data 2.21e-04 (3.72e-03)	Tok/s 105086 (96806)	Loss/tok 8.1027 (8.7883)	LR 1.726e-04
0: TRAIN [0][90/1885]	Time 0.147 (0.145)	Data 2.12e-04 (3.33e-03)	Tok/s 100300 (96352)	Loss/tok 7.9250 (8.7062)	LR 2.173e-04
0: TRAIN [0][100/1885]	Time 0.101 (0.145)	Data 1.87e-04 (3.02e-03)	Tok/s 91451 (96571)	Loss/tok 7.6971 (8.6247)	LR 2.736e-04
0: TRAIN [0][110/1885]	Time 0.195 (0.147)	Data 2.18e-04 (2.77e-03)	Tok/s 104394 (96743)	Loss/tok 7.9462 (8.5553)	LR 3.445e-04
0: TRAIN [0][120/1885]	Time 0.102 (0.146)	Data 2.29e-04 (2.56e-03)	Tok/s 89122 (96587)	Loss/tok 7.6930 (8.4980)	LR 4.337e-04
0: TRAIN [0][130/1885]	Time 0.103 (0.146)	Data 1.48e-04 (2.38e-03)	Tok/s 89700 (96309)	Loss/tok 7.5697 (8.4490)	LR 5.460e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][140/1885]	Time 0.245 (0.145)	Data 1.39e-04 (2.22e-03)	Tok/s 107219 (96086)	Loss/tok 8.0136 (8.4110)	LR 6.564e-04
0: TRAIN [0][150/1885]	Time 0.147 (0.145)	Data 2.16e-04 (2.09e-03)	Tok/s 99788 (96250)	Loss/tok 7.7169 (8.3666)	LR 8.263e-04
0: TRAIN [0][160/1885]	Time 0.103 (0.145)	Data 1.46e-04 (1.97e-03)	Tok/s 90021 (96012)	Loss/tok 7.3579 (8.3232)	LR 1.040e-03
0: TRAIN [0][170/1885]	Time 0.147 (0.146)	Data 2.11e-04 (1.87e-03)	Tok/s 100236 (96264)	Loss/tok 7.4815 (8.2763)	LR 1.310e-03
0: TRAIN [0][180/1885]	Time 0.146 (0.145)	Data 1.84e-04 (1.77e-03)	Tok/s 101105 (96121)	Loss/tok 7.4141 (8.2340)	LR 1.649e-03
0: TRAIN [0][190/1885]	Time 0.102 (0.145)	Data 1.06e-04 (1.69e-03)	Tok/s 88909 (96196)	Loss/tok 6.8692 (8.1797)	LR 2.076e-03
0: TRAIN [0][200/1885]	Time 0.194 (0.145)	Data 1.09e-04 (1.61e-03)	Tok/s 105126 (96108)	Loss/tok 7.0863 (8.1238)	LR 2.613e-03
0: TRAIN [0][210/1885]	Time 0.195 (0.146)	Data 2.24e-04 (1.54e-03)	Tok/s 104255 (96213)	Loss/tok 6.9698 (8.0572)	LR 2.800e-03
0: TRAIN [0][220/1885]	Time 0.102 (0.146)	Data 1.86e-04 (1.48e-03)	Tok/s 88110 (96245)	Loss/tok 6.2419 (7.9934)	LR 2.800e-03
0: TRAIN [0][230/1885]	Time 0.244 (0.147)	Data 2.15e-04 (1.42e-03)	Tok/s 106437 (96270)	Loss/tok 6.8469 (7.9286)	LR 2.800e-03
0: TRAIN [0][240/1885]	Time 0.194 (0.148)	Data 2.03e-04 (1.37e-03)	Tok/s 104654 (96402)	Loss/tok 6.5259 (7.8576)	LR 2.800e-03
0: TRAIN [0][250/1885]	Time 0.148 (0.147)	Data 2.17e-04 (1.32e-03)	Tok/s 100073 (96371)	Loss/tok 6.2440 (7.7989)	LR 2.800e-03
0: TRAIN [0][260/1885]	Time 0.146 (0.146)	Data 2.19e-04 (1.28e-03)	Tok/s 101187 (96172)	Loss/tok 6.2327 (7.7479)	LR 2.800e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][270/1885]	Time 0.195 (0.146)	Data 1.79e-04 (1.24e-03)	Tok/s 104902 (96087)	Loss/tok 6.1291 (7.6853)	LR 2.800e-03
0: TRAIN [0][280/1885]	Time 0.146 (0.147)	Data 2.18e-04 (1.20e-03)	Tok/s 101134 (96130)	Loss/tok 5.8652 (7.6213)	LR 2.800e-03
0: TRAIN [0][290/1885]	Time 0.149 (0.147)	Data 1.61e-04 (1.17e-03)	Tok/s 100188 (96148)	Loss/tok 5.8066 (7.5595)	LR 2.800e-03
0: TRAIN [0][300/1885]	Time 0.148 (0.147)	Data 1.51e-04 (1.14e-03)	Tok/s 100056 (96167)	Loss/tok 5.5934 (7.4963)	LR 2.800e-03
0: TRAIN [0][310/1885]	Time 0.148 (0.146)	Data 2.15e-04 (1.11e-03)	Tok/s 98449 (96022)	Loss/tok 5.5422 (7.4443)	LR 2.800e-03
0: TRAIN [0][320/1885]	Time 0.102 (0.147)	Data 2.14e-04 (1.08e-03)	Tok/s 86825 (96186)	Loss/tok 5.1425 (7.3759)	LR 2.800e-03
0: TRAIN [0][330/1885]	Time 0.103 (0.147)	Data 1.44e-04 (1.05e-03)	Tok/s 89797 (96190)	Loss/tok 5.0205 (7.3157)	LR 2.800e-03
0: TRAIN [0][340/1885]	Time 0.101 (0.146)	Data 2.14e-04 (1.03e-03)	Tok/s 90494 (96075)	Loss/tok 4.9190 (7.2639)	LR 2.800e-03
0: TRAIN [0][350/1885]	Time 0.103 (0.146)	Data 2.01e-04 (1.00e-03)	Tok/s 89060 (96044)	Loss/tok 4.8117 (7.2075)	LR 2.800e-03
0: TRAIN [0][360/1885]	Time 0.104 (0.146)	Data 1.45e-04 (9.82e-04)	Tok/s 87782 (95931)	Loss/tok 4.8110 (7.1581)	LR 2.800e-03
0: TRAIN [0][370/1885]	Time 0.102 (0.145)	Data 1.76e-04 (9.60e-04)	Tok/s 90009 (95841)	Loss/tok 4.7373 (7.1061)	LR 2.800e-03
0: TRAIN [0][380/1885]	Time 0.147 (0.146)	Data 2.17e-04 (9.41e-04)	Tok/s 100325 (95898)	Loss/tok 4.9453 (7.0471)	LR 2.800e-03
0: TRAIN [0][390/1885]	Time 0.148 (0.146)	Data 2.19e-04 (9.22e-04)	Tok/s 99717 (96004)	Loss/tok 4.8192 (6.9867)	LR 2.800e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][400/1885]	Time 0.105 (0.146)	Data 2.19e-04 (9.04e-04)	Tok/s 86008 (96002)	Loss/tok 4.4632 (6.9334)	LR 2.800e-03
0: TRAIN [0][410/1885]	Time 0.101 (0.146)	Data 2.17e-04 (8.87e-04)	Tok/s 88791 (95957)	Loss/tok 4.2472 (6.8825)	LR 2.800e-03
0: TRAIN [0][420/1885]	Time 0.149 (0.145)	Data 2.16e-04 (8.70e-04)	Tok/s 100025 (95897)	Loss/tok 4.5272 (6.8345)	LR 2.800e-03
0: TRAIN [0][430/1885]	Time 0.149 (0.146)	Data 1.81e-04 (8.55e-04)	Tok/s 98483 (95912)	Loss/tok 4.5840 (6.7812)	LR 2.800e-03
0: TRAIN [0][440/1885]	Time 0.197 (0.145)	Data 1.86e-04 (8.40e-04)	Tok/s 103049 (95843)	Loss/tok 4.7888 (6.7351)	LR 2.800e-03
0: TRAIN [0][450/1885]	Time 0.105 (0.145)	Data 2.18e-04 (8.26e-04)	Tok/s 86486 (95774)	Loss/tok 4.1712 (6.6911)	LR 2.800e-03
0: TRAIN [0][460/1885]	Time 0.149 (0.145)	Data 2.19e-04 (8.12e-04)	Tok/s 98237 (95736)	Loss/tok 4.4336 (6.6480)	LR 2.800e-03
0: TRAIN [0][470/1885]	Time 0.197 (0.145)	Data 1.10e-04 (7.99e-04)	Tok/s 103975 (95726)	Loss/tok 4.5137 (6.5977)	LR 2.800e-03
0: TRAIN [0][480/1885]	Time 0.102 (0.145)	Data 1.55e-04 (7.86e-04)	Tok/s 88008 (95685)	Loss/tok 4.0430 (6.5545)	LR 2.800e-03
0: TRAIN [0][490/1885]	Time 0.193 (0.145)	Data 1.11e-04 (7.74e-04)	Tok/s 106110 (95716)	Loss/tok 4.4037 (6.5090)	LR 2.800e-03
0: TRAIN [0][500/1885]	Time 0.100 (0.145)	Data 2.15e-04 (7.62e-04)	Tok/s 93003 (95704)	Loss/tok 3.8951 (6.4683)	LR 2.800e-03
0: TRAIN [0][510/1885]	Time 0.194 (0.145)	Data 2.15e-04 (7.52e-04)	Tok/s 103726 (95806)	Loss/tok 4.5800 (6.4205)	LR 2.800e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][520/1885]	Time 0.146 (0.145)	Data 2.22e-04 (7.41e-04)	Tok/s 99003 (95843)	Loss/tok 4.1671 (6.3772)	LR 2.800e-03
0: TRAIN [0][530/1885]	Time 0.102 (0.145)	Data 2.16e-04 (7.31e-04)	Tok/s 89144 (95850)	Loss/tok 3.8644 (6.3375)	LR 2.800e-03
0: TRAIN [0][540/1885]	Time 0.101 (0.145)	Data 2.17e-04 (7.22e-04)	Tok/s 90262 (95933)	Loss/tok 3.9542 (6.2946)	LR 2.800e-03
0: TRAIN [0][550/1885]	Time 0.102 (0.145)	Data 2.16e-04 (7.12e-04)	Tok/s 89085 (95932)	Loss/tok 3.8821 (6.2587)	LR 2.800e-03
0: TRAIN [0][560/1885]	Time 0.147 (0.146)	Data 2.15e-04 (7.04e-04)	Tok/s 99223 (95955)	Loss/tok 4.0774 (6.2187)	LR 2.800e-03
0: TRAIN [0][570/1885]	Time 0.102 (0.145)	Data 2.15e-04 (6.95e-04)	Tok/s 87362 (95955)	Loss/tok 3.7452 (6.1840)	LR 2.800e-03
0: TRAIN [0][580/1885]	Time 0.102 (0.145)	Data 2.16e-04 (6.87e-04)	Tok/s 87787 (95969)	Loss/tok 3.8541 (6.1490)	LR 2.800e-03
0: TRAIN [0][590/1885]	Time 0.102 (0.145)	Data 2.17e-04 (6.79e-04)	Tok/s 90370 (95915)	Loss/tok 3.8399 (6.1181)	LR 2.800e-03
0: TRAIN [0][600/1885]	Time 0.148 (0.145)	Data 2.19e-04 (6.71e-04)	Tok/s 98071 (95959)	Loss/tok 4.0150 (6.0837)	LR 2.800e-03
0: TRAIN [0][610/1885]	Time 0.102 (0.145)	Data 2.17e-04 (6.64e-04)	Tok/s 87216 (95963)	Loss/tok 3.7153 (6.0514)	LR 2.800e-03
0: TRAIN [0][620/1885]	Time 0.146 (0.145)	Data 1.07e-04 (6.56e-04)	Tok/s 99781 (96037)	Loss/tok 4.0883 (6.0172)	LR 2.800e-03
0: TRAIN [0][630/1885]	Time 0.059 (0.146)	Data 2.16e-04 (6.49e-04)	Tok/s 79188 (96082)	Loss/tok 3.1453 (5.9833)	LR 2.800e-03
0: TRAIN [0][640/1885]	Time 0.146 (0.146)	Data 2.13e-04 (6.42e-04)	Tok/s 100833 (96121)	Loss/tok 4.0727 (5.9519)	LR 2.800e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][650/1885]	Time 0.147 (0.146)	Data 2.17e-04 (6.35e-04)	Tok/s 100477 (96141)	Loss/tok 4.0361 (5.9229)	LR 2.800e-03
0: TRAIN [0][660/1885]	Time 0.060 (0.146)	Data 2.15e-04 (6.29e-04)	Tok/s 75729 (96172)	Loss/tok 3.0235 (5.8915)	LR 2.800e-03
0: TRAIN [0][670/1885]	Time 0.101 (0.146)	Data 1.08e-04 (6.22e-04)	Tok/s 90096 (96208)	Loss/tok 3.6821 (5.8621)	LR 2.800e-03
0: TRAIN [0][680/1885]	Time 0.146 (0.146)	Data 2.08e-04 (6.16e-04)	Tok/s 98692 (96221)	Loss/tok 3.9323 (5.8345)	LR 2.800e-03
0: TRAIN [0][690/1885]	Time 0.245 (0.146)	Data 2.18e-04 (6.10e-04)	Tok/s 108126 (96227)	Loss/tok 4.1934 (5.8076)	LR 2.800e-03
0: TRAIN [0][700/1885]	Time 0.146 (0.146)	Data 2.29e-04 (6.04e-04)	Tok/s 100881 (96201)	Loss/tok 3.9890 (5.7833)	LR 2.800e-03
0: TRAIN [0][710/1885]	Time 0.101 (0.146)	Data 2.15e-04 (5.99e-04)	Tok/s 89244 (96248)	Loss/tok 3.6707 (5.7563)	LR 2.800e-03
0: TRAIN [0][720/1885]	Time 0.101 (0.146)	Data 2.03e-04 (5.94e-04)	Tok/s 89107 (96243)	Loss/tok 3.6921 (5.7322)	LR 2.800e-03
0: TRAIN [0][730/1885]	Time 0.102 (0.146)	Data 2.16e-04 (5.88e-04)	Tok/s 87643 (96238)	Loss/tok 3.6595 (5.7076)	LR 2.800e-03
0: TRAIN [0][740/1885]	Time 0.195 (0.146)	Data 1.08e-04 (5.82e-04)	Tok/s 104343 (96237)	Loss/tok 4.0993 (5.6848)	LR 2.800e-03
0: TRAIN [0][750/1885]	Time 0.146 (0.146)	Data 1.05e-04 (5.76e-04)	Tok/s 101602 (96263)	Loss/tok 4.0002 (5.6613)	LR 2.800e-03
0: TRAIN [0][760/1885]	Time 0.101 (0.146)	Data 2.02e-04 (5.70e-04)	Tok/s 90416 (96274)	Loss/tok 3.7162 (5.6388)	LR 2.800e-03
0: TRAIN [0][770/1885]	Time 0.060 (0.146)	Data 2.17e-04 (5.65e-04)	Tok/s 75617 (96254)	Loss/tok 3.0463 (5.6169)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][780/1885]	Time 0.194 (0.146)	Data 2.15e-04 (5.61e-04)	Tok/s 105076 (96301)	Loss/tok 3.9610 (5.5929)	LR 2.800e-03
0: TRAIN [0][790/1885]	Time 0.147 (0.146)	Data 2.16e-04 (5.56e-04)	Tok/s 99478 (96309)	Loss/tok 3.9251 (5.5717)	LR 2.800e-03
0: TRAIN [0][800/1885]	Time 0.102 (0.146)	Data 2.16e-04 (5.52e-04)	Tok/s 90378 (96324)	Loss/tok 3.5042 (5.5503)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][810/1885]	Time 0.147 (0.146)	Data 2.15e-04 (5.48e-04)	Tok/s 99224 (96343)	Loss/tok 3.8838 (5.5295)	LR 2.800e-03
0: TRAIN [0][820/1885]	Time 0.060 (0.145)	Data 1.07e-04 (5.43e-04)	Tok/s 76565 (96278)	Loss/tok 2.8824 (5.5130)	LR 2.800e-03
0: TRAIN [0][830/1885]	Time 0.101 (0.145)	Data 2.12e-04 (5.39e-04)	Tok/s 88721 (96267)	Loss/tok 3.6046 (5.4942)	LR 2.800e-03
0: TRAIN [0][840/1885]	Time 0.101 (0.145)	Data 2.22e-04 (5.35e-04)	Tok/s 90134 (96270)	Loss/tok 3.4889 (5.4748)	LR 2.800e-03
0: TRAIN [0][850/1885]	Time 0.148 (0.145)	Data 2.18e-04 (5.31e-04)	Tok/s 100411 (96284)	Loss/tok 3.5955 (5.4542)	LR 2.800e-03
0: TRAIN [0][860/1885]	Time 0.146 (0.145)	Data 2.29e-04 (5.27e-04)	Tok/s 100975 (96257)	Loss/tok 3.7613 (5.4370)	LR 2.800e-03
0: TRAIN [0][870/1885]	Time 0.102 (0.145)	Data 2.17e-04 (5.24e-04)	Tok/s 88395 (96264)	Loss/tok 3.5177 (5.4186)	LR 2.800e-03
0: TRAIN [0][880/1885]	Time 0.101 (0.145)	Data 2.18e-04 (5.20e-04)	Tok/s 90717 (96303)	Loss/tok 3.4022 (5.3992)	LR 2.800e-03
0: TRAIN [0][890/1885]	Time 0.146 (0.145)	Data 2.20e-04 (5.17e-04)	Tok/s 99545 (96322)	Loss/tok 3.7700 (5.3812)	LR 2.800e-03
0: TRAIN [0][900/1885]	Time 0.146 (0.145)	Data 2.28e-04 (5.13e-04)	Tok/s 99808 (96325)	Loss/tok 3.8101 (5.3645)	LR 2.800e-03
0: TRAIN [0][910/1885]	Time 0.103 (0.145)	Data 2.16e-04 (5.10e-04)	Tok/s 89114 (96357)	Loss/tok 3.5074 (5.3468)	LR 2.800e-03
0: TRAIN [0][920/1885]	Time 0.101 (0.145)	Data 2.14e-04 (5.07e-04)	Tok/s 93761 (96368)	Loss/tok 3.5419 (5.3299)	LR 2.800e-03
0: TRAIN [0][930/1885]	Time 0.101 (0.145)	Data 2.15e-04 (5.04e-04)	Tok/s 87884 (96394)	Loss/tok 3.4559 (5.3127)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][940/1885]	Time 0.193 (0.145)	Data 2.18e-04 (5.01e-04)	Tok/s 106038 (96422)	Loss/tok 4.0929 (5.2959)	LR 2.800e-03
0: TRAIN [0][950/1885]	Time 0.194 (0.145)	Data 1.05e-04 (4.98e-04)	Tok/s 105361 (96433)	Loss/tok 3.7946 (5.2798)	LR 2.800e-03
0: TRAIN [0][960/1885]	Time 0.146 (0.145)	Data 2.15e-04 (4.94e-04)	Tok/s 101500 (96430)	Loss/tok 3.6262 (5.2646)	LR 2.800e-03
0: TRAIN [0][970/1885]	Time 0.101 (0.145)	Data 2.21e-04 (4.91e-04)	Tok/s 89247 (96431)	Loss/tok 3.4016 (5.2495)	LR 2.800e-03
0: TRAIN [0][980/1885]	Time 0.102 (0.145)	Data 2.15e-04 (4.89e-04)	Tok/s 87250 (96403)	Loss/tok 3.3599 (5.2355)	LR 2.800e-03
0: TRAIN [0][990/1885]	Time 0.194 (0.145)	Data 1.59e-04 (4.86e-04)	Tok/s 105064 (96386)	Loss/tok 3.9249 (5.2218)	LR 2.800e-03
0: TRAIN [0][1000/1885]	Time 0.192 (0.145)	Data 2.15e-04 (4.83e-04)	Tok/s 106047 (96375)	Loss/tok 3.8574 (5.2080)	LR 2.800e-03
0: TRAIN [0][1010/1885]	Time 0.147 (0.144)	Data 2.17e-04 (4.80e-04)	Tok/s 101096 (96368)	Loss/tok 3.6235 (5.1943)	LR 2.800e-03
0: TRAIN [0][1020/1885]	Time 0.194 (0.145)	Data 1.08e-04 (4.77e-04)	Tok/s 105811 (96411)	Loss/tok 3.8464 (5.1783)	LR 2.800e-03
0: TRAIN [0][1030/1885]	Time 0.194 (0.144)	Data 1.07e-04 (4.74e-04)	Tok/s 103750 (96402)	Loss/tok 4.0160 (5.1654)	LR 2.800e-03
0: TRAIN [0][1040/1885]	Time 0.060 (0.144)	Data 1.28e-04 (4.70e-04)	Tok/s 75580 (96380)	Loss/tok 2.8589 (5.1531)	LR 2.800e-03
0: TRAIN [0][1050/1885]	Time 0.101 (0.144)	Data 2.17e-04 (4.68e-04)	Tok/s 90891 (96329)	Loss/tok 3.5057 (5.1428)	LR 2.800e-03
0: TRAIN [0][1060/1885]	Time 0.101 (0.144)	Data 2.15e-04 (4.65e-04)	Tok/s 88980 (96326)	Loss/tok 3.4730 (5.1297)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1070/1885]	Time 0.147 (0.144)	Data 2.14e-04 (4.63e-04)	Tok/s 99930 (96311)	Loss/tok 3.6497 (5.1173)	LR 2.800e-03
0: TRAIN [0][1080/1885]	Time 0.246 (0.144)	Data 1.06e-04 (4.60e-04)	Tok/s 106818 (96337)	Loss/tok 3.9997 (5.1033)	LR 2.800e-03
0: TRAIN [0][1090/1885]	Time 0.147 (0.144)	Data 2.18e-04 (4.57e-04)	Tok/s 98779 (96339)	Loss/tok 3.7925 (5.0903)	LR 2.800e-03
0: TRAIN [0][1100/1885]	Time 0.194 (0.144)	Data 2.19e-04 (4.55e-04)	Tok/s 105665 (96326)	Loss/tok 3.9007 (5.0790)	LR 2.800e-03
0: TRAIN [0][1110/1885]	Time 0.146 (0.144)	Data 2.15e-04 (4.53e-04)	Tok/s 98980 (96327)	Loss/tok 3.6340 (5.0668)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1120/1885]	Time 0.193 (0.144)	Data 2.28e-04 (4.51e-04)	Tok/s 106258 (96324)	Loss/tok 3.9850 (5.0553)	LR 2.800e-03
0: TRAIN [0][1130/1885]	Time 0.196 (0.144)	Data 2.16e-04 (4.49e-04)	Tok/s 105335 (96337)	Loss/tok 3.7014 (5.0428)	LR 2.800e-03
0: TRAIN [0][1140/1885]	Time 0.060 (0.144)	Data 1.07e-04 (4.46e-04)	Tok/s 75935 (96320)	Loss/tok 2.8845 (5.0310)	LR 2.800e-03
0: TRAIN [0][1150/1885]	Time 0.146 (0.144)	Data 1.06e-04 (4.43e-04)	Tok/s 101117 (96296)	Loss/tok 3.5762 (5.0208)	LR 2.800e-03
0: TRAIN [0][1160/1885]	Time 0.102 (0.144)	Data 2.15e-04 (4.40e-04)	Tok/s 89384 (96291)	Loss/tok 3.3238 (5.0097)	LR 2.800e-03
0: TRAIN [0][1170/1885]	Time 0.194 (0.144)	Data 2.21e-04 (4.38e-04)	Tok/s 105833 (96312)	Loss/tok 3.8035 (4.9974)	LR 2.800e-03
0: TRAIN [0][1180/1885]	Time 0.147 (0.144)	Data 2.16e-04 (4.37e-04)	Tok/s 100205 (96350)	Loss/tok 3.6373 (4.9847)	LR 2.800e-03
0: TRAIN [0][1190/1885]	Time 0.146 (0.144)	Data 2.16e-04 (4.35e-04)	Tok/s 101421 (96368)	Loss/tok 3.6374 (4.9736)	LR 2.800e-03
0: TRAIN [0][1200/1885]	Time 0.101 (0.144)	Data 2.15e-04 (4.33e-04)	Tok/s 90340 (96376)	Loss/tok 3.2452 (4.9622)	LR 2.800e-03
0: TRAIN [0][1210/1885]	Time 0.101 (0.144)	Data 2.16e-04 (4.31e-04)	Tok/s 88171 (96411)	Loss/tok 3.3724 (4.9496)	LR 2.800e-03
0: TRAIN [0][1220/1885]	Time 0.145 (0.144)	Data 2.16e-04 (4.29e-04)	Tok/s 101438 (96407)	Loss/tok 3.7420 (4.9398)	LR 2.800e-03
0: TRAIN [0][1230/1885]	Time 0.101 (0.144)	Data 2.18e-04 (4.28e-04)	Tok/s 89457 (96382)	Loss/tok 3.3797 (4.9306)	LR 2.800e-03
0: TRAIN [0][1240/1885]	Time 0.101 (0.144)	Data 2.14e-04 (4.26e-04)	Tok/s 92248 (96417)	Loss/tok 3.2929 (4.9194)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1250/1885]	Time 0.101 (0.144)	Data 2.13e-04 (4.24e-04)	Tok/s 90414 (96434)	Loss/tok 3.3926 (4.9093)	LR 2.800e-03
0: TRAIN [0][1260/1885]	Time 0.195 (0.144)	Data 2.16e-04 (4.22e-04)	Tok/s 104763 (96439)	Loss/tok 3.8865 (4.8995)	LR 2.800e-03
0: TRAIN [0][1270/1885]	Time 0.147 (0.144)	Data 2.15e-04 (4.21e-04)	Tok/s 99646 (96433)	Loss/tok 3.6079 (4.8897)	LR 2.800e-03
0: TRAIN [0][1280/1885]	Time 0.193 (0.144)	Data 2.17e-04 (4.19e-04)	Tok/s 106997 (96445)	Loss/tok 3.7884 (4.8796)	LR 2.800e-03
0: TRAIN [0][1290/1885]	Time 0.195 (0.144)	Data 2.17e-04 (4.18e-04)	Tok/s 105139 (96467)	Loss/tok 3.8513 (4.8694)	LR 2.800e-03
0: TRAIN [0][1300/1885]	Time 0.102 (0.144)	Data 1.99e-04 (4.16e-04)	Tok/s 89035 (96460)	Loss/tok 3.3523 (4.8600)	LR 2.800e-03
0: TRAIN [0][1310/1885]	Time 0.102 (0.144)	Data 1.75e-04 (4.14e-04)	Tok/s 90599 (96461)	Loss/tok 3.3742 (4.8510)	LR 2.800e-03
0: TRAIN [0][1320/1885]	Time 0.101 (0.144)	Data 2.23e-04 (4.13e-04)	Tok/s 89725 (96454)	Loss/tok 3.4386 (4.8422)	LR 2.800e-03
0: TRAIN [0][1330/1885]	Time 0.194 (0.144)	Data 2.18e-04 (4.11e-04)	Tok/s 104293 (96447)	Loss/tok 3.7858 (4.8333)	LR 2.800e-03
0: TRAIN [0][1340/1885]	Time 0.195 (0.144)	Data 2.16e-04 (4.10e-04)	Tok/s 105482 (96428)	Loss/tok 3.8131 (4.8251)	LR 2.800e-03
0: TRAIN [0][1350/1885]	Time 0.101 (0.144)	Data 2.15e-04 (4.09e-04)	Tok/s 88132 (96422)	Loss/tok 3.3951 (4.8165)	LR 2.800e-03
0: TRAIN [0][1360/1885]	Time 0.060 (0.144)	Data 2.15e-04 (4.07e-04)	Tok/s 75534 (96420)	Loss/tok 2.7568 (4.8077)	LR 2.800e-03
0: TRAIN [0][1370/1885]	Time 0.195 (0.144)	Data 2.16e-04 (4.06e-04)	Tok/s 106112 (96430)	Loss/tok 3.7197 (4.7988)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [0][1380/1885]	Time 0.147 (0.144)	Data 1.06e-04 (4.04e-04)	Tok/s 99650 (96439)	Loss/tok 3.4769 (4.7900)	LR 2.800e-03
0: TRAIN [0][1390/1885]	Time 0.146 (0.144)	Data 1.07e-04 (4.02e-04)	Tok/s 101982 (96439)	Loss/tok 3.5446 (4.7818)	LR 2.800e-03
0: TRAIN [0][1400/1885]	Time 0.147 (0.144)	Data 1.08e-04 (4.00e-04)	Tok/s 99655 (96433)	Loss/tok 3.5132 (4.7738)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1410/1885]	Time 0.194 (0.144)	Data 1.07e-04 (3.98e-04)	Tok/s 106359 (96461)	Loss/tok 3.6187 (4.7644)	LR 2.800e-03
0: TRAIN [0][1420/1885]	Time 0.146 (0.144)	Data 1.06e-04 (3.96e-04)	Tok/s 99861 (96452)	Loss/tok 3.5400 (4.7566)	LR 2.800e-03
0: TRAIN [0][1430/1885]	Time 0.059 (0.144)	Data 2.18e-04 (3.95e-04)	Tok/s 76255 (96430)	Loss/tok 2.6992 (4.7492)	LR 2.800e-03
0: TRAIN [0][1440/1885]	Time 0.102 (0.144)	Data 2.05e-04 (3.93e-04)	Tok/s 88069 (96479)	Loss/tok 3.2599 (4.7392)	LR 2.800e-03
0: TRAIN [0][1450/1885]	Time 0.101 (0.144)	Data 2.15e-04 (3.92e-04)	Tok/s 92245 (96499)	Loss/tok 3.3353 (4.7306)	LR 2.800e-03
0: TRAIN [0][1460/1885]	Time 0.101 (0.144)	Data 2.15e-04 (3.91e-04)	Tok/s 92175 (96469)	Loss/tok 3.2616 (4.7237)	LR 2.800e-03
0: TRAIN [0][1470/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.90e-04)	Tok/s 100819 (96459)	Loss/tok 3.5658 (4.7161)	LR 2.800e-03
0: TRAIN [0][1480/1885]	Time 0.245 (0.144)	Data 2.13e-04 (3.88e-04)	Tok/s 107106 (96464)	Loss/tok 3.8999 (4.7085)	LR 2.800e-03
0: TRAIN [0][1490/1885]	Time 0.101 (0.144)	Data 2.19e-04 (3.87e-04)	Tok/s 88841 (96456)	Loss/tok 3.2102 (4.7015)	LR 2.800e-03
0: TRAIN [0][1500/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.86e-04)	Tok/s 100697 (96464)	Loss/tok 3.5108 (4.6938)	LR 2.800e-03
0: TRAIN [0][1510/1885]	Time 0.101 (0.144)	Data 2.19e-04 (3.85e-04)	Tok/s 89002 (96482)	Loss/tok 3.2406 (4.6862)	LR 2.800e-03
0: TRAIN [0][1520/1885]	Time 0.101 (0.144)	Data 2.15e-04 (3.84e-04)	Tok/s 91561 (96500)	Loss/tok 3.3083 (4.6784)	LR 2.800e-03
0: TRAIN [0][1530/1885]	Time 0.147 (0.144)	Data 2.19e-04 (3.83e-04)	Tok/s 98656 (96490)	Loss/tok 3.6169 (4.6718)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1540/1885]	Time 0.101 (0.144)	Data 2.14e-04 (3.82e-04)	Tok/s 89966 (96489)	Loss/tok 3.4023 (4.6649)	LR 2.800e-03
0: TRAIN [0][1550/1885]	Time 0.147 (0.144)	Data 2.15e-04 (3.81e-04)	Tok/s 99959 (96470)	Loss/tok 3.4442 (4.6584)	LR 2.800e-03
0: TRAIN [0][1560/1885]	Time 0.147 (0.144)	Data 2.14e-04 (3.80e-04)	Tok/s 98816 (96466)	Loss/tok 3.5752 (4.6513)	LR 2.800e-03
0: TRAIN [0][1570/1885]	Time 0.147 (0.144)	Data 2.13e-04 (3.78e-04)	Tok/s 100682 (96466)	Loss/tok 3.4244 (4.6445)	LR 2.800e-03
0: TRAIN [0][1580/1885]	Time 0.194 (0.144)	Data 2.17e-04 (3.77e-04)	Tok/s 106201 (96466)	Loss/tok 3.7213 (4.6376)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1590/1885]	Time 0.193 (0.144)	Data 2.14e-04 (3.76e-04)	Tok/s 106538 (96488)	Loss/tok 3.7073 (4.6302)	LR 2.800e-03
0: TRAIN [0][1600/1885]	Time 0.147 (0.144)	Data 2.15e-04 (3.75e-04)	Tok/s 98980 (96491)	Loss/tok 3.6013 (4.6238)	LR 2.800e-03
0: TRAIN [0][1610/1885]	Time 0.102 (0.144)	Data 2.13e-04 (3.74e-04)	Tok/s 90337 (96494)	Loss/tok 3.2838 (4.6172)	LR 2.800e-03
0: TRAIN [0][1620/1885]	Time 0.059 (0.144)	Data 2.15e-04 (3.73e-04)	Tok/s 78014 (96461)	Loss/tok 2.6494 (4.6116)	LR 2.800e-03
0: TRAIN [0][1630/1885]	Time 0.101 (0.144)	Data 2.15e-04 (3.72e-04)	Tok/s 91858 (96440)	Loss/tok 3.4532 (4.6060)	LR 2.800e-03
0: TRAIN [0][1640/1885]	Time 0.147 (0.144)	Data 2.25e-04 (3.71e-04)	Tok/s 100358 (96465)	Loss/tok 3.5567 (4.5991)	LR 2.800e-03
0: TRAIN [0][1650/1885]	Time 0.100 (0.144)	Data 2.18e-04 (3.71e-04)	Tok/s 91264 (96451)	Loss/tok 3.3387 (4.5932)	LR 2.800e-03
0: TRAIN [0][1660/1885]	Time 0.059 (0.144)	Data 2.17e-04 (3.70e-04)	Tok/s 77957 (96444)	Loss/tok 2.6487 (4.5874)	LR 2.800e-03
0: TRAIN [0][1670/1885]	Time 0.146 (0.144)	Data 2.15e-04 (3.69e-04)	Tok/s 100706 (96443)	Loss/tok 3.4672 (4.5813)	LR 2.800e-03
0: TRAIN [0][1680/1885]	Time 0.146 (0.143)	Data 2.18e-04 (3.68e-04)	Tok/s 99346 (96442)	Loss/tok 3.5441 (4.5752)	LR 2.800e-03
0: TRAIN [0][1690/1885]	Time 0.193 (0.144)	Data 2.17e-04 (3.67e-04)	Tok/s 105662 (96460)	Loss/tok 3.7312 (4.5687)	LR 2.800e-03
0: TRAIN [0][1700/1885]	Time 0.145 (0.143)	Data 2.16e-04 (3.66e-04)	Tok/s 99751 (96450)	Loss/tok 3.4546 (4.5630)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1710/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.65e-04)	Tok/s 89558 (96446)	Loss/tok 3.1822 (4.5572)	LR 2.800e-03
0: TRAIN [0][1720/1885]	Time 0.146 (0.143)	Data 2.16e-04 (3.64e-04)	Tok/s 100602 (96454)	Loss/tok 3.4685 (4.5514)	LR 2.800e-03
0: TRAIN [0][1730/1885]	Time 0.192 (0.143)	Data 2.16e-04 (3.63e-04)	Tok/s 106054 (96445)	Loss/tok 3.6490 (4.5456)	LR 2.800e-03
0: TRAIN [0][1740/1885]	Time 0.147 (0.143)	Data 2.14e-04 (3.62e-04)	Tok/s 100289 (96439)	Loss/tok 3.5071 (4.5403)	LR 2.800e-03
0: TRAIN [0][1750/1885]	Time 0.194 (0.143)	Data 2.15e-04 (3.62e-04)	Tok/s 104911 (96429)	Loss/tok 3.6604 (4.5347)	LR 2.800e-03
0: TRAIN [0][1760/1885]	Time 0.146 (0.143)	Data 2.13e-04 (3.61e-04)	Tok/s 99986 (96409)	Loss/tok 3.4772 (4.5295)	LR 2.800e-03
0: TRAIN [0][1770/1885]	Time 0.246 (0.143)	Data 1.50e-04 (3.60e-04)	Tok/s 106387 (96403)	Loss/tok 3.8153 (4.5240)	LR 2.800e-03
0: TRAIN [0][1780/1885]	Time 0.102 (0.143)	Data 1.57e-04 (3.59e-04)	Tok/s 86233 (96376)	Loss/tok 3.2389 (4.5190)	LR 2.800e-03
0: TRAIN [0][1790/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.58e-04)	Tok/s 99092 (96375)	Loss/tok 3.5855 (4.5136)	LR 2.800e-03
0: TRAIN [0][1800/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.57e-04)	Tok/s 99638 (96378)	Loss/tok 3.5602 (4.5082)	LR 2.800e-03
0: TRAIN [0][1810/1885]	Time 0.245 (0.143)	Data 2.32e-04 (3.57e-04)	Tok/s 106304 (96379)	Loss/tok 3.8967 (4.5027)	LR 2.800e-03
0: TRAIN [0][1820/1885]	Time 0.147 (0.143)	Data 2.15e-04 (3.56e-04)	Tok/s 98485 (96389)	Loss/tok 3.5283 (4.4972)	LR 2.800e-03
0: TRAIN [0][1830/1885]	Time 0.195 (0.143)	Data 2.18e-04 (3.55e-04)	Tok/s 104187 (96363)	Loss/tok 3.6872 (4.4923)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [0][1840/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.54e-04)	Tok/s 88408 (96345)	Loss/tok 3.1477 (4.4876)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [0][1850/1885]	Time 0.147 (0.143)	Data 2.15e-04 (3.54e-04)	Tok/s 98941 (96349)	Loss/tok 3.5190 (4.4823)	LR 2.800e-03
0: TRAIN [0][1860/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.53e-04)	Tok/s 91395 (96353)	Loss/tok 3.0697 (4.4769)	LR 2.800e-03
0: TRAIN [0][1870/1885]	Time 0.146 (0.143)	Data 2.22e-04 (3.52e-04)	Tok/s 100368 (96366)	Loss/tok 3.4512 (4.4716)	LR 2.800e-03
0: TRAIN [0][1880/1885]	Time 0.146 (0.143)	Data 2.16e-04 (3.51e-04)	Tok/s 100730 (96380)	Loss/tok 3.5164 (4.4660)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593832061022, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593832061023, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.674 (0.674)	Decoder iters 149.0 (149.0)	Tok/s 24312 (24312)
0: Running moses detokenizer
0: BLEU(score=19.607083981507174, counts=[34886, 15976, 8465, 4667], totals=[66721, 63718, 60715, 57718], precisions=[52.28638659492513, 25.072977808468565, 13.942188915424524, 8.085865761114384], bp=1.0, sys_len=66721, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593832062938, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1961, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593832062938, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.4620	Test BLEU: 19.61
0: Performance: Epoch: 0	Training: 770658 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593832062938, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593832062938, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593832062938, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 4183990009
0: TRAIN [1][0/1885]	Time 0.331 (0.331)	Data 2.13e-01 (2.13e-01)	Tok/s 27545 (27545)	Loss/tok 3.1980 (3.1980)	LR 2.800e-03
0: TRAIN [1][10/1885]	Time 0.148 (0.173)	Data 1.65e-04 (1.96e-02)	Tok/s 100729 (92097)	Loss/tok 3.3891 (3.4902)	LR 2.800e-03
0: TRAIN [1][20/1885]	Time 0.243 (0.154)	Data 1.88e-04 (1.03e-02)	Tok/s 106508 (93842)	Loss/tok 3.7632 (3.4511)	LR 2.800e-03
0: TRAIN [1][30/1885]	Time 0.192 (0.157)	Data 1.87e-04 (7.07e-03)	Tok/s 106546 (95915)	Loss/tok 3.5694 (3.4704)	LR 2.800e-03
0: TRAIN [1][40/1885]	Time 0.101 (0.147)	Data 2.17e-04 (5.39e-03)	Tok/s 90603 (95455)	Loss/tok 3.2300 (3.4363)	LR 2.800e-03
0: TRAIN [1][50/1885]	Time 0.147 (0.147)	Data 2.18e-04 (4.38e-03)	Tok/s 100023 (95887)	Loss/tok 3.3141 (3.4312)	LR 2.800e-03
0: TRAIN [1][60/1885]	Time 0.194 (0.147)	Data 2.16e-04 (3.69e-03)	Tok/s 105205 (96072)	Loss/tok 3.5838 (3.4364)	LR 2.800e-03
0: TRAIN [1][70/1885]	Time 0.102 (0.147)	Data 2.29e-04 (3.20e-03)	Tok/s 87939 (96283)	Loss/tok 3.2445 (3.4414)	LR 2.800e-03
0: TRAIN [1][80/1885]	Time 0.102 (0.145)	Data 2.14e-04 (2.83e-03)	Tok/s 88746 (96004)	Loss/tok 3.2213 (3.4336)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][90/1885]	Time 0.244 (0.144)	Data 2.16e-04 (2.55e-03)	Tok/s 107242 (95837)	Loss/tok 3.7325 (3.4344)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][100/1885]	Time 0.146 (0.145)	Data 2.17e-04 (2.32e-03)	Tok/s 101152 (95959)	Loss/tok 3.4602 (3.4434)	LR 2.800e-03
0: TRAIN [1][110/1885]	Time 0.245 (0.144)	Data 2.16e-04 (2.13e-03)	Tok/s 105779 (95931)	Loss/tok 3.8452 (3.4442)	LR 2.800e-03
0: TRAIN [1][120/1885]	Time 0.101 (0.143)	Data 2.15e-04 (1.97e-03)	Tok/s 90127 (95849)	Loss/tok 3.1687 (3.4398)	LR 2.800e-03
0: TRAIN [1][130/1885]	Time 0.194 (0.144)	Data 2.16e-04 (1.83e-03)	Tok/s 104808 (96132)	Loss/tok 3.5271 (3.4412)	LR 2.800e-03
0: TRAIN [1][140/1885]	Time 0.060 (0.145)	Data 2.17e-04 (1.72e-03)	Tok/s 76604 (96262)	Loss/tok 2.6540 (3.4400)	LR 2.800e-03
0: TRAIN [1][150/1885]	Time 0.147 (0.144)	Data 2.18e-04 (1.62e-03)	Tok/s 100165 (96276)	Loss/tok 3.4745 (3.4398)	LR 2.800e-03
0: TRAIN [1][160/1885]	Time 0.101 (0.144)	Data 2.17e-04 (1.53e-03)	Tok/s 89899 (96259)	Loss/tok 3.1676 (3.4403)	LR 2.800e-03
0: TRAIN [1][170/1885]	Time 0.146 (0.143)	Data 2.17e-04 (1.46e-03)	Tok/s 101065 (96186)	Loss/tok 3.3915 (3.4354)	LR 2.800e-03
0: TRAIN [1][180/1885]	Time 0.195 (0.145)	Data 2.15e-04 (1.39e-03)	Tok/s 105087 (96527)	Loss/tok 3.5604 (3.4438)	LR 2.800e-03
0: TRAIN [1][190/1885]	Time 0.146 (0.145)	Data 2.18e-04 (1.33e-03)	Tok/s 100210 (96535)	Loss/tok 3.4743 (3.4434)	LR 2.800e-03
0: TRAIN [1][200/1885]	Time 0.101 (0.144)	Data 2.30e-04 (1.27e-03)	Tok/s 90304 (96511)	Loss/tok 3.2930 (3.4416)	LR 2.800e-03
0: TRAIN [1][210/1885]	Time 0.146 (0.145)	Data 2.28e-04 (1.22e-03)	Tok/s 100395 (96549)	Loss/tok 3.3799 (3.4428)	LR 2.800e-03
0: TRAIN [1][220/1885]	Time 0.102 (0.144)	Data 2.17e-04 (1.18e-03)	Tok/s 87852 (96532)	Loss/tok 3.2053 (3.4389)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][230/1885]	Time 0.102 (0.144)	Data 2.24e-04 (1.13e-03)	Tok/s 89219 (96495)	Loss/tok 3.2012 (3.4412)	LR 2.800e-03
0: TRAIN [1][240/1885]	Time 0.101 (0.144)	Data 2.15e-04 (1.10e-03)	Tok/s 90355 (96586)	Loss/tok 3.2038 (3.4425)	LR 2.800e-03
0: TRAIN [1][250/1885]	Time 0.101 (0.144)	Data 2.19e-04 (1.06e-03)	Tok/s 91300 (96614)	Loss/tok 3.0834 (3.4408)	LR 2.800e-03
0: TRAIN [1][260/1885]	Time 0.147 (0.144)	Data 2.19e-04 (1.03e-03)	Tok/s 98454 (96656)	Loss/tok 3.4286 (3.4381)	LR 2.800e-03
0: TRAIN [1][270/1885]	Time 0.146 (0.144)	Data 2.15e-04 (9.99e-04)	Tok/s 101031 (96663)	Loss/tok 3.3298 (3.4337)	LR 2.800e-03
0: TRAIN [1][280/1885]	Time 0.245 (0.144)	Data 2.17e-04 (9.71e-04)	Tok/s 105285 (96674)	Loss/tok 3.6630 (3.4326)	LR 2.800e-03
0: TRAIN [1][290/1885]	Time 0.147 (0.144)	Data 2.18e-04 (9.45e-04)	Tok/s 99828 (96759)	Loss/tok 3.4633 (3.4339)	LR 2.800e-03
0: TRAIN [1][300/1885]	Time 0.101 (0.145)	Data 2.18e-04 (9.21e-04)	Tok/s 90373 (96836)	Loss/tok 3.0924 (3.4354)	LR 2.800e-03
0: TRAIN [1][310/1885]	Time 0.102 (0.143)	Data 2.18e-04 (8.98e-04)	Tok/s 89234 (96611)	Loss/tok 3.1859 (3.4303)	LR 2.800e-03
0: TRAIN [1][320/1885]	Time 0.102 (0.144)	Data 2.15e-04 (8.77e-04)	Tok/s 88640 (96714)	Loss/tok 3.1284 (3.4329)	LR 2.800e-03
0: TRAIN [1][330/1885]	Time 0.193 (0.144)	Data 2.18e-04 (8.57e-04)	Tok/s 105665 (96714)	Loss/tok 3.6849 (3.4341)	LR 2.800e-03
0: TRAIN [1][340/1885]	Time 0.148 (0.144)	Data 2.20e-04 (8.38e-04)	Tok/s 98071 (96776)	Loss/tok 3.4594 (3.4354)	LR 2.800e-03
0: TRAIN [1][350/1885]	Time 0.148 (0.144)	Data 2.15e-04 (8.20e-04)	Tok/s 99412 (96719)	Loss/tok 3.3828 (3.4318)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][360/1885]	Time 0.146 (0.145)	Data 1.09e-04 (8.01e-04)	Tok/s 100455 (96868)	Loss/tok 3.4699 (3.4336)	LR 2.800e-03
0: TRAIN [1][370/1885]	Time 0.102 (0.145)	Data 1.15e-04 (7.82e-04)	Tok/s 88692 (96867)	Loss/tok 3.1926 (3.4336)	LR 2.800e-03
0: TRAIN [1][380/1885]	Time 0.194 (0.145)	Data 1.10e-04 (7.65e-04)	Tok/s 104560 (96933)	Loss/tok 3.5045 (3.4351)	LR 2.800e-03
0: TRAIN [1][390/1885]	Time 0.101 (0.145)	Data 1.08e-04 (7.48e-04)	Tok/s 90450 (96934)	Loss/tok 3.1070 (3.4334)	LR 2.800e-03
0: TRAIN [1][400/1885]	Time 0.101 (0.144)	Data 1.32e-04 (7.33e-04)	Tok/s 90156 (96828)	Loss/tok 3.1070 (3.4307)	LR 2.800e-03
0: TRAIN [1][410/1885]	Time 0.101 (0.144)	Data 1.08e-04 (7.19e-04)	Tok/s 89229 (96761)	Loss/tok 3.1919 (3.4287)	LR 2.800e-03
0: TRAIN [1][420/1885]	Time 0.193 (0.143)	Data 1.08e-04 (7.05e-04)	Tok/s 105240 (96769)	Loss/tok 3.5578 (3.4279)	LR 2.800e-03
0: TRAIN [1][430/1885]	Time 0.193 (0.144)	Data 1.09e-04 (6.91e-04)	Tok/s 104333 (96794)	Loss/tok 3.6758 (3.4303)	LR 2.800e-03
0: TRAIN [1][440/1885]	Time 0.245 (0.144)	Data 2.16e-04 (6.80e-04)	Tok/s 106258 (96770)	Loss/tok 3.7593 (3.4297)	LR 2.800e-03
0: TRAIN [1][450/1885]	Time 0.101 (0.144)	Data 2.19e-04 (6.70e-04)	Tok/s 90350 (96865)	Loss/tok 3.1798 (3.4313)	LR 2.800e-03
0: TRAIN [1][460/1885]	Time 0.146 (0.144)	Data 2.18e-04 (6.60e-04)	Tok/s 100560 (96906)	Loss/tok 3.4275 (3.4326)	LR 2.800e-03
0: TRAIN [1][470/1885]	Time 0.244 (0.145)	Data 2.17e-04 (6.51e-04)	Tok/s 107098 (96973)	Loss/tok 3.7664 (3.4338)	LR 2.800e-03
0: TRAIN [1][480/1885]	Time 0.146 (0.145)	Data 2.20e-04 (6.42e-04)	Tok/s 99768 (96997)	Loss/tok 3.3768 (3.4340)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][490/1885]	Time 0.146 (0.145)	Data 2.18e-04 (6.33e-04)	Tok/s 100449 (97001)	Loss/tok 3.4381 (3.4335)	LR 2.800e-03
0: TRAIN [1][500/1885]	Time 0.147 (0.145)	Data 2.16e-04 (6.25e-04)	Tok/s 99666 (97055)	Loss/tok 3.4923 (3.4340)	LR 2.800e-03
0: TRAIN [1][510/1885]	Time 0.146 (0.145)	Data 2.19e-04 (6.17e-04)	Tok/s 100671 (97027)	Loss/tok 3.4572 (3.4321)	LR 2.800e-03
0: TRAIN [1][520/1885]	Time 0.145 (0.145)	Data 2.18e-04 (6.09e-04)	Tok/s 100523 (97036)	Loss/tok 3.3851 (3.4345)	LR 2.800e-03
0: TRAIN [1][530/1885]	Time 0.102 (0.145)	Data 2.28e-04 (6.02e-04)	Tok/s 87997 (97019)	Loss/tok 3.1223 (3.4340)	LR 2.800e-03
0: TRAIN [1][540/1885]	Time 0.244 (0.144)	Data 2.18e-04 (5.94e-04)	Tok/s 107986 (96939)	Loss/tok 3.7267 (3.4341)	LR 2.800e-03
0: TRAIN [1][550/1885]	Time 0.102 (0.145)	Data 2.16e-04 (5.88e-04)	Tok/s 90708 (96949)	Loss/tok 3.1077 (3.4340)	LR 2.800e-03
0: TRAIN [1][560/1885]	Time 0.245 (0.145)	Data 2.20e-04 (5.81e-04)	Tok/s 106409 (97022)	Loss/tok 3.5975 (3.4349)	LR 2.800e-03
0: TRAIN [1][570/1885]	Time 0.101 (0.145)	Data 1.90e-04 (5.75e-04)	Tok/s 92122 (97013)	Loss/tok 3.1834 (3.4360)	LR 2.800e-03
0: TRAIN [1][580/1885]	Time 0.102 (0.145)	Data 2.20e-04 (5.68e-04)	Tok/s 88408 (96942)	Loss/tok 3.0518 (3.4347)	LR 2.800e-03
0: TRAIN [1][590/1885]	Time 0.102 (0.145)	Data 2.18e-04 (5.62e-04)	Tok/s 90292 (96995)	Loss/tok 3.1545 (3.4350)	LR 2.800e-03
0: TRAIN [1][600/1885]	Time 0.145 (0.145)	Data 2.19e-04 (5.57e-04)	Tok/s 101916 (97023)	Loss/tok 3.3073 (3.4341)	LR 2.800e-03
0: TRAIN [1][610/1885]	Time 0.193 (0.145)	Data 2.18e-04 (5.51e-04)	Tok/s 104772 (97028)	Loss/tok 3.4828 (3.4333)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][620/1885]	Time 0.147 (0.145)	Data 2.29e-04 (5.46e-04)	Tok/s 100104 (96973)	Loss/tok 3.4057 (3.4321)	LR 2.800e-03
0: TRAIN [1][630/1885]	Time 0.245 (0.145)	Data 2.02e-04 (5.40e-04)	Tok/s 106305 (97010)	Loss/tok 3.6933 (3.4331)	LR 2.800e-03
0: TRAIN [1][640/1885]	Time 0.102 (0.145)	Data 2.18e-04 (5.35e-04)	Tok/s 87685 (96999)	Loss/tok 3.1305 (3.4332)	LR 2.800e-03
0: TRAIN [1][650/1885]	Time 0.101 (0.145)	Data 2.16e-04 (5.31e-04)	Tok/s 91329 (96979)	Loss/tok 3.0955 (3.4334)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][660/1885]	Time 0.192 (0.145)	Data 1.11e-04 (5.25e-04)	Tok/s 105402 (96968)	Loss/tok 3.6129 (3.4340)	LR 2.800e-03
0: TRAIN [1][670/1885]	Time 0.195 (0.145)	Data 1.08e-04 (5.19e-04)	Tok/s 104684 (97034)	Loss/tok 3.5475 (3.4355)	LR 2.800e-03
0: TRAIN [1][680/1885]	Time 0.146 (0.146)	Data 1.10e-04 (5.13e-04)	Tok/s 98975 (97088)	Loss/tok 3.3633 (3.4362)	LR 2.800e-03
0: TRAIN [1][690/1885]	Time 0.101 (0.145)	Data 2.19e-04 (5.09e-04)	Tok/s 89746 (97053)	Loss/tok 3.1406 (3.4353)	LR 2.800e-03
0: TRAIN [1][700/1885]	Time 0.147 (0.146)	Data 2.32e-04 (5.04e-04)	Tok/s 98963 (97059)	Loss/tok 3.2605 (3.4359)	LR 2.800e-03
0: TRAIN [1][710/1885]	Time 0.195 (0.146)	Data 2.20e-04 (5.00e-04)	Tok/s 103604 (97043)	Loss/tok 3.5737 (3.4349)	LR 2.800e-03
0: TRAIN [1][720/1885]	Time 0.146 (0.146)	Data 2.18e-04 (4.96e-04)	Tok/s 101019 (97076)	Loss/tok 3.3405 (3.4342)	LR 2.800e-03
0: TRAIN [1][730/1885]	Time 0.102 (0.145)	Data 2.18e-04 (4.92e-04)	Tok/s 89413 (97065)	Loss/tok 3.1655 (3.4330)	LR 2.800e-03
0: TRAIN [1][740/1885]	Time 0.101 (0.146)	Data 2.18e-04 (4.88e-04)	Tok/s 89515 (97085)	Loss/tok 3.1367 (3.4324)	LR 2.800e-03
0: TRAIN [1][750/1885]	Time 0.147 (0.146)	Data 2.28e-04 (4.85e-04)	Tok/s 100223 (97095)	Loss/tok 3.2346 (3.4318)	LR 2.800e-03
0: TRAIN [1][760/1885]	Time 0.244 (0.146)	Data 2.21e-04 (4.81e-04)	Tok/s 107336 (97071)	Loss/tok 3.7464 (3.4318)	LR 2.800e-03
0: TRAIN [1][770/1885]	Time 0.059 (0.146)	Data 2.17e-04 (4.78e-04)	Tok/s 77476 (97081)	Loss/tok 2.6107 (3.4314)	LR 2.800e-03
0: TRAIN [1][780/1885]	Time 0.101 (0.145)	Data 2.16e-04 (4.74e-04)	Tok/s 88899 (97046)	Loss/tok 3.1675 (3.4300)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][790/1885]	Time 0.101 (0.145)	Data 2.18e-04 (4.71e-04)	Tok/s 89617 (97047)	Loss/tok 3.1875 (3.4295)	LR 2.800e-03
0: TRAIN [1][800/1885]	Time 0.060 (0.145)	Data 2.37e-04 (4.68e-04)	Tok/s 73320 (97005)	Loss/tok 2.4697 (3.4280)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][810/1885]	Time 0.147 (0.145)	Data 2.21e-04 (4.65e-04)	Tok/s 98204 (96942)	Loss/tok 3.4284 (3.4269)	LR 2.800e-03
0: TRAIN [1][820/1885]	Time 0.101 (0.144)	Data 2.17e-04 (4.62e-04)	Tok/s 90301 (96904)	Loss/tok 3.1754 (3.4252)	LR 2.800e-03
0: TRAIN [1][830/1885]	Time 0.146 (0.145)	Data 2.19e-04 (4.59e-04)	Tok/s 100080 (96934)	Loss/tok 3.4196 (3.4257)	LR 2.800e-03
0: TRAIN [1][840/1885]	Time 0.147 (0.145)	Data 2.17e-04 (4.56e-04)	Tok/s 100535 (96945)	Loss/tok 3.2862 (3.4245)	LR 2.800e-03
0: TRAIN [1][850/1885]	Time 0.102 (0.144)	Data 2.17e-04 (4.53e-04)	Tok/s 90445 (96938)	Loss/tok 3.1106 (3.4237)	LR 2.800e-03
0: TRAIN [1][860/1885]	Time 0.102 (0.145)	Data 2.16e-04 (4.50e-04)	Tok/s 89107 (96952)	Loss/tok 3.0809 (3.4236)	LR 2.800e-03
0: TRAIN [1][870/1885]	Time 0.100 (0.145)	Data 1.08e-04 (4.47e-04)	Tok/s 92800 (96959)	Loss/tok 3.1612 (3.4236)	LR 2.800e-03
0: TRAIN [1][880/1885]	Time 0.146 (0.145)	Data 2.16e-04 (4.45e-04)	Tok/s 100085 (96967)	Loss/tok 3.2597 (3.4230)	LR 2.800e-03
0: TRAIN [1][890/1885]	Time 0.101 (0.144)	Data 2.20e-04 (4.42e-04)	Tok/s 87940 (96941)	Loss/tok 3.0377 (3.4225)	LR 2.800e-03
0: TRAIN [1][900/1885]	Time 0.101 (0.145)	Data 2.18e-04 (4.40e-04)	Tok/s 90526 (96960)	Loss/tok 3.0286 (3.4228)	LR 2.800e-03
0: TRAIN [1][910/1885]	Time 0.145 (0.144)	Data 2.17e-04 (4.37e-04)	Tok/s 100920 (96931)	Loss/tok 3.2207 (3.4219)	LR 2.800e-03
0: TRAIN [1][920/1885]	Time 0.101 (0.144)	Data 2.19e-04 (4.35e-04)	Tok/s 89025 (96954)	Loss/tok 3.1051 (3.4213)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][930/1885]	Time 0.148 (0.144)	Data 2.32e-04 (4.32e-04)	Tok/s 100726 (96936)	Loss/tok 3.4195 (3.4204)	LR 2.800e-03
0: TRAIN [1][940/1885]	Time 0.101 (0.144)	Data 2.18e-04 (4.30e-04)	Tok/s 89050 (96912)	Loss/tok 3.1879 (3.4202)	LR 2.800e-03
0: TRAIN [1][950/1885]	Time 0.102 (0.144)	Data 2.21e-04 (4.28e-04)	Tok/s 90134 (96857)	Loss/tok 3.2112 (3.4186)	LR 2.800e-03
0: TRAIN [1][960/1885]	Time 0.102 (0.144)	Data 2.02e-04 (4.26e-04)	Tok/s 88072 (96828)	Loss/tok 3.1021 (3.4172)	LR 2.800e-03
0: TRAIN [1][970/1885]	Time 0.193 (0.144)	Data 1.09e-04 (4.23e-04)	Tok/s 106832 (96838)	Loss/tok 3.5435 (3.4172)	LR 2.800e-03
0: TRAIN [1][980/1885]	Time 0.145 (0.144)	Data 2.19e-04 (4.20e-04)	Tok/s 99803 (96820)	Loss/tok 3.3371 (3.4165)	LR 2.800e-03
0: TRAIN [1][990/1885]	Time 0.146 (0.144)	Data 2.20e-04 (4.18e-04)	Tok/s 100661 (96842)	Loss/tok 3.2918 (3.4161)	LR 2.800e-03
0: TRAIN [1][1000/1885]	Time 0.194 (0.144)	Data 1.09e-04 (4.16e-04)	Tok/s 105425 (96827)	Loss/tok 3.4642 (3.4155)	LR 2.800e-03
0: TRAIN [1][1010/1885]	Time 0.145 (0.144)	Data 1.09e-04 (4.13e-04)	Tok/s 102497 (96828)	Loss/tok 3.3495 (3.4159)	LR 2.800e-03
0: TRAIN [1][1020/1885]	Time 0.101 (0.144)	Data 2.14e-04 (4.10e-04)	Tok/s 89355 (96791)	Loss/tok 2.9846 (3.4154)	LR 2.800e-03
0: TRAIN [1][1030/1885]	Time 0.147 (0.144)	Data 2.06e-04 (4.08e-04)	Tok/s 99256 (96821)	Loss/tok 3.4158 (3.4160)	LR 2.800e-03
0: TRAIN [1][1040/1885]	Time 0.101 (0.144)	Data 2.21e-04 (4.07e-04)	Tok/s 89806 (96818)	Loss/tok 3.1857 (3.4159)	LR 2.800e-03
0: TRAIN [1][1050/1885]	Time 0.101 (0.144)	Data 2.20e-04 (4.05e-04)	Tok/s 88702 (96797)	Loss/tok 3.1702 (3.4151)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1060/1885]	Time 0.147 (0.144)	Data 2.22e-04 (4.03e-04)	Tok/s 100754 (96817)	Loss/tok 3.2760 (3.4155)	LR 2.800e-03
0: TRAIN [1][1070/1885]	Time 0.147 (0.144)	Data 2.18e-04 (4.01e-04)	Tok/s 99820 (96818)	Loss/tok 3.4490 (3.4145)	LR 2.800e-03
0: TRAIN [1][1080/1885]	Time 0.101 (0.144)	Data 2.17e-04 (4.00e-04)	Tok/s 89098 (96812)	Loss/tok 3.0627 (3.4142)	LR 2.800e-03
0: TRAIN [1][1090/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.98e-04)	Tok/s 89119 (96805)	Loss/tok 3.1535 (3.4138)	LR 2.800e-03
0: TRAIN [1][1100/1885]	Time 0.147 (0.144)	Data 2.18e-04 (3.96e-04)	Tok/s 99327 (96812)	Loss/tok 3.5146 (3.4141)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1110/1885]	Time 0.145 (0.144)	Data 2.15e-04 (3.94e-04)	Tok/s 101957 (96845)	Loss/tok 3.3675 (3.4144)	LR 2.800e-03
0: TRAIN [1][1120/1885]	Time 0.194 (0.144)	Data 1.10e-04 (3.93e-04)	Tok/s 104310 (96862)	Loss/tok 3.5336 (3.4142)	LR 2.800e-03
0: TRAIN [1][1130/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.91e-04)	Tok/s 99670 (96864)	Loss/tok 3.3395 (3.4134)	LR 2.800e-03
0: TRAIN [1][1140/1885]	Time 0.146 (0.144)	Data 2.17e-04 (3.89e-04)	Tok/s 101029 (96835)	Loss/tok 3.3274 (3.4122)	LR 2.800e-03
0: TRAIN [1][1150/1885]	Time 0.102 (0.144)	Data 2.01e-04 (3.88e-04)	Tok/s 87495 (96831)	Loss/tok 3.1264 (3.4116)	LR 2.800e-03
0: TRAIN [1][1160/1885]	Time 0.101 (0.144)	Data 2.16e-04 (3.86e-04)	Tok/s 88359 (96835)	Loss/tok 3.0373 (3.4113)	LR 2.800e-03
0: TRAIN [1][1170/1885]	Time 0.147 (0.144)	Data 1.77e-04 (3.85e-04)	Tok/s 98978 (96854)	Loss/tok 3.2942 (3.4119)	LR 2.800e-03
0: TRAIN [1][1180/1885]	Time 0.059 (0.144)	Data 2.16e-04 (3.84e-04)	Tok/s 77265 (96785)	Loss/tok 2.5907 (3.4102)	LR 2.800e-03
0: TRAIN [1][1190/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.82e-04)	Tok/s 101237 (96790)	Loss/tok 3.3569 (3.4099)	LR 2.800e-03
0: TRAIN [1][1200/1885]	Time 0.195 (0.144)	Data 2.02e-04 (3.81e-04)	Tok/s 105702 (96802)	Loss/tok 3.5355 (3.4098)	LR 2.800e-03
0: TRAIN [1][1210/1885]	Time 0.147 (0.144)	Data 2.19e-04 (3.79e-04)	Tok/s 99012 (96796)	Loss/tok 3.4400 (3.4095)	LR 2.800e-03
0: TRAIN [1][1220/1885]	Time 0.246 (0.144)	Data 2.19e-04 (3.78e-04)	Tok/s 107688 (96776)	Loss/tok 3.6020 (3.4086)	LR 2.800e-03
0: TRAIN [1][1230/1885]	Time 0.059 (0.144)	Data 2.18e-04 (3.77e-04)	Tok/s 79168 (96773)	Loss/tok 2.6877 (3.4089)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1240/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.75e-04)	Tok/s 89373 (96751)	Loss/tok 3.0609 (3.4079)	LR 2.800e-03
0: TRAIN [1][1250/1885]	Time 0.147 (0.143)	Data 1.62e-04 (3.74e-04)	Tok/s 100132 (96731)	Loss/tok 3.2815 (3.4076)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1260/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.73e-04)	Tok/s 100888 (96734)	Loss/tok 3.2661 (3.4070)	LR 2.800e-03
0: TRAIN [1][1270/1885]	Time 0.148 (0.143)	Data 2.19e-04 (3.72e-04)	Tok/s 98355 (96686)	Loss/tok 3.2982 (3.4056)	LR 2.800e-03
0: TRAIN [1][1280/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.70e-04)	Tok/s 101204 (96675)	Loss/tok 3.3395 (3.4045)	LR 2.800e-03
0: TRAIN [1][1290/1885]	Time 0.102 (0.143)	Data 2.33e-04 (3.69e-04)	Tok/s 89360 (96654)	Loss/tok 3.0785 (3.4040)	LR 2.800e-03
0: TRAIN [1][1300/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.68e-04)	Tok/s 90258 (96639)	Loss/tok 3.1903 (3.4032)	LR 2.800e-03
0: TRAIN [1][1310/1885]	Time 0.245 (0.143)	Data 2.20e-04 (3.67e-04)	Tok/s 106747 (96654)	Loss/tok 3.8333 (3.4038)	LR 2.800e-03
0: TRAIN [1][1320/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.66e-04)	Tok/s 89333 (96652)	Loss/tok 3.1973 (3.4033)	LR 2.800e-03
0: TRAIN [1][1330/1885]	Time 0.102 (0.143)	Data 2.30e-04 (3.64e-04)	Tok/s 89338 (96619)	Loss/tok 3.0537 (3.4021)	LR 2.800e-03
0: TRAIN [1][1340/1885]	Time 0.195 (0.143)	Data 2.19e-04 (3.63e-04)	Tok/s 104476 (96640)	Loss/tok 3.5823 (3.4022)	LR 2.800e-03
0: TRAIN [1][1350/1885]	Time 0.102 (0.143)	Data 1.91e-04 (3.62e-04)	Tok/s 90811 (96634)	Loss/tok 3.1645 (3.4020)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1360/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.61e-04)	Tok/s 91231 (96626)	Loss/tok 3.0801 (3.4019)	LR 2.800e-03
0: TRAIN [1][1370/1885]	Time 0.102 (0.143)	Data 1.09e-04 (3.60e-04)	Tok/s 87644 (96602)	Loss/tok 3.0925 (3.4012)	LR 2.800e-03
0: TRAIN [1][1380/1885]	Time 0.193 (0.143)	Data 2.34e-04 (3.59e-04)	Tok/s 105836 (96598)	Loss/tok 3.6302 (3.4009)	LR 2.800e-03
0: TRAIN [1][1390/1885]	Time 0.244 (0.143)	Data 2.20e-04 (3.58e-04)	Tok/s 106195 (96590)	Loss/tok 3.6871 (3.4008)	LR 2.800e-03
0: TRAIN [1][1400/1885]	Time 0.244 (0.143)	Data 2.19e-04 (3.57e-04)	Tok/s 106725 (96588)	Loss/tok 3.7141 (3.4007)	LR 2.800e-03
0: TRAIN [1][1410/1885]	Time 0.146 (0.143)	Data 2.17e-04 (3.56e-04)	Tok/s 99854 (96610)	Loss/tok 3.2745 (3.4006)	LR 2.800e-03
0: TRAIN [1][1420/1885]	Time 0.193 (0.143)	Data 2.18e-04 (3.55e-04)	Tok/s 104554 (96610)	Loss/tok 3.5420 (3.4000)	LR 2.800e-03
0: TRAIN [1][1430/1885]	Time 0.246 (0.143)	Data 1.92e-04 (3.53e-04)	Tok/s 107464 (96617)	Loss/tok 3.4854 (3.3995)	LR 2.800e-03
0: TRAIN [1][1440/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.53e-04)	Tok/s 89679 (96609)	Loss/tok 3.1176 (3.3995)	LR 2.800e-03
0: TRAIN [1][1450/1885]	Time 0.146 (0.143)	Data 1.80e-04 (3.52e-04)	Tok/s 101778 (96603)	Loss/tok 3.3131 (3.3992)	LR 2.800e-03
0: TRAIN [1][1460/1885]	Time 0.102 (0.143)	Data 1.70e-04 (3.51e-04)	Tok/s 88898 (96608)	Loss/tok 3.1153 (3.3989)	LR 2.800e-03
0: TRAIN [1][1470/1885]	Time 0.244 (0.143)	Data 2.22e-04 (3.50e-04)	Tok/s 107132 (96602)	Loss/tok 3.7344 (3.3989)	LR 2.800e-03
0: TRAIN [1][1480/1885]	Time 0.194 (0.143)	Data 2.20e-04 (3.49e-04)	Tok/s 104803 (96644)	Loss/tok 3.5339 (3.4000)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1490/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.48e-04)	Tok/s 90205 (96663)	Loss/tok 3.1184 (3.3999)	LR 2.800e-03
0: TRAIN [1][1500/1885]	Time 0.101 (0.143)	Data 2.23e-04 (3.47e-04)	Tok/s 89088 (96644)	Loss/tok 3.0888 (3.3992)	LR 2.800e-03
0: TRAIN [1][1510/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.46e-04)	Tok/s 99396 (96645)	Loss/tok 3.3772 (3.3988)	LR 2.800e-03
0: TRAIN [1][1520/1885]	Time 0.059 (0.143)	Data 2.20e-04 (3.45e-04)	Tok/s 75979 (96637)	Loss/tok 2.5177 (3.3982)	LR 2.800e-03
0: TRAIN [1][1530/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.44e-04)	Tok/s 88076 (96652)	Loss/tok 3.2640 (3.3980)	LR 2.800e-03
0: TRAIN [1][1540/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.43e-04)	Tok/s 89241 (96661)	Loss/tok 3.1579 (3.3979)	LR 2.800e-03
0: TRAIN [1][1550/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.43e-04)	Tok/s 100448 (96642)	Loss/tok 3.4085 (3.3976)	LR 2.800e-03
0: TRAIN [1][1560/1885]	Time 0.195 (0.143)	Data 2.21e-04 (3.42e-04)	Tok/s 104955 (96625)	Loss/tok 3.3993 (3.3970)	LR 2.800e-03
0: TRAIN [1][1570/1885]	Time 0.147 (0.143)	Data 2.01e-04 (3.41e-04)	Tok/s 98067 (96596)	Loss/tok 3.4298 (3.3963)	LR 2.800e-03
0: TRAIN [1][1580/1885]	Time 0.147 (0.143)	Data 2.36e-04 (3.40e-04)	Tok/s 101405 (96617)	Loss/tok 3.2486 (3.3963)	LR 2.800e-03
0: TRAIN [1][1590/1885]	Time 0.244 (0.143)	Data 2.22e-04 (3.40e-04)	Tok/s 107778 (96644)	Loss/tok 3.6848 (3.3969)	LR 2.800e-03
0: TRAIN [1][1600/1885]	Time 0.195 (0.143)	Data 2.20e-04 (3.39e-04)	Tok/s 105318 (96624)	Loss/tok 3.5207 (3.3963)	LR 2.800e-03
0: TRAIN [1][1610/1885]	Time 0.195 (0.143)	Data 2.20e-04 (3.38e-04)	Tok/s 105119 (96620)	Loss/tok 3.4396 (3.3959)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1620/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.37e-04)	Tok/s 88957 (96629)	Loss/tok 3.1925 (3.3959)	LR 2.800e-03
0: TRAIN [1][1630/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.37e-04)	Tok/s 100531 (96625)	Loss/tok 3.3451 (3.3955)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1640/1885]	Time 0.101 (0.143)	Data 2.24e-04 (3.36e-04)	Tok/s 88153 (96642)	Loss/tok 3.0142 (3.3957)	LR 2.800e-03
0: TRAIN [1][1650/1885]	Time 0.195 (0.143)	Data 1.11e-04 (3.35e-04)	Tok/s 104783 (96622)	Loss/tok 3.4446 (3.3952)	LR 2.800e-03
0: TRAIN [1][1660/1885]	Time 0.060 (0.143)	Data 2.22e-04 (3.34e-04)	Tok/s 74220 (96614)	Loss/tok 2.4876 (3.3944)	LR 2.800e-03
0: TRAIN [1][1670/1885]	Time 0.195 (0.143)	Data 1.92e-04 (3.34e-04)	Tok/s 106231 (96613)	Loss/tok 3.3304 (3.3939)	LR 2.800e-03
0: TRAIN [1][1680/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.33e-04)	Tok/s 88034 (96607)	Loss/tok 3.0671 (3.3934)	LR 2.800e-03
0: TRAIN [1][1690/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.32e-04)	Tok/s 88234 (96574)	Loss/tok 2.9878 (3.3926)	LR 2.800e-03
0: TRAIN [1][1700/1885]	Time 0.103 (0.143)	Data 2.06e-04 (3.32e-04)	Tok/s 87541 (96582)	Loss/tok 3.1796 (3.3923)	LR 2.800e-03
0: TRAIN [1][1710/1885]	Time 0.102 (0.143)	Data 1.40e-04 (3.31e-04)	Tok/s 91897 (96553)	Loss/tok 3.0232 (3.3917)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1720/1885]	Time 0.103 (0.143)	Data 1.56e-04 (3.30e-04)	Tok/s 87340 (96543)	Loss/tok 2.9837 (3.3921)	LR 2.800e-03
0: TRAIN [1][1730/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.29e-04)	Tok/s 100612 (96545)	Loss/tok 3.2921 (3.3922)	LR 2.800e-03
0: TRAIN [1][1740/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.29e-04)	Tok/s 90336 (96559)	Loss/tok 3.0670 (3.3915)	LR 2.800e-03
0: TRAIN [1][1750/1885]	Time 0.194 (0.143)	Data 2.20e-04 (3.28e-04)	Tok/s 103877 (96566)	Loss/tok 3.5358 (3.3916)	LR 2.800e-03
0: TRAIN [1][1760/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.27e-04)	Tok/s 104821 (96562)	Loss/tok 3.5050 (3.3910)	LR 2.800e-03
0: TRAIN [1][1770/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.27e-04)	Tok/s 99099 (96559)	Loss/tok 3.2997 (3.3909)	LR 2.800e-03
0: TRAIN [1][1780/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.26e-04)	Tok/s 88716 (96546)	Loss/tok 3.1731 (3.3903)	LR 2.800e-03
0: TRAIN [1][1790/1885]	Time 0.101 (0.143)	Data 2.22e-04 (3.26e-04)	Tok/s 90916 (96528)	Loss/tok 3.0424 (3.3895)	LR 2.800e-03
0: TRAIN [1][1800/1885]	Time 0.193 (0.143)	Data 2.19e-04 (3.25e-04)	Tok/s 105322 (96519)	Loss/tok 3.5259 (3.3890)	LR 2.800e-03
0: TRAIN [1][1810/1885]	Time 0.193 (0.143)	Data 2.20e-04 (3.24e-04)	Tok/s 105328 (96534)	Loss/tok 3.4922 (3.3891)	LR 2.800e-03
0: TRAIN [1][1820/1885]	Time 0.102 (0.143)	Data 2.19e-04 (3.24e-04)	Tok/s 89440 (96542)	Loss/tok 3.0608 (3.3889)	LR 2.800e-03
0: TRAIN [1][1830/1885]	Time 0.245 (0.143)	Data 2.17e-04 (3.23e-04)	Tok/s 105904 (96548)	Loss/tok 3.6738 (3.3888)	LR 2.800e-03
0: TRAIN [1][1840/1885]	Time 0.195 (0.143)	Data 2.19e-04 (3.23e-04)	Tok/s 104683 (96528)	Loss/tok 3.4102 (3.3879)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1850/1885]	Time 0.060 (0.143)	Data 2.20e-04 (3.22e-04)	Tok/s 76488 (96528)	Loss/tok 2.4939 (3.3878)	LR 2.800e-03
0: TRAIN [1][1860/1885]	Time 0.102 (0.143)	Data 2.23e-04 (3.22e-04)	Tok/s 89246 (96532)	Loss/tok 3.1382 (3.3874)	LR 2.800e-03
0: TRAIN [1][1870/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.21e-04)	Tok/s 99718 (96529)	Loss/tok 3.3174 (3.3869)	LR 2.800e-03
0: TRAIN [1][1880/1885]	Time 0.060 (0.143)	Data 2.32e-04 (3.21e-04)	Tok/s 76577 (96534)	Loss/tok 2.6057 (3.3868)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593832332703, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593832332704, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.589 (0.589)	Decoder iters 109.0 (109.0)	Tok/s 27711 (27711)
0: Running moses detokenizer
0: BLEU(score=21.856883536322744, counts=[35756, 17151, 9459, 5448], totals=[65597, 62594, 59591, 56594], precisions=[54.508590331874935, 27.400389813720164, 15.873202329210788, 9.626462169134538], bp=1.0, sys_len=65597, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593832334482, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2186, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593832334482, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3855	Test BLEU: 21.86
0: Performance: Epoch: 1	Training: 772098 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593832334483, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593832334483, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593832334483, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 856872929
0: TRAIN [2][0/1885]	Time 0.435 (0.435)	Data 2.20e-01 (2.20e-01)	Tok/s 46830 (46830)	Loss/tok 3.3511 (3.3511)	LR 2.800e-03
0: TRAIN [2][10/1885]	Time 0.146 (0.157)	Data 2.30e-04 (2.02e-02)	Tok/s 99354 (91225)	Loss/tok 3.2010 (3.1841)	LR 2.800e-03
0: TRAIN [2][20/1885]	Time 0.146 (0.157)	Data 2.13e-04 (1.07e-02)	Tok/s 99496 (94397)	Loss/tok 3.1837 (3.2590)	LR 2.800e-03
0: TRAIN [2][30/1885]	Time 0.147 (0.153)	Data 2.16e-04 (7.31e-03)	Tok/s 98966 (95048)	Loss/tok 3.2771 (3.2697)	LR 2.800e-03
0: TRAIN [2][40/1885]	Time 0.102 (0.154)	Data 2.14e-04 (5.58e-03)	Tok/s 88205 (95978)	Loss/tok 2.9547 (3.2797)	LR 2.800e-03
0: TRAIN [2][50/1885]	Time 0.101 (0.151)	Data 1.75e-04 (4.53e-03)	Tok/s 89439 (96039)	Loss/tok 3.0602 (3.2669)	LR 2.800e-03
0: TRAIN [2][60/1885]	Time 0.191 (0.151)	Data 1.44e-04 (3.81e-03)	Tok/s 106292 (95832)	Loss/tok 3.3564 (3.2705)	LR 2.800e-03
0: TRAIN [2][70/1885]	Time 0.102 (0.150)	Data 1.78e-04 (3.31e-03)	Tok/s 90055 (96010)	Loss/tok 3.0208 (3.2672)	LR 2.800e-03
0: TRAIN [2][80/1885]	Time 0.101 (0.149)	Data 2.13e-04 (2.92e-03)	Tok/s 88917 (96125)	Loss/tok 3.0177 (3.2678)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][90/1885]	Time 0.102 (0.150)	Data 2.16e-04 (2.62e-03)	Tok/s 87461 (96353)	Loss/tok 3.0345 (3.2657)	LR 2.800e-03
0: TRAIN [2][100/1885]	Time 0.193 (0.146)	Data 2.12e-04 (2.39e-03)	Tok/s 103605 (95813)	Loss/tok 3.3773 (3.2549)	LR 2.800e-03
0: TRAIN [2][110/1885]	Time 0.194 (0.146)	Data 2.13e-04 (2.19e-03)	Tok/s 105564 (95850)	Loss/tok 3.3878 (3.2552)	LR 2.800e-03
0: TRAIN [2][120/1885]	Time 0.245 (0.146)	Data 2.12e-04 (2.03e-03)	Tok/s 107061 (95644)	Loss/tok 3.4848 (3.2589)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][130/1885]	Time 0.244 (0.147)	Data 2.11e-04 (1.89e-03)	Tok/s 108474 (95859)	Loss/tok 3.4541 (3.2646)	LR 2.800e-03
0: TRAIN [2][140/1885]	Time 0.146 (0.147)	Data 2.12e-04 (1.77e-03)	Tok/s 99292 (95900)	Loss/tok 3.3140 (3.2618)	LR 2.800e-03
0: TRAIN [2][150/1885]	Time 0.101 (0.146)	Data 2.13e-04 (1.67e-03)	Tok/s 89194 (95969)	Loss/tok 2.9854 (3.2588)	LR 2.800e-03
0: TRAIN [2][160/1885]	Time 0.148 (0.146)	Data 2.13e-04 (1.58e-03)	Tok/s 98301 (96047)	Loss/tok 3.2262 (3.2570)	LR 2.800e-03
0: TRAIN [2][170/1885]	Time 0.194 (0.147)	Data 2.11e-04 (1.50e-03)	Tok/s 105488 (96199)	Loss/tok 3.4617 (3.2657)	LR 2.800e-03
0: TRAIN [2][180/1885]	Time 0.102 (0.147)	Data 2.15e-04 (1.42e-03)	Tok/s 90014 (96234)	Loss/tok 3.0674 (3.2612)	LR 2.800e-03
0: TRAIN [2][190/1885]	Time 0.102 (0.146)	Data 2.14e-04 (1.36e-03)	Tok/s 90620 (96180)	Loss/tok 2.9082 (3.2572)	LR 2.800e-03
0: TRAIN [2][200/1885]	Time 0.101 (0.145)	Data 2.14e-04 (1.30e-03)	Tok/s 91842 (96122)	Loss/tok 2.9454 (3.2532)	LR 2.800e-03
0: TRAIN [2][210/1885]	Time 0.060 (0.144)	Data 2.12e-04 (1.25e-03)	Tok/s 75421 (95950)	Loss/tok 2.5164 (3.2512)	LR 2.800e-03
0: TRAIN [2][220/1885]	Time 0.102 (0.145)	Data 2.14e-04 (1.21e-03)	Tok/s 89352 (96109)	Loss/tok 2.9651 (3.2546)	LR 2.800e-03
0: TRAIN [2][230/1885]	Time 0.146 (0.146)	Data 2.14e-04 (1.16e-03)	Tok/s 99689 (96284)	Loss/tok 3.1419 (3.2568)	LR 2.800e-03
0: TRAIN [2][240/1885]	Time 0.101 (0.145)	Data 2.16e-04 (1.12e-03)	Tok/s 90828 (96130)	Loss/tok 3.0396 (3.2523)	LR 2.800e-03
0: TRAIN [2][250/1885]	Time 0.102 (0.145)	Data 2.11e-04 (1.09e-03)	Tok/s 89613 (96225)	Loss/tok 3.0526 (3.2569)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][260/1885]	Time 0.060 (0.145)	Data 1.07e-04 (1.05e-03)	Tok/s 75206 (96111)	Loss/tok 2.4738 (3.2560)	LR 2.800e-03
0: TRAIN [2][270/1885]	Time 0.194 (0.145)	Data 2.16e-04 (1.02e-03)	Tok/s 103745 (96184)	Loss/tok 3.4488 (3.2576)	LR 2.800e-03
0: TRAIN [2][280/1885]	Time 0.194 (0.144)	Data 2.15e-04 (9.93e-04)	Tok/s 105475 (96133)	Loss/tok 3.3371 (3.2577)	LR 2.800e-03
0: TRAIN [2][290/1885]	Time 0.102 (0.145)	Data 2.12e-04 (9.66e-04)	Tok/s 88451 (96209)	Loss/tok 3.0611 (3.2587)	LR 2.800e-03
0: TRAIN [2][300/1885]	Time 0.148 (0.145)	Data 2.16e-04 (9.41e-04)	Tok/s 99876 (96299)	Loss/tok 3.2817 (3.2609)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][310/1885]	Time 0.193 (0.145)	Data 2.16e-04 (9.17e-04)	Tok/s 105497 (96321)	Loss/tok 3.4682 (3.2608)	LR 2.800e-03
0: TRAIN [2][320/1885]	Time 0.147 (0.145)	Data 2.15e-04 (8.95e-04)	Tok/s 99067 (96344)	Loss/tok 3.2356 (3.2610)	LR 2.800e-03
0: TRAIN [2][330/1885]	Time 0.059 (0.145)	Data 2.15e-04 (8.75e-04)	Tok/s 77380 (96238)	Loss/tok 2.5003 (3.2629)	LR 2.800e-03
0: TRAIN [2][340/1885]	Time 0.147 (0.144)	Data 2.14e-04 (8.55e-04)	Tok/s 99294 (96147)	Loss/tok 3.1375 (3.2604)	LR 2.800e-03
0: TRAIN [2][350/1885]	Time 0.248 (0.144)	Data 2.15e-04 (8.37e-04)	Tok/s 106678 (96185)	Loss/tok 3.5756 (3.2603)	LR 2.800e-03
0: TRAIN [2][360/1885]	Time 0.101 (0.144)	Data 2.15e-04 (8.20e-04)	Tok/s 89804 (96198)	Loss/tok 3.0389 (3.2595)	LR 2.800e-03
0: TRAIN [2][370/1885]	Time 0.146 (0.144)	Data 2.14e-04 (8.03e-04)	Tok/s 98957 (96189)	Loss/tok 3.2527 (3.2604)	LR 2.800e-03
0: TRAIN [2][380/1885]	Time 0.103 (0.144)	Data 2.12e-04 (7.88e-04)	Tok/s 89195 (96205)	Loss/tok 2.9178 (3.2590)	LR 2.800e-03
0: TRAIN [2][390/1885]	Time 0.060 (0.144)	Data 2.13e-04 (7.73e-04)	Tok/s 78149 (96200)	Loss/tok 2.5989 (3.2589)	LR 2.800e-03
0: TRAIN [2][400/1885]	Time 0.146 (0.143)	Data 2.17e-04 (7.59e-04)	Tok/s 100252 (96128)	Loss/tok 3.2658 (3.2573)	LR 2.800e-03
0: TRAIN [2][410/1885]	Time 0.147 (0.143)	Data 2.17e-04 (7.46e-04)	Tok/s 100619 (96154)	Loss/tok 3.2612 (3.2573)	LR 2.800e-03
0: TRAIN [2][420/1885]	Time 0.146 (0.143)	Data 2.21e-04 (7.33e-04)	Tok/s 98964 (96118)	Loss/tok 3.2126 (3.2550)	LR 2.800e-03
0: TRAIN [2][430/1885]	Time 0.102 (0.143)	Data 2.17e-04 (7.21e-04)	Tok/s 88653 (96063)	Loss/tok 3.1195 (3.2568)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][440/1885]	Time 0.147 (0.143)	Data 2.17e-04 (7.10e-04)	Tok/s 99798 (96058)	Loss/tok 3.0975 (3.2556)	LR 2.800e-03
0: TRAIN [2][450/1885]	Time 0.246 (0.143)	Data 2.20e-04 (6.99e-04)	Tok/s 104140 (96088)	Loss/tok 3.5393 (3.2563)	LR 2.800e-03
0: TRAIN [2][460/1885]	Time 0.060 (0.142)	Data 2.17e-04 (6.88e-04)	Tok/s 75474 (96020)	Loss/tok 2.5131 (3.2547)	LR 2.800e-03
0: TRAIN [2][470/1885]	Time 0.192 (0.143)	Data 2.19e-04 (6.78e-04)	Tok/s 105228 (96048)	Loss/tok 3.3795 (3.2549)	LR 2.800e-03
0: TRAIN [2][480/1885]	Time 0.195 (0.143)	Data 2.15e-04 (6.69e-04)	Tok/s 103802 (96080)	Loss/tok 3.5084 (3.2556)	LR 2.800e-03
0: TRAIN [2][490/1885]	Time 0.102 (0.143)	Data 2.19e-04 (6.59e-04)	Tok/s 89451 (96063)	Loss/tok 2.9840 (3.2551)	LR 2.800e-03
0: TRAIN [2][500/1885]	Time 0.147 (0.142)	Data 2.16e-04 (6.51e-04)	Tok/s 100914 (96056)	Loss/tok 3.3405 (3.2552)	LR 2.800e-03
0: TRAIN [2][510/1885]	Time 0.246 (0.142)	Data 1.67e-04 (6.42e-04)	Tok/s 105541 (96028)	Loss/tok 3.5192 (3.2556)	LR 2.800e-03
0: TRAIN [2][520/1885]	Time 0.145 (0.142)	Data 2.19e-04 (6.34e-04)	Tok/s 101902 (96032)	Loss/tok 3.2479 (3.2555)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][530/1885]	Time 0.244 (0.143)	Data 2.17e-04 (6.26e-04)	Tok/s 108338 (96087)	Loss/tok 3.5097 (3.2573)	LR 2.800e-03
0: TRAIN [2][540/1885]	Time 0.244 (0.143)	Data 2.16e-04 (6.18e-04)	Tok/s 106850 (96131)	Loss/tok 3.5376 (3.2587)	LR 2.800e-03
0: TRAIN [2][550/1885]	Time 0.103 (0.143)	Data 2.11e-04 (6.11e-04)	Tok/s 86860 (96094)	Loss/tok 2.9293 (3.2578)	LR 2.800e-03
0: TRAIN [2][560/1885]	Time 0.102 (0.143)	Data 2.29e-04 (6.04e-04)	Tok/s 88628 (96114)	Loss/tok 2.9645 (3.2581)	LR 2.800e-03
0: TRAIN [2][570/1885]	Time 0.244 (0.144)	Data 2.17e-04 (5.97e-04)	Tok/s 105532 (96156)	Loss/tok 3.6702 (3.2610)	LR 2.800e-03
0: TRAIN [2][580/1885]	Time 0.147 (0.144)	Data 2.17e-04 (5.91e-04)	Tok/s 101720 (96287)	Loss/tok 3.2331 (3.2646)	LR 2.800e-03
0: TRAIN [2][590/1885]	Time 0.148 (0.144)	Data 2.15e-04 (5.84e-04)	Tok/s 100404 (96284)	Loss/tok 3.1893 (3.2637)	LR 2.800e-03
0: TRAIN [2][600/1885]	Time 0.245 (0.145)	Data 2.20e-04 (5.78e-04)	Tok/s 107808 (96335)	Loss/tok 3.5446 (3.2653)	LR 2.800e-03
0: TRAIN [2][610/1885]	Time 0.146 (0.144)	Data 2.17e-04 (5.72e-04)	Tok/s 101419 (96290)	Loss/tok 3.3460 (3.2655)	LR 2.800e-03
0: TRAIN [2][620/1885]	Time 0.195 (0.144)	Data 2.01e-04 (5.66e-04)	Tok/s 104395 (96312)	Loss/tok 3.3728 (3.2657)	LR 2.800e-03
0: TRAIN [2][630/1885]	Time 0.146 (0.145)	Data 2.24e-04 (5.61e-04)	Tok/s 101498 (96361)	Loss/tok 3.2027 (3.2670)	LR 2.800e-03
0: TRAIN [2][640/1885]	Time 0.101 (0.145)	Data 2.15e-04 (5.55e-04)	Tok/s 89638 (96364)	Loss/tok 3.1463 (3.2657)	LR 2.800e-03
0: TRAIN [2][650/1885]	Time 0.102 (0.144)	Data 2.16e-04 (5.50e-04)	Tok/s 91032 (96352)	Loss/tok 3.0582 (3.2651)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][660/1885]	Time 0.147 (0.144)	Data 2.15e-04 (5.45e-04)	Tok/s 99639 (96278)	Loss/tok 3.2148 (3.2630)	LR 2.800e-03
0: TRAIN [2][670/1885]	Time 0.147 (0.144)	Data 2.14e-04 (5.40e-04)	Tok/s 100428 (96319)	Loss/tok 3.2557 (3.2632)	LR 2.800e-03
0: TRAIN [2][680/1885]	Time 0.102 (0.144)	Data 2.14e-04 (5.35e-04)	Tok/s 88300 (96335)	Loss/tok 2.9430 (3.2643)	LR 2.800e-03
0: TRAIN [2][690/1885]	Time 0.102 (0.144)	Data 2.13e-04 (5.31e-04)	Tok/s 90457 (96291)	Loss/tok 3.1486 (3.2644)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][700/1885]	Time 0.103 (0.144)	Data 2.14e-04 (5.26e-04)	Tok/s 87296 (96277)	Loss/tok 2.9601 (3.2644)	LR 2.800e-03
0: TRAIN [2][710/1885]	Time 0.194 (0.144)	Data 2.16e-04 (5.22e-04)	Tok/s 104581 (96278)	Loss/tok 3.4324 (3.2642)	LR 2.800e-03
0: TRAIN [2][720/1885]	Time 0.194 (0.144)	Data 2.16e-04 (5.17e-04)	Tok/s 105060 (96302)	Loss/tok 3.4028 (3.2653)	LR 2.800e-03
0: TRAIN [2][730/1885]	Time 0.147 (0.144)	Data 2.13e-04 (5.13e-04)	Tok/s 99155 (96338)	Loss/tok 3.2307 (3.2655)	LR 2.800e-03
0: TRAIN [2][740/1885]	Time 0.195 (0.145)	Data 2.18e-04 (5.09e-04)	Tok/s 102883 (96371)	Loss/tok 3.4616 (3.2664)	LR 2.800e-03
0: TRAIN [2][750/1885]	Time 0.196 (0.145)	Data 2.16e-04 (5.05e-04)	Tok/s 103966 (96394)	Loss/tok 3.4471 (3.2666)	LR 2.800e-03
0: TRAIN [2][760/1885]	Time 0.101 (0.145)	Data 2.15e-04 (5.01e-04)	Tok/s 91763 (96413)	Loss/tok 3.0526 (3.2671)	LR 2.800e-03
0: TRAIN [2][770/1885]	Time 0.101 (0.145)	Data 2.14e-04 (4.98e-04)	Tok/s 90393 (96398)	Loss/tok 2.9814 (3.2667)	LR 2.800e-03
0: TRAIN [2][780/1885]	Time 0.195 (0.144)	Data 2.19e-04 (4.94e-04)	Tok/s 104330 (96365)	Loss/tok 3.3285 (3.2667)	LR 2.800e-03
0: TRAIN [2][790/1885]	Time 0.101 (0.144)	Data 2.13e-04 (4.90e-04)	Tok/s 88939 (96325)	Loss/tok 3.0711 (3.2661)	LR 2.800e-03
0: TRAIN [2][800/1885]	Time 0.101 (0.144)	Data 2.16e-04 (4.87e-04)	Tok/s 90829 (96282)	Loss/tok 3.0325 (3.2647)	LR 2.800e-03
0: TRAIN [2][810/1885]	Time 0.194 (0.144)	Data 2.13e-04 (4.84e-04)	Tok/s 104849 (96238)	Loss/tok 3.3442 (3.2637)	LR 2.800e-03
0: TRAIN [2][820/1885]	Time 0.102 (0.144)	Data 2.16e-04 (4.80e-04)	Tok/s 90602 (96236)	Loss/tok 2.9159 (3.2628)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][830/1885]	Time 0.147 (0.144)	Data 1.86e-04 (4.77e-04)	Tok/s 100476 (96241)	Loss/tok 3.1627 (3.2622)	LR 2.800e-03
0: TRAIN [2][840/1885]	Time 0.060 (0.143)	Data 2.16e-04 (4.74e-04)	Tok/s 76579 (96239)	Loss/tok 2.6082 (3.2620)	LR 2.800e-03
0: TRAIN [2][850/1885]	Time 0.102 (0.143)	Data 2.19e-04 (4.71e-04)	Tok/s 88989 (96241)	Loss/tok 3.0338 (3.2615)	LR 2.800e-03
0: TRAIN [2][860/1885]	Time 0.102 (0.144)	Data 2.29e-04 (4.68e-04)	Tok/s 89829 (96274)	Loss/tok 3.0562 (3.2617)	LR 2.800e-03
0: TRAIN [2][870/1885]	Time 0.102 (0.144)	Data 2.26e-04 (4.65e-04)	Tok/s 89553 (96328)	Loss/tok 2.9469 (3.2638)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][880/1885]	Time 0.146 (0.144)	Data 2.21e-04 (4.62e-04)	Tok/s 100820 (96296)	Loss/tok 3.2100 (3.2634)	LR 2.800e-03
0: TRAIN [2][890/1885]	Time 0.195 (0.144)	Data 2.20e-04 (4.59e-04)	Tok/s 104155 (96272)	Loss/tok 3.4507 (3.2631)	LR 2.800e-03
0: TRAIN [2][900/1885]	Time 0.146 (0.144)	Data 2.16e-04 (4.57e-04)	Tok/s 100799 (96285)	Loss/tok 3.2950 (3.2634)	LR 2.800e-03
0: TRAIN [2][910/1885]	Time 0.147 (0.144)	Data 2.22e-04 (4.54e-04)	Tok/s 99293 (96314)	Loss/tok 3.1790 (3.2636)	LR 2.800e-03
0: TRAIN [2][920/1885]	Time 0.145 (0.144)	Data 1.07e-04 (4.51e-04)	Tok/s 100436 (96287)	Loss/tok 3.4357 (3.2628)	LR 2.800e-03
0: TRAIN [2][930/1885]	Time 0.102 (0.144)	Data 2.16e-04 (4.48e-04)	Tok/s 87331 (96314)	Loss/tok 3.0595 (3.2637)	LR 2.800e-03
0: TRAIN [2][940/1885]	Time 0.195 (0.144)	Data 2.16e-04 (4.45e-04)	Tok/s 104903 (96304)	Loss/tok 3.2818 (3.2629)	LR 2.800e-03
0: TRAIN [2][950/1885]	Time 0.194 (0.144)	Data 2.15e-04 (4.43e-04)	Tok/s 104713 (96320)	Loss/tok 3.4349 (3.2647)	LR 2.800e-03
0: TRAIN [2][960/1885]	Time 0.061 (0.144)	Data 2.14e-04 (4.40e-04)	Tok/s 76051 (96269)	Loss/tok 2.5235 (3.2636)	LR 2.800e-03
0: TRAIN [2][970/1885]	Time 0.101 (0.144)	Data 2.17e-04 (4.38e-04)	Tok/s 89310 (96280)	Loss/tok 3.1158 (3.2643)	LR 2.800e-03
0: TRAIN [2][980/1885]	Time 0.102 (0.143)	Data 2.17e-04 (4.36e-04)	Tok/s 89792 (96232)	Loss/tok 2.9454 (3.2638)	LR 2.800e-03
0: TRAIN [2][990/1885]	Time 0.193 (0.144)	Data 2.19e-04 (4.34e-04)	Tok/s 107381 (96265)	Loss/tok 3.2438 (3.2647)	LR 2.800e-03
0: TRAIN [2][1000/1885]	Time 0.102 (0.144)	Data 2.18e-04 (4.31e-04)	Tok/s 87559 (96260)	Loss/tok 2.8609 (3.2645)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1010/1885]	Time 0.195 (0.144)	Data 2.02e-04 (4.29e-04)	Tok/s 104745 (96284)	Loss/tok 3.3383 (3.2647)	LR 2.800e-03
0: TRAIN [2][1020/1885]	Time 0.147 (0.144)	Data 2.09e-04 (4.27e-04)	Tok/s 98566 (96250)	Loss/tok 3.1371 (3.2644)	LR 2.800e-03
0: TRAIN [2][1030/1885]	Time 0.245 (0.144)	Data 2.17e-04 (4.25e-04)	Tok/s 106597 (96265)	Loss/tok 3.5708 (3.2647)	LR 2.800e-03
0: TRAIN [2][1040/1885]	Time 0.103 (0.144)	Data 2.16e-04 (4.23e-04)	Tok/s 88223 (96258)	Loss/tok 2.9584 (3.2644)	LR 2.800e-03
0: TRAIN [2][1050/1885]	Time 0.102 (0.143)	Data 2.15e-04 (4.21e-04)	Tok/s 89629 (96237)	Loss/tok 3.0013 (3.2636)	LR 2.800e-03
0: TRAIN [2][1060/1885]	Time 0.195 (0.143)	Data 2.17e-04 (4.19e-04)	Tok/s 103912 (96261)	Loss/tok 3.4548 (3.2639)	LR 2.800e-03
0: TRAIN [2][1070/1885]	Time 0.147 (0.144)	Data 2.18e-04 (4.17e-04)	Tok/s 99306 (96272)	Loss/tok 3.2457 (3.2642)	LR 2.800e-03
0: TRAIN [2][1080/1885]	Time 0.195 (0.143)	Data 2.22e-04 (4.16e-04)	Tok/s 103793 (96283)	Loss/tok 3.4217 (3.2639)	LR 2.800e-03
0: TRAIN [2][1090/1885]	Time 0.147 (0.143)	Data 1.08e-04 (4.13e-04)	Tok/s 101323 (96269)	Loss/tok 3.1593 (3.2642)	LR 2.800e-03
0: TRAIN [2][1100/1885]	Time 0.243 (0.143)	Data 2.19e-04 (4.11e-04)	Tok/s 106190 (96252)	Loss/tok 3.5752 (3.2641)	LR 2.800e-03
0: TRAIN [2][1110/1885]	Time 0.062 (0.143)	Data 2.16e-04 (4.10e-04)	Tok/s 73154 (96237)	Loss/tok 2.4178 (3.2635)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1120/1885]	Time 0.060 (0.143)	Data 2.18e-04 (4.08e-04)	Tok/s 76095 (96217)	Loss/tok 2.4100 (3.2631)	LR 2.800e-03
0: TRAIN [2][1130/1885]	Time 0.246 (0.143)	Data 2.19e-04 (4.06e-04)	Tok/s 105644 (96227)	Loss/tok 3.4985 (3.2631)	LR 2.800e-03
0: TRAIN [2][1140/1885]	Time 0.101 (0.143)	Data 2.21e-04 (4.05e-04)	Tok/s 90061 (96215)	Loss/tok 2.9628 (3.2635)	LR 2.800e-03
0: TRAIN [2][1150/1885]	Time 0.146 (0.143)	Data 2.16e-04 (4.03e-04)	Tok/s 99366 (96193)	Loss/tok 3.2022 (3.2629)	LR 2.800e-03
0: TRAIN [2][1160/1885]	Time 0.147 (0.143)	Data 2.20e-04 (4.01e-04)	Tok/s 98217 (96160)	Loss/tok 3.2459 (3.2620)	LR 2.800e-03
0: TRAIN [2][1170/1885]	Time 0.195 (0.143)	Data 2.17e-04 (4.00e-04)	Tok/s 105222 (96155)	Loss/tok 3.4071 (3.2621)	LR 2.800e-03
0: TRAIN [2][1180/1885]	Time 0.195 (0.143)	Data 2.16e-04 (3.98e-04)	Tok/s 104708 (96164)	Loss/tok 3.3988 (3.2615)	LR 2.800e-03
0: TRAIN [2][1190/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.97e-04)	Tok/s 90907 (96184)	Loss/tok 2.8359 (3.2612)	LR 2.800e-03
0: TRAIN [2][1200/1885]	Time 0.103 (0.143)	Data 2.15e-04 (3.95e-04)	Tok/s 88792 (96194)	Loss/tok 2.9383 (3.2612)	LR 2.800e-03
0: TRAIN [2][1210/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.94e-04)	Tok/s 88893 (96214)	Loss/tok 3.1340 (3.2622)	LR 2.800e-03
0: TRAIN [2][1220/1885]	Time 0.195 (0.143)	Data 2.21e-04 (3.92e-04)	Tok/s 104469 (96221)	Loss/tok 3.3780 (3.2623)	LR 2.800e-03
0: TRAIN [2][1230/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.91e-04)	Tok/s 98416 (96202)	Loss/tok 3.3929 (3.2619)	LR 2.800e-03
0: TRAIN [2][1240/1885]	Time 0.147 (0.143)	Data 2.14e-04 (3.90e-04)	Tok/s 99175 (96189)	Loss/tok 3.3190 (3.2615)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1250/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.88e-04)	Tok/s 100494 (96197)	Loss/tok 3.2889 (3.2612)	LR 2.800e-03
0: TRAIN [2][1260/1885]	Time 0.148 (0.143)	Data 2.17e-04 (3.87e-04)	Tok/s 100861 (96201)	Loss/tok 3.1758 (3.2612)	LR 2.800e-03
0: TRAIN [2][1270/1885]	Time 0.247 (0.143)	Data 2.17e-04 (3.86e-04)	Tok/s 107317 (96199)	Loss/tok 3.4542 (3.2614)	LR 2.800e-03
0: TRAIN [2][1280/1885]	Time 0.101 (0.143)	Data 2.16e-04 (3.84e-04)	Tok/s 89898 (96162)	Loss/tok 3.0090 (3.2606)	LR 2.800e-03
0: TRAIN [2][1290/1885]	Time 0.195 (0.143)	Data 2.28e-04 (3.83e-04)	Tok/s 104248 (96169)	Loss/tok 3.4217 (3.2603)	LR 2.800e-03
0: TRAIN [2][1300/1885]	Time 0.147 (0.143)	Data 2.15e-04 (3.82e-04)	Tok/s 100387 (96195)	Loss/tok 3.2590 (3.2604)	LR 2.800e-03
0: TRAIN [2][1310/1885]	Time 0.195 (0.143)	Data 2.16e-04 (3.80e-04)	Tok/s 104018 (96203)	Loss/tok 3.3897 (3.2607)	LR 2.800e-03
0: TRAIN [2][1320/1885]	Time 0.244 (0.143)	Data 2.16e-04 (3.79e-04)	Tok/s 106784 (96185)	Loss/tok 3.5744 (3.2605)	LR 2.800e-03
0: TRAIN [2][1330/1885]	Time 0.103 (0.143)	Data 2.22e-04 (3.78e-04)	Tok/s 87747 (96148)	Loss/tok 3.0971 (3.2598)	LR 2.800e-03
0: TRAIN [2][1340/1885]	Time 0.101 (0.143)	Data 2.24e-04 (3.77e-04)	Tok/s 90793 (96142)	Loss/tok 3.0673 (3.2599)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1350/1885]	Time 0.245 (0.143)	Data 2.14e-04 (3.76e-04)	Tok/s 106482 (96134)	Loss/tok 3.5933 (3.2597)	LR 2.800e-03
0: TRAIN [2][1360/1885]	Time 0.196 (0.143)	Data 1.10e-04 (3.74e-04)	Tok/s 103961 (96157)	Loss/tok 3.5305 (3.2602)	LR 2.800e-03
0: TRAIN [2][1370/1885]	Time 0.245 (0.143)	Data 2.20e-04 (3.73e-04)	Tok/s 106057 (96171)	Loss/tok 3.5421 (3.2609)	LR 2.800e-03
0: TRAIN [2][1380/1885]	Time 0.195 (0.143)	Data 1.91e-04 (3.72e-04)	Tok/s 104739 (96220)	Loss/tok 3.4073 (3.2620)	LR 2.800e-03
0: TRAIN [2][1390/1885]	Time 0.146 (0.143)	Data 2.16e-04 (3.70e-04)	Tok/s 100308 (96208)	Loss/tok 3.2042 (3.2616)	LR 2.800e-03
0: TRAIN [2][1400/1885]	Time 0.195 (0.143)	Data 2.14e-04 (3.69e-04)	Tok/s 105765 (96220)	Loss/tok 3.3070 (3.2615)	LR 2.800e-03
0: TRAIN [2][1410/1885]	Time 0.101 (0.143)	Data 8.56e-05 (3.67e-04)	Tok/s 90433 (96249)	Loss/tok 2.9812 (3.2622)	LR 2.800e-03
0: TRAIN [2][1420/1885]	Time 0.100 (0.143)	Data 8.63e-05 (3.65e-04)	Tok/s 92087 (96269)	Loss/tok 2.9718 (3.2627)	LR 2.800e-03
0: TRAIN [2][1430/1885]	Time 0.147 (0.143)	Data 8.32e-05 (3.64e-04)	Tok/s 100028 (96283)	Loss/tok 3.2643 (3.2625)	LR 2.800e-03
0: TRAIN [2][1440/1885]	Time 0.102 (0.143)	Data 8.32e-05 (3.62e-04)	Tok/s 88306 (96258)	Loss/tok 3.0936 (3.2623)	LR 2.800e-03
0: TRAIN [2][1450/1885]	Time 0.246 (0.143)	Data 8.49e-05 (3.60e-04)	Tok/s 107466 (96284)	Loss/tok 3.4893 (3.2629)	LR 2.800e-03
0: TRAIN [2][1460/1885]	Time 0.146 (0.143)	Data 8.44e-05 (3.58e-04)	Tok/s 100480 (96272)	Loss/tok 3.2168 (3.2624)	LR 2.800e-03
0: TRAIN [2][1470/1885]	Time 0.146 (0.143)	Data 8.42e-05 (3.56e-04)	Tok/s 101132 (96254)	Loss/tok 3.2161 (3.2615)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1480/1885]	Time 0.102 (0.143)	Data 8.58e-05 (3.54e-04)	Tok/s 88757 (96254)	Loss/tok 2.9393 (3.2609)	LR 2.800e-03
0: TRAIN [2][1490/1885]	Time 0.246 (0.143)	Data 8.44e-05 (3.52e-04)	Tok/s 106806 (96252)	Loss/tok 3.5523 (3.2609)	LR 2.800e-03
0: TRAIN [2][1500/1885]	Time 0.147 (0.143)	Data 8.27e-05 (3.50e-04)	Tok/s 99559 (96254)	Loss/tok 3.3019 (3.2610)	LR 2.800e-03
0: TRAIN [2][1510/1885]	Time 0.101 (0.143)	Data 8.23e-05 (3.49e-04)	Tok/s 90469 (96237)	Loss/tok 3.0152 (3.2604)	LR 2.800e-03
0: TRAIN [2][1520/1885]	Time 0.146 (0.143)	Data 8.42e-05 (3.47e-04)	Tok/s 99567 (96246)	Loss/tok 3.2691 (3.2607)	LR 2.800e-03
0: TRAIN [2][1530/1885]	Time 0.245 (0.143)	Data 8.49e-05 (3.45e-04)	Tok/s 107528 (96251)	Loss/tok 3.4790 (3.2609)	LR 2.800e-03
0: TRAIN [2][1540/1885]	Time 0.147 (0.143)	Data 8.44e-05 (3.44e-04)	Tok/s 100981 (96249)	Loss/tok 3.1416 (3.2605)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1550/1885]	Time 0.102 (0.143)	Data 8.94e-05 (3.42e-04)	Tok/s 90222 (96245)	Loss/tok 3.0285 (3.2602)	LR 2.800e-03
0: TRAIN [2][1560/1885]	Time 0.101 (0.143)	Data 8.34e-05 (3.40e-04)	Tok/s 90349 (96231)	Loss/tok 2.9120 (3.2595)	LR 2.800e-03
0: TRAIN [2][1570/1885]	Time 0.193 (0.143)	Data 9.01e-05 (3.39e-04)	Tok/s 104286 (96232)	Loss/tok 3.3015 (3.2589)	LR 2.800e-03
0: TRAIN [2][1580/1885]	Time 0.102 (0.143)	Data 8.46e-05 (3.37e-04)	Tok/s 90633 (96231)	Loss/tok 2.9262 (3.2589)	LR 2.800e-03
0: TRAIN [2][1590/1885]	Time 0.101 (0.143)	Data 8.30e-05 (3.35e-04)	Tok/s 90232 (96230)	Loss/tok 2.9902 (3.2584)	LR 2.800e-03
0: TRAIN [2][1600/1885]	Time 0.146 (0.143)	Data 8.30e-05 (3.34e-04)	Tok/s 100035 (96218)	Loss/tok 3.2514 (3.2579)	LR 2.800e-03
0: TRAIN [2][1610/1885]	Time 0.101 (0.143)	Data 8.32e-05 (3.32e-04)	Tok/s 87652 (96193)	Loss/tok 3.0810 (3.2576)	LR 2.800e-03
0: TRAIN [2][1620/1885]	Time 0.147 (0.142)	Data 1.16e-04 (3.31e-04)	Tok/s 100726 (96178)	Loss/tok 3.3230 (3.2574)	LR 2.800e-03
0: TRAIN [2][1630/1885]	Time 0.147 (0.142)	Data 8.44e-05 (3.29e-04)	Tok/s 100364 (96198)	Loss/tok 3.1624 (3.2572)	LR 2.800e-03
0: TRAIN [2][1640/1885]	Time 0.194 (0.142)	Data 8.23e-05 (3.28e-04)	Tok/s 104701 (96197)	Loss/tok 3.5835 (3.2568)	LR 2.800e-03
0: TRAIN [2][1650/1885]	Time 0.147 (0.142)	Data 8.34e-05 (3.26e-04)	Tok/s 100245 (96186)	Loss/tok 3.2077 (3.2564)	LR 2.800e-03
0: TRAIN [2][1660/1885]	Time 0.102 (0.142)	Data 8.34e-05 (3.25e-04)	Tok/s 90044 (96196)	Loss/tok 3.0559 (3.2566)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1670/1885]	Time 0.245 (0.143)	Data 8.46e-05 (3.23e-04)	Tok/s 106640 (96200)	Loss/tok 3.5752 (3.2571)	LR 2.800e-03
0: TRAIN [2][1680/1885]	Time 0.195 (0.143)	Data 8.25e-05 (3.22e-04)	Tok/s 104470 (96219)	Loss/tok 3.3136 (3.2573)	LR 2.800e-03
0: TRAIN [2][1690/1885]	Time 0.147 (0.143)	Data 8.32e-05 (3.21e-04)	Tok/s 100349 (96219)	Loss/tok 3.4207 (3.2572)	LR 2.800e-03
0: TRAIN [2][1700/1885]	Time 0.148 (0.143)	Data 8.39e-05 (3.19e-04)	Tok/s 99638 (96243)	Loss/tok 3.1965 (3.2580)	LR 2.800e-03
0: TRAIN [2][1710/1885]	Time 0.146 (0.143)	Data 8.49e-05 (3.18e-04)	Tok/s 100018 (96229)	Loss/tok 3.2157 (3.2577)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1720/1885]	Time 0.147 (0.143)	Data 8.73e-05 (3.16e-04)	Tok/s 98956 (96241)	Loss/tok 3.2683 (3.2580)	LR 2.800e-03
0: TRAIN [2][1730/1885]	Time 0.193 (0.143)	Data 8.27e-05 (3.15e-04)	Tok/s 106040 (96238)	Loss/tok 3.4410 (3.2581)	LR 2.800e-03
0: TRAIN [2][1740/1885]	Time 0.246 (0.143)	Data 8.34e-05 (3.14e-04)	Tok/s 104871 (96227)	Loss/tok 3.6202 (3.2582)	LR 2.800e-03
0: TRAIN [2][1750/1885]	Time 0.148 (0.143)	Data 8.34e-05 (3.12e-04)	Tok/s 97623 (96205)	Loss/tok 3.2591 (3.2580)	LR 2.800e-03
0: TRAIN [2][1760/1885]	Time 0.247 (0.143)	Data 8.37e-05 (3.11e-04)	Tok/s 104788 (96230)	Loss/tok 3.4716 (3.2585)	LR 2.800e-03
0: TRAIN [2][1770/1885]	Time 0.101 (0.143)	Data 8.49e-05 (3.10e-04)	Tok/s 90499 (96197)	Loss/tok 3.0213 (3.2577)	LR 2.800e-03
0: TRAIN [2][1780/1885]	Time 0.245 (0.143)	Data 8.49e-05 (3.09e-04)	Tok/s 106178 (96199)	Loss/tok 3.5228 (3.2578)	LR 2.800e-03
0: TRAIN [2][1790/1885]	Time 0.147 (0.142)	Data 8.49e-05 (3.07e-04)	Tok/s 100311 (96178)	Loss/tok 3.2366 (3.2575)	LR 2.800e-03
0: TRAIN [2][1800/1885]	Time 0.102 (0.143)	Data 8.34e-05 (3.06e-04)	Tok/s 88937 (96181)	Loss/tok 2.9449 (3.2579)	LR 2.800e-03
0: TRAIN [2][1810/1885]	Time 0.102 (0.142)	Data 8.25e-05 (3.05e-04)	Tok/s 92317 (96176)	Loss/tok 2.8987 (3.2578)	LR 2.800e-03
0: TRAIN [2][1820/1885]	Time 0.148 (0.143)	Data 8.61e-05 (3.04e-04)	Tok/s 98862 (96181)	Loss/tok 3.1143 (3.2579)	LR 2.800e-03
0: TRAIN [2][1830/1885]	Time 0.244 (0.143)	Data 8.56e-05 (3.02e-04)	Tok/s 108237 (96196)	Loss/tok 3.5658 (3.2587)	LR 2.800e-03
0: TRAIN [2][1840/1885]	Time 0.146 (0.143)	Data 8.42e-05 (3.01e-04)	Tok/s 98489 (96208)	Loss/tok 3.1885 (3.2591)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1850/1885]	Time 0.147 (0.143)	Data 8.80e-05 (3.00e-04)	Tok/s 98882 (96215)	Loss/tok 3.2417 (3.2593)	LR 2.800e-03
0: TRAIN [2][1860/1885]	Time 0.101 (0.143)	Data 8.49e-05 (2.99e-04)	Tok/s 90173 (96216)	Loss/tok 3.1429 (3.2592)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1870/1885]	Time 0.147 (0.143)	Data 8.68e-05 (2.98e-04)	Tok/s 98911 (96219)	Loss/tok 3.1339 (3.2591)	LR 2.800e-03
0: TRAIN [2][1880/1885]	Time 0.147 (0.143)	Data 8.51e-05 (2.97e-04)	Tok/s 99451 (96247)	Loss/tok 3.1392 (3.2594)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593832604910, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593832604911, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.555 (0.555)	Decoder iters 96.0 (96.0)	Tok/s 29233 (29233)
0: Running moses detokenizer
0: BLEU(score=22.557317760617167, counts=[36291, 17659, 9778, 5670], totals=[65460, 62457, 59454, 56456], precisions=[55.43996333638864, 28.273852410458396, 16.446328253776027, 10.043219498370412], bp=1.0, sys_len=65460, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593832606479, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2256, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593832606480, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2593	Test BLEU: 22.56
0: Performance: Epoch: 2	Training: 770107 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593832606480, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593832606480, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593832606481, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3595620521
0: TRAIN [3][0/1885]	Time 0.434 (0.434)	Data 2.23e-01 (2.23e-01)	Tok/s 46662 (46662)	Loss/tok 3.3165 (3.3165)	LR 2.800e-03
0: TRAIN [3][10/1885]	Time 0.101 (0.163)	Data 8.30e-05 (2.03e-02)	Tok/s 90091 (89282)	Loss/tok 2.8779 (3.1790)	LR 2.800e-03
0: TRAIN [3][20/1885]	Time 0.146 (0.156)	Data 8.30e-05 (1.07e-02)	Tok/s 99027 (92951)	Loss/tok 3.2155 (3.1637)	LR 2.800e-03
0: TRAIN [3][30/1885]	Time 0.147 (0.147)	Data 8.32e-05 (7.27e-03)	Tok/s 99469 (92450)	Loss/tok 3.1066 (3.1484)	LR 2.800e-03
0: TRAIN [3][40/1885]	Time 0.106 (0.149)	Data 8.25e-05 (5.52e-03)	Tok/s 85818 (93045)	Loss/tok 2.9787 (3.1693)	LR 2.800e-03
0: TRAIN [3][50/1885]	Time 0.247 (0.149)	Data 8.37e-05 (4.45e-03)	Tok/s 106106 (93626)	Loss/tok 3.4169 (3.1706)	LR 2.800e-03
0: TRAIN [3][60/1885]	Time 0.060 (0.149)	Data 8.68e-05 (3.74e-03)	Tok/s 77471 (93832)	Loss/tok 2.4072 (3.1692)	LR 2.800e-03
0: TRAIN [3][70/1885]	Time 0.101 (0.144)	Data 8.34e-05 (3.22e-03)	Tok/s 89421 (93638)	Loss/tok 2.9528 (3.1517)	LR 2.800e-03
0: TRAIN [3][80/1885]	Time 0.146 (0.147)	Data 8.39e-05 (2.84e-03)	Tok/s 100766 (94522)	Loss/tok 3.1250 (3.1670)	LR 2.800e-03
0: TRAIN [3][90/1885]	Time 0.194 (0.148)	Data 8.46e-05 (2.53e-03)	Tok/s 105734 (95046)	Loss/tok 3.2121 (3.1684)	LR 2.800e-03
0: TRAIN [3][100/1885]	Time 0.245 (0.147)	Data 8.46e-05 (2.29e-03)	Tok/s 107362 (94997)	Loss/tok 3.3834 (3.1671)	LR 2.800e-03
0: TRAIN [3][110/1885]	Time 0.194 (0.147)	Data 8.32e-05 (2.09e-03)	Tok/s 105276 (95160)	Loss/tok 3.3347 (3.1754)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][120/1885]	Time 0.102 (0.147)	Data 8.34e-05 (1.93e-03)	Tok/s 87615 (95262)	Loss/tok 2.9740 (3.1764)	LR 2.800e-03
0: TRAIN [3][130/1885]	Time 0.101 (0.146)	Data 8.39e-05 (1.79e-03)	Tok/s 89603 (95194)	Loss/tok 2.9548 (3.1736)	LR 2.800e-03
0: TRAIN [3][140/1885]	Time 0.147 (0.145)	Data 8.65e-05 (1.67e-03)	Tok/s 99531 (95276)	Loss/tok 3.2642 (3.1721)	LR 2.800e-03
0: TRAIN [3][150/1885]	Time 0.102 (0.145)	Data 8.27e-05 (1.56e-03)	Tok/s 90126 (95323)	Loss/tok 2.8554 (3.1732)	LR 2.800e-03
0: TRAIN [3][160/1885]	Time 0.195 (0.145)	Data 8.42e-05 (1.47e-03)	Tok/s 104867 (95364)	Loss/tok 3.2435 (3.1728)	LR 2.800e-03
0: TRAIN [3][170/1885]	Time 0.060 (0.146)	Data 8.44e-05 (1.39e-03)	Tok/s 77610 (95580)	Loss/tok 2.4997 (3.1750)	LR 2.800e-03
0: TRAIN [3][180/1885]	Time 0.102 (0.146)	Data 8.49e-05 (1.32e-03)	Tok/s 89393 (95599)	Loss/tok 2.9549 (3.1764)	LR 2.800e-03
0: TRAIN [3][190/1885]	Time 0.146 (0.145)	Data 8.23e-05 (1.25e-03)	Tok/s 99249 (95496)	Loss/tok 3.1936 (3.1729)	LR 2.800e-03
0: TRAIN [3][200/1885]	Time 0.100 (0.146)	Data 8.49e-05 (1.19e-03)	Tok/s 89178 (95753)	Loss/tok 3.0575 (3.1796)	LR 2.800e-03
0: TRAIN [3][210/1885]	Time 0.102 (0.147)	Data 8.39e-05 (1.14e-03)	Tok/s 87448 (95890)	Loss/tok 2.8568 (3.1817)	LR 2.800e-03
0: TRAIN [3][220/1885]	Time 0.193 (0.146)	Data 8.20e-05 (1.09e-03)	Tok/s 106046 (95786)	Loss/tok 3.1676 (3.1772)	LR 2.800e-03
0: TRAIN [3][230/1885]	Time 0.148 (0.146)	Data 8.54e-05 (1.05e-03)	Tok/s 99046 (95922)	Loss/tok 3.1669 (3.1810)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][240/1885]	Time 0.060 (0.144)	Data 8.39e-05 (1.01e-03)	Tok/s 78052 (95682)	Loss/tok 2.4740 (3.1755)	LR 2.800e-03
0: TRAIN [3][250/1885]	Time 0.101 (0.144)	Data 8.46e-05 (9.73e-04)	Tok/s 90020 (95724)	Loss/tok 2.9165 (3.1744)	LR 2.800e-03
0: TRAIN [3][260/1885]	Time 0.147 (0.144)	Data 8.49e-05 (9.39e-04)	Tok/s 99292 (95724)	Loss/tok 3.1217 (3.1740)	LR 2.800e-03
0: TRAIN [3][270/1885]	Time 0.102 (0.143)	Data 8.44e-05 (9.07e-04)	Tok/s 88314 (95669)	Loss/tok 2.8038 (3.1706)	LR 2.800e-03
0: TRAIN [3][280/1885]	Time 0.147 (0.144)	Data 8.37e-05 (8.78e-04)	Tok/s 99504 (95829)	Loss/tok 3.1623 (3.1711)	LR 2.800e-03
0: TRAIN [3][290/1885]	Time 0.193 (0.143)	Data 8.58e-05 (8.51e-04)	Tok/s 107726 (95774)	Loss/tok 3.2981 (3.1685)	LR 2.800e-03
0: TRAIN [3][300/1885]	Time 0.195 (0.143)	Data 8.27e-05 (8.25e-04)	Tok/s 105141 (95793)	Loss/tok 3.3299 (3.1716)	LR 2.800e-03
0: TRAIN [3][310/1885]	Time 0.059 (0.144)	Data 8.80e-05 (8.02e-04)	Tok/s 76827 (95841)	Loss/tok 2.4722 (3.1727)	LR 2.800e-03
0: TRAIN [3][320/1885]	Time 0.101 (0.144)	Data 8.44e-05 (7.79e-04)	Tok/s 89721 (95859)	Loss/tok 2.8974 (3.1729)	LR 2.800e-03
0: TRAIN [3][330/1885]	Time 0.102 (0.144)	Data 8.44e-05 (7.58e-04)	Tok/s 88474 (95934)	Loss/tok 2.9911 (3.1755)	LR 2.800e-03
0: TRAIN [3][340/1885]	Time 0.060 (0.143)	Data 8.27e-05 (7.39e-04)	Tok/s 76605 (95887)	Loss/tok 2.5195 (3.1737)	LR 2.800e-03
0: TRAIN [3][350/1885]	Time 0.146 (0.142)	Data 8.34e-05 (7.20e-04)	Tok/s 99950 (95730)	Loss/tok 3.1503 (3.1700)	LR 2.800e-03
0: TRAIN [3][360/1885]	Time 0.101 (0.142)	Data 8.54e-05 (7.02e-04)	Tok/s 89910 (95653)	Loss/tok 2.9249 (3.1689)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][370/1885]	Time 0.060 (0.142)	Data 8.42e-05 (6.86e-04)	Tok/s 75751 (95696)	Loss/tok 2.4798 (3.1719)	LR 2.800e-03
0: TRAIN [3][380/1885]	Time 0.146 (0.142)	Data 8.39e-05 (6.70e-04)	Tok/s 99677 (95667)	Loss/tok 3.0571 (3.1717)	LR 2.800e-03
0: TRAIN [3][390/1885]	Time 0.148 (0.143)	Data 8.56e-05 (6.55e-04)	Tok/s 99286 (95781)	Loss/tok 3.0709 (3.1733)	LR 2.800e-03
0: TRAIN [3][400/1885]	Time 0.146 (0.142)	Data 8.32e-05 (6.41e-04)	Tok/s 100178 (95772)	Loss/tok 3.1750 (3.1722)	LR 2.800e-03
0: TRAIN [3][410/1885]	Time 0.147 (0.142)	Data 8.32e-05 (6.27e-04)	Tok/s 100434 (95744)	Loss/tok 3.1011 (3.1719)	LR 2.800e-03
0: TRAIN [3][420/1885]	Time 0.146 (0.142)	Data 8.70e-05 (6.14e-04)	Tok/s 99591 (95718)	Loss/tok 3.1539 (3.1710)	LR 2.800e-03
0: TRAIN [3][430/1885]	Time 0.146 (0.142)	Data 8.30e-05 (6.02e-04)	Tok/s 100614 (95812)	Loss/tok 3.1951 (3.1736)	LR 1.400e-03
0: TRAIN [3][440/1885]	Time 0.146 (0.142)	Data 8.51e-05 (5.90e-04)	Tok/s 99164 (95761)	Loss/tok 3.0223 (3.1724)	LR 1.400e-03
0: TRAIN [3][450/1885]	Time 0.104 (0.142)	Data 9.08e-05 (5.79e-04)	Tok/s 87931 (95723)	Loss/tok 2.8635 (3.1713)	LR 1.400e-03
0: TRAIN [3][460/1885]	Time 0.195 (0.141)	Data 8.46e-05 (5.69e-04)	Tok/s 105047 (95649)	Loss/tok 3.2173 (3.1693)	LR 1.400e-03
0: TRAIN [3][470/1885]	Time 0.194 (0.142)	Data 8.70e-05 (5.58e-04)	Tok/s 106834 (95713)	Loss/tok 3.1597 (3.1703)	LR 1.400e-03
0: TRAIN [3][480/1885]	Time 0.103 (0.142)	Data 8.58e-05 (5.49e-04)	Tok/s 88610 (95774)	Loss/tok 2.7549 (3.1716)	LR 1.400e-03
0: TRAIN [3][490/1885]	Time 0.102 (0.142)	Data 8.34e-05 (5.39e-04)	Tok/s 88400 (95787)	Loss/tok 2.9683 (3.1728)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][500/1885]	Time 0.102 (0.142)	Data 8.51e-05 (5.30e-04)	Tok/s 88485 (95745)	Loss/tok 2.8950 (3.1714)	LR 1.400e-03
0: TRAIN [3][510/1885]	Time 0.146 (0.142)	Data 8.49e-05 (5.21e-04)	Tok/s 100545 (95764)	Loss/tok 3.1302 (3.1707)	LR 1.400e-03
0: TRAIN [3][520/1885]	Time 0.059 (0.142)	Data 8.42e-05 (5.13e-04)	Tok/s 75732 (95773)	Loss/tok 2.3758 (3.1715)	LR 1.400e-03
0: TRAIN [3][530/1885]	Time 0.102 (0.142)	Data 8.30e-05 (5.05e-04)	Tok/s 88726 (95820)	Loss/tok 2.8977 (3.1725)	LR 1.400e-03
0: TRAIN [3][540/1885]	Time 0.146 (0.142)	Data 8.51e-05 (4.97e-04)	Tok/s 101289 (95852)	Loss/tok 3.0320 (3.1732)	LR 1.400e-03
0: TRAIN [3][550/1885]	Time 0.194 (0.142)	Data 8.68e-05 (4.90e-04)	Tok/s 105827 (95933)	Loss/tok 3.2859 (3.1732)	LR 1.400e-03
0: TRAIN [3][560/1885]	Time 0.146 (0.142)	Data 8.30e-05 (4.82e-04)	Tok/s 101417 (95932)	Loss/tok 3.1297 (3.1718)	LR 1.400e-03
0: TRAIN [3][570/1885]	Time 0.147 (0.143)	Data 8.54e-05 (4.75e-04)	Tok/s 99831 (96016)	Loss/tok 3.0832 (3.1732)	LR 1.400e-03
0: TRAIN [3][580/1885]	Time 0.148 (0.143)	Data 8.70e-05 (4.69e-04)	Tok/s 98994 (96093)	Loss/tok 3.2563 (3.1737)	LR 1.400e-03
0: TRAIN [3][590/1885]	Time 0.147 (0.143)	Data 8.30e-05 (4.62e-04)	Tok/s 99764 (96150)	Loss/tok 3.1603 (3.1740)	LR 1.400e-03
0: TRAIN [3][600/1885]	Time 0.101 (0.143)	Data 8.39e-05 (4.56e-04)	Tok/s 87592 (96175)	Loss/tok 2.9224 (3.1730)	LR 1.400e-03
0: TRAIN [3][610/1885]	Time 0.147 (0.143)	Data 8.46e-05 (4.50e-04)	Tok/s 101819 (96200)	Loss/tok 3.1744 (3.1727)	LR 1.400e-03
0: TRAIN [3][620/1885]	Time 0.146 (0.143)	Data 8.30e-05 (4.44e-04)	Tok/s 102278 (96223)	Loss/tok 3.0608 (3.1726)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][630/1885]	Time 0.245 (0.144)	Data 8.54e-05 (4.38e-04)	Tok/s 106902 (96232)	Loss/tok 3.3985 (3.1734)	LR 1.400e-03
0: TRAIN [3][640/1885]	Time 0.146 (0.144)	Data 8.49e-05 (4.33e-04)	Tok/s 99960 (96243)	Loss/tok 3.1066 (3.1732)	LR 1.400e-03
0: TRAIN [3][650/1885]	Time 0.146 (0.144)	Data 8.44e-05 (4.28e-04)	Tok/s 100569 (96232)	Loss/tok 3.1272 (3.1722)	LR 1.400e-03
0: TRAIN [3][660/1885]	Time 0.102 (0.144)	Data 8.54e-05 (4.22e-04)	Tok/s 88672 (96220)	Loss/tok 2.8533 (3.1723)	LR 1.400e-03
0: TRAIN [3][670/1885]	Time 0.195 (0.143)	Data 8.37e-05 (4.17e-04)	Tok/s 104847 (96179)	Loss/tok 3.1905 (3.1707)	LR 1.400e-03
0: TRAIN [3][680/1885]	Time 0.101 (0.143)	Data 8.39e-05 (4.12e-04)	Tok/s 89590 (96153)	Loss/tok 2.8499 (3.1706)	LR 1.400e-03
0: TRAIN [3][690/1885]	Time 0.102 (0.143)	Data 8.54e-05 (4.08e-04)	Tok/s 90612 (96185)	Loss/tok 2.9241 (3.1716)	LR 1.400e-03
0: TRAIN [3][700/1885]	Time 0.146 (0.143)	Data 8.51e-05 (4.03e-04)	Tok/s 100242 (96171)	Loss/tok 3.0863 (3.1707)	LR 1.400e-03
0: TRAIN [3][710/1885]	Time 0.146 (0.143)	Data 8.42e-05 (3.99e-04)	Tok/s 101822 (96172)	Loss/tok 3.0927 (3.1703)	LR 1.400e-03
0: TRAIN [3][720/1885]	Time 0.101 (0.143)	Data 8.37e-05 (3.94e-04)	Tok/s 91497 (96177)	Loss/tok 2.9019 (3.1699)	LR 1.400e-03
0: TRAIN [3][730/1885]	Time 0.102 (0.143)	Data 8.70e-05 (3.90e-04)	Tok/s 89999 (96180)	Loss/tok 2.9189 (3.1688)	LR 1.400e-03
0: TRAIN [3][740/1885]	Time 0.246 (0.143)	Data 8.61e-05 (3.86e-04)	Tok/s 105210 (96178)	Loss/tok 3.4851 (3.1697)	LR 1.400e-03
0: TRAIN [3][750/1885]	Time 0.147 (0.143)	Data 8.44e-05 (3.82e-04)	Tok/s 99562 (96173)	Loss/tok 3.0630 (3.1687)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][760/1885]	Time 0.149 (0.143)	Data 8.30e-05 (3.78e-04)	Tok/s 98286 (96130)	Loss/tok 3.1414 (3.1672)	LR 1.400e-03
0: TRAIN [3][770/1885]	Time 0.194 (0.143)	Data 8.34e-05 (3.74e-04)	Tok/s 106002 (96176)	Loss/tok 3.2108 (3.1672)	LR 1.400e-03
0: TRAIN [3][780/1885]	Time 0.103 (0.143)	Data 8.44e-05 (3.71e-04)	Tok/s 89145 (96172)	Loss/tok 2.9240 (3.1675)	LR 1.400e-03
0: TRAIN [3][790/1885]	Time 0.147 (0.143)	Data 8.46e-05 (3.67e-04)	Tok/s 101304 (96161)	Loss/tok 3.0736 (3.1673)	LR 1.400e-03
0: TRAIN [3][800/1885]	Time 0.103 (0.143)	Data 8.61e-05 (3.63e-04)	Tok/s 88582 (96181)	Loss/tok 2.8754 (3.1664)	LR 1.400e-03
0: TRAIN [3][810/1885]	Time 0.194 (0.144)	Data 8.54e-05 (3.60e-04)	Tok/s 104971 (96236)	Loss/tok 3.2274 (3.1668)	LR 1.400e-03
0: TRAIN [3][820/1885]	Time 0.147 (0.144)	Data 8.30e-05 (3.57e-04)	Tok/s 99110 (96280)	Loss/tok 3.1708 (3.1673)	LR 1.400e-03
0: TRAIN [3][830/1885]	Time 0.102 (0.144)	Data 8.39e-05 (3.53e-04)	Tok/s 90510 (96278)	Loss/tok 2.8376 (3.1670)	LR 1.400e-03
0: TRAIN [3][840/1885]	Time 0.195 (0.144)	Data 8.65e-05 (3.50e-04)	Tok/s 106022 (96274)	Loss/tok 3.2196 (3.1666)	LR 1.400e-03
0: TRAIN [3][850/1885]	Time 0.100 (0.144)	Data 8.32e-05 (3.47e-04)	Tok/s 90471 (96340)	Loss/tok 2.9432 (3.1689)	LR 1.400e-03
0: TRAIN [3][860/1885]	Time 0.193 (0.144)	Data 8.23e-05 (3.44e-04)	Tok/s 104832 (96356)	Loss/tok 3.2851 (3.1690)	LR 1.400e-03
0: TRAIN [3][870/1885]	Time 0.101 (0.144)	Data 8.63e-05 (3.41e-04)	Tok/s 87546 (96308)	Loss/tok 2.9153 (3.1677)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][880/1885]	Time 0.246 (0.144)	Data 8.34e-05 (3.38e-04)	Tok/s 107323 (96319)	Loss/tok 3.3535 (3.1676)	LR 1.400e-03
0: TRAIN [3][890/1885]	Time 0.194 (0.144)	Data 8.32e-05 (3.35e-04)	Tok/s 105913 (96321)	Loss/tok 3.2734 (3.1675)	LR 1.400e-03
0: TRAIN [3][900/1885]	Time 0.102 (0.144)	Data 8.34e-05 (3.33e-04)	Tok/s 88994 (96292)	Loss/tok 2.9095 (3.1660)	LR 1.400e-03
0: TRAIN [3][910/1885]	Time 0.196 (0.144)	Data 8.39e-05 (3.30e-04)	Tok/s 104357 (96313)	Loss/tok 3.1986 (3.1656)	LR 1.400e-03
0: TRAIN [3][920/1885]	Time 0.146 (0.144)	Data 8.49e-05 (3.27e-04)	Tok/s 101109 (96287)	Loss/tok 3.0720 (3.1643)	LR 1.400e-03
0: TRAIN [3][930/1885]	Time 0.145 (0.144)	Data 8.54e-05 (3.25e-04)	Tok/s 100358 (96291)	Loss/tok 3.0921 (3.1642)	LR 1.400e-03
0: TRAIN [3][940/1885]	Time 0.147 (0.144)	Data 8.49e-05 (3.22e-04)	Tok/s 100882 (96297)	Loss/tok 3.1132 (3.1642)	LR 1.400e-03
0: TRAIN [3][950/1885]	Time 0.101 (0.144)	Data 8.54e-05 (3.20e-04)	Tok/s 90477 (96282)	Loss/tok 2.7393 (3.1633)	LR 1.400e-03
0: TRAIN [3][960/1885]	Time 0.146 (0.144)	Data 8.61e-05 (3.17e-04)	Tok/s 100226 (96268)	Loss/tok 3.0825 (3.1630)	LR 1.400e-03
0: TRAIN [3][970/1885]	Time 0.146 (0.144)	Data 8.37e-05 (3.15e-04)	Tok/s 101689 (96258)	Loss/tok 3.1320 (3.1620)	LR 1.400e-03
0: TRAIN [3][980/1885]	Time 0.101 (0.143)	Data 8.56e-05 (3.12e-04)	Tok/s 91242 (96241)	Loss/tok 2.9078 (3.1609)	LR 1.400e-03
0: TRAIN [3][990/1885]	Time 0.102 (0.144)	Data 8.39e-05 (3.10e-04)	Tok/s 88987 (96279)	Loss/tok 2.8632 (3.1623)	LR 1.400e-03
0: TRAIN [3][1000/1885]	Time 0.102 (0.144)	Data 8.37e-05 (3.08e-04)	Tok/s 87758 (96282)	Loss/tok 2.8721 (3.1620)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1010/1885]	Time 0.146 (0.144)	Data 8.61e-05 (3.06e-04)	Tok/s 102213 (96264)	Loss/tok 2.9787 (3.1617)	LR 1.400e-03
0: TRAIN [3][1020/1885]	Time 0.147 (0.144)	Data 8.39e-05 (3.04e-04)	Tok/s 98617 (96304)	Loss/tok 3.0847 (3.1622)	LR 1.400e-03
0: TRAIN [3][1030/1885]	Time 0.101 (0.144)	Data 8.34e-05 (3.01e-04)	Tok/s 90590 (96275)	Loss/tok 2.8637 (3.1615)	LR 1.400e-03
0: TRAIN [3][1040/1885]	Time 0.195 (0.144)	Data 8.58e-05 (2.99e-04)	Tok/s 104386 (96282)	Loss/tok 3.2370 (3.1608)	LR 1.400e-03
0: TRAIN [3][1050/1885]	Time 0.102 (0.144)	Data 8.44e-05 (2.97e-04)	Tok/s 88739 (96291)	Loss/tok 2.9415 (3.1616)	LR 1.400e-03
0: TRAIN [3][1060/1885]	Time 0.146 (0.144)	Data 8.46e-05 (2.95e-04)	Tok/s 100713 (96282)	Loss/tok 3.0250 (3.1606)	LR 1.400e-03
0: TRAIN [3][1070/1885]	Time 0.061 (0.144)	Data 8.56e-05 (2.93e-04)	Tok/s 74614 (96260)	Loss/tok 2.4774 (3.1593)	LR 1.400e-03
0: TRAIN [3][1080/1885]	Time 0.245 (0.144)	Data 8.37e-05 (2.91e-04)	Tok/s 107204 (96293)	Loss/tok 3.3745 (3.1600)	LR 1.400e-03
0: TRAIN [3][1090/1885]	Time 0.146 (0.144)	Data 8.46e-05 (2.90e-04)	Tok/s 99868 (96276)	Loss/tok 3.0608 (3.1591)	LR 1.400e-03
0: TRAIN [3][1100/1885]	Time 0.102 (0.144)	Data 8.61e-05 (2.88e-04)	Tok/s 88617 (96245)	Loss/tok 2.8809 (3.1584)	LR 1.400e-03
0: TRAIN [3][1110/1885]	Time 0.101 (0.144)	Data 8.56e-05 (2.86e-04)	Tok/s 89863 (96248)	Loss/tok 2.9223 (3.1583)	LR 1.400e-03
0: TRAIN [3][1120/1885]	Time 0.102 (0.144)	Data 8.44e-05 (2.84e-04)	Tok/s 88149 (96251)	Loss/tok 2.9703 (3.1579)	LR 1.400e-03
0: TRAIN [3][1130/1885]	Time 0.102 (0.143)	Data 8.34e-05 (2.82e-04)	Tok/s 91311 (96195)	Loss/tok 2.9834 (3.1562)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1140/1885]	Time 0.102 (0.143)	Data 8.42e-05 (2.81e-04)	Tok/s 88045 (96163)	Loss/tok 2.8241 (3.1554)	LR 1.400e-03
0: TRAIN [3][1150/1885]	Time 0.146 (0.143)	Data 8.56e-05 (2.79e-04)	Tok/s 100487 (96186)	Loss/tok 3.1377 (3.1550)	LR 1.400e-03
0: TRAIN [3][1160/1885]	Time 0.104 (0.143)	Data 8.42e-05 (2.77e-04)	Tok/s 88429 (96190)	Loss/tok 2.8950 (3.1549)	LR 1.400e-03
0: TRAIN [3][1170/1885]	Time 0.148 (0.143)	Data 8.54e-05 (2.76e-04)	Tok/s 99195 (96193)	Loss/tok 3.0665 (3.1548)	LR 1.400e-03
0: TRAIN [3][1180/1885]	Time 0.147 (0.143)	Data 8.39e-05 (2.74e-04)	Tok/s 100115 (96176)	Loss/tok 3.1230 (3.1540)	LR 1.400e-03
0: TRAIN [3][1190/1885]	Time 0.060 (0.143)	Data 8.37e-05 (2.72e-04)	Tok/s 76228 (96140)	Loss/tok 2.4604 (3.1532)	LR 1.400e-03
0: TRAIN [3][1200/1885]	Time 0.196 (0.143)	Data 8.77e-05 (2.71e-04)	Tok/s 102440 (96160)	Loss/tok 3.3413 (3.1541)	LR 1.400e-03
0: TRAIN [3][1210/1885]	Time 0.101 (0.143)	Data 8.32e-05 (2.69e-04)	Tok/s 88470 (96177)	Loss/tok 3.0172 (3.1542)	LR 1.400e-03
0: TRAIN [3][1220/1885]	Time 0.147 (0.143)	Data 8.39e-05 (2.68e-04)	Tok/s 99470 (96182)	Loss/tok 2.9616 (3.1538)	LR 1.400e-03
0: TRAIN [3][1230/1885]	Time 0.146 (0.143)	Data 8.61e-05 (2.66e-04)	Tok/s 100246 (96206)	Loss/tok 3.1217 (3.1537)	LR 1.400e-03
0: TRAIN [3][1240/1885]	Time 0.147 (0.143)	Data 8.30e-05 (2.65e-04)	Tok/s 99172 (96158)	Loss/tok 2.9765 (3.1532)	LR 1.400e-03
0: TRAIN [3][1250/1885]	Time 0.147 (0.143)	Data 8.56e-05 (2.63e-04)	Tok/s 99926 (96138)	Loss/tok 2.9697 (3.1520)	LR 1.400e-03
0: TRAIN [3][1260/1885]	Time 0.193 (0.143)	Data 8.49e-05 (2.62e-04)	Tok/s 105310 (96127)	Loss/tok 3.1393 (3.1516)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1270/1885]	Time 0.148 (0.143)	Data 8.30e-05 (2.61e-04)	Tok/s 99104 (96120)	Loss/tok 3.1619 (3.1511)	LR 1.400e-03
0: TRAIN [3][1280/1885]	Time 0.147 (0.143)	Data 9.39e-05 (2.59e-04)	Tok/s 99838 (96125)	Loss/tok 3.1650 (3.1506)	LR 1.400e-03
0: TRAIN [3][1290/1885]	Time 0.244 (0.143)	Data 8.63e-05 (2.58e-04)	Tok/s 108117 (96127)	Loss/tok 3.3273 (3.1509)	LR 7.000e-04
0: TRAIN [3][1300/1885]	Time 0.146 (0.143)	Data 8.46e-05 (2.57e-04)	Tok/s 99645 (96130)	Loss/tok 3.2169 (3.1508)	LR 7.000e-04
0: TRAIN [3][1310/1885]	Time 0.245 (0.143)	Data 9.39e-05 (2.55e-04)	Tok/s 105946 (96115)	Loss/tok 3.4046 (3.1506)	LR 7.000e-04
0: TRAIN [3][1320/1885]	Time 0.102 (0.142)	Data 8.49e-05 (2.54e-04)	Tok/s 90648 (96100)	Loss/tok 2.9334 (3.1500)	LR 7.000e-04
0: TRAIN [3][1330/1885]	Time 0.102 (0.142)	Data 8.46e-05 (2.53e-04)	Tok/s 89301 (96089)	Loss/tok 2.8028 (3.1491)	LR 7.000e-04
0: TRAIN [3][1340/1885]	Time 0.146 (0.142)	Data 8.34e-05 (2.52e-04)	Tok/s 100343 (96100)	Loss/tok 3.1562 (3.1492)	LR 7.000e-04
0: TRAIN [3][1350/1885]	Time 0.102 (0.142)	Data 8.32e-05 (2.50e-04)	Tok/s 90190 (96076)	Loss/tok 2.9475 (3.1487)	LR 7.000e-04
0: TRAIN [3][1360/1885]	Time 0.101 (0.142)	Data 8.30e-05 (2.49e-04)	Tok/s 86739 (96093)	Loss/tok 2.7917 (3.1488)	LR 7.000e-04
0: TRAIN [3][1370/1885]	Time 0.147 (0.143)	Data 9.18e-05 (2.48e-04)	Tok/s 99729 (96127)	Loss/tok 3.0071 (3.1488)	LR 7.000e-04
0: TRAIN [3][1380/1885]	Time 0.102 (0.143)	Data 8.42e-05 (2.47e-04)	Tok/s 89825 (96142)	Loss/tok 2.8636 (3.1483)	LR 7.000e-04
0: TRAIN [3][1390/1885]	Time 0.147 (0.143)	Data 8.39e-05 (2.46e-04)	Tok/s 100431 (96167)	Loss/tok 3.0778 (3.1484)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1400/1885]	Time 0.195 (0.143)	Data 8.42e-05 (2.44e-04)	Tok/s 104901 (96169)	Loss/tok 3.2578 (3.1482)	LR 7.000e-04
0: TRAIN [3][1410/1885]	Time 0.102 (0.143)	Data 8.49e-05 (2.43e-04)	Tok/s 90130 (96175)	Loss/tok 2.8386 (3.1476)	LR 7.000e-04
0: TRAIN [3][1420/1885]	Time 0.147 (0.143)	Data 8.65e-05 (2.42e-04)	Tok/s 101579 (96191)	Loss/tok 3.1161 (3.1479)	LR 7.000e-04
0: TRAIN [3][1430/1885]	Time 0.146 (0.143)	Data 8.42e-05 (2.41e-04)	Tok/s 100432 (96203)	Loss/tok 3.0168 (3.1475)	LR 7.000e-04
0: TRAIN [3][1440/1885]	Time 0.147 (0.143)	Data 9.23e-05 (2.40e-04)	Tok/s 100509 (96198)	Loss/tok 3.0033 (3.1474)	LR 7.000e-04
0: TRAIN [3][1450/1885]	Time 0.103 (0.143)	Data 8.42e-05 (2.39e-04)	Tok/s 87074 (96223)	Loss/tok 2.8625 (3.1477)	LR 7.000e-04
0: TRAIN [3][1460/1885]	Time 0.195 (0.143)	Data 8.39e-05 (2.38e-04)	Tok/s 104015 (96224)	Loss/tok 3.2151 (3.1473)	LR 7.000e-04
0: TRAIN [3][1470/1885]	Time 0.061 (0.143)	Data 8.65e-05 (2.37e-04)	Tok/s 74610 (96203)	Loss/tok 2.4789 (3.1467)	LR 7.000e-04
0: TRAIN [3][1480/1885]	Time 0.146 (0.143)	Data 8.49e-05 (2.36e-04)	Tok/s 99871 (96198)	Loss/tok 3.0798 (3.1462)	LR 7.000e-04
0: TRAIN [3][1490/1885]	Time 0.195 (0.143)	Data 8.18e-05 (2.35e-04)	Tok/s 106275 (96196)	Loss/tok 3.3199 (3.1457)	LR 7.000e-04
0: TRAIN [3][1500/1885]	Time 0.146 (0.143)	Data 8.37e-05 (2.34e-04)	Tok/s 99643 (96180)	Loss/tok 3.1694 (3.1451)	LR 7.000e-04
0: TRAIN [3][1510/1885]	Time 0.102 (0.143)	Data 8.39e-05 (2.33e-04)	Tok/s 87574 (96182)	Loss/tok 2.9146 (3.1449)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1520/1885]	Time 0.147 (0.143)	Data 8.54e-05 (2.32e-04)	Tok/s 99670 (96174)	Loss/tok 3.1027 (3.1445)	LR 7.000e-04
0: TRAIN [3][1530/1885]	Time 0.102 (0.143)	Data 8.44e-05 (2.31e-04)	Tok/s 87709 (96195)	Loss/tok 2.8973 (3.1449)	LR 7.000e-04
0: TRAIN [3][1540/1885]	Time 0.146 (0.143)	Data 8.61e-05 (2.30e-04)	Tok/s 102036 (96208)	Loss/tok 3.0116 (3.1446)	LR 7.000e-04
0: TRAIN [3][1550/1885]	Time 0.147 (0.143)	Data 8.49e-05 (2.29e-04)	Tok/s 99060 (96226)	Loss/tok 3.1476 (3.1445)	LR 7.000e-04
0: TRAIN [3][1560/1885]	Time 0.059 (0.143)	Data 8.70e-05 (2.28e-04)	Tok/s 76184 (96215)	Loss/tok 2.4198 (3.1445)	LR 7.000e-04
0: TRAIN [3][1570/1885]	Time 0.147 (0.143)	Data 8.63e-05 (2.27e-04)	Tok/s 99087 (96227)	Loss/tok 3.0546 (3.1442)	LR 7.000e-04
0: TRAIN [3][1580/1885]	Time 0.145 (0.143)	Data 8.58e-05 (2.26e-04)	Tok/s 100729 (96235)	Loss/tok 3.0547 (3.1443)	LR 7.000e-04
0: TRAIN [3][1590/1885]	Time 0.244 (0.143)	Data 8.44e-05 (2.25e-04)	Tok/s 106219 (96227)	Loss/tok 3.3483 (3.1440)	LR 7.000e-04
0: TRAIN [3][1600/1885]	Time 0.148 (0.143)	Data 8.25e-05 (2.25e-04)	Tok/s 97646 (96230)	Loss/tok 3.0564 (3.1439)	LR 7.000e-04
0: TRAIN [3][1610/1885]	Time 0.102 (0.143)	Data 8.39e-05 (2.24e-04)	Tok/s 89665 (96234)	Loss/tok 2.8355 (3.1437)	LR 7.000e-04
0: TRAIN [3][1620/1885]	Time 0.102 (0.143)	Data 8.61e-05 (2.23e-04)	Tok/s 92004 (96209)	Loss/tok 2.9010 (3.1434)	LR 7.000e-04
0: TRAIN [3][1630/1885]	Time 0.059 (0.143)	Data 9.16e-05 (2.22e-04)	Tok/s 76466 (96173)	Loss/tok 2.4464 (3.1425)	LR 7.000e-04
0: TRAIN [3][1640/1885]	Time 0.246 (0.143)	Data 8.44e-05 (2.21e-04)	Tok/s 106480 (96201)	Loss/tok 3.4291 (3.1434)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1650/1885]	Time 0.101 (0.143)	Data 8.37e-05 (2.20e-04)	Tok/s 89234 (96185)	Loss/tok 3.0598 (3.1431)	LR 7.000e-04
0: TRAIN [3][1660/1885]	Time 0.101 (0.143)	Data 8.61e-05 (2.19e-04)	Tok/s 91720 (96211)	Loss/tok 2.8858 (3.1436)	LR 7.000e-04
0: TRAIN [3][1670/1885]	Time 0.102 (0.143)	Data 8.39e-05 (2.19e-04)	Tok/s 89245 (96197)	Loss/tok 2.9034 (3.1433)	LR 7.000e-04
0: TRAIN [3][1680/1885]	Time 0.194 (0.143)	Data 8.61e-05 (2.18e-04)	Tok/s 105713 (96198)	Loss/tok 3.2421 (3.1429)	LR 7.000e-04
0: TRAIN [3][1690/1885]	Time 0.147 (0.143)	Data 8.44e-05 (2.17e-04)	Tok/s 101276 (96202)	Loss/tok 3.1032 (3.1427)	LR 7.000e-04
0: TRAIN [3][1700/1885]	Time 0.196 (0.143)	Data 8.46e-05 (2.16e-04)	Tok/s 104150 (96225)	Loss/tok 3.1797 (3.1429)	LR 7.000e-04
0: TRAIN [3][1710/1885]	Time 0.146 (0.143)	Data 8.61e-05 (2.16e-04)	Tok/s 100954 (96236)	Loss/tok 3.0377 (3.1426)	LR 7.000e-04
0: TRAIN [3][1720/1885]	Time 0.147 (0.143)	Data 8.37e-05 (2.15e-04)	Tok/s 99884 (96226)	Loss/tok 3.1140 (3.1421)	LR 7.000e-04
0: TRAIN [3][1730/1885]	Time 0.149 (0.143)	Data 8.44e-05 (2.14e-04)	Tok/s 99646 (96241)	Loss/tok 3.0872 (3.1426)	LR 7.000e-04
0: TRAIN [3][1740/1885]	Time 0.148 (0.143)	Data 8.37e-05 (2.13e-04)	Tok/s 100397 (96232)	Loss/tok 3.0138 (3.1422)	LR 7.000e-04
0: TRAIN [3][1750/1885]	Time 0.148 (0.143)	Data 8.34e-05 (2.13e-04)	Tok/s 99104 (96226)	Loss/tok 3.0337 (3.1417)	LR 7.000e-04
0: TRAIN [3][1760/1885]	Time 0.194 (0.143)	Data 8.46e-05 (2.12e-04)	Tok/s 106268 (96250)	Loss/tok 3.2320 (3.1421)	LR 7.000e-04
0: TRAIN [3][1770/1885]	Time 0.102 (0.143)	Data 8.54e-05 (2.11e-04)	Tok/s 89518 (96228)	Loss/tok 2.9122 (3.1414)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1780/1885]	Time 0.195 (0.143)	Data 8.51e-05 (2.10e-04)	Tok/s 103328 (96212)	Loss/tok 3.2372 (3.1410)	LR 7.000e-04
0: TRAIN [3][1790/1885]	Time 0.247 (0.143)	Data 8.44e-05 (2.10e-04)	Tok/s 105565 (96202)	Loss/tok 3.3768 (3.1403)	LR 7.000e-04
0: TRAIN [3][1800/1885]	Time 0.243 (0.143)	Data 8.63e-05 (2.09e-04)	Tok/s 107901 (96221)	Loss/tok 3.4178 (3.1408)	LR 7.000e-04
0: TRAIN [3][1810/1885]	Time 0.102 (0.143)	Data 8.49e-05 (2.08e-04)	Tok/s 86465 (96204)	Loss/tok 2.8697 (3.1404)	LR 7.000e-04
0: TRAIN [3][1820/1885]	Time 0.101 (0.143)	Data 8.68e-05 (2.08e-04)	Tok/s 89560 (96210)	Loss/tok 2.8595 (3.1399)	LR 7.000e-04
0: TRAIN [3][1830/1885]	Time 0.102 (0.143)	Data 8.32e-05 (2.07e-04)	Tok/s 89310 (96217)	Loss/tok 2.7347 (3.1396)	LR 7.000e-04
0: TRAIN [3][1840/1885]	Time 0.147 (0.143)	Data 8.46e-05 (2.06e-04)	Tok/s 99106 (96226)	Loss/tok 3.1155 (3.1395)	LR 7.000e-04
0: TRAIN [3][1850/1885]	Time 0.194 (0.143)	Data 8.68e-05 (2.06e-04)	Tok/s 105090 (96244)	Loss/tok 3.2230 (3.1397)	LR 7.000e-04
0: TRAIN [3][1860/1885]	Time 0.147 (0.143)	Data 8.56e-05 (2.05e-04)	Tok/s 98315 (96255)	Loss/tok 3.1557 (3.1396)	LR 7.000e-04
0: TRAIN [3][1870/1885]	Time 0.147 (0.143)	Data 8.23e-05 (2.04e-04)	Tok/s 99451 (96254)	Loss/tok 3.0567 (3.1391)	LR 7.000e-04
0: TRAIN [3][1880/1885]	Time 0.102 (0.143)	Data 8.70e-05 (2.04e-04)	Tok/s 87708 (96245)	Loss/tok 2.8207 (3.1390)	LR 7.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593832877030, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593832877031, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.558 (0.558)	Decoder iters 96.0 (96.0)	Tok/s 29353 (29353)
0: Running moses detokenizer
0: BLEU(score=24.17680290774154, counts=[36940, 18535, 10553, 6291], totals=[64991, 61988, 58985, 55987], precisions=[56.8386391961964, 29.900948570691103, 17.890989234551157, 11.236537053244502], bp=1.0, sys_len=64991, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593832878562, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2418, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593832878563, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1379	Test BLEU: 24.18
0: Performance: Epoch: 3	Training: 769827 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593832878563, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593832878563, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-07-04 03:21:24 AM
RESULT,RNN_TRANSLATOR,,1110,Fujitsu,2020-07-04 03:02:54 AM
