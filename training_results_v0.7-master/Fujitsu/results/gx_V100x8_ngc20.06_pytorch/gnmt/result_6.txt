Beginning trial 1 of 1
Gathering sys log on gx2570-03
:::MLLOG {"namespace": "", "time_ms": 1593836784170, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593836784206, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Fujitsu", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593836784206, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593836784206, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593836784206, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xGX2570M5", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
Clearing caches
:::MLLOG {"namespace": "", "time_ms": 1593836787463, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/opt/conda/lib/python3.6/site-packages/mlperf_logging/mllog/mllog.py", "lineno": 261}}
Launching on node gx2570-03
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e PGSYSTEM=PG -e 'MULTI_NODE= --master_port=4260' -e LR=2.8e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6053 -e DECAY_INTERVAL=859 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=65 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200704114252288086675 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200704114252288086675 ./run_and_time.sh
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.8e-3
+ TRAIN_BATCH_SIZE=256
STARTING TIMING RUN AT 2020-07-04 04:26:28 AM
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6053
+ DECAY_INTERVAL=859
+ TARGET=24.0
+ MAX_SEQ_LEN=65
+ NUMEPOCHS=8
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ PGSYSTEM=PG
+ [[ -f config_PG.sh ]]
+ source config_PG.sh
++ export PGNNODES=1
++ PGNNODES=1
+++ sed 's/^config_//'
+++ sed 's/\.sh$//'
++++ readlink -f config_PG.sh
+++ basename /workspace/rnn_translator/config_PG.sh
++ export PGSYSTEM=PG
++ PGSYSTEM=PG
++ export WALLTIME=00:30:00
++ WALLTIME=00:30:00
++ export LR=2.8e-3
++ LR=2.8e-3
++ export TRAIN_BATCH_SIZE=256
++ TRAIN_BATCH_SIZE=256
++ export TEST_BATCH_SIZE=128
++ TEST_BATCH_SIZE=128
++ export WARMUP_STEPS=200
++ WARMUP_STEPS=200
++ export REMAIN_STEPS=6053
++ REMAIN_STEPS=6053
++ export DECAY_INTERVAL=859
++ DECAY_INTERVAL=859
++ export TARGET=24.0
++ TARGET=24.0
++ export MAX_SEQ_LEN=65
++ MAX_SEQ_LEN=65
++ export NUMEPOCHS=8
++ NUMEPOCHS=8
++ export MATH=fp16
++ MATH=fp16
++ export DIST_OPTS=
++ DIST_OPTS=
++ export 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ export PGNGPU=8
++ PGNGPU=8
++ export PGSOCKETCORES=24
++ PGSOCKETCORES=24
++ export PGHT=2
++ PGHT=2
++ export PGNSOCKET=2
++ PGNSOCKET=2
+ declare -a CMD
+ '[' -n '' ']'
+ CMD=('python' '-u' '-m' 'bind_launch' "--nsockets_per_node=${PGNSOCKET}" "--ncores_per_socket=${PGSOCKETCORES}" "--nproc_per_node=${PGNGPU}")
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ PG == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ python -u -m bind_launch --nsockets_per_node=2 --ncores_per_socket=24 --nproc_per_node=8 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 65 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.8e-3 --warmup-steps 200 --remain-steps 6053 --decay-interval 859 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593836790500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593836790500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593836790500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593836790511, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593836790515, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593836790519, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593836790522, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593836790522, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=859, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.0028, math='fp16', max_length_test=150, max_length_train=65, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6053, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 254399671
:::MLLOG {"namespace": "", "time_ms": 1593836798925, "event_type": "POINT_IN_TIME", "key": "seed", "value": 254399671, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3938627963
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.0028}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.0028
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593836801803, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593836801804, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0028, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593836801804, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593836801804, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593836801804, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593836804385, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593836804410, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593836804411, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 65, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3866375 min length: 0 max length: 65 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593836804668, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593836804669, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3860480, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593836804669, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6053, 'decay_interval': 859, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6053
0: Scheduler decay interval: 859
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593836804669, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593836804670, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593836804670, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 859, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593836804670, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593836804670, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593836804670, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6053, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593836804670, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593836804671, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593836804671, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1678475482
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1885]	Time 0.516 (0.516)	Data 2.72e-01 (2.72e-01)	Tok/s 28287 (28287)	Loss/tok 10.7112 (10.7112)	LR 2.865e-05
0: TRAIN [0][10/1885]	Time 0.193 (0.163)	Data 2.31e-04 (2.49e-02)	Tok/s 105394 (90229)	Loss/tok 9.6791 (9.9931)	LR 3.607e-05
0: TRAIN [0][20/1885]	Time 0.100 (0.155)	Data 2.19e-04 (1.32e-02)	Tok/s 91212 (94571)	Loss/tok 9.0425 (9.6303)	LR 4.541e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][30/1885]	Time 0.243 (0.152)	Data 2.28e-04 (8.98e-03)	Tok/s 107358 (95406)	Loss/tok 9.4048 (9.4424)	LR 5.460e-05
0: TRAIN [0][40/1885]	Time 0.100 (0.153)	Data 2.20e-04 (6.84e-03)	Tok/s 89425 (96728)	Loss/tok 8.6335 (9.3217)	LR 6.873e-05
0: TRAIN [0][50/1885]	Time 0.145 (0.148)	Data 2.20e-04 (5.54e-03)	Tok/s 101456 (96772)	Loss/tok 8.4491 (9.1869)	LR 8.653e-05
0: TRAIN [0][60/1885]	Time 0.100 (0.149)	Data 2.17e-04 (4.67e-03)	Tok/s 90753 (97181)	Loss/tok 8.2352 (9.0396)	LR 1.089e-04
0: TRAIN [0][70/1885]	Time 0.101 (0.144)	Data 2.16e-04 (4.04e-03)	Tok/s 90665 (96674)	Loss/tok 7.9790 (8.9413)	LR 1.371e-04
0: TRAIN [0][80/1885]	Time 0.146 (0.144)	Data 1.77e-04 (3.57e-03)	Tok/s 99652 (97036)	Loss/tok 8.0201 (8.8245)	LR 1.726e-04
0: TRAIN [0][90/1885]	Time 0.243 (0.145)	Data 2.19e-04 (3.20e-03)	Tok/s 109237 (97279)	Loss/tok 8.0274 (8.7230)	LR 2.173e-04
0: TRAIN [0][100/1885]	Time 0.242 (0.149)	Data 2.20e-04 (2.91e-03)	Tok/s 108100 (97943)	Loss/tok 8.0536 (8.6239)	LR 2.736e-04
0: TRAIN [0][110/1885]	Time 0.191 (0.147)	Data 2.24e-04 (2.66e-03)	Tok/s 105693 (97456)	Loss/tok 7.9513 (8.5627)	LR 3.445e-04
0: TRAIN [0][120/1885]	Time 0.192 (0.146)	Data 2.01e-04 (2.46e-03)	Tok/s 105180 (97364)	Loss/tok 7.9798 (8.5074)	LR 4.337e-04
0: TRAIN [0][130/1885]	Time 0.242 (0.149)	Data 2.28e-04 (2.29e-03)	Tok/s 108906 (97830)	Loss/tok 7.9264 (8.4452)	LR 5.460e-04
0: TRAIN [0][140/1885]	Time 0.101 (0.148)	Data 2.20e-04 (2.14e-03)	Tok/s 87994 (97935)	Loss/tok 7.4732 (8.3934)	LR 6.873e-04
0: TRAIN [0][150/1885]	Time 0.146 (0.149)	Data 2.21e-04 (2.01e-03)	Tok/s 101433 (97931)	Loss/tok 7.6364 (8.3543)	LR 8.653e-04
0: Upscaling, new scale: 512.0
0: TRAIN [0][160/1885]	Time 0.145 (0.148)	Data 2.16e-04 (1.90e-03)	Tok/s 99750 (97910)	Loss/tok 7.5808 (8.3071)	LR 1.089e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][170/1885]	Time 0.146 (0.148)	Data 2.15e-04 (1.80e-03)	Tok/s 99036 (97850)	Loss/tok 7.3264 (8.2572)	LR 1.340e-03
0: TRAIN [0][180/1885]	Time 0.101 (0.147)	Data 2.17e-04 (1.72e-03)	Tok/s 90980 (97655)	Loss/tok 6.8626 (8.2079)	LR 1.687e-03
0: TRAIN [0][190/1885]	Time 0.101 (0.146)	Data 2.17e-04 (1.64e-03)	Tok/s 90137 (97640)	Loss/tok 6.6972 (8.1515)	LR 2.124e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][200/1885]	Time 0.144 (0.147)	Data 2.15e-04 (1.57e-03)	Tok/s 101957 (97664)	Loss/tok 7.1506 (8.0949)	LR 2.554e-03
0: TRAIN [0][210/1885]	Time 0.100 (0.146)	Data 2.24e-04 (1.50e-03)	Tok/s 90437 (97443)	Loss/tok 6.6609 (8.0480)	LR 2.800e-03
0: TRAIN [0][220/1885]	Time 0.101 (0.145)	Data 2.14e-04 (1.44e-03)	Tok/s 93164 (97387)	Loss/tok 6.3146 (7.9911)	LR 2.800e-03
0: TRAIN [0][230/1885]	Time 0.145 (0.144)	Data 2.16e-04 (1.39e-03)	Tok/s 102357 (97232)	Loss/tok 6.4870 (7.9362)	LR 2.800e-03
0: TRAIN [0][240/1885]	Time 0.243 (0.144)	Data 2.16e-04 (1.34e-03)	Tok/s 107910 (97272)	Loss/tok 6.6997 (7.8737)	LR 2.800e-03
0: TRAIN [0][250/1885]	Time 0.243 (0.144)	Data 2.16e-04 (1.30e-03)	Tok/s 108662 (97218)	Loss/tok 6.4748 (7.8109)	LR 2.800e-03
0: TRAIN [0][260/1885]	Time 0.145 (0.144)	Data 2.21e-04 (1.26e-03)	Tok/s 101334 (97259)	Loss/tok 6.0615 (7.7457)	LR 2.800e-03
0: TRAIN [0][270/1885]	Time 0.192 (0.144)	Data 2.18e-04 (1.22e-03)	Tok/s 106273 (97264)	Loss/tok 6.1736 (7.6813)	LR 2.800e-03
0: TRAIN [0][280/1885]	Time 0.192 (0.145)	Data 2.17e-04 (1.18e-03)	Tok/s 107404 (97483)	Loss/tok 5.9665 (7.6046)	LR 2.800e-03
0: TRAIN [0][290/1885]	Time 0.101 (0.146)	Data 2.16e-04 (1.15e-03)	Tok/s 90996 (97615)	Loss/tok 5.4559 (7.5348)	LR 2.800e-03
0: TRAIN [0][300/1885]	Time 0.101 (0.145)	Data 2.18e-04 (1.12e-03)	Tok/s 90206 (97588)	Loss/tok 5.2807 (7.4775)	LR 2.800e-03
0: TRAIN [0][310/1885]	Time 0.146 (0.145)	Data 2.18e-04 (1.09e-03)	Tok/s 99902 (97643)	Loss/tok 5.5204 (7.4117)	LR 2.800e-03
0: TRAIN [0][320/1885]	Time 0.193 (0.146)	Data 2.19e-04 (1.06e-03)	Tok/s 105409 (97683)	Loss/tok 5.8469 (7.3478)	LR 2.800e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][330/1885]	Time 0.144 (0.146)	Data 2.15e-04 (1.04e-03)	Tok/s 100736 (97674)	Loss/tok 5.3539 (7.2896)	LR 2.800e-03
0: TRAIN [0][340/1885]	Time 0.146 (0.146)	Data 2.16e-04 (1.01e-03)	Tok/s 99766 (97741)	Loss/tok 5.0768 (7.2243)	LR 2.800e-03
0: TRAIN [0][350/1885]	Time 0.101 (0.146)	Data 2.23e-04 (9.89e-04)	Tok/s 90232 (97605)	Loss/tok 4.6040 (7.1714)	LR 2.800e-03
0: TRAIN [0][360/1885]	Time 0.101 (0.145)	Data 2.15e-04 (9.67e-04)	Tok/s 91138 (97642)	Loss/tok 4.6174 (7.1139)	LR 2.800e-03
0: TRAIN [0][370/1885]	Time 0.101 (0.146)	Data 2.17e-04 (9.47e-04)	Tok/s 88678 (97705)	Loss/tok 4.5165 (7.0538)	LR 2.800e-03
0: TRAIN [0][380/1885]	Time 0.060 (0.145)	Data 2.03e-04 (9.28e-04)	Tok/s 75417 (97616)	Loss/tok 3.6601 (7.0016)	LR 2.800e-03
0: TRAIN [0][390/1885]	Time 0.244 (0.146)	Data 2.20e-04 (9.09e-04)	Tok/s 106376 (97632)	Loss/tok 5.1637 (6.9437)	LR 2.800e-03
0: TRAIN [0][400/1885]	Time 0.100 (0.145)	Data 2.18e-04 (8.92e-04)	Tok/s 89852 (97562)	Loss/tok 4.3454 (6.8950)	LR 2.800e-03
0: TRAIN [0][410/1885]	Time 0.192 (0.145)	Data 2.52e-04 (8.76e-04)	Tok/s 106474 (97630)	Loss/tok 4.6646 (6.8354)	LR 2.800e-03
0: TRAIN [0][420/1885]	Time 0.102 (0.145)	Data 2.18e-04 (8.60e-04)	Tok/s 89288 (97571)	Loss/tok 4.1560 (6.7887)	LR 2.800e-03
0: TRAIN [0][430/1885]	Time 0.245 (0.145)	Data 1.78e-04 (8.45e-04)	Tok/s 108122 (97464)	Loss/tok 4.8588 (6.7426)	LR 2.800e-03
0: TRAIN [0][440/1885]	Time 0.059 (0.145)	Data 2.16e-04 (8.31e-04)	Tok/s 78678 (97434)	Loss/tok 3.4521 (6.6940)	LR 2.800e-03
0: TRAIN [0][450/1885]	Time 0.146 (0.144)	Data 2.19e-04 (8.17e-04)	Tok/s 101541 (97385)	Loss/tok 4.4778 (6.6502)	LR 2.800e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][460/1885]	Time 0.101 (0.144)	Data 2.16e-04 (8.04e-04)	Tok/s 90518 (97281)	Loss/tok 4.0243 (6.6106)	LR 2.800e-03
0: TRAIN [0][470/1885]	Time 0.101 (0.143)	Data 2.17e-04 (7.91e-04)	Tok/s 87769 (97240)	Loss/tok 3.9804 (6.5666)	LR 2.800e-03
0: TRAIN [0][480/1885]	Time 0.102 (0.143)	Data 2.01e-04 (7.79e-04)	Tok/s 91057 (97193)	Loss/tok 3.8978 (6.5258)	LR 2.800e-03
0: TRAIN [0][490/1885]	Time 0.194 (0.143)	Data 2.19e-04 (7.68e-04)	Tok/s 104603 (97188)	Loss/tok 4.4946 (6.4824)	LR 2.800e-03
0: TRAIN [0][500/1885]	Time 0.101 (0.143)	Data 2.17e-04 (7.57e-04)	Tok/s 91028 (97208)	Loss/tok 3.8986 (6.4356)	LR 2.800e-03
0: TRAIN [0][510/1885]	Time 0.146 (0.143)	Data 2.20e-04 (7.46e-04)	Tok/s 100136 (97235)	Loss/tok 4.1855 (6.3913)	LR 2.800e-03
0: TRAIN [0][520/1885]	Time 0.102 (0.144)	Data 2.18e-04 (7.36e-04)	Tok/s 89536 (97319)	Loss/tok 3.8766 (6.3437)	LR 2.800e-03
0: TRAIN [0][530/1885]	Time 0.193 (0.144)	Data 2.18e-04 (7.26e-04)	Tok/s 106179 (97277)	Loss/tok 4.4550 (6.3062)	LR 2.800e-03
0: TRAIN [0][540/1885]	Time 0.101 (0.144)	Data 2.19e-04 (7.17e-04)	Tok/s 90116 (97300)	Loss/tok 3.7656 (6.2674)	LR 2.800e-03
0: TRAIN [0][550/1885]	Time 0.101 (0.144)	Data 2.17e-04 (7.07e-04)	Tok/s 89326 (97321)	Loss/tok 3.7929 (6.2280)	LR 2.800e-03
0: TRAIN [0][560/1885]	Time 0.146 (0.144)	Data 2.19e-04 (6.99e-04)	Tok/s 99679 (97343)	Loss/tok 4.1529 (6.1916)	LR 2.800e-03
0: TRAIN [0][570/1885]	Time 0.059 (0.144)	Data 2.17e-04 (6.90e-04)	Tok/s 78595 (97309)	Loss/tok 3.0618 (6.1591)	LR 2.800e-03
0: TRAIN [0][580/1885]	Time 0.147 (0.143)	Data 2.03e-04 (6.82e-04)	Tok/s 98727 (97282)	Loss/tok 4.1511 (6.1256)	LR 2.800e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][590/1885]	Time 0.101 (0.143)	Data 2.17e-04 (6.74e-04)	Tok/s 89706 (97266)	Loss/tok 3.7720 (6.0936)	LR 2.800e-03
0: TRAIN [0][600/1885]	Time 0.146 (0.143)	Data 2.17e-04 (6.67e-04)	Tok/s 101609 (97241)	Loss/tok 4.0755 (6.0627)	LR 2.800e-03
0: TRAIN [0][610/1885]	Time 0.244 (0.143)	Data 2.16e-04 (6.59e-04)	Tok/s 107128 (97278)	Loss/tok 4.4856 (6.0266)	LR 2.800e-03
0: TRAIN [0][620/1885]	Time 0.145 (0.144)	Data 2.17e-04 (6.52e-04)	Tok/s 100367 (97330)	Loss/tok 3.9671 (5.9917)	LR 2.800e-03
0: TRAIN [0][630/1885]	Time 0.102 (0.144)	Data 2.17e-04 (6.45e-04)	Tok/s 87880 (97344)	Loss/tok 3.7131 (5.9587)	LR 2.800e-03
0: TRAIN [0][640/1885]	Time 0.102 (0.144)	Data 2.17e-04 (6.38e-04)	Tok/s 89542 (97359)	Loss/tok 3.7300 (5.9269)	LR 2.800e-03
0: TRAIN [0][650/1885]	Time 0.058 (0.144)	Data 2.19e-04 (6.32e-04)	Tok/s 77864 (97281)	Loss/tok 2.8834 (5.9026)	LR 2.800e-03
0: TRAIN [0][660/1885]	Time 0.244 (0.144)	Data 2.30e-04 (6.26e-04)	Tok/s 107751 (97355)	Loss/tok 4.2482 (5.8684)	LR 2.800e-03
0: TRAIN [0][670/1885]	Time 0.193 (0.144)	Data 2.19e-04 (6.19e-04)	Tok/s 106328 (97290)	Loss/tok 4.1323 (5.8453)	LR 2.800e-03
0: TRAIN [0][680/1885]	Time 0.145 (0.144)	Data 2.16e-04 (6.14e-04)	Tok/s 99988 (97324)	Loss/tok 3.9364 (5.8159)	LR 2.800e-03
0: TRAIN [0][690/1885]	Time 0.100 (0.144)	Data 2.17e-04 (6.08e-04)	Tok/s 89388 (97306)	Loss/tok 3.6476 (5.7905)	LR 2.800e-03
0: TRAIN [0][700/1885]	Time 0.193 (0.144)	Data 2.16e-04 (6.02e-04)	Tok/s 105140 (97321)	Loss/tok 4.1338 (5.7646)	LR 2.800e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][710/1885]	Time 0.102 (0.143)	Data 2.18e-04 (5.97e-04)	Tok/s 89860 (97284)	Loss/tok 3.6549 (5.7422)	LR 2.800e-03
0: TRAIN [0][720/1885]	Time 0.146 (0.143)	Data 2.16e-04 (5.91e-04)	Tok/s 99394 (97276)	Loss/tok 4.0592 (5.7186)	LR 2.800e-03
0: TRAIN [0][730/1885]	Time 0.146 (0.143)	Data 2.17e-04 (5.86e-04)	Tok/s 99114 (97230)	Loss/tok 3.9952 (5.6977)	LR 2.800e-03
0: TRAIN [0][740/1885]	Time 0.059 (0.143)	Data 2.19e-04 (5.81e-04)	Tok/s 78620 (97233)	Loss/tok 3.1083 (5.6728)	LR 2.800e-03
0: TRAIN [0][750/1885]	Time 0.193 (0.143)	Data 2.18e-04 (5.76e-04)	Tok/s 105402 (97248)	Loss/tok 4.0276 (5.6493)	LR 2.800e-03
0: TRAIN [0][760/1885]	Time 0.146 (0.143)	Data 2.29e-04 (5.72e-04)	Tok/s 101159 (97260)	Loss/tok 3.8537 (5.6260)	LR 2.800e-03
0: TRAIN [0][770/1885]	Time 0.145 (0.143)	Data 2.18e-04 (5.67e-04)	Tok/s 101968 (97251)	Loss/tok 3.8093 (5.6030)	LR 2.800e-03
0: TRAIN [0][780/1885]	Time 0.101 (0.143)	Data 2.18e-04 (5.63e-04)	Tok/s 90504 (97224)	Loss/tok 3.5774 (5.5827)	LR 2.800e-03
0: TRAIN [0][790/1885]	Time 0.101 (0.143)	Data 2.13e-04 (5.58e-04)	Tok/s 87832 (97179)	Loss/tok 3.6809 (5.5638)	LR 2.800e-03
0: TRAIN [0][800/1885]	Time 0.101 (0.143)	Data 2.17e-04 (5.54e-04)	Tok/s 90760 (97164)	Loss/tok 3.6220 (5.5436)	LR 2.800e-03
0: TRAIN [0][810/1885]	Time 0.145 (0.143)	Data 2.18e-04 (5.50e-04)	Tok/s 101814 (97155)	Loss/tok 3.7918 (5.5230)	LR 2.800e-03
0: TRAIN [0][820/1885]	Time 0.147 (0.143)	Data 2.19e-04 (5.46e-04)	Tok/s 101450 (97164)	Loss/tok 3.8638 (5.5018)	LR 2.800e-03
0: TRAIN [0][830/1885]	Time 0.145 (0.142)	Data 2.15e-04 (5.42e-04)	Tok/s 100827 (97101)	Loss/tok 3.8112 (5.4849)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][840/1885]	Time 0.102 (0.142)	Data 2.18e-04 (5.38e-04)	Tok/s 89813 (97075)	Loss/tok 3.4994 (5.4665)	LR 2.800e-03
0: TRAIN [0][850/1885]	Time 0.244 (0.142)	Data 2.17e-04 (5.34e-04)	Tok/s 107227 (97057)	Loss/tok 4.1014 (5.4479)	LR 2.800e-03
0: TRAIN [0][860/1885]	Time 0.101 (0.142)	Data 2.20e-04 (5.30e-04)	Tok/s 89377 (97009)	Loss/tok 3.5081 (5.4311)	LR 2.800e-03
0: TRAIN [0][870/1885]	Time 0.147 (0.142)	Data 2.19e-04 (5.27e-04)	Tok/s 99784 (97007)	Loss/tok 3.9278 (5.4133)	LR 2.800e-03
0: TRAIN [0][880/1885]	Time 0.059 (0.142)	Data 2.18e-04 (5.23e-04)	Tok/s 77838 (96987)	Loss/tok 2.9722 (5.3956)	LR 2.800e-03
0: TRAIN [0][890/1885]	Time 0.059 (0.142)	Data 2.17e-04 (5.20e-04)	Tok/s 76319 (96970)	Loss/tok 2.9027 (5.3778)	LR 2.800e-03
0: TRAIN [0][900/1885]	Time 0.195 (0.142)	Data 2.18e-04 (5.16e-04)	Tok/s 103986 (96982)	Loss/tok 3.9786 (5.3596)	LR 2.800e-03
0: TRAIN [0][910/1885]	Time 0.146 (0.142)	Data 2.18e-04 (5.13e-04)	Tok/s 99734 (96975)	Loss/tok 3.7145 (5.3429)	LR 2.800e-03
0: TRAIN [0][920/1885]	Time 0.148 (0.142)	Data 2.18e-04 (5.10e-04)	Tok/s 99006 (96980)	Loss/tok 3.7873 (5.3254)	LR 2.800e-03
0: TRAIN [0][930/1885]	Time 0.145 (0.142)	Data 2.20e-04 (5.07e-04)	Tok/s 102049 (97002)	Loss/tok 3.7967 (5.3081)	LR 2.800e-03
0: TRAIN [0][940/1885]	Time 0.145 (0.142)	Data 2.18e-04 (5.03e-04)	Tok/s 100611 (96950)	Loss/tok 3.6568 (5.2937)	LR 2.800e-03
0: TRAIN [0][950/1885]	Time 0.147 (0.142)	Data 2.20e-04 (5.00e-04)	Tok/s 99828 (96916)	Loss/tok 3.7226 (5.2789)	LR 2.800e-03
0: TRAIN [0][960/1885]	Time 0.101 (0.142)	Data 2.20e-04 (4.97e-04)	Tok/s 91325 (96930)	Loss/tok 3.3983 (5.2626)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][970/1885]	Time 0.060 (0.141)	Data 2.18e-04 (4.94e-04)	Tok/s 77118 (96894)	Loss/tok 2.9544 (5.2485)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][980/1885]	Time 0.145 (0.142)	Data 2.16e-04 (4.92e-04)	Tok/s 101312 (96910)	Loss/tok 3.7805 (5.2327)	LR 2.800e-03
0: TRAIN [0][990/1885]	Time 0.147 (0.141)	Data 2.18e-04 (4.89e-04)	Tok/s 100128 (96898)	Loss/tok 3.6001 (5.2183)	LR 2.800e-03
0: TRAIN [0][1000/1885]	Time 0.193 (0.142)	Data 2.08e-04 (4.86e-04)	Tok/s 104532 (96919)	Loss/tok 3.9764 (5.2023)	LR 2.800e-03
0: TRAIN [0][1010/1885]	Time 0.146 (0.141)	Data 2.16e-04 (4.83e-04)	Tok/s 100304 (96882)	Loss/tok 3.7742 (5.1894)	LR 2.800e-03
0: TRAIN [0][1020/1885]	Time 0.102 (0.141)	Data 1.89e-04 (4.81e-04)	Tok/s 86880 (96862)	Loss/tok 3.5086 (5.1763)	LR 2.800e-03
0: TRAIN [0][1030/1885]	Time 0.243 (0.141)	Data 2.17e-04 (4.78e-04)	Tok/s 107150 (96855)	Loss/tok 4.0970 (5.1623)	LR 2.800e-03
0: TRAIN [0][1040/1885]	Time 0.244 (0.141)	Data 2.36e-04 (4.76e-04)	Tok/s 107877 (96834)	Loss/tok 4.0343 (5.1490)	LR 2.800e-03
0: TRAIN [0][1050/1885]	Time 0.101 (0.141)	Data 2.18e-04 (4.73e-04)	Tok/s 90723 (96847)	Loss/tok 3.4052 (5.1350)	LR 2.800e-03
0: TRAIN [0][1060/1885]	Time 0.193 (0.142)	Data 2.18e-04 (4.71e-04)	Tok/s 104414 (96871)	Loss/tok 3.9549 (5.1205)	LR 2.800e-03
0: TRAIN [0][1070/1885]	Time 0.147 (0.142)	Data 2.28e-04 (4.68e-04)	Tok/s 100843 (96887)	Loss/tok 3.5919 (5.1062)	LR 2.800e-03
0: TRAIN [0][1080/1885]	Time 0.146 (0.142)	Data 2.31e-04 (4.66e-04)	Tok/s 100243 (96884)	Loss/tok 3.7053 (5.0937)	LR 2.800e-03
0: TRAIN [0][1090/1885]	Time 0.145 (0.142)	Data 2.11e-04 (4.64e-04)	Tok/s 99329 (96913)	Loss/tok 3.7229 (5.0799)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1100/1885]	Time 0.147 (0.142)	Data 2.18e-04 (4.61e-04)	Tok/s 98874 (96924)	Loss/tok 3.6793 (5.0669)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1110/1885]	Time 0.193 (0.142)	Data 2.18e-04 (4.59e-04)	Tok/s 105667 (96918)	Loss/tok 3.7307 (5.0540)	LR 2.800e-03
0: TRAIN [0][1120/1885]	Time 0.194 (0.142)	Data 2.17e-04 (4.57e-04)	Tok/s 104965 (96904)	Loss/tok 3.8683 (5.0424)	LR 2.800e-03
0: TRAIN [0][1130/1885]	Time 0.193 (0.142)	Data 2.19e-04 (4.55e-04)	Tok/s 106637 (96907)	Loss/tok 3.7550 (5.0300)	LR 2.800e-03
0: TRAIN [0][1140/1885]	Time 0.102 (0.142)	Data 2.20e-04 (4.53e-04)	Tok/s 89891 (96925)	Loss/tok 3.4157 (5.0179)	LR 2.800e-03
0: TRAIN [0][1150/1885]	Time 0.102 (0.142)	Data 2.18e-04 (4.51e-04)	Tok/s 88529 (96910)	Loss/tok 3.5070 (5.0070)	LR 2.800e-03
0: TRAIN [0][1160/1885]	Time 0.101 (0.142)	Data 2.17e-04 (4.49e-04)	Tok/s 91601 (96919)	Loss/tok 3.4855 (4.9954)	LR 2.800e-03
0: TRAIN [0][1170/1885]	Time 0.101 (0.142)	Data 2.18e-04 (4.47e-04)	Tok/s 90406 (96940)	Loss/tok 3.4068 (4.9833)	LR 2.800e-03
0: TRAIN [0][1180/1885]	Time 0.194 (0.142)	Data 2.19e-04 (4.45e-04)	Tok/s 104770 (96971)	Loss/tok 3.8450 (4.9706)	LR 2.800e-03
0: TRAIN [0][1190/1885]	Time 0.102 (0.142)	Data 2.17e-04 (4.43e-04)	Tok/s 88032 (96960)	Loss/tok 3.2214 (4.9597)	LR 2.800e-03
0: TRAIN [0][1200/1885]	Time 0.101 (0.142)	Data 2.30e-04 (4.41e-04)	Tok/s 88288 (96904)	Loss/tok 3.4841 (4.9510)	LR 2.800e-03
0: TRAIN [0][1210/1885]	Time 0.102 (0.142)	Data 2.18e-04 (4.39e-04)	Tok/s 91093 (96903)	Loss/tok 3.3163 (4.9405)	LR 2.800e-03
0: TRAIN [0][1220/1885]	Time 0.245 (0.142)	Data 2.19e-04 (4.37e-04)	Tok/s 106879 (96909)	Loss/tok 3.8643 (4.9292)	LR 2.800e-03
0: TRAIN [0][1230/1885]	Time 0.146 (0.142)	Data 2.18e-04 (4.36e-04)	Tok/s 100728 (96895)	Loss/tok 3.6666 (4.9194)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1240/1885]	Time 0.147 (0.142)	Data 2.19e-04 (4.34e-04)	Tok/s 98666 (96917)	Loss/tok 3.6416 (4.9082)	LR 2.800e-03
0: TRAIN [0][1250/1885]	Time 0.101 (0.142)	Data 2.18e-04 (4.32e-04)	Tok/s 92504 (96903)	Loss/tok 3.4392 (4.8989)	LR 2.800e-03
0: TRAIN [0][1260/1885]	Time 0.145 (0.142)	Data 2.18e-04 (4.30e-04)	Tok/s 101397 (96891)	Loss/tok 3.6191 (4.8897)	LR 2.800e-03
0: TRAIN [0][1270/1885]	Time 0.146 (0.142)	Data 2.17e-04 (4.29e-04)	Tok/s 101442 (96889)	Loss/tok 3.6753 (4.8799)	LR 2.800e-03
0: TRAIN [0][1280/1885]	Time 0.243 (0.142)	Data 2.18e-04 (4.27e-04)	Tok/s 109202 (96862)	Loss/tok 3.8462 (4.8709)	LR 2.800e-03
0: TRAIN [0][1290/1885]	Time 0.194 (0.141)	Data 1.91e-04 (4.25e-04)	Tok/s 105779 (96849)	Loss/tok 3.8204 (4.8619)	LR 2.800e-03
0: TRAIN [0][1300/1885]	Time 0.147 (0.142)	Data 2.06e-04 (4.24e-04)	Tok/s 100564 (96885)	Loss/tok 3.5949 (4.8513)	LR 2.800e-03
0: TRAIN [0][1310/1885]	Time 0.101 (0.142)	Data 2.19e-04 (4.22e-04)	Tok/s 88727 (96864)	Loss/tok 3.2833 (4.8425)	LR 2.800e-03
0: TRAIN [0][1320/1885]	Time 0.193 (0.142)	Data 2.22e-04 (4.21e-04)	Tok/s 104119 (96868)	Loss/tok 3.8185 (4.8331)	LR 2.800e-03
0: TRAIN [0][1330/1885]	Time 0.147 (0.141)	Data 2.19e-04 (4.19e-04)	Tok/s 100436 (96844)	Loss/tok 3.6424 (4.8249)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1340/1885]	Time 0.102 (0.141)	Data 1.80e-04 (4.17e-04)	Tok/s 87374 (96852)	Loss/tok 3.3473 (4.8155)	LR 2.800e-03
0: TRAIN [0][1350/1885]	Time 0.147 (0.141)	Data 2.34e-04 (4.16e-04)	Tok/s 100483 (96806)	Loss/tok 3.6552 (4.8079)	LR 2.800e-03
0: TRAIN [0][1360/1885]	Time 0.194 (0.141)	Data 1.26e-04 (4.14e-04)	Tok/s 104747 (96827)	Loss/tok 3.7314 (4.7979)	LR 2.800e-03
0: TRAIN [0][1370/1885]	Time 0.101 (0.141)	Data 1.99e-04 (4.13e-04)	Tok/s 91123 (96816)	Loss/tok 3.2836 (4.7896)	LR 2.800e-03
0: TRAIN [0][1380/1885]	Time 0.244 (0.141)	Data 1.78e-04 (4.11e-04)	Tok/s 107572 (96829)	Loss/tok 3.9099 (4.7806)	LR 2.800e-03
0: TRAIN [0][1390/1885]	Time 0.059 (0.142)	Data 2.17e-04 (4.10e-04)	Tok/s 76306 (96848)	Loss/tok 2.6839 (4.7708)	LR 2.800e-03
0: TRAIN [0][1400/1885]	Time 0.101 (0.142)	Data 2.20e-04 (4.08e-04)	Tok/s 89869 (96857)	Loss/tok 3.3034 (4.7621)	LR 2.800e-03
0: TRAIN [0][1410/1885]	Time 0.148 (0.142)	Data 1.71e-04 (4.07e-04)	Tok/s 99201 (96881)	Loss/tok 3.5141 (4.7528)	LR 2.800e-03
0: TRAIN [0][1420/1885]	Time 0.101 (0.142)	Data 2.21e-04 (4.06e-04)	Tok/s 89830 (96877)	Loss/tok 3.2692 (4.7448)	LR 2.800e-03
0: TRAIN [0][1430/1885]	Time 0.195 (0.142)	Data 2.18e-04 (4.04e-04)	Tok/s 104876 (96890)	Loss/tok 3.7250 (4.7361)	LR 2.800e-03
0: TRAIN [0][1440/1885]	Time 0.194 (0.142)	Data 2.18e-04 (4.03e-04)	Tok/s 106024 (96910)	Loss/tok 3.7579 (4.7276)	LR 2.800e-03
0: TRAIN [0][1450/1885]	Time 0.102 (0.142)	Data 2.32e-04 (4.02e-04)	Tok/s 89935 (96912)	Loss/tok 3.2431 (4.7195)	LR 2.800e-03
0: TRAIN [0][1460/1885]	Time 0.059 (0.142)	Data 2.19e-04 (4.00e-04)	Tok/s 80057 (96900)	Loss/tok 2.7001 (4.7119)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1470/1885]	Time 0.193 (0.142)	Data 2.21e-04 (3.99e-04)	Tok/s 106009 (96918)	Loss/tok 3.7781 (4.7036)	LR 2.800e-03
0: TRAIN [0][1480/1885]	Time 0.059 (0.142)	Data 1.79e-04 (3.98e-04)	Tok/s 76676 (96915)	Loss/tok 2.8170 (4.6958)	LR 2.800e-03
0: TRAIN [0][1490/1885]	Time 0.193 (0.142)	Data 2.22e-04 (3.97e-04)	Tok/s 105142 (96937)	Loss/tok 3.7451 (4.6877)	LR 2.800e-03
0: TRAIN [0][1500/1885]	Time 0.193 (0.142)	Data 2.13e-04 (3.95e-04)	Tok/s 105442 (96939)	Loss/tok 3.7335 (4.6803)	LR 2.800e-03
0: TRAIN [0][1510/1885]	Time 0.147 (0.142)	Data 2.17e-04 (3.94e-04)	Tok/s 100878 (96926)	Loss/tok 3.5855 (4.6736)	LR 2.800e-03
0: TRAIN [0][1520/1885]	Time 0.146 (0.142)	Data 2.20e-04 (3.93e-04)	Tok/s 99955 (96942)	Loss/tok 3.5269 (4.6658)	LR 2.800e-03
0: TRAIN [0][1530/1885]	Time 0.146 (0.142)	Data 2.18e-04 (3.92e-04)	Tok/s 100234 (96952)	Loss/tok 3.5470 (4.6585)	LR 2.800e-03
0: TRAIN [0][1540/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.91e-04)	Tok/s 89909 (96973)	Loss/tok 3.3223 (4.6505)	LR 2.800e-03
0: TRAIN [0][1550/1885]	Time 0.146 (0.143)	Data 2.18e-04 (3.90e-04)	Tok/s 100638 (96979)	Loss/tok 3.6337 (4.6439)	LR 2.800e-03
0: TRAIN [0][1560/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.89e-04)	Tok/s 89413 (96978)	Loss/tok 3.3432 (4.6371)	LR 2.800e-03
0: TRAIN [0][1570/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.87e-04)	Tok/s 90951 (97000)	Loss/tok 3.3291 (4.6295)	LR 2.800e-03
0: TRAIN [0][1580/1885]	Time 0.060 (0.143)	Data 2.23e-04 (3.86e-04)	Tok/s 76988 (96988)	Loss/tok 2.6672 (4.6228)	LR 2.800e-03
0: TRAIN [0][1590/1885]	Time 0.245 (0.143)	Data 2.22e-04 (3.85e-04)	Tok/s 107740 (96980)	Loss/tok 3.8612 (4.6163)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1600/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.84e-04)	Tok/s 90321 (96972)	Loss/tok 3.1856 (4.6098)	LR 2.800e-03
0: TRAIN [0][1610/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.83e-04)	Tok/s 89380 (96943)	Loss/tok 3.3078 (4.6041)	LR 2.800e-03
0: TRAIN [0][1620/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.82e-04)	Tok/s 99703 (96953)	Loss/tok 3.5698 (4.5974)	LR 2.800e-03
0: TRAIN [0][1630/1885]	Time 0.100 (0.142)	Data 2.18e-04 (3.81e-04)	Tok/s 89347 (96915)	Loss/tok 3.1386 (4.5918)	LR 2.800e-03
0: TRAIN [0][1640/1885]	Time 0.101 (0.142)	Data 2.19e-04 (3.80e-04)	Tok/s 89688 (96908)	Loss/tok 3.3108 (4.5857)	LR 2.800e-03
0: TRAIN [0][1650/1885]	Time 0.102 (0.142)	Data 2.21e-04 (3.79e-04)	Tok/s 88636 (96890)	Loss/tok 3.3772 (4.5802)	LR 2.800e-03
0: TRAIN [0][1660/1885]	Time 0.146 (0.142)	Data 2.39e-04 (3.78e-04)	Tok/s 100500 (96876)	Loss/tok 3.6268 (4.5743)	LR 2.800e-03
0: TRAIN [0][1670/1885]	Time 0.101 (0.142)	Data 2.30e-04 (3.77e-04)	Tok/s 89760 (96882)	Loss/tok 3.2634 (4.5679)	LR 2.800e-03
0: TRAIN [0][1680/1885]	Time 0.101 (0.142)	Data 2.21e-04 (3.76e-04)	Tok/s 90263 (96847)	Loss/tok 3.2159 (4.5627)	LR 2.800e-03
0: TRAIN [0][1690/1885]	Time 0.147 (0.142)	Data 2.16e-04 (3.75e-04)	Tok/s 99491 (96826)	Loss/tok 3.5741 (4.5571)	LR 2.800e-03
0: TRAIN [0][1700/1885]	Time 0.193 (0.142)	Data 2.19e-04 (3.74e-04)	Tok/s 106965 (96854)	Loss/tok 3.6739 (4.5501)	LR 2.800e-03
0: TRAIN [0][1710/1885]	Time 0.146 (0.142)	Data 2.18e-04 (3.74e-04)	Tok/s 100553 (96869)	Loss/tok 3.4157 (4.5439)	LR 2.800e-03
0: TRAIN [0][1720/1885]	Time 0.102 (0.142)	Data 2.19e-04 (3.73e-04)	Tok/s 88596 (96860)	Loss/tok 3.2738 (4.5384)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [0][1730/1885]	Time 0.101 (0.142)	Data 2.32e-04 (3.72e-04)	Tok/s 88442 (96843)	Loss/tok 3.1586 (4.5329)	LR 2.800e-03
0: TRAIN [0][1740/1885]	Time 0.146 (0.142)	Data 2.32e-04 (3.71e-04)	Tok/s 101808 (96829)	Loss/tok 3.5007 (4.5275)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1750/1885]	Time 0.146 (0.142)	Data 2.17e-04 (3.70e-04)	Tok/s 100144 (96840)	Loss/tok 3.5776 (4.5213)	LR 2.800e-03
0: TRAIN [0][1760/1885]	Time 0.146 (0.142)	Data 2.20e-04 (3.69e-04)	Tok/s 99975 (96849)	Loss/tok 3.4482 (4.5155)	LR 2.800e-03
0: TRAIN [0][1770/1885]	Time 0.102 (0.142)	Data 2.16e-04 (3.68e-04)	Tok/s 88241 (96840)	Loss/tok 3.2547 (4.5100)	LR 2.800e-03
0: TRAIN [0][1780/1885]	Time 0.146 (0.142)	Data 2.18e-04 (3.67e-04)	Tok/s 101113 (96832)	Loss/tok 3.3959 (4.5046)	LR 2.800e-03
0: TRAIN [0][1790/1885]	Time 0.147 (0.142)	Data 2.16e-04 (3.66e-04)	Tok/s 99085 (96860)	Loss/tok 3.6102 (4.4983)	LR 2.800e-03
0: TRAIN [0][1800/1885]	Time 0.146 (0.142)	Data 2.17e-04 (3.66e-04)	Tok/s 99746 (96852)	Loss/tok 3.4751 (4.4930)	LR 2.800e-03
0: TRAIN [0][1810/1885]	Time 0.102 (0.142)	Data 1.44e-04 (3.65e-04)	Tok/s 90776 (96860)	Loss/tok 3.2808 (4.4876)	LR 2.800e-03
0: TRAIN [0][1820/1885]	Time 0.100 (0.142)	Data 2.20e-04 (3.64e-04)	Tok/s 92054 (96876)	Loss/tok 3.1935 (4.4819)	LR 2.800e-03
0: TRAIN [0][1830/1885]	Time 0.101 (0.142)	Data 2.20e-04 (3.63e-04)	Tok/s 89611 (96898)	Loss/tok 3.2229 (4.4758)	LR 2.800e-03
0: TRAIN [0][1840/1885]	Time 0.100 (0.142)	Data 2.19e-04 (3.62e-04)	Tok/s 91028 (96889)	Loss/tok 3.2392 (4.4708)	LR 2.800e-03
0: TRAIN [0][1850/1885]	Time 0.102 (0.142)	Data 1.09e-04 (3.61e-04)	Tok/s 87269 (96891)	Loss/tok 3.1538 (4.4654)	LR 2.800e-03
0: TRAIN [0][1860/1885]	Time 0.146 (0.142)	Data 2.18e-04 (3.60e-04)	Tok/s 101627 (96919)	Loss/tok 3.4049 (4.4596)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1870/1885]	Time 0.194 (0.142)	Data 2.26e-04 (3.59e-04)	Tok/s 105020 (96913)	Loss/tok 3.4984 (4.4547)	LR 2.800e-03
0: TRAIN [0][1880/1885]	Time 0.102 (0.142)	Data 2.35e-04 (3.59e-04)	Tok/s 88747 (96897)	Loss/tok 3.3423 (4.4501)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593837073583, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593837073584, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.660 (0.660)	Decoder iters 149.0 (149.0)	Tok/s 23962 (23962)
0: Running moses detokenizer
0: BLEU(score=19.927542628653836, counts=[34073, 15691, 8330, 4584], totals=[63156, 60153, 57151, 54154], precisions=[53.95053518272215, 26.08514953535152, 14.575423002222184, 8.464748679691251], bp=0.9762199213612177, sys_len=63156, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593837075434, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1993, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593837075434, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.4488	Test BLEU: 19.93
0: Performance: Epoch: 0	Training: 774764 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593837075434, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593837075434, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593837075435, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 4087588155
0: TRAIN [1][0/1885]	Time 0.408 (0.408)	Data 2.26e-01 (2.26e-01)	Tok/s 35574 (35574)	Loss/tok 3.4158 (3.4158)	LR 2.800e-03
0: TRAIN [1][10/1885]	Time 0.101 (0.150)	Data 2.29e-04 (2.07e-02)	Tok/s 89376 (88532)	Loss/tok 3.1054 (3.2846)	LR 2.800e-03
0: TRAIN [1][20/1885]	Time 0.100 (0.146)	Data 2.23e-04 (1.09e-02)	Tok/s 89022 (92121)	Loss/tok 3.2612 (3.3753)	LR 2.800e-03
0: TRAIN [1][30/1885]	Time 0.101 (0.153)	Data 2.16e-04 (7.48e-03)	Tok/s 90673 (94786)	Loss/tok 3.2823 (3.4351)	LR 2.800e-03
0: TRAIN [1][40/1885]	Time 0.147 (0.150)	Data 1.89e-04 (5.71e-03)	Tok/s 98808 (95304)	Loss/tok 3.3987 (3.4358)	LR 2.800e-03
0: TRAIN [1][50/1885]	Time 0.146 (0.150)	Data 2.18e-04 (4.63e-03)	Tok/s 100618 (95715)	Loss/tok 3.6356 (3.4546)	LR 2.800e-03
0: TRAIN [1][60/1885]	Time 0.101 (0.148)	Data 2.18e-04 (3.90e-03)	Tok/s 90374 (95606)	Loss/tok 3.1156 (3.4554)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][70/1885]	Time 0.101 (0.147)	Data 2.16e-04 (3.38e-03)	Tok/s 90517 (95747)	Loss/tok 3.1633 (3.4584)	LR 2.800e-03
0: TRAIN [1][80/1885]	Time 0.147 (0.148)	Data 1.74e-04 (2.99e-03)	Tok/s 99947 (96100)	Loss/tok 3.3335 (3.4614)	LR 2.800e-03
0: TRAIN [1][90/1885]	Time 0.102 (0.145)	Data 1.70e-04 (2.68e-03)	Tok/s 89389 (95789)	Loss/tok 3.0653 (3.4468)	LR 2.800e-03
0: TRAIN [1][100/1885]	Time 0.146 (0.142)	Data 2.23e-04 (2.44e-03)	Tok/s 100436 (95575)	Loss/tok 3.4191 (3.4346)	LR 2.800e-03
0: TRAIN [1][110/1885]	Time 0.244 (0.144)	Data 2.19e-04 (2.24e-03)	Tok/s 107141 (95692)	Loss/tok 3.7727 (3.4450)	LR 2.800e-03
0: TRAIN [1][120/1885]	Time 0.101 (0.144)	Data 1.68e-04 (2.07e-03)	Tok/s 90089 (95892)	Loss/tok 3.1227 (3.4439)	LR 2.800e-03
0: TRAIN [1][130/1885]	Time 0.102 (0.145)	Data 1.41e-04 (1.93e-03)	Tok/s 89110 (96115)	Loss/tok 3.1320 (3.4483)	LR 2.800e-03
0: TRAIN [1][140/1885]	Time 0.060 (0.145)	Data 1.90e-04 (1.80e-03)	Tok/s 77445 (96091)	Loss/tok 2.7540 (3.4462)	LR 2.800e-03
0: TRAIN [1][150/1885]	Time 0.147 (0.145)	Data 2.21e-04 (1.70e-03)	Tok/s 100437 (96163)	Loss/tok 3.4099 (3.4423)	LR 2.800e-03
0: TRAIN [1][160/1885]	Time 0.145 (0.146)	Data 2.16e-04 (1.61e-03)	Tok/s 101835 (96391)	Loss/tok 3.4081 (3.4494)	LR 2.800e-03
0: TRAIN [1][170/1885]	Time 0.145 (0.146)	Data 2.18e-04 (1.52e-03)	Tok/s 102032 (96598)	Loss/tok 3.3761 (3.4502)	LR 2.800e-03
0: TRAIN [1][180/1885]	Time 0.147 (0.146)	Data 2.15e-04 (1.45e-03)	Tok/s 100665 (96661)	Loss/tok 3.3444 (3.4466)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][190/1885]	Time 0.147 (0.145)	Data 1.58e-04 (1.39e-03)	Tok/s 100743 (96663)	Loss/tok 3.5070 (3.4424)	LR 2.800e-03
0: TRAIN [1][200/1885]	Time 0.101 (0.144)	Data 2.20e-04 (1.33e-03)	Tok/s 87817 (96500)	Loss/tok 3.3012 (3.4395)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][210/1885]	Time 0.193 (0.145)	Data 2.21e-04 (1.27e-03)	Tok/s 105470 (96720)	Loss/tok 3.6033 (3.4434)	LR 2.800e-03
0: TRAIN [1][220/1885]	Time 0.193 (0.147)	Data 2.15e-04 (1.23e-03)	Tok/s 105794 (97064)	Loss/tok 3.5723 (3.4518)	LR 2.800e-03
0: TRAIN [1][230/1885]	Time 0.146 (0.146)	Data 2.17e-04 (1.18e-03)	Tok/s 100026 (96889)	Loss/tok 3.3363 (3.4464)	LR 2.800e-03
0: TRAIN [1][240/1885]	Time 0.146 (0.147)	Data 2.15e-04 (1.14e-03)	Tok/s 102062 (97087)	Loss/tok 3.2460 (3.4502)	LR 2.800e-03
0: TRAIN [1][250/1885]	Time 0.193 (0.147)	Data 2.15e-04 (1.11e-03)	Tok/s 105034 (96971)	Loss/tok 3.3775 (3.4472)	LR 2.800e-03
0: TRAIN [1][260/1885]	Time 0.146 (0.147)	Data 2.18e-04 (1.07e-03)	Tok/s 100057 (97052)	Loss/tok 3.3587 (3.4478)	LR 2.800e-03
0: TRAIN [1][270/1885]	Time 0.193 (0.147)	Data 2.19e-04 (1.04e-03)	Tok/s 106677 (97129)	Loss/tok 3.5348 (3.4476)	LR 2.800e-03
0: TRAIN [1][280/1885]	Time 0.059 (0.146)	Data 2.16e-04 (1.01e-03)	Tok/s 78060 (96918)	Loss/tok 2.5998 (3.4413)	LR 2.800e-03
0: TRAIN [1][290/1885]	Time 0.193 (0.147)	Data 2.18e-04 (9.84e-04)	Tok/s 105595 (97087)	Loss/tok 3.6361 (3.4451)	LR 2.800e-03
0: TRAIN [1][300/1885]	Time 0.101 (0.146)	Data 2.16e-04 (9.58e-04)	Tok/s 90101 (97058)	Loss/tok 3.1974 (3.4452)	LR 2.800e-03
0: TRAIN [1][310/1885]	Time 0.100 (0.146)	Data 2.15e-04 (9.34e-04)	Tok/s 91934 (97058)	Loss/tok 3.1340 (3.4446)	LR 2.800e-03
0: TRAIN [1][320/1885]	Time 0.147 (0.146)	Data 2.22e-04 (9.12e-04)	Tok/s 100833 (97058)	Loss/tok 3.3832 (3.4433)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][330/1885]	Time 0.195 (0.146)	Data 2.31e-04 (8.91e-04)	Tok/s 104396 (97089)	Loss/tok 3.5201 (3.4428)	LR 2.800e-03
0: TRAIN [1][340/1885]	Time 0.147 (0.146)	Data 2.21e-04 (8.71e-04)	Tok/s 98839 (97089)	Loss/tok 3.4679 (3.4434)	LR 2.800e-03
0: TRAIN [1][350/1885]	Time 0.194 (0.146)	Data 2.17e-04 (8.52e-04)	Tok/s 104754 (97036)	Loss/tok 3.6335 (3.4447)	LR 2.800e-03
0: TRAIN [1][360/1885]	Time 0.146 (0.147)	Data 2.17e-04 (8.35e-04)	Tok/s 99951 (97065)	Loss/tok 3.3054 (3.4473)	LR 2.800e-03
0: TRAIN [1][370/1885]	Time 0.194 (0.146)	Data 2.16e-04 (8.18e-04)	Tok/s 104097 (97065)	Loss/tok 3.5710 (3.4464)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][380/1885]	Time 0.101 (0.147)	Data 2.19e-04 (8.02e-04)	Tok/s 88987 (97043)	Loss/tok 3.1514 (3.4469)	LR 2.800e-03
0: TRAIN [1][390/1885]	Time 0.147 (0.146)	Data 2.19e-04 (7.87e-04)	Tok/s 99177 (96993)	Loss/tok 3.4654 (3.4459)	LR 2.800e-03
0: TRAIN [1][400/1885]	Time 0.246 (0.146)	Data 2.02e-04 (7.73e-04)	Tok/s 107160 (97010)	Loss/tok 3.7634 (3.4469)	LR 2.800e-03
0: TRAIN [1][410/1885]	Time 0.146 (0.146)	Data 2.18e-04 (7.59e-04)	Tok/s 100736 (96965)	Loss/tok 3.3316 (3.4459)	LR 2.800e-03
0: TRAIN [1][420/1885]	Time 0.101 (0.146)	Data 2.20e-04 (7.47e-04)	Tok/s 90169 (96921)	Loss/tok 3.1996 (3.4438)	LR 2.800e-03
0: TRAIN [1][430/1885]	Time 0.100 (0.145)	Data 2.18e-04 (7.34e-04)	Tok/s 90315 (96800)	Loss/tok 3.2478 (3.4429)	LR 2.800e-03
0: TRAIN [1][440/1885]	Time 0.196 (0.145)	Data 2.16e-04 (7.23e-04)	Tok/s 103727 (96731)	Loss/tok 3.6003 (3.4404)	LR 2.800e-03
0: TRAIN [1][450/1885]	Time 0.101 (0.144)	Data 2.15e-04 (7.11e-04)	Tok/s 89658 (96646)	Loss/tok 3.1784 (3.4376)	LR 2.800e-03
0: TRAIN [1][460/1885]	Time 0.146 (0.144)	Data 2.17e-04 (7.01e-04)	Tok/s 99902 (96577)	Loss/tok 3.3472 (3.4346)	LR 2.800e-03
0: TRAIN [1][470/1885]	Time 0.194 (0.144)	Data 2.17e-04 (6.90e-04)	Tok/s 103867 (96669)	Loss/tok 3.6052 (3.4365)	LR 2.800e-03
0: TRAIN [1][480/1885]	Time 0.101 (0.144)	Data 2.19e-04 (6.81e-04)	Tok/s 89654 (96600)	Loss/tok 3.1207 (3.4355)	LR 2.800e-03
0: TRAIN [1][490/1885]	Time 0.147 (0.144)	Data 2.17e-04 (6.71e-04)	Tok/s 100818 (96631)	Loss/tok 3.2884 (3.4353)	LR 2.800e-03
0: TRAIN [1][500/1885]	Time 0.193 (0.144)	Data 2.19e-04 (6.62e-04)	Tok/s 106263 (96662)	Loss/tok 3.4407 (3.4347)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][510/1885]	Time 0.194 (0.144)	Data 2.18e-04 (6.53e-04)	Tok/s 105354 (96634)	Loss/tok 3.5562 (3.4345)	LR 2.800e-03
0: TRAIN [1][520/1885]	Time 0.194 (0.144)	Data 2.26e-04 (6.45e-04)	Tok/s 106362 (96591)	Loss/tok 3.4272 (3.4336)	LR 2.800e-03
0: TRAIN [1][530/1885]	Time 0.101 (0.144)	Data 2.27e-04 (6.37e-04)	Tok/s 88694 (96649)	Loss/tok 3.0115 (3.4332)	LR 2.800e-03
0: TRAIN [1][540/1885]	Time 0.146 (0.144)	Data 2.25e-04 (6.30e-04)	Tok/s 101203 (96660)	Loss/tok 3.3891 (3.4318)	LR 2.800e-03
0: TRAIN [1][550/1885]	Time 0.103 (0.144)	Data 2.28e-04 (6.23e-04)	Tok/s 88350 (96643)	Loss/tok 3.0352 (3.4316)	LR 2.800e-03
0: TRAIN [1][560/1885]	Time 0.101 (0.144)	Data 2.26e-04 (6.15e-04)	Tok/s 90666 (96703)	Loss/tok 3.1527 (3.4331)	LR 2.800e-03
0: TRAIN [1][570/1885]	Time 0.101 (0.144)	Data 2.25e-04 (6.09e-04)	Tok/s 89777 (96643)	Loss/tok 3.0812 (3.4314)	LR 2.800e-03
0: TRAIN [1][580/1885]	Time 0.194 (0.144)	Data 2.27e-04 (6.02e-04)	Tok/s 104674 (96707)	Loss/tok 3.5728 (3.4314)	LR 2.800e-03
0: TRAIN [1][590/1885]	Time 0.147 (0.144)	Data 2.30e-04 (5.96e-04)	Tok/s 100206 (96681)	Loss/tok 3.3118 (3.4312)	LR 2.800e-03
0: TRAIN [1][600/1885]	Time 0.102 (0.144)	Data 2.28e-04 (5.90e-04)	Tok/s 89220 (96647)	Loss/tok 3.2514 (3.4289)	LR 2.800e-03
0: TRAIN [1][610/1885]	Time 0.103 (0.144)	Data 2.29e-04 (5.84e-04)	Tok/s 88073 (96705)	Loss/tok 3.1249 (3.4303)	LR 2.800e-03
0: TRAIN [1][620/1885]	Time 0.147 (0.144)	Data 2.17e-04 (5.78e-04)	Tok/s 100267 (96761)	Loss/tok 3.4035 (3.4317)	LR 2.800e-03
0: TRAIN [1][630/1885]	Time 0.145 (0.144)	Data 3.24e-04 (5.73e-04)	Tok/s 102413 (96676)	Loss/tok 3.4159 (3.4302)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][640/1885]	Time 0.102 (0.143)	Data 2.20e-04 (5.67e-04)	Tok/s 89080 (96627)	Loss/tok 3.0076 (3.4287)	LR 2.800e-03
0: TRAIN [1][650/1885]	Time 0.102 (0.144)	Data 1.91e-04 (5.62e-04)	Tok/s 86580 (96637)	Loss/tok 3.0883 (3.4279)	LR 2.800e-03
0: TRAIN [1][660/1885]	Time 0.146 (0.144)	Data 2.17e-04 (5.56e-04)	Tok/s 100143 (96661)	Loss/tok 3.3546 (3.4280)	LR 2.800e-03
0: TRAIN [1][670/1885]	Time 0.147 (0.144)	Data 2.17e-04 (5.52e-04)	Tok/s 100428 (96645)	Loss/tok 3.3715 (3.4269)	LR 2.800e-03
0: TRAIN [1][680/1885]	Time 0.102 (0.143)	Data 2.22e-04 (5.47e-04)	Tok/s 91327 (96595)	Loss/tok 3.0381 (3.4251)	LR 2.800e-03
0: TRAIN [1][690/1885]	Time 0.244 (0.143)	Data 2.19e-04 (5.42e-04)	Tok/s 107731 (96586)	Loss/tok 3.6863 (3.4255)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][700/1885]	Time 0.146 (0.143)	Data 3.00e-04 (5.38e-04)	Tok/s 99585 (96565)	Loss/tok 3.4864 (3.4247)	LR 2.800e-03
0: TRAIN [1][710/1885]	Time 0.147 (0.144)	Data 2.19e-04 (5.33e-04)	Tok/s 99067 (96651)	Loss/tok 3.4033 (3.4271)	LR 2.800e-03
0: TRAIN [1][720/1885]	Time 0.193 (0.144)	Data 2.17e-04 (5.29e-04)	Tok/s 106690 (96689)	Loss/tok 3.4802 (3.4270)	LR 2.800e-03
0: TRAIN [1][730/1885]	Time 0.244 (0.144)	Data 2.20e-04 (5.24e-04)	Tok/s 106271 (96707)	Loss/tok 3.7596 (3.4285)	LR 2.800e-03
0: TRAIN [1][740/1885]	Time 0.103 (0.144)	Data 2.83e-04 (5.20e-04)	Tok/s 87247 (96697)	Loss/tok 3.1000 (3.4267)	LR 2.800e-03
0: TRAIN [1][750/1885]	Time 0.244 (0.144)	Data 2.29e-04 (5.16e-04)	Tok/s 106542 (96689)	Loss/tok 3.6824 (3.4269)	LR 2.800e-03
0: TRAIN [1][760/1885]	Time 0.244 (0.144)	Data 2.19e-04 (5.13e-04)	Tok/s 105976 (96680)	Loss/tok 3.7520 (3.4264)	LR 2.800e-03
0: TRAIN [1][770/1885]	Time 0.147 (0.144)	Data 2.21e-04 (5.09e-04)	Tok/s 99133 (96673)	Loss/tok 3.4083 (3.4261)	LR 2.800e-03
0: TRAIN [1][780/1885]	Time 0.194 (0.144)	Data 2.16e-04 (5.05e-04)	Tok/s 104887 (96670)	Loss/tok 3.5099 (3.4255)	LR 2.800e-03
0: TRAIN [1][790/1885]	Time 0.146 (0.144)	Data 2.20e-04 (5.02e-04)	Tok/s 99833 (96682)	Loss/tok 3.3792 (3.4247)	LR 2.800e-03
0: TRAIN [1][800/1885]	Time 0.101 (0.144)	Data 2.34e-04 (4.99e-04)	Tok/s 91071 (96653)	Loss/tok 3.1718 (3.4244)	LR 2.800e-03
0: TRAIN [1][810/1885]	Time 0.102 (0.144)	Data 2.20e-04 (4.95e-04)	Tok/s 88051 (96619)	Loss/tok 3.2217 (3.4234)	LR 2.800e-03
0: TRAIN [1][820/1885]	Time 0.193 (0.144)	Data 2.21e-04 (4.92e-04)	Tok/s 105125 (96633)	Loss/tok 3.6006 (3.4233)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][830/1885]	Time 0.102 (0.144)	Data 2.20e-04 (4.89e-04)	Tok/s 91057 (96597)	Loss/tok 3.0867 (3.4219)	LR 2.800e-03
0: TRAIN [1][840/1885]	Time 0.146 (0.144)	Data 2.19e-04 (4.86e-04)	Tok/s 100636 (96604)	Loss/tok 3.3655 (3.4212)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][850/1885]	Time 0.247 (0.144)	Data 2.21e-04 (4.83e-04)	Tok/s 106398 (96648)	Loss/tok 3.7345 (3.4231)	LR 2.800e-03
0: TRAIN [1][860/1885]	Time 0.146 (0.144)	Data 2.20e-04 (4.80e-04)	Tok/s 101041 (96633)	Loss/tok 3.3900 (3.4217)	LR 2.800e-03
0: TRAIN [1][870/1885]	Time 0.102 (0.143)	Data 2.16e-04 (4.77e-04)	Tok/s 90708 (96574)	Loss/tok 3.1811 (3.4198)	LR 2.800e-03
0: TRAIN [1][880/1885]	Time 0.102 (0.143)	Data 2.22e-04 (4.74e-04)	Tok/s 87732 (96578)	Loss/tok 3.0738 (3.4190)	LR 2.800e-03
0: TRAIN [1][890/1885]	Time 0.147 (0.144)	Data 2.06e-04 (4.71e-04)	Tok/s 99410 (96619)	Loss/tok 3.3127 (3.4194)	LR 2.800e-03
0: TRAIN [1][900/1885]	Time 0.194 (0.143)	Data 2.22e-04 (4.68e-04)	Tok/s 107786 (96600)	Loss/tok 3.5023 (3.4189)	LR 2.800e-03
0: TRAIN [1][910/1885]	Time 0.147 (0.143)	Data 2.21e-04 (4.65e-04)	Tok/s 100862 (96609)	Loss/tok 3.3708 (3.4189)	LR 2.800e-03
0: TRAIN [1][920/1885]	Time 0.195 (0.143)	Data 1.91e-04 (4.63e-04)	Tok/s 103369 (96617)	Loss/tok 3.4970 (3.4184)	LR 2.800e-03
0: TRAIN [1][930/1885]	Time 0.145 (0.143)	Data 2.18e-04 (4.60e-04)	Tok/s 99650 (96597)	Loss/tok 3.3937 (3.4168)	LR 2.800e-03
0: TRAIN [1][940/1885]	Time 0.146 (0.144)	Data 2.17e-04 (4.58e-04)	Tok/s 100109 (96647)	Loss/tok 3.3064 (3.4171)	LR 2.800e-03
0: TRAIN [1][950/1885]	Time 0.146 (0.143)	Data 2.23e-04 (4.55e-04)	Tok/s 99698 (96634)	Loss/tok 3.2861 (3.4161)	LR 2.800e-03
0: TRAIN [1][960/1885]	Time 0.145 (0.143)	Data 2.20e-04 (4.53e-04)	Tok/s 101378 (96605)	Loss/tok 3.3671 (3.4156)	LR 2.800e-03
0: TRAIN [1][970/1885]	Time 0.194 (0.143)	Data 3.07e-04 (4.51e-04)	Tok/s 105560 (96607)	Loss/tok 3.4590 (3.4156)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][980/1885]	Time 0.101 (0.143)	Data 2.63e-04 (4.48e-04)	Tok/s 90748 (96559)	Loss/tok 3.1927 (3.4143)	LR 2.800e-03
0: TRAIN [1][990/1885]	Time 0.244 (0.143)	Data 2.21e-04 (4.46e-04)	Tok/s 106021 (96561)	Loss/tok 3.8439 (3.4142)	LR 2.800e-03
0: TRAIN [1][1000/1885]	Time 0.103 (0.143)	Data 2.19e-04 (4.44e-04)	Tok/s 88497 (96611)	Loss/tok 3.1314 (3.4144)	LR 2.800e-03
0: TRAIN [1][1010/1885]	Time 0.193 (0.143)	Data 2.21e-04 (4.42e-04)	Tok/s 105607 (96576)	Loss/tok 3.4418 (3.4139)	LR 2.800e-03
0: TRAIN [1][1020/1885]	Time 0.147 (0.143)	Data 2.24e-04 (4.40e-04)	Tok/s 98416 (96555)	Loss/tok 3.2898 (3.4142)	LR 2.800e-03
0: TRAIN [1][1030/1885]	Time 0.101 (0.143)	Data 2.19e-04 (4.37e-04)	Tok/s 91155 (96571)	Loss/tok 3.1225 (3.4134)	LR 2.800e-03
0: TRAIN [1][1040/1885]	Time 0.103 (0.143)	Data 2.22e-04 (4.35e-04)	Tok/s 88453 (96597)	Loss/tok 3.0690 (3.4139)	LR 2.800e-03
0: TRAIN [1][1050/1885]	Time 0.146 (0.143)	Data 3.25e-04 (4.33e-04)	Tok/s 102101 (96577)	Loss/tok 3.3626 (3.4129)	LR 2.800e-03
0: TRAIN [1][1060/1885]	Time 0.102 (0.144)	Data 2.19e-04 (4.31e-04)	Tok/s 88187 (96618)	Loss/tok 3.1190 (3.4138)	LR 2.800e-03
0: TRAIN [1][1070/1885]	Time 0.192 (0.144)	Data 2.19e-04 (4.30e-04)	Tok/s 107618 (96636)	Loss/tok 3.5400 (3.4132)	LR 2.800e-03
0: TRAIN [1][1080/1885]	Time 0.101 (0.144)	Data 2.20e-04 (4.28e-04)	Tok/s 89615 (96623)	Loss/tok 3.0763 (3.4129)	LR 2.800e-03
0: TRAIN [1][1090/1885]	Time 0.102 (0.144)	Data 2.18e-04 (4.26e-04)	Tok/s 89235 (96629)	Loss/tok 3.1804 (3.4124)	LR 2.800e-03
0: TRAIN [1][1100/1885]	Time 0.101 (0.143)	Data 3.26e-04 (4.24e-04)	Tok/s 89825 (96615)	Loss/tok 3.0115 (3.4117)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1110/1885]	Time 0.194 (0.144)	Data 2.21e-04 (4.22e-04)	Tok/s 106045 (96630)	Loss/tok 3.4987 (3.4113)	LR 2.800e-03
0: TRAIN [1][1120/1885]	Time 0.146 (0.144)	Data 2.20e-04 (4.20e-04)	Tok/s 100705 (96644)	Loss/tok 3.3347 (3.4107)	LR 2.800e-03
0: TRAIN [1][1130/1885]	Time 0.101 (0.144)	Data 2.20e-04 (4.19e-04)	Tok/s 90107 (96633)	Loss/tok 3.0576 (3.4103)	LR 2.800e-03
0: TRAIN [1][1140/1885]	Time 0.146 (0.143)	Data 2.21e-04 (4.17e-04)	Tok/s 100671 (96612)	Loss/tok 3.2927 (3.4095)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1150/1885]	Time 0.245 (0.143)	Data 2.19e-04 (4.15e-04)	Tok/s 106146 (96596)	Loss/tok 3.7107 (3.4094)	LR 2.800e-03
0: TRAIN [1][1160/1885]	Time 0.146 (0.143)	Data 2.17e-04 (4.14e-04)	Tok/s 99094 (96589)	Loss/tok 3.4555 (3.4087)	LR 2.800e-03
0: TRAIN [1][1170/1885]	Time 0.146 (0.143)	Data 2.18e-04 (4.12e-04)	Tok/s 100942 (96602)	Loss/tok 3.2604 (3.4080)	LR 2.800e-03
0: TRAIN [1][1180/1885]	Time 0.244 (0.143)	Data 2.18e-04 (4.10e-04)	Tok/s 106955 (96595)	Loss/tok 3.6805 (3.4083)	LR 2.800e-03
0: TRAIN [1][1190/1885]	Time 0.102 (0.143)	Data 2.21e-04 (4.09e-04)	Tok/s 88828 (96599)	Loss/tok 3.1187 (3.4077)	LR 2.800e-03
0: TRAIN [1][1200/1885]	Time 0.103 (0.143)	Data 2.21e-04 (4.07e-04)	Tok/s 88040 (96603)	Loss/tok 3.1520 (3.4074)	LR 2.800e-03
0: TRAIN [1][1210/1885]	Time 0.192 (0.144)	Data 2.17e-04 (4.06e-04)	Tok/s 106456 (96640)	Loss/tok 3.5053 (3.4077)	LR 2.800e-03
0: TRAIN [1][1220/1885]	Time 0.102 (0.143)	Data 2.17e-04 (4.04e-04)	Tok/s 87529 (96625)	Loss/tok 3.1791 (3.4072)	LR 2.800e-03
0: TRAIN [1][1230/1885]	Time 0.146 (0.143)	Data 2.21e-04 (4.03e-04)	Tok/s 102292 (96602)	Loss/tok 3.2763 (3.4064)	LR 2.800e-03
0: TRAIN [1][1240/1885]	Time 0.102 (0.143)	Data 2.19e-04 (4.01e-04)	Tok/s 89064 (96617)	Loss/tok 3.1246 (3.4062)	LR 2.800e-03
0: TRAIN [1][1250/1885]	Time 0.245 (0.143)	Data 2.21e-04 (4.00e-04)	Tok/s 106694 (96603)	Loss/tok 3.6604 (3.4058)	LR 2.800e-03
0: TRAIN [1][1260/1885]	Time 0.245 (0.144)	Data 2.25e-04 (3.99e-04)	Tok/s 107970 (96629)	Loss/tok 3.5460 (3.4062)	LR 2.800e-03
0: TRAIN [1][1270/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.97e-04)	Tok/s 100013 (96618)	Loss/tok 3.3695 (3.4056)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1280/1885]	Time 0.146 (0.143)	Data 2.26e-04 (3.96e-04)	Tok/s 101133 (96608)	Loss/tok 3.4060 (3.4055)	LR 2.800e-03
0: TRAIN [1][1290/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.95e-04)	Tok/s 100675 (96598)	Loss/tok 3.3361 (3.4047)	LR 2.800e-03
0: TRAIN [1][1300/1885]	Time 0.145 (0.143)	Data 2.18e-04 (3.93e-04)	Tok/s 100460 (96555)	Loss/tok 3.2334 (3.4035)	LR 2.800e-03
0: TRAIN [1][1310/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.92e-04)	Tok/s 99810 (96563)	Loss/tok 3.3785 (3.4032)	LR 2.800e-03
0: TRAIN [1][1320/1885]	Time 0.245 (0.143)	Data 2.21e-04 (3.91e-04)	Tok/s 105844 (96562)	Loss/tok 3.7235 (3.4027)	LR 2.800e-03
0: TRAIN [1][1330/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.90e-04)	Tok/s 98836 (96564)	Loss/tok 3.3610 (3.4027)	LR 2.800e-03
0: TRAIN [1][1340/1885]	Time 0.147 (0.143)	Data 2.37e-04 (3.88e-04)	Tok/s 98829 (96586)	Loss/tok 3.3994 (3.4033)	LR 2.800e-03
0: TRAIN [1][1350/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.87e-04)	Tok/s 90557 (96591)	Loss/tok 3.1506 (3.4030)	LR 2.800e-03
0: TRAIN [1][1360/1885]	Time 0.193 (0.143)	Data 2.19e-04 (3.86e-04)	Tok/s 107029 (96633)	Loss/tok 3.4351 (3.4029)	LR 2.800e-03
0: TRAIN [1][1370/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.85e-04)	Tok/s 99073 (96610)	Loss/tok 3.5037 (3.4022)	LR 2.800e-03
0: TRAIN [1][1380/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.83e-04)	Tok/s 90317 (96595)	Loss/tok 3.1004 (3.4010)	LR 2.800e-03
0: TRAIN [1][1390/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.82e-04)	Tok/s 105224 (96593)	Loss/tok 3.4808 (3.4004)	LR 2.800e-03
0: TRAIN [1][1400/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.81e-04)	Tok/s 101817 (96576)	Loss/tok 3.2749 (3.3996)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1410/1885]	Time 0.060 (0.143)	Data 2.20e-04 (3.80e-04)	Tok/s 76532 (96572)	Loss/tok 2.5343 (3.3991)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1420/1885]	Time 0.245 (0.143)	Data 1.80e-04 (3.79e-04)	Tok/s 106256 (96591)	Loss/tok 3.5985 (3.3990)	LR 2.800e-03
0: TRAIN [1][1430/1885]	Time 0.244 (0.143)	Data 2.19e-04 (3.78e-04)	Tok/s 108736 (96607)	Loss/tok 3.6449 (3.3993)	LR 2.800e-03
0: TRAIN [1][1440/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.77e-04)	Tok/s 90497 (96574)	Loss/tok 3.0657 (3.3978)	LR 2.800e-03
0: TRAIN [1][1450/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.76e-04)	Tok/s 99616 (96575)	Loss/tok 3.2992 (3.3971)	LR 2.800e-03
0: TRAIN [1][1460/1885]	Time 0.060 (0.143)	Data 2.18e-04 (3.75e-04)	Tok/s 76176 (96550)	Loss/tok 2.5239 (3.3967)	LR 2.800e-03
0: TRAIN [1][1470/1885]	Time 0.147 (0.143)	Data 2.53e-04 (3.74e-04)	Tok/s 99853 (96556)	Loss/tok 3.2202 (3.3961)	LR 2.800e-03
0: TRAIN [1][1480/1885]	Time 0.193 (0.143)	Data 2.21e-04 (3.73e-04)	Tok/s 106539 (96530)	Loss/tok 3.3459 (3.3954)	LR 2.800e-03
0: TRAIN [1][1490/1885]	Time 0.145 (0.143)	Data 2.22e-04 (3.71e-04)	Tok/s 99546 (96541)	Loss/tok 3.4522 (3.3950)	LR 2.800e-03
0: TRAIN [1][1500/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.70e-04)	Tok/s 89256 (96574)	Loss/tok 3.0787 (3.3952)	LR 2.800e-03
0: TRAIN [1][1510/1885]	Time 0.103 (0.143)	Data 3.28e-04 (3.70e-04)	Tok/s 89187 (96564)	Loss/tok 3.1322 (3.3944)	LR 2.800e-03
0: TRAIN [1][1520/1885]	Time 0.147 (0.143)	Data 1.93e-04 (3.69e-04)	Tok/s 100150 (96564)	Loss/tok 3.3759 (3.3938)	LR 2.800e-03
0: TRAIN [1][1530/1885]	Time 0.193 (0.143)	Data 2.19e-04 (3.68e-04)	Tok/s 105440 (96557)	Loss/tok 3.4894 (3.3934)	LR 2.800e-03
0: TRAIN [1][1540/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.67e-04)	Tok/s 100393 (96574)	Loss/tok 3.2785 (3.3931)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1550/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.66e-04)	Tok/s 99516 (96575)	Loss/tok 3.2434 (3.3923)	LR 2.800e-03
0: TRAIN [1][1560/1885]	Time 0.100 (0.143)	Data 2.17e-04 (3.65e-04)	Tok/s 90418 (96588)	Loss/tok 3.0691 (3.3919)	LR 2.800e-03
0: TRAIN [1][1570/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.64e-04)	Tok/s 101460 (96591)	Loss/tok 3.2651 (3.3912)	LR 2.800e-03
0: TRAIN [1][1580/1885]	Time 0.060 (0.143)	Data 2.32e-04 (3.63e-04)	Tok/s 76530 (96592)	Loss/tok 2.5708 (3.3909)	LR 2.800e-03
0: TRAIN [1][1590/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.62e-04)	Tok/s 87649 (96567)	Loss/tok 3.1345 (3.3903)	LR 2.800e-03
0: TRAIN [1][1600/1885]	Time 0.145 (0.143)	Data 2.23e-04 (3.61e-04)	Tok/s 102172 (96580)	Loss/tok 3.1719 (3.3902)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1610/1885]	Time 0.146 (0.143)	Data 2.60e-04 (3.60e-04)	Tok/s 101419 (96612)	Loss/tok 3.2575 (3.3906)	LR 2.800e-03
0: TRAIN [1][1620/1885]	Time 0.245 (0.143)	Data 2.19e-04 (3.59e-04)	Tok/s 105781 (96608)	Loss/tok 3.7616 (3.3913)	LR 2.800e-03
0: TRAIN [1][1630/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.59e-04)	Tok/s 99402 (96598)	Loss/tok 3.2755 (3.3906)	LR 2.800e-03
0: TRAIN [1][1640/1885]	Time 0.102 (0.143)	Data 2.24e-04 (3.58e-04)	Tok/s 89711 (96599)	Loss/tok 3.1421 (3.3904)	LR 2.800e-03
0: TRAIN [1][1650/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.57e-04)	Tok/s 100361 (96606)	Loss/tok 3.2956 (3.3903)	LR 2.800e-03
0: TRAIN [1][1660/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.56e-04)	Tok/s 99726 (96623)	Loss/tok 3.2122 (3.3899)	LR 2.800e-03
0: TRAIN [1][1670/1885]	Time 0.100 (0.143)	Data 2.80e-04 (3.55e-04)	Tok/s 91416 (96645)	Loss/tok 3.0012 (3.3905)	LR 2.800e-03
0: TRAIN [1][1680/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.55e-04)	Tok/s 99934 (96623)	Loss/tok 3.3130 (3.3897)	LR 2.800e-03
0: TRAIN [1][1690/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.54e-04)	Tok/s 89817 (96624)	Loss/tok 3.0669 (3.3892)	LR 2.800e-03
0: TRAIN [1][1700/1885]	Time 0.193 (0.143)	Data 2.23e-04 (3.53e-04)	Tok/s 105656 (96644)	Loss/tok 3.4474 (3.3893)	LR 2.800e-03
0: TRAIN [1][1710/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.52e-04)	Tok/s 100424 (96649)	Loss/tok 3.2056 (3.3890)	LR 2.800e-03
0: TRAIN [1][1720/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.52e-04)	Tok/s 88847 (96651)	Loss/tok 3.1611 (3.3886)	LR 2.800e-03
0: TRAIN [1][1730/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.51e-04)	Tok/s 87471 (96620)	Loss/tok 3.1560 (3.3880)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1740/1885]	Time 0.061 (0.143)	Data 2.19e-04 (3.50e-04)	Tok/s 75526 (96601)	Loss/tok 2.6723 (3.3872)	LR 2.800e-03
0: TRAIN [1][1750/1885]	Time 0.195 (0.143)	Data 2.20e-04 (3.49e-04)	Tok/s 104196 (96600)	Loss/tok 3.4422 (3.3870)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1760/1885]	Time 0.147 (0.143)	Data 2.75e-04 (3.49e-04)	Tok/s 99311 (96599)	Loss/tok 3.3060 (3.3868)	LR 2.800e-03
0: TRAIN [1][1770/1885]	Time 0.102 (0.143)	Data 2.23e-04 (3.48e-04)	Tok/s 89845 (96584)	Loss/tok 3.1544 (3.3861)	LR 2.800e-03
0: TRAIN [1][1780/1885]	Time 0.147 (0.143)	Data 2.23e-04 (3.47e-04)	Tok/s 99167 (96578)	Loss/tok 3.2812 (3.3859)	LR 2.800e-03
0: TRAIN [1][1790/1885]	Time 0.147 (0.143)	Data 2.22e-04 (3.47e-04)	Tok/s 99774 (96578)	Loss/tok 3.3208 (3.3854)	LR 2.800e-03
0: TRAIN [1][1800/1885]	Time 0.245 (0.143)	Data 1.83e-04 (3.46e-04)	Tok/s 106100 (96589)	Loss/tok 3.6212 (3.3855)	LR 2.800e-03
0: TRAIN [1][1810/1885]	Time 0.061 (0.143)	Data 2.54e-04 (3.45e-04)	Tok/s 76195 (96584)	Loss/tok 2.6432 (3.3852)	LR 2.800e-03
0: TRAIN [1][1820/1885]	Time 0.100 (0.143)	Data 2.23e-04 (3.45e-04)	Tok/s 88992 (96573)	Loss/tok 2.9295 (3.3846)	LR 2.800e-03
0: TRAIN [1][1830/1885]	Time 0.245 (0.143)	Data 2.21e-04 (3.44e-04)	Tok/s 105159 (96580)	Loss/tok 3.6354 (3.3847)	LR 2.800e-03
0: TRAIN [1][1840/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.43e-04)	Tok/s 90231 (96565)	Loss/tok 3.0043 (3.3843)	LR 2.800e-03
0: TRAIN [1][1850/1885]	Time 0.061 (0.143)	Data 2.07e-04 (3.43e-04)	Tok/s 75977 (96561)	Loss/tok 2.5713 (3.3845)	LR 2.800e-03
0: TRAIN [1][1860/1885]	Time 0.102 (0.143)	Data 2.29e-04 (3.42e-04)	Tok/s 91822 (96553)	Loss/tok 3.0926 (3.3840)	LR 2.800e-03
0: TRAIN [1][1870/1885]	Time 0.061 (0.143)	Data 2.24e-04 (3.42e-04)	Tok/s 73805 (96540)	Loss/tok 2.5804 (3.3836)	LR 2.800e-03
0: TRAIN [1][1880/1885]	Time 0.101 (0.143)	Data 2.27e-04 (3.41e-04)	Tok/s 89476 (96526)	Loss/tok 3.1357 (3.3832)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
:::MLLOG {"namespace": "", "time_ms": 1593837345154, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593837345155, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.678 (0.678)	Decoder iters 149.0 (149.0)	Tok/s 24045 (24045)
0: Running moses detokenizer
0: BLEU(score=21.928749928561096, counts=[35490, 17002, 9332, 5339], totals=[64232, 61229, 58226, 55227], precisions=[55.25283347863993, 27.767887765601266, 16.02720434170302, 9.667372842993464], bp=0.9931113935804001, sys_len=64232, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593837346827, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2193, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593837346827, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3823	Test BLEU: 21.93
0: Performance: Epoch: 1	Training: 771945 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593837346827, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593837346828, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593837346828, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2761053219
0: TRAIN [2][0/1885]	Time 0.338 (0.338)	Data 2.17e-01 (2.17e-01)	Tok/s 27001 (27001)	Loss/tok 2.9952 (2.9952)	LR 2.800e-03
0: TRAIN [2][10/1885]	Time 0.100 (0.145)	Data 2.16e-04 (1.99e-02)	Tok/s 90862 (87427)	Loss/tok 2.8949 (3.1936)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][20/1885]	Time 0.101 (0.142)	Data 3.08e-04 (1.05e-02)	Tok/s 89077 (91525)	Loss/tok 2.9942 (3.2019)	LR 2.800e-03
0: TRAIN [2][30/1885]	Time 0.101 (0.137)	Data 2.95e-04 (7.21e-03)	Tok/s 87763 (92920)	Loss/tok 2.9930 (3.1934)	LR 2.800e-03
0: TRAIN [2][40/1885]	Time 0.101 (0.136)	Data 3.04e-04 (5.51e-03)	Tok/s 89395 (93648)	Loss/tok 2.9465 (3.1984)	LR 2.800e-03
0: TRAIN [2][50/1885]	Time 0.101 (0.138)	Data 2.13e-04 (4.48e-03)	Tok/s 90225 (94089)	Loss/tok 2.9754 (3.2122)	LR 2.800e-03
0: TRAIN [2][60/1885]	Time 0.100 (0.138)	Data 1.81e-04 (3.78e-03)	Tok/s 91011 (94413)	Loss/tok 2.9627 (3.2163)	LR 2.800e-03
0: TRAIN [2][70/1885]	Time 0.193 (0.140)	Data 3.12e-04 (3.28e-03)	Tok/s 106524 (95070)	Loss/tok 3.3027 (3.2294)	LR 2.800e-03
0: TRAIN [2][80/1885]	Time 0.146 (0.143)	Data 2.18e-04 (2.90e-03)	Tok/s 99782 (95400)	Loss/tok 3.1845 (3.2390)	LR 2.800e-03
0: TRAIN [2][90/1885]	Time 0.101 (0.140)	Data 2.15e-04 (2.61e-03)	Tok/s 89147 (95173)	Loss/tok 2.9264 (3.2260)	LR 2.800e-03
0: TRAIN [2][100/1885]	Time 0.194 (0.141)	Data 2.47e-04 (2.37e-03)	Tok/s 104127 (95407)	Loss/tok 3.4069 (3.2356)	LR 2.800e-03
0: TRAIN [2][110/1885]	Time 0.059 (0.140)	Data 2.16e-04 (2.18e-03)	Tok/s 79133 (95322)	Loss/tok 2.5226 (3.2277)	LR 2.800e-03
0: TRAIN [2][120/1885]	Time 0.060 (0.140)	Data 3.06e-04 (2.02e-03)	Tok/s 75954 (95355)	Loss/tok 2.4678 (3.2288)	LR 2.800e-03
0: TRAIN [2][130/1885]	Time 0.146 (0.140)	Data 2.16e-04 (1.88e-03)	Tok/s 101668 (95555)	Loss/tok 3.2931 (3.2304)	LR 2.800e-03
0: TRAIN [2][140/1885]	Time 0.146 (0.142)	Data 2.96e-04 (1.76e-03)	Tok/s 101495 (95886)	Loss/tok 3.2825 (3.2346)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][150/1885]	Time 0.102 (0.142)	Data 2.76e-04 (1.66e-03)	Tok/s 89524 (96019)	Loss/tok 2.9537 (3.2350)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][160/1885]	Time 0.194 (0.141)	Data 2.17e-04 (1.57e-03)	Tok/s 104965 (95879)	Loss/tok 3.3976 (3.2371)	LR 2.800e-03
0: TRAIN [2][170/1885]	Time 0.195 (0.141)	Data 2.15e-04 (1.49e-03)	Tok/s 103606 (95836)	Loss/tok 3.4778 (3.2338)	LR 2.800e-03
0: TRAIN [2][180/1885]	Time 0.102 (0.140)	Data 2.16e-04 (1.42e-03)	Tok/s 89506 (95874)	Loss/tok 2.9478 (3.2312)	LR 2.800e-03
0: TRAIN [2][190/1885]	Time 0.245 (0.141)	Data 2.14e-04 (1.36e-03)	Tok/s 107883 (95969)	Loss/tok 3.4933 (3.2357)	LR 2.800e-03
0: TRAIN [2][200/1885]	Time 0.101 (0.141)	Data 2.15e-04 (1.30e-03)	Tok/s 89290 (96015)	Loss/tok 2.9607 (3.2357)	LR 2.800e-03
0: TRAIN [2][210/1885]	Time 0.060 (0.140)	Data 2.14e-04 (1.25e-03)	Tok/s 77287 (95907)	Loss/tok 2.5141 (3.2342)	LR 2.800e-03
0: TRAIN [2][220/1885]	Time 0.146 (0.139)	Data 2.15e-04 (1.21e-03)	Tok/s 99944 (95624)	Loss/tok 3.2605 (3.2286)	LR 2.800e-03
0: TRAIN [2][230/1885]	Time 0.101 (0.138)	Data 2.26e-04 (1.16e-03)	Tok/s 89789 (95585)	Loss/tok 3.0311 (3.2266)	LR 2.800e-03
0: TRAIN [2][240/1885]	Time 0.147 (0.139)	Data 2.16e-04 (1.12e-03)	Tok/s 100111 (95777)	Loss/tok 3.2712 (3.2286)	LR 2.800e-03
0: TRAIN [2][250/1885]	Time 0.244 (0.139)	Data 2.17e-04 (1.09e-03)	Tok/s 107121 (95829)	Loss/tok 3.4705 (3.2289)	LR 2.800e-03
0: TRAIN [2][260/1885]	Time 0.147 (0.139)	Data 2.20e-04 (1.05e-03)	Tok/s 99163 (95833)	Loss/tok 3.1813 (3.2263)	LR 2.800e-03
0: TRAIN [2][270/1885]	Time 0.145 (0.140)	Data 2.18e-04 (1.02e-03)	Tok/s 101156 (96004)	Loss/tok 3.1801 (3.2320)	LR 2.800e-03
0: TRAIN [2][280/1885]	Time 0.195 (0.140)	Data 2.19e-04 (9.96e-04)	Tok/s 105538 (96052)	Loss/tok 3.3564 (3.2354)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][290/1885]	Time 0.101 (0.141)	Data 2.18e-04 (9.69e-04)	Tok/s 89019 (96083)	Loss/tok 3.0525 (3.2376)	LR 2.800e-03
0: TRAIN [2][300/1885]	Time 0.101 (0.141)	Data 2.99e-04 (9.45e-04)	Tok/s 88895 (96189)	Loss/tok 2.9952 (3.2398)	LR 2.800e-03
0: TRAIN [2][310/1885]	Time 0.193 (0.142)	Data 2.21e-04 (9.21e-04)	Tok/s 105301 (96318)	Loss/tok 3.3781 (3.2411)	LR 2.800e-03
0: TRAIN [2][320/1885]	Time 0.101 (0.141)	Data 2.18e-04 (8.99e-04)	Tok/s 89730 (96222)	Loss/tok 3.0116 (3.2386)	LR 2.800e-03
0: TRAIN [2][330/1885]	Time 0.102 (0.142)	Data 2.17e-04 (8.79e-04)	Tok/s 88477 (96317)	Loss/tok 3.0794 (3.2432)	LR 2.800e-03
0: TRAIN [2][340/1885]	Time 0.059 (0.142)	Data 2.16e-04 (8.60e-04)	Tok/s 76975 (96189)	Loss/tok 2.4247 (3.2425)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][350/1885]	Time 0.148 (0.141)	Data 3.17e-04 (8.41e-04)	Tok/s 100848 (96048)	Loss/tok 3.1260 (3.2401)	LR 2.800e-03
0: TRAIN [2][360/1885]	Time 0.195 (0.140)	Data 2.20e-04 (8.24e-04)	Tok/s 105414 (95910)	Loss/tok 3.3647 (3.2382)	LR 2.800e-03
0: TRAIN [2][370/1885]	Time 0.101 (0.141)	Data 2.29e-04 (8.08e-04)	Tok/s 90423 (95994)	Loss/tok 3.0447 (3.2400)	LR 2.800e-03
0: TRAIN [2][380/1885]	Time 0.146 (0.141)	Data 2.19e-04 (7.93e-04)	Tok/s 100062 (95940)	Loss/tok 3.3125 (3.2397)	LR 2.800e-03
0: TRAIN [2][390/1885]	Time 0.147 (0.140)	Data 3.22e-04 (7.78e-04)	Tok/s 99421 (95886)	Loss/tok 3.2019 (3.2381)	LR 2.800e-03
0: TRAIN [2][400/1885]	Time 0.101 (0.140)	Data 2.20e-04 (7.64e-04)	Tok/s 89707 (95766)	Loss/tok 2.9793 (3.2357)	LR 2.800e-03
0: TRAIN [2][410/1885]	Time 0.147 (0.140)	Data 2.50e-04 (7.51e-04)	Tok/s 97771 (95777)	Loss/tok 3.3043 (3.2378)	LR 2.800e-03
0: TRAIN [2][420/1885]	Time 0.195 (0.139)	Data 2.18e-04 (7.39e-04)	Tok/s 104254 (95738)	Loss/tok 3.3538 (3.2365)	LR 2.800e-03
0: TRAIN [2][430/1885]	Time 0.102 (0.140)	Data 2.68e-04 (7.27e-04)	Tok/s 87301 (95794)	Loss/tok 2.9574 (3.2376)	LR 2.800e-03
0: TRAIN [2][440/1885]	Time 0.147 (0.140)	Data 3.22e-04 (7.16e-04)	Tok/s 99239 (95823)	Loss/tok 3.2023 (3.2387)	LR 2.800e-03
0: TRAIN [2][450/1885]	Time 0.103 (0.140)	Data 2.18e-04 (7.05e-04)	Tok/s 89493 (95872)	Loss/tok 2.8833 (3.2403)	LR 2.800e-03
0: TRAIN [2][460/1885]	Time 0.059 (0.140)	Data 2.31e-04 (6.95e-04)	Tok/s 79153 (95830)	Loss/tok 2.5178 (3.2410)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][470/1885]	Time 0.148 (0.141)	Data 2.77e-04 (6.85e-04)	Tok/s 98739 (95906)	Loss/tok 3.1718 (3.2436)	LR 2.800e-03
0: TRAIN [2][480/1885]	Time 0.101 (0.141)	Data 2.18e-04 (6.75e-04)	Tok/s 91749 (95932)	Loss/tok 2.9619 (3.2442)	LR 2.800e-03
0: TRAIN [2][490/1885]	Time 0.101 (0.141)	Data 2.18e-04 (6.66e-04)	Tok/s 89014 (95944)	Loss/tok 3.1035 (3.2454)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][500/1885]	Time 0.101 (0.141)	Data 2.18e-04 (6.57e-04)	Tok/s 91459 (95995)	Loss/tok 2.9989 (3.2462)	LR 2.800e-03
0: TRAIN [2][510/1885]	Time 0.102 (0.141)	Data 2.17e-04 (6.49e-04)	Tok/s 90849 (96003)	Loss/tok 3.0691 (3.2460)	LR 2.800e-03
0: TRAIN [2][520/1885]	Time 0.101 (0.141)	Data 2.20e-04 (6.41e-04)	Tok/s 88203 (96039)	Loss/tok 2.8833 (3.2450)	LR 2.800e-03
0: TRAIN [2][530/1885]	Time 0.194 (0.142)	Data 2.20e-04 (6.33e-04)	Tok/s 103828 (96172)	Loss/tok 3.5093 (3.2498)	LR 2.800e-03
0: TRAIN [2][540/1885]	Time 0.146 (0.142)	Data 2.15e-04 (6.25e-04)	Tok/s 100322 (96132)	Loss/tok 3.2646 (3.2481)	LR 2.800e-03
0: TRAIN [2][550/1885]	Time 0.246 (0.142)	Data 2.21e-04 (6.18e-04)	Tok/s 105908 (96150)	Loss/tok 3.5742 (3.2510)	LR 2.800e-03
0: TRAIN [2][560/1885]	Time 0.102 (0.142)	Data 2.20e-04 (6.11e-04)	Tok/s 88736 (96124)	Loss/tok 3.0704 (3.2510)	LR 2.800e-03
0: TRAIN [2][570/1885]	Time 0.060 (0.142)	Data 2.33e-04 (6.04e-04)	Tok/s 77020 (96185)	Loss/tok 2.4716 (3.2523)	LR 2.800e-03
0: TRAIN [2][580/1885]	Time 0.146 (0.142)	Data 2.22e-04 (5.97e-04)	Tok/s 98413 (96223)	Loss/tok 3.3369 (3.2518)	LR 2.800e-03
0: TRAIN [2][590/1885]	Time 0.102 (0.142)	Data 2.34e-04 (5.91e-04)	Tok/s 87972 (96159)	Loss/tok 2.9577 (3.2503)	LR 2.800e-03
0: TRAIN [2][600/1885]	Time 0.059 (0.142)	Data 2.18e-04 (5.85e-04)	Tok/s 80100 (96169)	Loss/tok 2.5401 (3.2507)	LR 2.800e-03
0: TRAIN [2][610/1885]	Time 0.193 (0.142)	Data 2.16e-04 (5.79e-04)	Tok/s 105447 (96228)	Loss/tok 3.3959 (3.2526)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][620/1885]	Time 0.102 (0.142)	Data 2.21e-04 (5.74e-04)	Tok/s 88648 (96195)	Loss/tok 3.1086 (3.2524)	LR 2.800e-03
0: TRAIN [2][630/1885]	Time 0.147 (0.142)	Data 2.41e-04 (5.68e-04)	Tok/s 100307 (96197)	Loss/tok 3.2545 (3.2533)	LR 2.800e-03
0: TRAIN [2][640/1885]	Time 0.102 (0.142)	Data 2.20e-04 (5.63e-04)	Tok/s 90312 (96196)	Loss/tok 3.0812 (3.2528)	LR 2.800e-03
0: TRAIN [2][650/1885]	Time 0.059 (0.142)	Data 2.18e-04 (5.57e-04)	Tok/s 75923 (96107)	Loss/tok 2.4465 (3.2515)	LR 2.800e-03
0: TRAIN [2][660/1885]	Time 0.193 (0.141)	Data 2.19e-04 (5.52e-04)	Tok/s 106200 (96062)	Loss/tok 3.4598 (3.2509)	LR 2.800e-03
0: TRAIN [2][670/1885]	Time 0.193 (0.141)	Data 2.21e-04 (5.47e-04)	Tok/s 105808 (96089)	Loss/tok 3.4725 (3.2513)	LR 2.800e-03
0: TRAIN [2][680/1885]	Time 0.101 (0.141)	Data 2.21e-04 (5.43e-04)	Tok/s 87930 (96106)	Loss/tok 2.9347 (3.2502)	LR 2.800e-03
0: TRAIN [2][690/1885]	Time 0.194 (0.141)	Data 2.19e-04 (5.38e-04)	Tok/s 104341 (96128)	Loss/tok 3.3869 (3.2502)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][700/1885]	Time 0.102 (0.141)	Data 2.20e-04 (5.33e-04)	Tok/s 87559 (96132)	Loss/tok 3.1372 (3.2508)	LR 2.800e-03
0: TRAIN [2][710/1885]	Time 0.060 (0.142)	Data 2.17e-04 (5.29e-04)	Tok/s 78086 (96153)	Loss/tok 2.5411 (3.2511)	LR 2.800e-03
0: TRAIN [2][720/1885]	Time 0.101 (0.142)	Data 2.21e-04 (5.25e-04)	Tok/s 90149 (96171)	Loss/tok 3.0191 (3.2506)	LR 2.800e-03
0: TRAIN [2][730/1885]	Time 0.146 (0.142)	Data 3.23e-04 (5.21e-04)	Tok/s 98118 (96154)	Loss/tok 3.3141 (3.2508)	LR 2.800e-03
0: TRAIN [2][740/1885]	Time 0.147 (0.141)	Data 2.17e-04 (5.17e-04)	Tok/s 99578 (96117)	Loss/tok 3.1721 (3.2504)	LR 2.800e-03
0: TRAIN [2][750/1885]	Time 0.192 (0.142)	Data 2.19e-04 (5.13e-04)	Tok/s 105971 (96134)	Loss/tok 3.4381 (3.2513)	LR 2.800e-03
0: TRAIN [2][760/1885]	Time 0.101 (0.142)	Data 2.17e-04 (5.09e-04)	Tok/s 88816 (96149)	Loss/tok 3.1171 (3.2516)	LR 2.800e-03
0: TRAIN [2][770/1885]	Time 0.101 (0.142)	Data 2.20e-04 (5.05e-04)	Tok/s 88275 (96174)	Loss/tok 3.0099 (3.2523)	LR 2.800e-03
0: TRAIN [2][780/1885]	Time 0.147 (0.142)	Data 2.19e-04 (5.02e-04)	Tok/s 98782 (96202)	Loss/tok 3.1556 (3.2537)	LR 2.800e-03
0: TRAIN [2][790/1885]	Time 0.101 (0.142)	Data 3.30e-04 (4.98e-04)	Tok/s 90354 (96161)	Loss/tok 2.9672 (3.2520)	LR 2.800e-03
0: TRAIN [2][800/1885]	Time 0.102 (0.142)	Data 2.16e-04 (4.95e-04)	Tok/s 90265 (96147)	Loss/tok 2.9977 (3.2508)	LR 2.800e-03
0: TRAIN [2][810/1885]	Time 0.102 (0.141)	Data 2.19e-04 (4.92e-04)	Tok/s 88912 (96128)	Loss/tok 3.0384 (3.2503)	LR 2.800e-03
0: TRAIN [2][820/1885]	Time 0.101 (0.141)	Data 2.18e-04 (4.88e-04)	Tok/s 90719 (96145)	Loss/tok 2.9615 (3.2499)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][830/1885]	Time 0.102 (0.142)	Data 1.91e-04 (4.85e-04)	Tok/s 89099 (96193)	Loss/tok 3.0081 (3.2514)	LR 2.800e-03
0: TRAIN [2][840/1885]	Time 0.146 (0.142)	Data 2.18e-04 (4.82e-04)	Tok/s 101320 (96192)	Loss/tok 3.1936 (3.2516)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][850/1885]	Time 0.147 (0.142)	Data 2.19e-04 (4.79e-04)	Tok/s 99960 (96217)	Loss/tok 3.1931 (3.2526)	LR 2.800e-03
0: TRAIN [2][860/1885]	Time 0.245 (0.142)	Data 2.21e-04 (4.76e-04)	Tok/s 106095 (96205)	Loss/tok 3.5592 (3.2527)	LR 2.800e-03
0: TRAIN [2][870/1885]	Time 0.101 (0.142)	Data 2.18e-04 (4.73e-04)	Tok/s 89123 (96226)	Loss/tok 3.1756 (3.2530)	LR 2.800e-03
0: TRAIN [2][880/1885]	Time 0.148 (0.142)	Data 2.22e-04 (4.70e-04)	Tok/s 99223 (96227)	Loss/tok 3.3074 (3.2534)	LR 2.800e-03
0: TRAIN [2][890/1885]	Time 0.194 (0.142)	Data 2.19e-04 (4.68e-04)	Tok/s 104548 (96218)	Loss/tok 3.4113 (3.2530)	LR 2.800e-03
0: TRAIN [2][900/1885]	Time 0.102 (0.142)	Data 2.21e-04 (4.65e-04)	Tok/s 89550 (96171)	Loss/tok 2.9162 (3.2519)	LR 2.800e-03
0: TRAIN [2][910/1885]	Time 0.147 (0.142)	Data 2.19e-04 (4.62e-04)	Tok/s 99619 (96193)	Loss/tok 3.2027 (3.2517)	LR 2.800e-03
0: TRAIN [2][920/1885]	Time 0.147 (0.142)	Data 3.30e-04 (4.60e-04)	Tok/s 100539 (96249)	Loss/tok 3.3132 (3.2537)	LR 2.800e-03
0: TRAIN [2][930/1885]	Time 0.146 (0.142)	Data 2.17e-04 (4.57e-04)	Tok/s 100999 (96259)	Loss/tok 3.2548 (3.2540)	LR 2.800e-03
0: TRAIN [2][940/1885]	Time 0.102 (0.142)	Data 2.17e-04 (4.55e-04)	Tok/s 87417 (96232)	Loss/tok 2.9576 (3.2531)	LR 2.800e-03
0: TRAIN [2][950/1885]	Time 0.101 (0.142)	Data 2.18e-04 (4.52e-04)	Tok/s 90808 (96182)	Loss/tok 2.9326 (3.2523)	LR 2.800e-03
0: TRAIN [2][960/1885]	Time 0.146 (0.142)	Data 2.21e-04 (4.50e-04)	Tok/s 99762 (96189)	Loss/tok 3.2555 (3.2529)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][970/1885]	Time 0.195 (0.142)	Data 1.91e-04 (4.48e-04)	Tok/s 104499 (96189)	Loss/tok 3.3932 (3.2533)	LR 2.800e-03
0: TRAIN [2][980/1885]	Time 0.059 (0.142)	Data 2.03e-04 (4.45e-04)	Tok/s 76910 (96165)	Loss/tok 2.4711 (3.2526)	LR 2.800e-03
0: TRAIN [2][990/1885]	Time 0.193 (0.142)	Data 2.20e-04 (4.43e-04)	Tok/s 104440 (96187)	Loss/tok 3.5567 (3.2535)	LR 2.800e-03
0: TRAIN [2][1000/1885]	Time 0.101 (0.142)	Data 2.18e-04 (4.41e-04)	Tok/s 89279 (96160)	Loss/tok 3.0738 (3.2529)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1010/1885]	Time 0.145 (0.142)	Data 2.25e-04 (4.39e-04)	Tok/s 102745 (96175)	Loss/tok 3.3553 (3.2540)	LR 2.800e-03
0: TRAIN [2][1020/1885]	Time 0.194 (0.141)	Data 2.16e-04 (4.37e-04)	Tok/s 105736 (96129)	Loss/tok 3.4308 (3.2530)	LR 2.800e-03
0: TRAIN [2][1030/1885]	Time 0.101 (0.141)	Data 2.19e-04 (4.35e-04)	Tok/s 91803 (96155)	Loss/tok 3.0279 (3.2531)	LR 2.800e-03
0: TRAIN [2][1040/1885]	Time 0.194 (0.141)	Data 2.20e-04 (4.33e-04)	Tok/s 103719 (96140)	Loss/tok 3.5033 (3.2530)	LR 2.800e-03
0: TRAIN [2][1050/1885]	Time 0.101 (0.141)	Data 2.17e-04 (4.31e-04)	Tok/s 89123 (96150)	Loss/tok 3.0319 (3.2526)	LR 2.800e-03
0: TRAIN [2][1060/1885]	Time 0.146 (0.141)	Data 2.20e-04 (4.29e-04)	Tok/s 99270 (96183)	Loss/tok 3.2552 (3.2530)	LR 2.800e-03
0: TRAIN [2][1070/1885]	Time 0.101 (0.141)	Data 2.17e-04 (4.27e-04)	Tok/s 87531 (96190)	Loss/tok 3.0893 (3.2527)	LR 2.800e-03
0: TRAIN [2][1080/1885]	Time 0.102 (0.141)	Data 3.05e-04 (4.25e-04)	Tok/s 88698 (96216)	Loss/tok 3.0974 (3.2527)	LR 2.800e-03
0: TRAIN [2][1090/1885]	Time 0.194 (0.141)	Data 2.18e-04 (4.23e-04)	Tok/s 105369 (96229)	Loss/tok 3.3943 (3.2525)	LR 2.800e-03
0: TRAIN [2][1100/1885]	Time 0.193 (0.141)	Data 2.20e-04 (4.21e-04)	Tok/s 105698 (96249)	Loss/tok 3.4131 (3.2525)	LR 2.800e-03
0: TRAIN [2][1110/1885]	Time 0.102 (0.141)	Data 2.12e-04 (4.19e-04)	Tok/s 90291 (96264)	Loss/tok 2.9952 (3.2524)	LR 2.800e-03
0: TRAIN [2][1120/1885]	Time 0.146 (0.142)	Data 3.27e-04 (4.18e-04)	Tok/s 100278 (96297)	Loss/tok 3.1529 (3.2527)	LR 2.800e-03
0: TRAIN [2][1130/1885]	Time 0.194 (0.142)	Data 2.20e-04 (4.16e-04)	Tok/s 106106 (96310)	Loss/tok 3.4225 (3.2539)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1140/1885]	Time 0.194 (0.142)	Data 2.19e-04 (4.14e-04)	Tok/s 105819 (96277)	Loss/tok 3.4833 (3.2538)	LR 2.800e-03
0: TRAIN [2][1150/1885]	Time 0.194 (0.142)	Data 2.17e-04 (4.13e-04)	Tok/s 105513 (96262)	Loss/tok 3.3808 (3.2543)	LR 2.800e-03
0: TRAIN [2][1160/1885]	Time 0.101 (0.142)	Data 2.19e-04 (4.11e-04)	Tok/s 89471 (96258)	Loss/tok 2.9786 (3.2544)	LR 2.800e-03
0: TRAIN [2][1170/1885]	Time 0.148 (0.142)	Data 3.20e-04 (4.09e-04)	Tok/s 98730 (96291)	Loss/tok 3.2402 (3.2555)	LR 2.800e-03
0: TRAIN [2][1180/1885]	Time 0.146 (0.142)	Data 2.19e-04 (4.08e-04)	Tok/s 101074 (96327)	Loss/tok 3.2396 (3.2562)	LR 2.800e-03
0: TRAIN [2][1190/1885]	Time 0.194 (0.142)	Data 2.19e-04 (4.06e-04)	Tok/s 106242 (96314)	Loss/tok 3.3908 (3.2557)	LR 2.800e-03
0: TRAIN [2][1200/1885]	Time 0.193 (0.142)	Data 2.18e-04 (4.05e-04)	Tok/s 106332 (96306)	Loss/tok 3.4787 (3.2558)	LR 2.800e-03
0: TRAIN [2][1210/1885]	Time 0.102 (0.142)	Data 2.18e-04 (4.03e-04)	Tok/s 89788 (96330)	Loss/tok 3.1250 (3.2559)	LR 2.800e-03
0: TRAIN [2][1220/1885]	Time 0.244 (0.142)	Data 2.18e-04 (4.02e-04)	Tok/s 106318 (96369)	Loss/tok 3.6441 (3.2568)	LR 2.800e-03
0: TRAIN [2][1230/1885]	Time 0.146 (0.142)	Data 2.17e-04 (4.00e-04)	Tok/s 99480 (96342)	Loss/tok 3.1589 (3.2560)	LR 2.800e-03
0: TRAIN [2][1240/1885]	Time 0.146 (0.142)	Data 2.18e-04 (3.99e-04)	Tok/s 101089 (96377)	Loss/tok 3.3301 (3.2571)	LR 2.800e-03
0: TRAIN [2][1250/1885]	Time 0.194 (0.142)	Data 2.20e-04 (3.97e-04)	Tok/s 105940 (96407)	Loss/tok 3.2999 (3.2583)	LR 2.800e-03
0: TRAIN [2][1260/1885]	Time 0.147 (0.142)	Data 2.16e-04 (3.96e-04)	Tok/s 100298 (96423)	Loss/tok 3.2234 (3.2585)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1270/1885]	Time 0.147 (0.142)	Data 2.17e-04 (3.95e-04)	Tok/s 99707 (96426)	Loss/tok 3.2043 (3.2583)	LR 2.800e-03
0: TRAIN [2][1280/1885]	Time 0.146 (0.142)	Data 2.15e-04 (3.93e-04)	Tok/s 101146 (96433)	Loss/tok 3.1275 (3.2579)	LR 2.800e-03
0: TRAIN [2][1290/1885]	Time 0.146 (0.142)	Data 2.18e-04 (3.92e-04)	Tok/s 100854 (96427)	Loss/tok 3.2421 (3.2577)	LR 2.800e-03
0: TRAIN [2][1300/1885]	Time 0.102 (0.142)	Data 2.20e-04 (3.91e-04)	Tok/s 89468 (96426)	Loss/tok 3.0562 (3.2574)	LR 2.800e-03
0: TRAIN [2][1310/1885]	Time 0.146 (0.142)	Data 2.19e-04 (3.89e-04)	Tok/s 100875 (96452)	Loss/tok 3.1590 (3.2581)	LR 2.800e-03
0: TRAIN [2][1320/1885]	Time 0.245 (0.142)	Data 2.68e-04 (3.88e-04)	Tok/s 107368 (96454)	Loss/tok 3.5731 (3.2583)	LR 2.800e-03
0: TRAIN [2][1330/1885]	Time 0.100 (0.142)	Data 2.21e-04 (3.87e-04)	Tok/s 89008 (96437)	Loss/tok 2.8767 (3.2580)	LR 2.800e-03
0: TRAIN [2][1340/1885]	Time 0.146 (0.142)	Data 2.19e-04 (3.86e-04)	Tok/s 101018 (96470)	Loss/tok 3.1935 (3.2590)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1350/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.84e-04)	Tok/s 99673 (96477)	Loss/tok 3.2970 (3.2598)	LR 2.800e-03
0: TRAIN [2][1360/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.83e-04)	Tok/s 106777 (96508)	Loss/tok 3.4303 (3.2611)	LR 2.800e-03
0: TRAIN [2][1370/1885]	Time 0.059 (0.143)	Data 2.16e-04 (3.82e-04)	Tok/s 76854 (96521)	Loss/tok 2.4972 (3.2615)	LR 2.800e-03
0: TRAIN [2][1380/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.81e-04)	Tok/s 106575 (96532)	Loss/tok 3.2980 (3.2620)	LR 2.800e-03
0: TRAIN [2][1390/1885]	Time 0.243 (0.143)	Data 2.16e-04 (3.80e-04)	Tok/s 108777 (96556)	Loss/tok 3.4844 (3.2623)	LR 2.800e-03
0: TRAIN [2][1400/1885]	Time 0.146 (0.143)	Data 2.16e-04 (3.78e-04)	Tok/s 100336 (96533)	Loss/tok 3.2987 (3.2620)	LR 2.800e-03
0: TRAIN [2][1410/1885]	Time 0.146 (0.143)	Data 2.16e-04 (3.77e-04)	Tok/s 100241 (96527)	Loss/tok 3.1576 (3.2612)	LR 2.800e-03
0: TRAIN [2][1420/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.76e-04)	Tok/s 88237 (96525)	Loss/tok 2.9724 (3.2617)	LR 2.800e-03
0: TRAIN [2][1430/1885]	Time 0.146 (0.143)	Data 2.17e-04 (3.75e-04)	Tok/s 100076 (96542)	Loss/tok 3.1205 (3.2613)	LR 2.800e-03
0: TRAIN [2][1440/1885]	Time 0.145 (0.143)	Data 2.14e-04 (3.74e-04)	Tok/s 101204 (96551)	Loss/tok 3.4010 (3.2614)	LR 2.800e-03
0: TRAIN [2][1450/1885]	Time 0.145 (0.143)	Data 2.18e-04 (3.73e-04)	Tok/s 101008 (96567)	Loss/tok 3.2426 (3.2614)	LR 2.800e-03
0: TRAIN [2][1460/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.72e-04)	Tok/s 100367 (96562)	Loss/tok 3.3067 (3.2616)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1470/1885]	Time 0.192 (0.143)	Data 2.16e-04 (3.71e-04)	Tok/s 106586 (96568)	Loss/tok 3.3196 (3.2616)	LR 2.800e-03
0: TRAIN [2][1480/1885]	Time 0.194 (0.143)	Data 2.17e-04 (3.70e-04)	Tok/s 104854 (96590)	Loss/tok 3.3708 (3.2619)	LR 2.800e-03
0: TRAIN [2][1490/1885]	Time 0.101 (0.143)	Data 2.69e-04 (3.69e-04)	Tok/s 88464 (96567)	Loss/tok 3.0174 (3.2615)	LR 2.800e-03
0: TRAIN [2][1500/1885]	Time 0.193 (0.143)	Data 2.37e-04 (3.68e-04)	Tok/s 106397 (96562)	Loss/tok 3.2899 (3.2611)	LR 2.800e-03
0: TRAIN [2][1510/1885]	Time 0.145 (0.143)	Data 2.54e-04 (3.67e-04)	Tok/s 101675 (96569)	Loss/tok 3.1440 (3.2615)	LR 2.800e-03
0: TRAIN [2][1520/1885]	Time 0.146 (0.143)	Data 2.17e-04 (3.66e-04)	Tok/s 99228 (96575)	Loss/tok 3.1915 (3.2614)	LR 2.800e-03
0: TRAIN [2][1530/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.65e-04)	Tok/s 89646 (96583)	Loss/tok 3.0859 (3.2616)	LR 2.800e-03
0: TRAIN [2][1540/1885]	Time 0.243 (0.143)	Data 2.19e-04 (3.64e-04)	Tok/s 107305 (96602)	Loss/tok 3.4477 (3.2624)	LR 2.800e-03
0: TRAIN [2][1550/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.63e-04)	Tok/s 88484 (96626)	Loss/tok 2.9756 (3.2628)	LR 2.800e-03
0: TRAIN [2][1560/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.62e-04)	Tok/s 89814 (96657)	Loss/tok 2.9215 (3.2631)	LR 2.800e-03
0: TRAIN [2][1570/1885]	Time 0.145 (0.143)	Data 3.29e-04 (3.62e-04)	Tok/s 100697 (96681)	Loss/tok 3.2590 (3.2636)	LR 2.800e-03
0: TRAIN [2][1580/1885]	Time 0.194 (0.143)	Data 2.18e-04 (3.61e-04)	Tok/s 106213 (96671)	Loss/tok 3.2438 (3.2629)	LR 2.800e-03
0: TRAIN [2][1590/1885]	Time 0.059 (0.143)	Data 2.57e-04 (3.60e-04)	Tok/s 76843 (96660)	Loss/tok 2.5220 (3.2626)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1600/1885]	Time 0.146 (0.143)	Data 2.18e-04 (3.59e-04)	Tok/s 101997 (96664)	Loss/tok 3.2510 (3.2624)	LR 2.800e-03
0: TRAIN [2][1610/1885]	Time 0.145 (0.143)	Data 2.19e-04 (3.58e-04)	Tok/s 100594 (96666)	Loss/tok 3.1911 (3.2625)	LR 2.800e-03
0: TRAIN [2][1620/1885]	Time 0.060 (0.143)	Data 2.21e-04 (3.57e-04)	Tok/s 74896 (96659)	Loss/tok 2.3901 (3.2622)	LR 2.800e-03
0: TRAIN [2][1630/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.57e-04)	Tok/s 99616 (96645)	Loss/tok 3.1479 (3.2617)	LR 2.800e-03
0: TRAIN [2][1640/1885]	Time 0.100 (0.143)	Data 2.17e-04 (3.56e-04)	Tok/s 91002 (96631)	Loss/tok 2.9736 (3.2612)	LR 2.800e-03
0: TRAIN [2][1650/1885]	Time 0.146 (0.143)	Data 2.17e-04 (3.55e-04)	Tok/s 102075 (96643)	Loss/tok 3.2360 (3.2610)	LR 2.800e-03
0: TRAIN [2][1660/1885]	Time 0.193 (0.143)	Data 2.19e-04 (3.54e-04)	Tok/s 107473 (96657)	Loss/tok 3.4256 (3.2611)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1670/1885]	Time 0.194 (0.143)	Data 2.21e-04 (3.53e-04)	Tok/s 105019 (96651)	Loss/tok 3.3384 (3.2608)	LR 2.800e-03
0: TRAIN [2][1680/1885]	Time 0.147 (0.143)	Data 2.52e-04 (3.53e-04)	Tok/s 98392 (96638)	Loss/tok 3.1606 (3.2606)	LR 2.800e-03
0: TRAIN [2][1690/1885]	Time 0.245 (0.143)	Data 2.16e-04 (3.52e-04)	Tok/s 107310 (96652)	Loss/tok 3.4027 (3.2609)	LR 2.800e-03
0: TRAIN [2][1700/1885]	Time 0.100 (0.143)	Data 2.20e-04 (3.51e-04)	Tok/s 91813 (96641)	Loss/tok 2.9832 (3.2605)	LR 2.800e-03
0: TRAIN [2][1710/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.50e-04)	Tok/s 100082 (96611)	Loss/tok 3.2054 (3.2601)	LR 2.800e-03
0: TRAIN [2][1720/1885]	Time 0.060 (0.143)	Data 2.19e-04 (3.50e-04)	Tok/s 75212 (96588)	Loss/tok 2.4409 (3.2595)	LR 2.800e-03
0: TRAIN [2][1730/1885]	Time 0.147 (0.142)	Data 2.18e-04 (3.49e-04)	Tok/s 101051 (96563)	Loss/tok 3.2797 (3.2588)	LR 2.800e-03
0: TRAIN [2][1740/1885]	Time 0.101 (0.142)	Data 2.18e-04 (3.48e-04)	Tok/s 92054 (96548)	Loss/tok 3.0586 (3.2585)	LR 2.800e-03
0: TRAIN [2][1750/1885]	Time 0.147 (0.142)	Data 2.81e-04 (3.47e-04)	Tok/s 99887 (96556)	Loss/tok 3.2490 (3.2587)	LR 2.800e-03
0: TRAIN [2][1760/1885]	Time 0.145 (0.143)	Data 2.18e-04 (3.47e-04)	Tok/s 101107 (96589)	Loss/tok 3.1629 (3.2594)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1770/1885]	Time 0.145 (0.143)	Data 2.21e-04 (3.46e-04)	Tok/s 100748 (96599)	Loss/tok 3.2475 (3.2598)	LR 2.800e-03
0: TRAIN [2][1780/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.45e-04)	Tok/s 91221 (96628)	Loss/tok 3.0346 (3.2600)	LR 2.800e-03
0: TRAIN [2][1790/1885]	Time 0.146 (0.143)	Data 2.33e-04 (3.45e-04)	Tok/s 99307 (96652)	Loss/tok 3.2432 (3.2601)	LR 2.800e-03
0: TRAIN [2][1800/1885]	Time 0.193 (0.143)	Data 2.16e-04 (3.44e-04)	Tok/s 106400 (96675)	Loss/tok 3.4479 (3.2605)	LR 2.800e-03
0: TRAIN [2][1810/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.43e-04)	Tok/s 89322 (96677)	Loss/tok 2.8590 (3.2600)	LR 2.800e-03
0: TRAIN [2][1820/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.43e-04)	Tok/s 101439 (96672)	Loss/tok 3.1330 (3.2599)	LR 2.800e-03
0: TRAIN [2][1830/1885]	Time 0.145 (0.143)	Data 2.19e-04 (3.42e-04)	Tok/s 101422 (96672)	Loss/tok 3.1658 (3.2594)	LR 2.800e-03
0: TRAIN [2][1840/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.41e-04)	Tok/s 91390 (96665)	Loss/tok 3.0322 (3.2591)	LR 2.800e-03
0: TRAIN [2][1850/1885]	Time 0.193 (0.143)	Data 2.45e-04 (3.41e-04)	Tok/s 104910 (96668)	Loss/tok 3.3335 (3.2589)	LR 2.800e-03
0: TRAIN [2][1860/1885]	Time 0.193 (0.143)	Data 2.16e-04 (3.40e-04)	Tok/s 106049 (96668)	Loss/tok 3.3948 (3.2584)	LR 2.800e-03
0: TRAIN [2][1870/1885]	Time 0.195 (0.143)	Data 2.20e-04 (3.39e-04)	Tok/s 105339 (96649)	Loss/tok 3.3750 (3.2578)	LR 2.800e-03
0: TRAIN [2][1880/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.39e-04)	Tok/s 89398 (96642)	Loss/tok 2.8918 (3.2578)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593837616242, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593837616243, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.672 (0.672)	Decoder iters 149.0 (149.0)	Tok/s 24679 (24679)
0: Running moses detokenizer
0: BLEU(score=22.950821360084763, counts=[36254, 17740, 9919, 5737], totals=[64862, 61859, 58856, 55858], precisions=[55.89405198729611, 28.678122827721108, 16.852997145575642, 10.270686383329156], bp=1.0, sys_len=64862, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593837617898, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22949999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593837617899, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2571	Test BLEU: 22.95
0: Performance: Epoch: 2	Training: 773097 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593837617899, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593837617899, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593837617899, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1156082410
0: TRAIN [3][0/1885]	Time 0.344 (0.344)	Data 2.32e-01 (2.32e-01)	Tok/s 26507 (26507)	Loss/tok 2.8865 (2.8865)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][10/1885]	Time 0.059 (0.170)	Data 2.36e-04 (2.13e-02)	Tok/s 76001 (91251)	Loss/tok 2.4292 (3.1815)	LR 2.800e-03
0: TRAIN [3][20/1885]	Time 0.193 (0.159)	Data 2.21e-04 (1.12e-02)	Tok/s 105161 (94176)	Loss/tok 3.3058 (3.1714)	LR 2.800e-03
0: TRAIN [3][30/1885]	Time 0.244 (0.163)	Data 2.20e-04 (7.69e-03)	Tok/s 107336 (95913)	Loss/tok 3.2367 (3.2082)	LR 2.800e-03
0: TRAIN [3][40/1885]	Time 0.195 (0.161)	Data 2.22e-04 (5.87e-03)	Tok/s 104956 (96610)	Loss/tok 3.2937 (3.2026)	LR 2.800e-03
0: TRAIN [3][50/1885]	Time 0.101 (0.151)	Data 2.21e-04 (4.76e-03)	Tok/s 89029 (95627)	Loss/tok 3.0840 (3.1765)	LR 2.800e-03
0: TRAIN [3][60/1885]	Time 0.194 (0.150)	Data 2.32e-04 (4.02e-03)	Tok/s 105894 (95803)	Loss/tok 3.2588 (3.1782)	LR 2.800e-03
0: TRAIN [3][70/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.48e-03)	Tok/s 91344 (95028)	Loss/tok 2.9096 (3.1590)	LR 2.800e-03
0: TRAIN [3][80/1885]	Time 0.060 (0.141)	Data 2.21e-04 (3.08e-03)	Tok/s 79097 (94676)	Loss/tok 2.4469 (3.1503)	LR 2.800e-03
0: TRAIN [3][90/1885]	Time 0.103 (0.140)	Data 2.21e-04 (2.76e-03)	Tok/s 89614 (94592)	Loss/tok 2.8628 (3.1443)	LR 2.800e-03
0: TRAIN [3][100/1885]	Time 0.101 (0.140)	Data 2.22e-04 (2.51e-03)	Tok/s 90503 (94980)	Loss/tok 2.9649 (3.1398)	LR 2.800e-03
0: TRAIN [3][110/1885]	Time 0.101 (0.142)	Data 2.22e-04 (2.31e-03)	Tok/s 89836 (95361)	Loss/tok 2.9476 (3.1478)	LR 2.800e-03
0: TRAIN [3][120/1885]	Time 0.101 (0.143)	Data 2.22e-04 (2.13e-03)	Tok/s 91900 (95608)	Loss/tok 2.8387 (3.1511)	LR 2.800e-03
0: TRAIN [3][130/1885]	Time 0.148 (0.144)	Data 1.51e-04 (1.99e-03)	Tok/s 98698 (95918)	Loss/tok 3.1831 (3.1615)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][140/1885]	Time 0.147 (0.144)	Data 2.26e-04 (1.86e-03)	Tok/s 100113 (95686)	Loss/tok 3.1650 (3.1609)	LR 2.800e-03
0: TRAIN [3][150/1885]	Time 0.146 (0.143)	Data 2.21e-04 (1.75e-03)	Tok/s 100919 (95638)	Loss/tok 3.0289 (3.1585)	LR 2.800e-03
0: TRAIN [3][160/1885]	Time 0.146 (0.142)	Data 2.19e-04 (1.66e-03)	Tok/s 100704 (95297)	Loss/tok 3.1925 (3.1591)	LR 2.800e-03
0: TRAIN [3][170/1885]	Time 0.104 (0.141)	Data 2.20e-04 (1.57e-03)	Tok/s 85763 (95142)	Loss/tok 2.9025 (3.1540)	LR 2.800e-03
0: TRAIN [3][180/1885]	Time 0.146 (0.140)	Data 2.20e-04 (1.50e-03)	Tok/s 100982 (95118)	Loss/tok 3.2684 (3.1507)	LR 2.800e-03
0: TRAIN [3][190/1885]	Time 0.195 (0.141)	Data 2.04e-04 (1.43e-03)	Tok/s 104181 (95351)	Loss/tok 3.3195 (3.1579)	LR 2.800e-03
0: TRAIN [3][200/1885]	Time 0.244 (0.142)	Data 2.21e-04 (1.37e-03)	Tok/s 106508 (95359)	Loss/tok 3.4668 (3.1611)	LR 2.800e-03
0: TRAIN [3][210/1885]	Time 0.148 (0.142)	Data 2.27e-04 (1.32e-03)	Tok/s 99884 (95529)	Loss/tok 3.1529 (3.1623)	LR 2.800e-03
0: TRAIN [3][220/1885]	Time 0.146 (0.142)	Data 2.17e-04 (1.27e-03)	Tok/s 101065 (95500)	Loss/tok 3.0906 (3.1635)	LR 2.800e-03
0: TRAIN [3][230/1885]	Time 0.195 (0.143)	Data 2.23e-04 (1.22e-03)	Tok/s 104736 (95700)	Loss/tok 3.3038 (3.1673)	LR 2.800e-03
0: TRAIN [3][240/1885]	Time 0.148 (0.144)	Data 2.20e-04 (1.18e-03)	Tok/s 98102 (95814)	Loss/tok 3.1127 (3.1699)	LR 2.800e-03
0: TRAIN [3][250/1885]	Time 0.146 (0.144)	Data 2.24e-04 (1.14e-03)	Tok/s 100109 (95893)	Loss/tok 3.1033 (3.1723)	LR 2.800e-03
0: TRAIN [3][260/1885]	Time 0.145 (0.145)	Data 2.21e-04 (1.11e-03)	Tok/s 101113 (95968)	Loss/tok 3.1676 (3.1733)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][270/1885]	Time 0.102 (0.144)	Data 2.21e-04 (1.07e-03)	Tok/s 88643 (95899)	Loss/tok 3.0375 (3.1701)	LR 2.800e-03
0: TRAIN [3][280/1885]	Time 0.195 (0.144)	Data 2.21e-04 (1.04e-03)	Tok/s 103170 (95966)	Loss/tok 3.3562 (3.1720)	LR 2.800e-03
0: TRAIN [3][290/1885]	Time 0.148 (0.144)	Data 2.20e-04 (1.01e-03)	Tok/s 97878 (96033)	Loss/tok 3.1665 (3.1730)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][300/1885]	Time 0.194 (0.145)	Data 2.23e-04 (9.87e-04)	Tok/s 103990 (96070)	Loss/tok 3.3756 (3.1757)	LR 2.800e-03
0: TRAIN [3][310/1885]	Time 0.147 (0.145)	Data 2.21e-04 (9.63e-04)	Tok/s 99073 (96160)	Loss/tok 3.0162 (3.1763)	LR 2.800e-03
0: TRAIN [3][320/1885]	Time 0.102 (0.145)	Data 2.21e-04 (9.40e-04)	Tok/s 89181 (96112)	Loss/tok 2.9176 (3.1749)	LR 2.800e-03
0: TRAIN [3][330/1885]	Time 0.195 (0.145)	Data 2.23e-04 (9.18e-04)	Tok/s 104156 (96133)	Loss/tok 3.2527 (3.1746)	LR 2.800e-03
0: TRAIN [3][340/1885]	Time 0.147 (0.145)	Data 1.91e-04 (8.97e-04)	Tok/s 100571 (96119)	Loss/tok 3.1237 (3.1724)	LR 2.800e-03
0: TRAIN [3][350/1885]	Time 0.103 (0.145)	Data 2.20e-04 (8.78e-04)	Tok/s 88496 (96142)	Loss/tok 2.8353 (3.1735)	LR 2.800e-03
0: TRAIN [3][360/1885]	Time 0.148 (0.144)	Data 2.18e-04 (8.59e-04)	Tok/s 98438 (96129)	Loss/tok 3.1881 (3.1730)	LR 2.800e-03
0: TRAIN [3][370/1885]	Time 0.196 (0.145)	Data 2.21e-04 (8.42e-04)	Tok/s 104266 (96164)	Loss/tok 3.2388 (3.1733)	LR 2.800e-03
0: TRAIN [3][380/1885]	Time 0.145 (0.144)	Data 2.22e-04 (8.25e-04)	Tok/s 101032 (96095)	Loss/tok 3.1678 (3.1714)	LR 2.800e-03
0: TRAIN [3][390/1885]	Time 0.147 (0.144)	Data 2.23e-04 (8.10e-04)	Tok/s 98677 (96106)	Loss/tok 3.1342 (3.1732)	LR 2.800e-03
0: TRAIN [3][400/1885]	Time 0.196 (0.144)	Data 2.23e-04 (7.95e-04)	Tok/s 102800 (96081)	Loss/tok 3.4004 (3.1737)	LR 2.800e-03
0: TRAIN [3][410/1885]	Time 0.102 (0.145)	Data 2.22e-04 (7.81e-04)	Tok/s 87827 (96209)	Loss/tok 2.8899 (3.1762)	LR 2.800e-03
0: TRAIN [3][420/1885]	Time 0.147 (0.145)	Data 2.18e-04 (7.68e-04)	Tok/s 99581 (96252)	Loss/tok 3.1319 (3.1768)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][430/1885]	Time 0.102 (0.145)	Data 2.19e-04 (7.55e-04)	Tok/s 90375 (96356)	Loss/tok 3.0467 (3.1785)	LR 2.800e-03
0: TRAIN [3][440/1885]	Time 0.103 (0.145)	Data 1.72e-04 (7.43e-04)	Tok/s 87660 (96285)	Loss/tok 2.9731 (3.1774)	LR 1.400e-03
0: TRAIN [3][450/1885]	Time 0.148 (0.145)	Data 2.27e-04 (7.31e-04)	Tok/s 99547 (96248)	Loss/tok 3.2381 (3.1760)	LR 1.400e-03
0: TRAIN [3][460/1885]	Time 0.194 (0.145)	Data 2.19e-04 (7.20e-04)	Tok/s 106494 (96210)	Loss/tok 3.1576 (3.1756)	LR 1.400e-03
0: TRAIN [3][470/1885]	Time 0.245 (0.145)	Data 2.21e-04 (7.09e-04)	Tok/s 106203 (96308)	Loss/tok 3.4651 (3.1773)	LR 1.400e-03
0: TRAIN [3][480/1885]	Time 0.195 (0.145)	Data 2.21e-04 (6.99e-04)	Tok/s 104921 (96285)	Loss/tok 3.4053 (3.1761)	LR 1.400e-03
0: TRAIN [3][490/1885]	Time 0.195 (0.145)	Data 2.33e-04 (6.89e-04)	Tok/s 104927 (96350)	Loss/tok 3.2402 (3.1770)	LR 1.400e-03
0: TRAIN [3][500/1885]	Time 0.104 (0.145)	Data 2.22e-04 (6.80e-04)	Tok/s 86789 (96309)	Loss/tok 2.8984 (3.1757)	LR 1.400e-03
0: TRAIN [3][510/1885]	Time 0.193 (0.145)	Data 2.22e-04 (6.70e-04)	Tok/s 105834 (96369)	Loss/tok 3.1878 (3.1766)	LR 1.400e-03
0: TRAIN [3][520/1885]	Time 0.193 (0.146)	Data 2.32e-04 (6.62e-04)	Tok/s 105351 (96459)	Loss/tok 3.2905 (3.1778)	LR 1.400e-03
0: TRAIN [3][530/1885]	Time 0.102 (0.146)	Data 2.20e-04 (6.53e-04)	Tok/s 88582 (96501)	Loss/tok 2.9481 (3.1769)	LR 1.400e-03
0: TRAIN [3][540/1885]	Time 0.147 (0.146)	Data 2.20e-04 (6.45e-04)	Tok/s 100844 (96554)	Loss/tok 3.1418 (3.1764)	LR 1.400e-03
0: TRAIN [3][550/1885]	Time 0.101 (0.146)	Data 2.27e-04 (6.38e-04)	Tok/s 90164 (96479)	Loss/tok 2.8849 (3.1753)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1885]	Time 0.147 (0.146)	Data 2.24e-04 (6.30e-04)	Tok/s 100263 (96548)	Loss/tok 3.1181 (3.1752)	LR 1.400e-03
0: TRAIN [3][570/1885]	Time 0.059 (0.145)	Data 2.24e-04 (6.23e-04)	Tok/s 77364 (96507)	Loss/tok 2.3817 (3.1738)	LR 1.400e-03
0: TRAIN [3][580/1885]	Time 0.102 (0.145)	Data 2.24e-04 (6.16e-04)	Tok/s 88483 (96472)	Loss/tok 2.9698 (3.1719)	LR 1.400e-03
0: TRAIN [3][590/1885]	Time 0.102 (0.145)	Data 2.21e-04 (6.09e-04)	Tok/s 86352 (96503)	Loss/tok 2.9369 (3.1730)	LR 1.400e-03
0: TRAIN [3][600/1885]	Time 0.146 (0.145)	Data 2.21e-04 (6.03e-04)	Tok/s 99739 (96518)	Loss/tok 3.2666 (3.1751)	LR 1.400e-03
0: TRAIN [3][610/1885]	Time 0.147 (0.146)	Data 2.20e-04 (5.97e-04)	Tok/s 99809 (96553)	Loss/tok 3.0927 (3.1753)	LR 1.400e-03
0: TRAIN [3][620/1885]	Time 0.101 (0.145)	Data 2.22e-04 (5.91e-04)	Tok/s 91220 (96491)	Loss/tok 2.9324 (3.1731)	LR 1.400e-03
0: TRAIN [3][630/1885]	Time 0.193 (0.145)	Data 2.17e-04 (5.85e-04)	Tok/s 105934 (96490)	Loss/tok 3.2734 (3.1721)	LR 1.400e-03
0: TRAIN [3][640/1885]	Time 0.102 (0.145)	Data 2.22e-04 (5.79e-04)	Tok/s 90085 (96479)	Loss/tok 2.8539 (3.1713)	LR 1.400e-03
0: TRAIN [3][650/1885]	Time 0.245 (0.145)	Data 2.22e-04 (5.74e-04)	Tok/s 106775 (96535)	Loss/tok 3.3663 (3.1720)	LR 1.400e-03
0: TRAIN [3][660/1885]	Time 0.101 (0.145)	Data 2.20e-04 (5.68e-04)	Tok/s 90103 (96472)	Loss/tok 2.8521 (3.1700)	LR 1.400e-03
0: TRAIN [3][670/1885]	Time 0.147 (0.145)	Data 2.23e-04 (5.63e-04)	Tok/s 101054 (96500)	Loss/tok 3.0594 (3.1691)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][680/1885]	Time 0.060 (0.144)	Data 2.05e-04 (5.58e-04)	Tok/s 76331 (96445)	Loss/tok 2.4264 (3.1675)	LR 1.400e-03
0: TRAIN [3][690/1885]	Time 0.101 (0.145)	Data 2.20e-04 (5.53e-04)	Tok/s 91051 (96473)	Loss/tok 2.8836 (3.1688)	LR 1.400e-03
0: TRAIN [3][700/1885]	Time 0.102 (0.145)	Data 2.21e-04 (5.48e-04)	Tok/s 87982 (96489)	Loss/tok 2.8983 (3.1683)	LR 1.400e-03
0: TRAIN [3][710/1885]	Time 0.195 (0.145)	Data 2.21e-04 (5.44e-04)	Tok/s 104269 (96471)	Loss/tok 3.2062 (3.1668)	LR 1.400e-03
0: TRAIN [3][720/1885]	Time 0.146 (0.144)	Data 2.24e-04 (5.39e-04)	Tok/s 99188 (96477)	Loss/tok 3.2112 (3.1658)	LR 1.400e-03
0: TRAIN [3][730/1885]	Time 0.101 (0.144)	Data 2.23e-04 (5.35e-04)	Tok/s 89156 (96481)	Loss/tok 2.8258 (3.1653)	LR 1.400e-03
0: TRAIN [3][740/1885]	Time 0.101 (0.144)	Data 2.20e-04 (5.31e-04)	Tok/s 89020 (96503)	Loss/tok 2.8815 (3.1650)	LR 1.400e-03
0: TRAIN [3][750/1885]	Time 0.246 (0.145)	Data 2.25e-04 (5.27e-04)	Tok/s 105809 (96516)	Loss/tok 3.4961 (3.1660)	LR 1.400e-03
0: TRAIN [3][760/1885]	Time 0.100 (0.145)	Data 2.22e-04 (5.23e-04)	Tok/s 89940 (96531)	Loss/tok 2.9646 (3.1662)	LR 1.400e-03
0: TRAIN [3][770/1885]	Time 0.101 (0.145)	Data 2.22e-04 (5.19e-04)	Tok/s 88422 (96557)	Loss/tok 2.9947 (3.1663)	LR 1.400e-03
0: TRAIN [3][780/1885]	Time 0.102 (0.145)	Data 2.23e-04 (5.15e-04)	Tok/s 89786 (96527)	Loss/tok 2.7334 (3.1657)	LR 1.400e-03
0: TRAIN [3][790/1885]	Time 0.147 (0.145)	Data 2.19e-04 (5.11e-04)	Tok/s 99465 (96536)	Loss/tok 3.0332 (3.1660)	LR 1.400e-03
0: TRAIN [3][800/1885]	Time 0.102 (0.144)	Data 2.20e-04 (5.08e-04)	Tok/s 89522 (96447)	Loss/tok 2.9771 (3.1645)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][810/1885]	Time 0.147 (0.144)	Data 2.22e-04 (5.04e-04)	Tok/s 98316 (96429)	Loss/tok 3.1672 (3.1647)	LR 1.400e-03
0: TRAIN [3][820/1885]	Time 0.102 (0.144)	Data 1.82e-04 (5.00e-04)	Tok/s 90731 (96415)	Loss/tok 2.9642 (3.1643)	LR 1.400e-03
0: TRAIN [3][830/1885]	Time 0.101 (0.144)	Data 2.21e-04 (4.97e-04)	Tok/s 91444 (96445)	Loss/tok 2.9831 (3.1656)	LR 1.400e-03
0: TRAIN [3][840/1885]	Time 0.148 (0.144)	Data 2.20e-04 (4.94e-04)	Tok/s 96998 (96434)	Loss/tok 3.2334 (3.1646)	LR 1.400e-03
0: TRAIN [3][850/1885]	Time 0.194 (0.144)	Data 2.20e-04 (4.91e-04)	Tok/s 104741 (96445)	Loss/tok 3.2817 (3.1641)	LR 1.400e-03
0: TRAIN [3][860/1885]	Time 0.245 (0.144)	Data 2.20e-04 (4.87e-04)	Tok/s 107151 (96446)	Loss/tok 3.4818 (3.1644)	LR 1.400e-03
0: TRAIN [3][870/1885]	Time 0.147 (0.144)	Data 2.26e-04 (4.84e-04)	Tok/s 99033 (96448)	Loss/tok 3.1528 (3.1632)	LR 1.400e-03
0: TRAIN [3][880/1885]	Time 0.195 (0.144)	Data 2.26e-04 (4.81e-04)	Tok/s 103926 (96437)	Loss/tok 3.1467 (3.1626)	LR 1.400e-03
0: TRAIN [3][890/1885]	Time 0.147 (0.144)	Data 2.22e-04 (4.78e-04)	Tok/s 100335 (96446)	Loss/tok 3.1841 (3.1623)	LR 1.400e-03
0: TRAIN [3][900/1885]	Time 0.147 (0.144)	Data 2.21e-04 (4.75e-04)	Tok/s 101394 (96437)	Loss/tok 3.0732 (3.1610)	LR 1.400e-03
0: TRAIN [3][910/1885]	Time 0.193 (0.144)	Data 2.21e-04 (4.73e-04)	Tok/s 106684 (96465)	Loss/tok 3.2311 (3.1611)	LR 1.400e-03
0: TRAIN [3][920/1885]	Time 0.146 (0.144)	Data 2.22e-04 (4.70e-04)	Tok/s 100211 (96458)	Loss/tok 3.0862 (3.1607)	LR 1.400e-03
0: TRAIN [3][930/1885]	Time 0.102 (0.144)	Data 2.23e-04 (4.67e-04)	Tok/s 90431 (96458)	Loss/tok 2.9008 (3.1599)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][940/1885]	Time 0.146 (0.144)	Data 2.23e-04 (4.65e-04)	Tok/s 99475 (96476)	Loss/tok 3.1018 (3.1604)	LR 1.400e-03
0: TRAIN [3][950/1885]	Time 0.101 (0.144)	Data 2.20e-04 (4.62e-04)	Tok/s 86361 (96482)	Loss/tok 2.7762 (3.1597)	LR 1.400e-03
0: TRAIN [3][960/1885]	Time 0.148 (0.144)	Data 2.19e-04 (4.60e-04)	Tok/s 100277 (96488)	Loss/tok 3.0793 (3.1594)	LR 1.400e-03
0: TRAIN [3][970/1885]	Time 0.101 (0.144)	Data 2.27e-04 (4.57e-04)	Tok/s 88761 (96477)	Loss/tok 2.9639 (3.1593)	LR 1.400e-03
0: TRAIN [3][980/1885]	Time 0.194 (0.144)	Data 2.33e-04 (4.55e-04)	Tok/s 104285 (96515)	Loss/tok 3.2208 (3.1591)	LR 1.400e-03
0: TRAIN [3][990/1885]	Time 0.102 (0.144)	Data 2.21e-04 (4.52e-04)	Tok/s 87043 (96498)	Loss/tok 2.8924 (3.1583)	LR 1.400e-03
0: TRAIN [3][1000/1885]	Time 0.194 (0.144)	Data 2.21e-04 (4.50e-04)	Tok/s 106155 (96516)	Loss/tok 3.1969 (3.1575)	LR 1.400e-03
0: TRAIN [3][1010/1885]	Time 0.146 (0.144)	Data 2.21e-04 (4.48e-04)	Tok/s 98907 (96544)	Loss/tok 3.1297 (3.1575)	LR 1.400e-03
0: TRAIN [3][1020/1885]	Time 0.102 (0.144)	Data 2.21e-04 (4.46e-04)	Tok/s 86804 (96494)	Loss/tok 2.8685 (3.1567)	LR 1.400e-03
0: TRAIN [3][1030/1885]	Time 0.194 (0.144)	Data 2.21e-04 (4.43e-04)	Tok/s 105659 (96535)	Loss/tok 3.2412 (3.1577)	LR 1.400e-03
0: TRAIN [3][1040/1885]	Time 0.102 (0.144)	Data 1.91e-04 (4.41e-04)	Tok/s 86698 (96478)	Loss/tok 2.9986 (3.1572)	LR 1.400e-03
0: TRAIN [3][1050/1885]	Time 0.146 (0.144)	Data 2.27e-04 (4.39e-04)	Tok/s 102310 (96438)	Loss/tok 3.1390 (3.1559)	LR 1.400e-03
0: TRAIN [3][1060/1885]	Time 0.147 (0.144)	Data 2.20e-04 (4.37e-04)	Tok/s 100202 (96440)	Loss/tok 3.1062 (3.1555)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1070/1885]	Time 0.101 (0.144)	Data 2.23e-04 (4.35e-04)	Tok/s 89149 (96430)	Loss/tok 2.8065 (3.1546)	LR 1.400e-03
0: TRAIN [3][1080/1885]	Time 0.101 (0.144)	Data 2.25e-04 (4.33e-04)	Tok/s 90409 (96452)	Loss/tok 3.0842 (3.1538)	LR 1.400e-03
0: TRAIN [3][1090/1885]	Time 0.102 (0.144)	Data 2.21e-04 (4.31e-04)	Tok/s 89355 (96470)	Loss/tok 2.8289 (3.1535)	LR 1.400e-03
0: TRAIN [3][1100/1885]	Time 0.194 (0.144)	Data 2.18e-04 (4.29e-04)	Tok/s 106042 (96511)	Loss/tok 3.2420 (3.1545)	LR 1.400e-03
0: TRAIN [3][1110/1885]	Time 0.102 (0.144)	Data 2.21e-04 (4.27e-04)	Tok/s 88751 (96482)	Loss/tok 2.8930 (3.1535)	LR 1.400e-03
0: TRAIN [3][1120/1885]	Time 0.102 (0.144)	Data 2.22e-04 (4.26e-04)	Tok/s 91512 (96475)	Loss/tok 2.8680 (3.1529)	LR 1.400e-03
0: TRAIN [3][1130/1885]	Time 0.146 (0.144)	Data 2.21e-04 (4.24e-04)	Tok/s 100117 (96464)	Loss/tok 3.2017 (3.1523)	LR 1.400e-03
0: TRAIN [3][1140/1885]	Time 0.059 (0.143)	Data 2.20e-04 (4.22e-04)	Tok/s 78827 (96434)	Loss/tok 2.4516 (3.1514)	LR 1.400e-03
0: TRAIN [3][1150/1885]	Time 0.101 (0.143)	Data 2.20e-04 (4.20e-04)	Tok/s 89465 (96421)	Loss/tok 2.8287 (3.1510)	LR 1.400e-03
0: TRAIN [3][1160/1885]	Time 0.102 (0.143)	Data 2.21e-04 (4.18e-04)	Tok/s 89741 (96400)	Loss/tok 2.9454 (3.1506)	LR 1.400e-03
0: TRAIN [3][1170/1885]	Time 0.245 (0.143)	Data 2.29e-04 (4.17e-04)	Tok/s 107141 (96431)	Loss/tok 3.2874 (3.1504)	LR 1.400e-03
0: TRAIN [3][1180/1885]	Time 0.101 (0.143)	Data 2.21e-04 (4.15e-04)	Tok/s 90030 (96388)	Loss/tok 2.7457 (3.1493)	LR 1.400e-03
0: TRAIN [3][1190/1885]	Time 0.101 (0.143)	Data 2.26e-04 (4.14e-04)	Tok/s 88929 (96378)	Loss/tok 2.9719 (3.1495)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1200/1885]	Time 0.245 (0.143)	Data 2.21e-04 (4.12e-04)	Tok/s 106685 (96383)	Loss/tok 3.3646 (3.1496)	LR 1.400e-03
0: TRAIN [3][1210/1885]	Time 0.193 (0.143)	Data 2.23e-04 (4.10e-04)	Tok/s 105211 (96364)	Loss/tok 3.2566 (3.1493)	LR 1.400e-03
0: TRAIN [3][1220/1885]	Time 0.101 (0.143)	Data 2.21e-04 (4.09e-04)	Tok/s 91322 (96350)	Loss/tok 2.8495 (3.1485)	LR 1.400e-03
0: TRAIN [3][1230/1885]	Time 0.102 (0.143)	Data 2.20e-04 (4.07e-04)	Tok/s 87960 (96357)	Loss/tok 2.8551 (3.1484)	LR 1.400e-03
0: TRAIN [3][1240/1885]	Time 0.147 (0.143)	Data 2.22e-04 (4.06e-04)	Tok/s 100174 (96366)	Loss/tok 3.0301 (3.1482)	LR 1.400e-03
0: TRAIN [3][1250/1885]	Time 0.146 (0.143)	Data 2.23e-04 (4.04e-04)	Tok/s 101195 (96382)	Loss/tok 3.1311 (3.1483)	LR 1.400e-03
0: TRAIN [3][1260/1885]	Time 0.192 (0.143)	Data 2.22e-04 (4.03e-04)	Tok/s 106169 (96410)	Loss/tok 3.2134 (3.1486)	LR 1.400e-03
0: TRAIN [3][1270/1885]	Time 0.102 (0.143)	Data 2.22e-04 (4.01e-04)	Tok/s 90774 (96444)	Loss/tok 2.9042 (3.1494)	LR 1.400e-03
0: TRAIN [3][1280/1885]	Time 0.146 (0.143)	Data 2.27e-04 (4.00e-04)	Tok/s 100880 (96454)	Loss/tok 3.1249 (3.1494)	LR 1.400e-03
0: TRAIN [3][1290/1885]	Time 0.101 (0.143)	Data 2.27e-04 (3.99e-04)	Tok/s 89750 (96438)	Loss/tok 2.9433 (3.1491)	LR 1.400e-03
0: TRAIN [3][1300/1885]	Time 0.101 (0.143)	Data 2.24e-04 (3.97e-04)	Tok/s 89761 (96448)	Loss/tok 2.8753 (3.1494)	LR 7.000e-04
0: TRAIN [3][1310/1885]	Time 0.102 (0.143)	Data 2.23e-04 (3.96e-04)	Tok/s 87860 (96441)	Loss/tok 2.8671 (3.1492)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1320/1885]	Time 0.148 (0.143)	Data 2.22e-04 (3.94e-04)	Tok/s 98771 (96437)	Loss/tok 3.1041 (3.1489)	LR 7.000e-04
0: TRAIN [3][1330/1885]	Time 0.147 (0.144)	Data 2.20e-04 (3.93e-04)	Tok/s 100796 (96465)	Loss/tok 2.9769 (3.1489)	LR 7.000e-04
0: TRAIN [3][1340/1885]	Time 0.102 (0.144)	Data 1.80e-04 (3.92e-04)	Tok/s 90835 (96462)	Loss/tok 2.9077 (3.1487)	LR 7.000e-04
0: TRAIN [3][1350/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.91e-04)	Tok/s 99123 (96428)	Loss/tok 3.1649 (3.1478)	LR 7.000e-04
0: TRAIN [3][1360/1885]	Time 0.245 (0.143)	Data 2.21e-04 (3.89e-04)	Tok/s 105111 (96430)	Loss/tok 3.4742 (3.1483)	LR 7.000e-04
0: TRAIN [3][1370/1885]	Time 0.194 (0.144)	Data 2.15e-04 (3.88e-04)	Tok/s 104809 (96465)	Loss/tok 3.3155 (3.1483)	LR 7.000e-04
0: TRAIN [3][1380/1885]	Time 0.147 (0.144)	Data 2.21e-04 (3.87e-04)	Tok/s 99113 (96474)	Loss/tok 3.0018 (3.1477)	LR 7.000e-04
0: TRAIN [3][1390/1885]	Time 0.102 (0.143)	Data 2.21e-04 (3.86e-04)	Tok/s 87098 (96449)	Loss/tok 2.9370 (3.1470)	LR 7.000e-04
0: TRAIN [3][1400/1885]	Time 0.147 (0.143)	Data 2.22e-04 (3.84e-04)	Tok/s 99626 (96455)	Loss/tok 3.0404 (3.1466)	LR 7.000e-04
0: TRAIN [3][1410/1885]	Time 0.146 (0.144)	Data 2.21e-04 (3.83e-04)	Tok/s 100240 (96459)	Loss/tok 3.1646 (3.1467)	LR 7.000e-04
0: TRAIN [3][1420/1885]	Time 0.194 (0.144)	Data 2.34e-04 (3.82e-04)	Tok/s 104737 (96475)	Loss/tok 3.2457 (3.1464)	LR 7.000e-04
0: TRAIN [3][1430/1885]	Time 0.245 (0.144)	Data 2.20e-04 (3.81e-04)	Tok/s 107735 (96479)	Loss/tok 3.4225 (3.1467)	LR 7.000e-04
0: TRAIN [3][1440/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.80e-04)	Tok/s 90785 (96469)	Loss/tok 2.9273 (3.1463)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1450/1885]	Time 0.245 (0.143)	Data 2.20e-04 (3.79e-04)	Tok/s 106511 (96458)	Loss/tok 3.4213 (3.1460)	LR 7.000e-04
0: TRAIN [3][1460/1885]	Time 0.101 (0.143)	Data 2.24e-04 (3.78e-04)	Tok/s 89396 (96458)	Loss/tok 2.8976 (3.1455)	LR 7.000e-04
0: TRAIN [3][1470/1885]	Time 0.193 (0.144)	Data 2.27e-04 (3.77e-04)	Tok/s 106119 (96490)	Loss/tok 3.2431 (3.1460)	LR 7.000e-04
0: TRAIN [3][1480/1885]	Time 0.146 (0.144)	Data 2.09e-04 (3.76e-04)	Tok/s 99318 (96479)	Loss/tok 3.0049 (3.1457)	LR 7.000e-04
0: TRAIN [3][1490/1885]	Time 0.245 (0.144)	Data 2.19e-04 (3.75e-04)	Tok/s 105998 (96464)	Loss/tok 3.5124 (3.1454)	LR 7.000e-04
0: TRAIN [3][1500/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.74e-04)	Tok/s 88754 (96458)	Loss/tok 2.9142 (3.1451)	LR 7.000e-04
0: TRAIN [3][1510/1885]	Time 0.195 (0.144)	Data 2.23e-04 (3.73e-04)	Tok/s 104308 (96480)	Loss/tok 3.2196 (3.1449)	LR 7.000e-04
0: TRAIN [3][1520/1885]	Time 0.193 (0.144)	Data 2.20e-04 (3.72e-04)	Tok/s 107758 (96493)	Loss/tok 3.1384 (3.1448)	LR 7.000e-04
0: TRAIN [3][1530/1885]	Time 0.147 (0.144)	Data 2.18e-04 (3.71e-04)	Tok/s 101213 (96531)	Loss/tok 3.0625 (3.1451)	LR 7.000e-04
0: TRAIN [3][1540/1885]	Time 0.146 (0.144)	Data 2.19e-04 (3.70e-04)	Tok/s 100984 (96504)	Loss/tok 3.1177 (3.1443)	LR 7.000e-04
0: TRAIN [3][1550/1885]	Time 0.101 (0.144)	Data 2.23e-04 (3.69e-04)	Tok/s 89242 (96500)	Loss/tok 2.8026 (3.1443)	LR 7.000e-04
0: TRAIN [3][1560/1885]	Time 0.147 (0.144)	Data 2.09e-04 (3.68e-04)	Tok/s 100074 (96511)	Loss/tok 3.1553 (3.1437)	LR 7.000e-04
0: TRAIN [3][1570/1885]	Time 0.146 (0.144)	Data 2.24e-04 (3.67e-04)	Tok/s 101158 (96531)	Loss/tok 3.0228 (3.1438)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1580/1885]	Time 0.148 (0.144)	Data 2.19e-04 (3.66e-04)	Tok/s 99831 (96506)	Loss/tok 3.0519 (3.1432)	LR 7.000e-04
0: TRAIN [3][1590/1885]	Time 0.060 (0.144)	Data 2.21e-04 (3.65e-04)	Tok/s 78593 (96511)	Loss/tok 2.3891 (3.1429)	LR 7.000e-04
0: TRAIN [3][1600/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.64e-04)	Tok/s 101133 (96498)	Loss/tok 2.9246 (3.1422)	LR 7.000e-04
0: TRAIN [3][1610/1885]	Time 0.101 (0.143)	Data 2.31e-04 (3.63e-04)	Tok/s 88607 (96493)	Loss/tok 2.8909 (3.1417)	LR 7.000e-04
0: TRAIN [3][1620/1885]	Time 0.102 (0.143)	Data 2.24e-04 (3.62e-04)	Tok/s 89721 (96505)	Loss/tok 2.7185 (3.1415)	LR 7.000e-04
0: TRAIN [3][1630/1885]	Time 0.194 (0.143)	Data 2.20e-04 (3.61e-04)	Tok/s 104375 (96499)	Loss/tok 3.2750 (3.1416)	LR 7.000e-04
0: TRAIN [3][1640/1885]	Time 0.145 (0.143)	Data 2.23e-04 (3.61e-04)	Tok/s 100058 (96490)	Loss/tok 3.0205 (3.1413)	LR 7.000e-04
0: TRAIN [3][1650/1885]	Time 0.101 (0.143)	Data 1.73e-04 (3.60e-04)	Tok/s 88986 (96473)	Loss/tok 2.7946 (3.1409)	LR 7.000e-04
0: TRAIN [3][1660/1885]	Time 0.244 (0.144)	Data 2.23e-04 (3.59e-04)	Tok/s 107220 (96507)	Loss/tok 3.2523 (3.1415)	LR 7.000e-04
0: TRAIN [3][1670/1885]	Time 0.245 (0.143)	Data 2.24e-04 (3.58e-04)	Tok/s 107151 (96489)	Loss/tok 3.3263 (3.1411)	LR 7.000e-04
0: TRAIN [3][1680/1885]	Time 0.195 (0.144)	Data 2.22e-04 (3.57e-04)	Tok/s 105317 (96505)	Loss/tok 3.0862 (3.1413)	LR 7.000e-04
0: TRAIN [3][1690/1885]	Time 0.101 (0.144)	Data 2.23e-04 (3.56e-04)	Tok/s 88973 (96517)	Loss/tok 2.8368 (3.1415)	LR 7.000e-04
0: TRAIN [3][1700/1885]	Time 0.195 (0.144)	Data 2.23e-04 (3.56e-04)	Tok/s 104712 (96510)	Loss/tok 3.3396 (3.1410)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1710/1885]	Time 0.245 (0.144)	Data 2.24e-04 (3.55e-04)	Tok/s 106580 (96507)	Loss/tok 3.5047 (3.1410)	LR 7.000e-04
0: TRAIN [3][1720/1885]	Time 0.145 (0.144)	Data 2.21e-04 (3.54e-04)	Tok/s 99518 (96504)	Loss/tok 3.1603 (3.1406)	LR 7.000e-04
0: TRAIN [3][1730/1885]	Time 0.100 (0.144)	Data 2.21e-04 (3.53e-04)	Tok/s 89790 (96520)	Loss/tok 2.9221 (3.1409)	LR 7.000e-04
0: TRAIN [3][1740/1885]	Time 0.101 (0.144)	Data 2.24e-04 (3.53e-04)	Tok/s 90237 (96524)	Loss/tok 2.9041 (3.1405)	LR 7.000e-04
0: TRAIN [3][1750/1885]	Time 0.194 (0.144)	Data 2.20e-04 (3.52e-04)	Tok/s 105457 (96522)	Loss/tok 3.1944 (3.1402)	LR 7.000e-04
0: TRAIN [3][1760/1885]	Time 0.146 (0.144)	Data 2.21e-04 (3.51e-04)	Tok/s 100608 (96505)	Loss/tok 3.0620 (3.1397)	LR 7.000e-04
0: TRAIN [3][1770/1885]	Time 0.101 (0.143)	Data 2.23e-04 (3.50e-04)	Tok/s 88971 (96461)	Loss/tok 2.8713 (3.1390)	LR 7.000e-04
0: TRAIN [3][1780/1885]	Time 0.246 (0.143)	Data 1.80e-04 (3.50e-04)	Tok/s 107760 (96463)	Loss/tok 3.3120 (3.1395)	LR 7.000e-04
0: TRAIN [3][1790/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.49e-04)	Tok/s 100741 (96438)	Loss/tok 3.0763 (3.1388)	LR 7.000e-04
0: TRAIN [3][1800/1885]	Time 0.194 (0.143)	Data 2.22e-04 (3.48e-04)	Tok/s 104589 (96431)	Loss/tok 3.1193 (3.1383)	LR 7.000e-04
0: TRAIN [3][1810/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.47e-04)	Tok/s 89386 (96414)	Loss/tok 2.7888 (3.1376)	LR 7.000e-04
0: TRAIN [3][1820/1885]	Time 0.101 (0.143)	Data 2.22e-04 (3.47e-04)	Tok/s 87918 (96406)	Loss/tok 2.8017 (3.1370)	LR 7.000e-04
0: TRAIN [3][1830/1885]	Time 0.102 (0.143)	Data 2.25e-04 (3.46e-04)	Tok/s 90174 (96406)	Loss/tok 2.8004 (3.1368)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1840/1885]	Time 0.195 (0.143)	Data 2.20e-04 (3.45e-04)	Tok/s 103240 (96408)	Loss/tok 3.2511 (3.1367)	LR 7.000e-04
0: TRAIN [3][1850/1885]	Time 0.193 (0.143)	Data 2.21e-04 (3.45e-04)	Tok/s 107089 (96414)	Loss/tok 3.1299 (3.1368)	LR 7.000e-04
0: TRAIN [3][1860/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.44e-04)	Tok/s 89207 (96396)	Loss/tok 2.9091 (3.1364)	LR 7.000e-04
0: TRAIN [3][1870/1885]	Time 0.244 (0.143)	Data 2.18e-04 (3.43e-04)	Tok/s 106425 (96388)	Loss/tok 3.3726 (3.1362)	LR 7.000e-04
0: TRAIN [3][1880/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.43e-04)	Tok/s 90612 (96398)	Loss/tok 3.0093 (3.1359)	LR 7.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593837887955, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593837887956, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.579 (0.579)	Decoder iters 109.0 (109.0)	Tok/s 28160 (28160)
0: Running moses detokenizer
0: BLEU(score=24.16654488728229, counts=[36979, 18537, 10557, 6264], totals=[64975, 61972, 58969, 55971], precisions=[56.91265871489034, 29.911895694829923, 17.902626803913922, 11.191509889049687], bp=1.0, sys_len=64975, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593837889530, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24170000000000003, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593837889530, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1360	Test BLEU: 24.17
0: Performance: Epoch: 3	Training: 771228 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593837889531, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593837889531, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-07-04 04:44:55 AM
RESULT,RNN_TRANSLATOR,,1107,Fujitsu,2020-07-04 04:26:28 AM
