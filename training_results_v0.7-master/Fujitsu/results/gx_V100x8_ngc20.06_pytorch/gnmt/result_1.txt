Beginning trial 1 of 1
Gathering sys log on gx2570-03
:::MLLOG {"namespace": "", "time_ms": 1593830656544, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593830656580, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Fujitsu", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593830656580, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593830656580, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593830656580, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xGX2570M5", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
Clearing caches
:::MLLOG {"namespace": "", "time_ms": 1593830660031, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/opt/conda/lib/python3.6/site-packages/mlperf_logging/mllog/mllog.py", "lineno": 261}}
Launching on node gx2570-03
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e PGSYSTEM=PG -e 'MULTI_NODE= --master_port=4260' -e LR=2.8e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6053 -e DECAY_INTERVAL=859 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=65 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=200704114252288086675 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_200704114252288086675 ./run_and_time.sh
STARTING TIMING RUN AT 2020-07-04 02:44:21 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.8e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6053
+ DECAY_INTERVAL=859
+ TARGET=24.0
+ MAX_SEQ_LEN=65
+ NUMEPOCHS=8
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ PGSYSTEM=PG
+ [[ -f config_PG.sh ]]
+ source config_PG.sh
++ export PGNNODES=1
++ PGNNODES=1
+++ sed 's/^config_//'
+++ sed 's/\.sh$//'
++++ readlink -f config_PG.sh
+++ basename /workspace/rnn_translator/config_PG.sh
++ export PGSYSTEM=PG
++ PGSYSTEM=PG
++ export WALLTIME=00:30:00
++ WALLTIME=00:30:00
++ export LR=2.8e-3
++ LR=2.8e-3
++ export TRAIN_BATCH_SIZE=256
++ TRAIN_BATCH_SIZE=256
++ export TEST_BATCH_SIZE=128
++ TEST_BATCH_SIZE=128
++ export WARMUP_STEPS=200
++ WARMUP_STEPS=200
++ export REMAIN_STEPS=6053
++ REMAIN_STEPS=6053
++ export DECAY_INTERVAL=859
++ DECAY_INTERVAL=859
++ export TARGET=24.0
++ TARGET=24.0
++ export MAX_SEQ_LEN=65
++ MAX_SEQ_LEN=65
++ export NUMEPOCHS=8
++ NUMEPOCHS=8
++ export MATH=fp16
++ MATH=fp16
++ export DIST_OPTS=
++ DIST_OPTS=
++ export 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
++ export PGNGPU=8
++ PGNGPU=8
++ export PGSOCKETCORES=24
++ PGSOCKETCORES=24
++ export PGHT=2
++ PGHT=2
++ export PGNSOCKET=2
++ PGNSOCKET=2
+ declare -a CMD
+ '[' -n '' ']'
+ CMD=('python' '-u' '-m' 'bind_launch' "--nsockets_per_node=${PGNSOCKET}" "--ncores_per_socket=${PGSOCKETCORES}" "--nproc_per_node=${PGNGPU}")
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ PG == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ python -u -m bind_launch --nsockets_per_node=2 --ncores_per_socket=24 --nproc_per_node=8 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 65 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.8e-3 --warmup-steps 200 --remain-steps 6053 --decay-interval 859 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593830663063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593830663064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593830663067, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593830663091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593830663091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593830663092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593830663099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593830663101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=859, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.0028, math='fp16', max_length_test=150, max_length_train=65, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6053, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3200533610
:::MLLOG {"namespace": "", "time_ms": 1593830671291, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3200533610, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 2673909710
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.0028}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.0028
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593830674153, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593830674154, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0028, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593830674154, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593830674154, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593830674155, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593830676510, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593830676522, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593830676523, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 65, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3866375 min length: 0 max length: 65 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593830676789, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593830676789, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3860480, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593830676790, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6053, 'decay_interval': 859, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6053
0: Scheduler decay interval: 859
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593830676790, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593830676790, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593830676791, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 859, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593830676791, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593830676791, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593830676791, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6053, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593830676791, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593830676791, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593830676791, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1686840833
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1885]	Time 0.343 (0.343)	Data 2.71e-01 (2.71e-01)	Tok/s 13212 (13212)	Loss/tok 10.6042 (10.6042)	LR 2.865e-05
0: TRAIN [0][10/1885]	Time 0.144 (0.162)	Data 2.20e-04 (2.48e-02)	Tok/s 102633 (92412)	Loss/tok 9.5620 (10.0079)	LR 3.607e-05
0: TRAIN [0][20/1885]	Time 0.144 (0.156)	Data 2.18e-04 (1.31e-02)	Tok/s 101570 (96250)	Loss/tok 9.1796 (9.6836)	LR 4.541e-05
0: TRAIN [0][30/1885]	Time 0.143 (0.151)	Data 1.84e-04 (8.94e-03)	Tok/s 102272 (97106)	Loss/tok 8.8577 (9.4744)	LR 5.717e-05
0: TRAIN [0][40/1885]	Time 0.059 (0.146)	Data 2.22e-04 (6.82e-03)	Tok/s 77179 (97052)	Loss/tok 8.2576 (9.3089)	LR 7.197e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][50/1885]	Time 0.100 (0.144)	Data 2.24e-04 (5.52e-03)	Tok/s 90438 (97063)	Loss/tok 8.2828 (9.1617)	LR 8.854e-05
0: TRAIN [0][60/1885]	Time 0.144 (0.147)	Data 2.32e-04 (4.65e-03)	Tok/s 101174 (98071)	Loss/tok 8.1398 (9.0000)	LR 1.115e-04
0: TRAIN [0][70/1885]	Time 0.100 (0.146)	Data 2.15e-04 (4.03e-03)	Tok/s 89329 (97989)	Loss/tok 8.0117 (8.8849)	LR 1.403e-04
0: TRAIN [0][80/1885]	Time 0.100 (0.148)	Data 2.16e-04 (3.56e-03)	Tok/s 90394 (98541)	Loss/tok 7.8048 (8.7651)	LR 1.767e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][90/1885]	Time 0.100 (0.148)	Data 2.19e-04 (3.19e-03)	Tok/s 92850 (98559)	Loss/tok 7.7443 (8.6810)	LR 2.173e-04
0: TRAIN [0][100/1885]	Time 0.144 (0.145)	Data 2.18e-04 (2.90e-03)	Tok/s 101210 (98294)	Loss/tok 7.8294 (8.6139)	LR 2.736e-04
0: TRAIN [0][110/1885]	Time 0.144 (0.145)	Data 2.16e-04 (2.66e-03)	Tok/s 102766 (98460)	Loss/tok 7.8564 (8.5494)	LR 3.445e-04
0: TRAIN [0][120/1885]	Time 0.145 (0.145)	Data 2.17e-04 (2.45e-03)	Tok/s 101784 (98379)	Loss/tok 7.9080 (8.4959)	LR 4.337e-04
0: TRAIN [0][130/1885]	Time 0.240 (0.146)	Data 2.16e-04 (2.28e-03)	Tok/s 107951 (98606)	Loss/tok 8.0307 (8.4428)	LR 5.460e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][140/1885]	Time 0.191 (0.146)	Data 2.15e-04 (2.14e-03)	Tok/s 107636 (98569)	Loss/tok 7.8631 (8.3958)	LR 6.717e-04
0: TRAIN [0][150/1885]	Time 0.144 (0.145)	Data 2.17e-04 (2.01e-03)	Tok/s 102239 (98631)	Loss/tok 7.6388 (8.3504)	LR 8.456e-04
0: TRAIN [0][160/1885]	Time 0.058 (0.145)	Data 2.15e-04 (1.90e-03)	Tok/s 78932 (98589)	Loss/tok 6.4585 (8.3016)	LR 1.065e-03
0: TRAIN [0][170/1885]	Time 0.099 (0.144)	Data 2.19e-04 (1.80e-03)	Tok/s 92470 (98517)	Loss/tok 7.1768 (8.2555)	LR 1.340e-03
0: TRAIN [0][180/1885]	Time 0.100 (0.144)	Data 2.19e-04 (1.71e-03)	Tok/s 89082 (98469)	Loss/tok 6.9215 (8.2023)	LR 1.687e-03
0: TRAIN [0][190/1885]	Time 0.191 (0.146)	Data 2.13e-04 (1.63e-03)	Tok/s 106510 (98676)	Loss/tok 7.1946 (8.1424)	LR 2.124e-03
0: TRAIN [0][200/1885]	Time 0.144 (0.146)	Data 2.29e-04 (1.56e-03)	Tok/s 101968 (98671)	Loss/tok 6.9128 (8.0891)	LR 2.674e-03
0: TRAIN [0][210/1885]	Time 0.100 (0.146)	Data 2.26e-04 (1.50e-03)	Tok/s 91864 (98660)	Loss/tok 6.6260 (8.0336)	LR 2.800e-03
0: TRAIN [0][220/1885]	Time 0.144 (0.146)	Data 2.13e-04 (1.44e-03)	Tok/s 101987 (98657)	Loss/tok 6.6354 (7.9747)	LR 2.800e-03
0: TRAIN [0][230/1885]	Time 0.242 (0.146)	Data 2.14e-04 (1.39e-03)	Tok/s 109262 (98736)	Loss/tok 6.7310 (7.9107)	LR 2.800e-03
0: TRAIN [0][240/1885]	Time 0.145 (0.145)	Data 2.16e-04 (1.34e-03)	Tok/s 100696 (98526)	Loss/tok 6.4395 (7.8572)	LR 2.800e-03
0: TRAIN [0][250/1885]	Time 0.100 (0.145)	Data 2.17e-04 (1.29e-03)	Tok/s 90051 (98411)	Loss/tok 5.8206 (7.7997)	LR 2.800e-03
0: TRAIN [0][260/1885]	Time 0.143 (0.144)	Data 2.18e-04 (1.25e-03)	Tok/s 101861 (98312)	Loss/tok 6.1777 (7.7438)	LR 2.800e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][270/1885]	Time 0.145 (0.143)	Data 2.20e-04 (1.21e-03)	Tok/s 100792 (98256)	Loss/tok 5.9544 (7.6853)	LR 2.800e-03
0: TRAIN [0][280/1885]	Time 0.100 (0.143)	Data 2.19e-04 (1.18e-03)	Tok/s 90334 (98166)	Loss/tok 5.6138 (7.6275)	LR 2.800e-03
0: TRAIN [0][290/1885]	Time 0.192 (0.144)	Data 2.17e-04 (1.15e-03)	Tok/s 107186 (98342)	Loss/tok 5.8401 (7.5522)	LR 2.800e-03
0: TRAIN [0][300/1885]	Time 0.143 (0.144)	Data 2.14e-04 (1.11e-03)	Tok/s 103031 (98358)	Loss/tok 5.5712 (7.4889)	LR 2.800e-03
0: TRAIN [0][310/1885]	Time 0.145 (0.144)	Data 2.17e-04 (1.09e-03)	Tok/s 100487 (98313)	Loss/tok 5.4304 (7.4294)	LR 2.800e-03
0: TRAIN [0][320/1885]	Time 0.101 (0.143)	Data 2.29e-04 (1.06e-03)	Tok/s 89022 (98187)	Loss/tok 5.0833 (7.3762)	LR 2.800e-03
0: TRAIN [0][330/1885]	Time 0.191 (0.144)	Data 2.17e-04 (1.03e-03)	Tok/s 106577 (98307)	Loss/tok 5.4885 (7.3063)	LR 2.800e-03
0: TRAIN [0][340/1885]	Time 0.101 (0.144)	Data 2.33e-04 (1.01e-03)	Tok/s 92070 (98200)	Loss/tok 4.9049 (7.2531)	LR 2.800e-03
0: TRAIN [0][350/1885]	Time 0.145 (0.143)	Data 1.79e-04 (9.86e-04)	Tok/s 102091 (98177)	Loss/tok 5.1704 (7.1966)	LR 2.800e-03
0: TRAIN [0][360/1885]	Time 0.101 (0.142)	Data 2.18e-04 (9.64e-04)	Tok/s 90309 (97956)	Loss/tok 4.5958 (7.1508)	LR 2.800e-03
0: TRAIN [0][370/1885]	Time 0.102 (0.142)	Data 2.19e-04 (9.44e-04)	Tok/s 91350 (97893)	Loss/tok 4.5155 (7.0973)	LR 2.800e-03
0: TRAIN [0][380/1885]	Time 0.101 (0.142)	Data 2.17e-04 (9.25e-04)	Tok/s 91615 (97833)	Loss/tok 4.5073 (7.0429)	LR 2.800e-03
0: TRAIN [0][390/1885]	Time 0.145 (0.142)	Data 2.17e-04 (9.07e-04)	Tok/s 100570 (97907)	Loss/tok 4.8270 (6.9808)	LR 2.800e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][400/1885]	Time 0.102 (0.143)	Data 2.18e-04 (8.90e-04)	Tok/s 89155 (97927)	Loss/tok 4.3745 (6.9198)	LR 2.800e-03
0: TRAIN [0][410/1885]	Time 0.146 (0.142)	Data 2.16e-04 (8.73e-04)	Tok/s 100936 (97791)	Loss/tok 4.5226 (6.8743)	LR 2.800e-03
0: TRAIN [0][420/1885]	Time 0.060 (0.141)	Data 2.17e-04 (8.58e-04)	Tok/s 76464 (97622)	Loss/tok 3.3636 (6.8335)	LR 2.800e-03
0: TRAIN [0][430/1885]	Time 0.247 (0.141)	Data 9.63e-05 (8.42e-04)	Tok/s 105406 (97496)	Loss/tok 4.9952 (6.7876)	LR 2.800e-03
0: TRAIN [0][440/1885]	Time 0.101 (0.141)	Data 2.16e-04 (8.28e-04)	Tok/s 92151 (97503)	Loss/tok 4.1561 (6.7353)	LR 2.800e-03
0: TRAIN [0][450/1885]	Time 0.100 (0.141)	Data 2.15e-04 (8.15e-04)	Tok/s 90074 (97508)	Loss/tok 4.1084 (6.6850)	LR 2.800e-03
0: TRAIN [0][460/1885]	Time 0.192 (0.141)	Data 2.16e-04 (8.02e-04)	Tok/s 106887 (97471)	Loss/tok 4.7134 (6.6396)	LR 2.800e-03
0: TRAIN [0][470/1885]	Time 0.242 (0.141)	Data 2.20e-04 (7.89e-04)	Tok/s 108766 (97503)	Loss/tok 4.8092 (6.5897)	LR 2.800e-03
0: TRAIN [0][480/1885]	Time 0.101 (0.141)	Data 2.14e-04 (7.77e-04)	Tok/s 89573 (97513)	Loss/tok 4.0129 (6.5421)	LR 2.800e-03
0: TRAIN [0][490/1885]	Time 0.146 (0.141)	Data 2.17e-04 (7.66e-04)	Tok/s 102438 (97521)	Loss/tok 4.4134 (6.4987)	LR 2.800e-03
0: TRAIN [0][500/1885]	Time 0.060 (0.141)	Data 2.17e-04 (7.55e-04)	Tok/s 76816 (97407)	Loss/tok 3.3314 (6.4629)	LR 2.800e-03
0: TRAIN [0][510/1885]	Time 0.192 (0.141)	Data 2.18e-04 (7.44e-04)	Tok/s 106257 (97388)	Loss/tok 4.4425 (6.4213)	LR 2.800e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][520/1885]	Time 0.101 (0.141)	Data 2.15e-04 (7.34e-04)	Tok/s 87963 (97419)	Loss/tok 3.9812 (6.3779)	LR 2.800e-03
0: TRAIN [0][530/1885]	Time 0.102 (0.141)	Data 2.16e-04 (7.24e-04)	Tok/s 90145 (97390)	Loss/tok 3.9818 (6.3404)	LR 2.800e-03
0: TRAIN [0][540/1885]	Time 0.101 (0.141)	Data 2.16e-04 (7.15e-04)	Tok/s 89227 (97437)	Loss/tok 3.8640 (6.2980)	LR 2.800e-03
0: TRAIN [0][550/1885]	Time 0.061 (0.141)	Data 1.47e-04 (7.06e-04)	Tok/s 75987 (97343)	Loss/tok 3.1608 (6.2660)	LR 2.800e-03
0: TRAIN [0][560/1885]	Time 0.102 (0.141)	Data 2.15e-04 (6.97e-04)	Tok/s 87964 (97376)	Loss/tok 3.7301 (6.2260)	LR 2.800e-03
0: TRAIN [0][570/1885]	Time 0.102 (0.141)	Data 2.17e-04 (6.89e-04)	Tok/s 90803 (97348)	Loss/tok 3.7709 (6.1930)	LR 2.800e-03
0: TRAIN [0][580/1885]	Time 0.101 (0.140)	Data 2.15e-04 (6.80e-04)	Tok/s 89483 (97237)	Loss/tok 3.7007 (6.1646)	LR 2.800e-03
0: TRAIN [0][590/1885]	Time 0.192 (0.140)	Data 2.16e-04 (6.73e-04)	Tok/s 106228 (97222)	Loss/tok 4.2980 (6.1313)	LR 2.800e-03
0: TRAIN [0][600/1885]	Time 0.146 (0.140)	Data 2.21e-04 (6.65e-04)	Tok/s 100648 (97238)	Loss/tok 3.9813 (6.0956)	LR 2.800e-03
0: TRAIN [0][610/1885]	Time 0.193 (0.140)	Data 2.18e-04 (6.58e-04)	Tok/s 105646 (97242)	Loss/tok 4.1884 (6.0610)	LR 2.800e-03
0: TRAIN [0][620/1885]	Time 0.192 (0.141)	Data 2.18e-04 (6.51e-04)	Tok/s 105904 (97299)	Loss/tok 4.2976 (6.0251)	LR 2.800e-03
0: TRAIN [0][630/1885]	Time 0.101 (0.140)	Data 2.18e-04 (6.44e-04)	Tok/s 91553 (97245)	Loss/tok 3.7886 (5.9970)	LR 2.800e-03
0: TRAIN [0][640/1885]	Time 0.102 (0.140)	Data 2.16e-04 (6.37e-04)	Tok/s 88977 (97224)	Loss/tok 3.5999 (5.9685)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][650/1885]	Time 0.192 (0.140)	Data 2.16e-04 (6.30e-04)	Tok/s 107453 (97184)	Loss/tok 4.1635 (5.9409)	LR 2.800e-03
0: TRAIN [0][660/1885]	Time 0.060 (0.140)	Data 2.17e-04 (6.24e-04)	Tok/s 78369 (97173)	Loss/tok 3.0863 (5.9121)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][670/1885]	Time 0.191 (0.140)	Data 2.19e-04 (6.18e-04)	Tok/s 107072 (97202)	Loss/tok 4.2844 (5.8808)	LR 2.800e-03
0: TRAIN [0][680/1885]	Time 0.101 (0.140)	Data 2.14e-04 (6.12e-04)	Tok/s 90139 (97129)	Loss/tok 3.6575 (5.8585)	LR 2.800e-03
0: TRAIN [0][690/1885]	Time 0.192 (0.140)	Data 2.17e-04 (6.06e-04)	Tok/s 105456 (97166)	Loss/tok 4.0444 (5.8287)	LR 2.800e-03
0: TRAIN [0][700/1885]	Time 0.101 (0.140)	Data 2.19e-04 (6.01e-04)	Tok/s 89744 (97163)	Loss/tok 3.7455 (5.8019)	LR 2.800e-03
0: TRAIN [0][710/1885]	Time 0.146 (0.140)	Data 2.21e-04 (5.95e-04)	Tok/s 101581 (97131)	Loss/tok 3.8898 (5.7772)	LR 2.800e-03
0: TRAIN [0][720/1885]	Time 0.101 (0.140)	Data 2.16e-04 (5.90e-04)	Tok/s 90206 (97117)	Loss/tok 3.8025 (5.7521)	LR 2.800e-03
0: TRAIN [0][730/1885]	Time 0.244 (0.141)	Data 2.18e-04 (5.85e-04)	Tok/s 106990 (97156)	Loss/tok 4.2164 (5.7246)	LR 2.800e-03
0: TRAIN [0][740/1885]	Time 0.101 (0.141)	Data 2.18e-04 (5.80e-04)	Tok/s 89812 (97139)	Loss/tok 3.5536 (5.7004)	LR 2.800e-03
0: TRAIN [0][750/1885]	Time 0.101 (0.141)	Data 2.17e-04 (5.75e-04)	Tok/s 90480 (97182)	Loss/tok 3.6076 (5.6735)	LR 2.800e-03
0: TRAIN [0][760/1885]	Time 0.146 (0.141)	Data 2.22e-04 (5.70e-04)	Tok/s 100443 (97215)	Loss/tok 3.9120 (5.6476)	LR 2.800e-03
0: TRAIN [0][770/1885]	Time 0.192 (0.141)	Data 2.19e-04 (5.66e-04)	Tok/s 106950 (97234)	Loss/tok 4.1366 (5.6233)	LR 2.800e-03
0: TRAIN [0][780/1885]	Time 0.192 (0.141)	Data 2.16e-04 (5.61e-04)	Tok/s 105189 (97224)	Loss/tok 4.1680 (5.6016)	LR 2.800e-03
0: TRAIN [0][790/1885]	Time 0.146 (0.142)	Data 2.18e-04 (5.57e-04)	Tok/s 100329 (97277)	Loss/tok 3.8030 (5.5767)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][800/1885]	Time 0.100 (0.141)	Data 2.15e-04 (5.53e-04)	Tok/s 92098 (97242)	Loss/tok 3.6215 (5.5570)	LR 2.800e-03
0: TRAIN [0][810/1885]	Time 0.145 (0.141)	Data 2.17e-04 (5.48e-04)	Tok/s 100239 (97192)	Loss/tok 3.8472 (5.5377)	LR 2.800e-03
0: TRAIN [0][820/1885]	Time 0.101 (0.141)	Data 2.15e-04 (5.44e-04)	Tok/s 89654 (97168)	Loss/tok 3.5419 (5.5193)	LR 2.800e-03
0: TRAIN [0][830/1885]	Time 0.145 (0.141)	Data 2.17e-04 (5.40e-04)	Tok/s 101928 (97129)	Loss/tok 3.7826 (5.5011)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][840/1885]	Time 0.243 (0.141)	Data 2.22e-04 (5.37e-04)	Tok/s 107234 (97131)	Loss/tok 4.3789 (5.4808)	LR 2.800e-03
0: TRAIN [0][850/1885]	Time 0.192 (0.141)	Data 2.19e-04 (5.33e-04)	Tok/s 105959 (97171)	Loss/tok 4.0359 (5.4605)	LR 2.800e-03
0: TRAIN [0][860/1885]	Time 0.102 (0.141)	Data 2.15e-04 (5.29e-04)	Tok/s 89921 (97180)	Loss/tok 3.4813 (5.4410)	LR 2.800e-03
0: TRAIN [0][870/1885]	Time 0.193 (0.141)	Data 2.17e-04 (5.26e-04)	Tok/s 105223 (97192)	Loss/tok 3.8785 (5.4219)	LR 2.800e-03
0: TRAIN [0][880/1885]	Time 0.101 (0.141)	Data 2.17e-04 (5.22e-04)	Tok/s 89474 (97181)	Loss/tok 3.4867 (5.4042)	LR 2.800e-03
0: TRAIN [0][890/1885]	Time 0.146 (0.141)	Data 2.19e-04 (5.19e-04)	Tok/s 102066 (97212)	Loss/tok 3.8475 (5.3851)	LR 2.800e-03
0: TRAIN [0][900/1885]	Time 0.192 (0.141)	Data 2.30e-04 (5.15e-04)	Tok/s 106762 (97225)	Loss/tok 3.9579 (5.3667)	LR 2.800e-03
0: TRAIN [0][910/1885]	Time 0.242 (0.142)	Data 2.16e-04 (5.12e-04)	Tok/s 108559 (97259)	Loss/tok 4.0995 (5.3474)	LR 2.800e-03
0: TRAIN [0][920/1885]	Time 0.244 (0.142)	Data 2.19e-04 (5.09e-04)	Tok/s 107432 (97240)	Loss/tok 4.0962 (5.3310)	LR 2.800e-03
0: TRAIN [0][930/1885]	Time 0.192 (0.142)	Data 2.16e-04 (5.06e-04)	Tok/s 105457 (97221)	Loss/tok 4.0782 (5.3159)	LR 2.800e-03
0: TRAIN [0][940/1885]	Time 0.101 (0.141)	Data 2.16e-04 (5.03e-04)	Tok/s 90239 (97185)	Loss/tok 3.5056 (5.3018)	LR 2.800e-03
0: TRAIN [0][950/1885]	Time 0.194 (0.141)	Data 2.17e-04 (5.00e-04)	Tok/s 105617 (97175)	Loss/tok 3.9963 (5.2864)	LR 2.800e-03
0: TRAIN [0][960/1885]	Time 0.146 (0.142)	Data 2.31e-04 (4.97e-04)	Tok/s 100319 (97227)	Loss/tok 3.7941 (5.2675)	LR 2.800e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][970/1885]	Time 0.101 (0.141)	Data 2.16e-04 (4.94e-04)	Tok/s 89886 (97180)	Loss/tok 3.4727 (5.2542)	LR 2.800e-03
0: TRAIN [0][980/1885]	Time 0.192 (0.141)	Data 2.48e-04 (4.91e-04)	Tok/s 105052 (97154)	Loss/tok 3.9841 (5.2400)	LR 2.800e-03
0: TRAIN [0][990/1885]	Time 0.102 (0.141)	Data 2.21e-04 (4.88e-04)	Tok/s 88281 (97149)	Loss/tok 3.4759 (5.2243)	LR 2.800e-03
0: TRAIN [0][1000/1885]	Time 0.103 (0.141)	Data 2.18e-04 (4.85e-04)	Tok/s 88060 (97152)	Loss/tok 3.4485 (5.2097)	LR 2.800e-03
0: TRAIN [0][1010/1885]	Time 0.101 (0.141)	Data 2.22e-04 (4.83e-04)	Tok/s 88576 (97138)	Loss/tok 3.4711 (5.1953)	LR 2.800e-03
0: TRAIN [0][1020/1885]	Time 0.193 (0.141)	Data 2.15e-04 (4.80e-04)	Tok/s 103415 (97139)	Loss/tok 3.9380 (5.1803)	LR 2.800e-03
0: TRAIN [0][1030/1885]	Time 0.101 (0.142)	Data 2.17e-04 (4.78e-04)	Tok/s 89092 (97158)	Loss/tok 3.4222 (5.1654)	LR 2.800e-03
0: TRAIN [0][1040/1885]	Time 0.146 (0.142)	Data 2.36e-04 (4.75e-04)	Tok/s 100297 (97197)	Loss/tok 3.6096 (5.1497)	LR 2.800e-03
0: TRAIN [0][1050/1885]	Time 0.101 (0.142)	Data 2.17e-04 (4.73e-04)	Tok/s 90514 (97146)	Loss/tok 3.4018 (5.1382)	LR 2.800e-03
0: TRAIN [0][1060/1885]	Time 0.101 (0.142)	Data 2.19e-04 (4.70e-04)	Tok/s 90926 (97168)	Loss/tok 3.3006 (5.1235)	LR 2.800e-03
0: TRAIN [0][1070/1885]	Time 0.193 (0.142)	Data 2.23e-04 (4.68e-04)	Tok/s 105902 (97193)	Loss/tok 3.8630 (5.1087)	LR 2.800e-03
0: TRAIN [0][1080/1885]	Time 0.243 (0.142)	Data 2.18e-04 (4.66e-04)	Tok/s 107381 (97232)	Loss/tok 3.9864 (5.0941)	LR 2.800e-03
0: TRAIN [0][1090/1885]	Time 0.101 (0.143)	Data 2.17e-04 (4.63e-04)	Tok/s 89834 (97255)	Loss/tok 3.3927 (5.0803)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1100/1885]	Time 0.146 (0.142)	Data 2.19e-04 (4.61e-04)	Tok/s 99905 (97229)	Loss/tok 3.6605 (5.0689)	LR 2.800e-03
0: TRAIN [0][1110/1885]	Time 0.145 (0.143)	Data 2.18e-04 (4.59e-04)	Tok/s 102499 (97247)	Loss/tok 3.5291 (5.0557)	LR 2.800e-03
0: TRAIN [0][1120/1885]	Time 0.146 (0.143)	Data 2.16e-04 (4.57e-04)	Tok/s 99626 (97278)	Loss/tok 3.6109 (5.0417)	LR 2.800e-03
0: TRAIN [0][1130/1885]	Time 0.101 (0.143)	Data 2.32e-04 (4.55e-04)	Tok/s 88499 (97253)	Loss/tok 3.4639 (5.0304)	LR 2.800e-03
0: TRAIN [0][1140/1885]	Time 0.102 (0.142)	Data 2.21e-04 (4.53e-04)	Tok/s 88636 (97208)	Loss/tok 3.5471 (5.0204)	LR 2.800e-03
0: TRAIN [0][1150/1885]	Time 0.100 (0.142)	Data 2.16e-04 (4.51e-04)	Tok/s 91067 (97222)	Loss/tok 3.4047 (5.0083)	LR 2.800e-03
0: TRAIN [0][1160/1885]	Time 0.146 (0.142)	Data 2.17e-04 (4.48e-04)	Tok/s 99396 (97194)	Loss/tok 3.7394 (4.9979)	LR 2.800e-03
0: TRAIN [0][1170/1885]	Time 0.060 (0.142)	Data 2.19e-04 (4.46e-04)	Tok/s 76550 (97133)	Loss/tok 2.9333 (4.9891)	LR 2.800e-03
0: TRAIN [0][1180/1885]	Time 0.101 (0.142)	Data 2.21e-04 (4.45e-04)	Tok/s 88824 (97135)	Loss/tok 3.3316 (4.9780)	LR 2.800e-03
0: TRAIN [0][1190/1885]	Time 0.102 (0.142)	Data 2.14e-04 (4.43e-04)	Tok/s 90290 (97146)	Loss/tok 3.4453 (4.9666)	LR 2.800e-03
0: TRAIN [0][1200/1885]	Time 0.192 (0.142)	Data 2.03e-04 (4.41e-04)	Tok/s 106554 (97139)	Loss/tok 3.8966 (4.9559)	LR 2.800e-03
0: TRAIN [0][1210/1885]	Time 0.147 (0.142)	Data 2.18e-04 (4.39e-04)	Tok/s 100775 (97140)	Loss/tok 3.6117 (4.9446)	LR 2.800e-03
0: TRAIN [0][1220/1885]	Time 0.193 (0.142)	Data 2.16e-04 (4.37e-04)	Tok/s 105563 (97149)	Loss/tok 3.9092 (4.9337)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [0][1230/1885]	Time 0.146 (0.142)	Data 2.18e-04 (4.35e-04)	Tok/s 100146 (97163)	Loss/tok 3.6313 (4.9228)	LR 2.800e-03
0: TRAIN [0][1240/1885]	Time 0.147 (0.142)	Data 2.16e-04 (4.33e-04)	Tok/s 98661 (97185)	Loss/tok 3.6257 (4.9119)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [0][1250/1885]	Time 0.194 (0.142)	Data 2.19e-04 (4.32e-04)	Tok/s 103269 (97169)	Loss/tok 3.7313 (4.9019)	LR 2.800e-03
0: TRAIN [0][1260/1885]	Time 0.193 (0.142)	Data 2.50e-04 (4.30e-04)	Tok/s 104710 (97170)	Loss/tok 3.8458 (4.8918)	LR 2.800e-03
0: TRAIN [0][1270/1885]	Time 0.146 (0.142)	Data 2.19e-04 (4.28e-04)	Tok/s 99110 (97171)	Loss/tok 3.5605 (4.8818)	LR 2.800e-03
0: TRAIN [0][1280/1885]	Time 0.194 (0.142)	Data 1.88e-04 (4.27e-04)	Tok/s 105159 (97179)	Loss/tok 3.7764 (4.8717)	LR 2.800e-03
0: TRAIN [0][1290/1885]	Time 0.145 (0.142)	Data 2.17e-04 (4.25e-04)	Tok/s 101265 (97171)	Loss/tok 3.5424 (4.8621)	LR 2.800e-03
0: TRAIN [0][1300/1885]	Time 0.193 (0.142)	Data 2.18e-04 (4.23e-04)	Tok/s 104069 (97173)	Loss/tok 3.8647 (4.8522)	LR 2.800e-03
0: TRAIN [0][1310/1885]	Time 0.193 (0.142)	Data 2.21e-04 (4.22e-04)	Tok/s 106344 (97139)	Loss/tok 3.7002 (4.8437)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1320/1885]	Time 0.145 (0.142)	Data 2.19e-04 (4.20e-04)	Tok/s 100324 (97135)	Loss/tok 3.6210 (4.8346)	LR 2.800e-03
0: TRAIN [0][1330/1885]	Time 0.146 (0.142)	Data 2.20e-04 (4.19e-04)	Tok/s 99562 (97150)	Loss/tok 3.6493 (4.8256)	LR 2.800e-03
0: TRAIN [0][1340/1885]	Time 0.100 (0.142)	Data 2.14e-04 (4.17e-04)	Tok/s 89924 (97121)	Loss/tok 3.3338 (4.8171)	LR 2.800e-03
0: TRAIN [0][1350/1885]	Time 0.146 (0.142)	Data 2.16e-04 (4.16e-04)	Tok/s 99766 (97121)	Loss/tok 3.6653 (4.8081)	LR 2.800e-03
0: TRAIN [0][1360/1885]	Time 0.101 (0.142)	Data 2.17e-04 (4.14e-04)	Tok/s 89378 (97104)	Loss/tok 3.2917 (4.7997)	LR 2.800e-03
0: TRAIN [0][1370/1885]	Time 0.102 (0.142)	Data 2.17e-04 (4.13e-04)	Tok/s 87889 (97079)	Loss/tok 3.2485 (4.7917)	LR 2.800e-03
0: TRAIN [0][1380/1885]	Time 0.061 (0.142)	Data 2.17e-04 (4.11e-04)	Tok/s 73946 (97067)	Loss/tok 2.6621 (4.7833)	LR 2.800e-03
0: TRAIN [0][1390/1885]	Time 0.102 (0.142)	Data 2.16e-04 (4.10e-04)	Tok/s 89260 (97028)	Loss/tok 3.2172 (4.7759)	LR 2.800e-03
0: TRAIN [0][1400/1885]	Time 0.146 (0.141)	Data 2.21e-04 (4.08e-04)	Tok/s 100203 (97007)	Loss/tok 3.5000 (4.7682)	LR 2.800e-03
0: TRAIN [0][1410/1885]	Time 0.145 (0.141)	Data 2.18e-04 (4.07e-04)	Tok/s 100384 (97022)	Loss/tok 3.7109 (4.7597)	LR 2.800e-03
0: TRAIN [0][1420/1885]	Time 0.147 (0.141)	Data 2.18e-04 (4.06e-04)	Tok/s 99897 (97026)	Loss/tok 3.5724 (4.7514)	LR 2.800e-03
0: TRAIN [0][1430/1885]	Time 0.101 (0.141)	Data 2.30e-04 (4.04e-04)	Tok/s 89908 (96992)	Loss/tok 3.2711 (4.7441)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1440/1885]	Time 0.193 (0.141)	Data 2.16e-04 (4.03e-04)	Tok/s 106292 (96994)	Loss/tok 3.8157 (4.7364)	LR 2.800e-03
0: TRAIN [0][1450/1885]	Time 0.101 (0.141)	Data 2.18e-04 (4.02e-04)	Tok/s 90924 (97005)	Loss/tok 3.2906 (4.7282)	LR 2.800e-03
0: TRAIN [0][1460/1885]	Time 0.192 (0.142)	Data 2.17e-04 (4.01e-04)	Tok/s 107720 (97019)	Loss/tok 3.7767 (4.7203)	LR 2.800e-03
0: TRAIN [0][1470/1885]	Time 0.146 (0.142)	Data 2.19e-04 (3.99e-04)	Tok/s 100189 (97044)	Loss/tok 3.5574 (4.7121)	LR 2.800e-03
0: TRAIN [0][1480/1885]	Time 0.194 (0.142)	Data 2.21e-04 (3.98e-04)	Tok/s 105761 (97052)	Loss/tok 3.7096 (4.7040)	LR 2.800e-03
0: TRAIN [0][1490/1885]	Time 0.147 (0.142)	Data 2.21e-04 (3.97e-04)	Tok/s 99578 (97080)	Loss/tok 3.6041 (4.6957)	LR 2.800e-03
0: TRAIN [0][1500/1885]	Time 0.145 (0.142)	Data 2.30e-04 (3.96e-04)	Tok/s 100279 (97057)	Loss/tok 3.5554 (4.6889)	LR 2.800e-03
0: TRAIN [0][1510/1885]	Time 0.102 (0.142)	Data 2.16e-04 (3.94e-04)	Tok/s 89734 (97049)	Loss/tok 3.2900 (4.6819)	LR 2.800e-03
0: TRAIN [0][1520/1885]	Time 0.193 (0.142)	Data 2.17e-04 (3.93e-04)	Tok/s 105040 (97051)	Loss/tok 3.6393 (4.6744)	LR 2.800e-03
0: TRAIN [0][1530/1885]	Time 0.102 (0.142)	Data 2.21e-04 (3.92e-04)	Tok/s 89391 (97051)	Loss/tok 3.2176 (4.6667)	LR 2.800e-03
0: TRAIN [0][1540/1885]	Time 0.146 (0.142)	Data 2.18e-04 (3.91e-04)	Tok/s 99220 (97020)	Loss/tok 3.5634 (4.6604)	LR 2.800e-03
0: TRAIN [0][1550/1885]	Time 0.243 (0.142)	Data 2.05e-04 (3.90e-04)	Tok/s 107592 (97034)	Loss/tok 3.7938 (4.6525)	LR 2.800e-03
0: TRAIN [0][1560/1885]	Time 0.243 (0.142)	Data 1.60e-04 (3.89e-04)	Tok/s 109028 (97041)	Loss/tok 3.6770 (4.6450)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [0][1570/1885]	Time 0.147 (0.142)	Data 2.24e-04 (3.88e-04)	Tok/s 99628 (97042)	Loss/tok 3.3668 (4.6378)	LR 2.800e-03
0: TRAIN [0][1580/1885]	Time 0.193 (0.142)	Data 2.20e-04 (3.87e-04)	Tok/s 107047 (97061)	Loss/tok 3.7346 (4.6303)	LR 2.800e-03
0: TRAIN [0][1590/1885]	Time 0.102 (0.142)	Data 2.03e-04 (3.85e-04)	Tok/s 88158 (97067)	Loss/tok 3.2623 (4.6231)	LR 2.800e-03
0: TRAIN [0][1600/1885]	Time 0.061 (0.142)	Data 2.21e-04 (3.84e-04)	Tok/s 75588 (97029)	Loss/tok 2.7105 (4.6174)	LR 2.800e-03
0: TRAIN [0][1610/1885]	Time 0.146 (0.142)	Data 2.17e-04 (3.83e-04)	Tok/s 100135 (97039)	Loss/tok 3.6359 (4.6103)	LR 2.800e-03
0: TRAIN [0][1620/1885]	Time 0.193 (0.142)	Data 2.17e-04 (3.82e-04)	Tok/s 104820 (97061)	Loss/tok 3.8070 (4.6034)	LR 2.800e-03
0: TRAIN [0][1630/1885]	Time 0.103 (0.142)	Data 2.15e-04 (3.81e-04)	Tok/s 87154 (97053)	Loss/tok 3.3563 (4.5971)	LR 2.800e-03
0: TRAIN [0][1640/1885]	Time 0.100 (0.142)	Data 2.18e-04 (3.80e-04)	Tok/s 90630 (97034)	Loss/tok 3.3877 (4.5912)	LR 2.800e-03
0: TRAIN [0][1650/1885]	Time 0.145 (0.142)	Data 2.18e-04 (3.79e-04)	Tok/s 101201 (97044)	Loss/tok 3.4538 (4.5844)	LR 2.800e-03
0: TRAIN [0][1660/1885]	Time 0.101 (0.142)	Data 2.16e-04 (3.78e-04)	Tok/s 91195 (97029)	Loss/tok 3.2740 (4.5786)	LR 2.800e-03
0: TRAIN [0][1670/1885]	Time 0.102 (0.142)	Data 2.20e-04 (3.77e-04)	Tok/s 89351 (97039)	Loss/tok 3.1549 (4.5718)	LR 2.800e-03
0: TRAIN [0][1680/1885]	Time 0.145 (0.142)	Data 2.19e-04 (3.76e-04)	Tok/s 101352 (97010)	Loss/tok 3.5474 (4.5666)	LR 2.800e-03
0: TRAIN [0][1690/1885]	Time 0.192 (0.142)	Data 2.25e-04 (3.76e-04)	Tok/s 105000 (97010)	Loss/tok 3.7079 (4.5603)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [0][1700/1885]	Time 0.102 (0.142)	Data 2.19e-04 (3.75e-04)	Tok/s 87888 (97015)	Loss/tok 3.2949 (4.5542)	LR 2.800e-03
0: TRAIN [0][1710/1885]	Time 0.243 (0.142)	Data 2.21e-04 (3.74e-04)	Tok/s 108871 (97010)	Loss/tok 3.7261 (4.5483)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [0][1720/1885]	Time 0.102 (0.142)	Data 2.19e-04 (3.73e-04)	Tok/s 87974 (97000)	Loss/tok 3.2462 (4.5424)	LR 2.800e-03
0: TRAIN [0][1730/1885]	Time 0.146 (0.142)	Data 2.18e-04 (3.72e-04)	Tok/s 100403 (97024)	Loss/tok 3.5109 (4.5360)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1740/1885]	Time 0.146 (0.142)	Data 2.20e-04 (3.71e-04)	Tok/s 99176 (97055)	Loss/tok 3.4777 (4.5293)	LR 2.800e-03
0: TRAIN [0][1750/1885]	Time 0.101 (0.142)	Data 2.20e-04 (3.70e-04)	Tok/s 90630 (97046)	Loss/tok 3.1255 (4.5236)	LR 2.800e-03
0: TRAIN [0][1760/1885]	Time 0.145 (0.142)	Data 2.19e-04 (3.69e-04)	Tok/s 99768 (97063)	Loss/tok 3.4342 (4.5174)	LR 2.800e-03
0: TRAIN [0][1770/1885]	Time 0.146 (0.142)	Data 2.17e-04 (3.68e-04)	Tok/s 100096 (97076)	Loss/tok 3.4556 (4.5114)	LR 2.800e-03
0: TRAIN [0][1780/1885]	Time 0.193 (0.142)	Data 2.18e-04 (3.68e-04)	Tok/s 106136 (97084)	Loss/tok 3.7987 (4.5057)	LR 2.800e-03
0: TRAIN [0][1790/1885]	Time 0.145 (0.142)	Data 2.21e-04 (3.67e-04)	Tok/s 100467 (97073)	Loss/tok 3.5830 (4.5004)	LR 2.800e-03
0: TRAIN [0][1800/1885]	Time 0.193 (0.142)	Data 2.18e-04 (3.66e-04)	Tok/s 106407 (97066)	Loss/tok 3.6316 (4.4949)	LR 2.800e-03
0: TRAIN [0][1810/1885]	Time 0.145 (0.142)	Data 2.17e-04 (3.65e-04)	Tok/s 99511 (97064)	Loss/tok 3.5365 (4.4893)	LR 2.800e-03
0: TRAIN [0][1820/1885]	Time 0.145 (0.142)	Data 2.29e-04 (3.64e-04)	Tok/s 100824 (97057)	Loss/tok 3.4002 (4.4841)	LR 2.800e-03
0: TRAIN [0][1830/1885]	Time 0.146 (0.142)	Data 2.52e-04 (3.63e-04)	Tok/s 100162 (97037)	Loss/tok 3.5425 (4.4794)	LR 2.800e-03
0: TRAIN [0][1840/1885]	Time 0.146 (0.142)	Data 2.20e-04 (3.63e-04)	Tok/s 100953 (97050)	Loss/tok 3.4701 (4.4743)	LR 2.800e-03
0: TRAIN [0][1850/1885]	Time 0.102 (0.142)	Data 2.18e-04 (3.62e-04)	Tok/s 87539 (97029)	Loss/tok 3.2981 (4.4695)	LR 2.800e-03
0: TRAIN [0][1860/1885]	Time 0.100 (0.142)	Data 2.19e-04 (3.61e-04)	Tok/s 89473 (97006)	Loss/tok 3.0972 (4.4647)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1870/1885]	Time 0.145 (0.142)	Data 2.16e-04 (3.60e-04)	Tok/s 100214 (97013)	Loss/tok 3.4739 (4.4591)	LR 2.800e-03
0: TRAIN [0][1880/1885]	Time 0.102 (0.142)	Data 2.20e-04 (3.60e-04)	Tok/s 87292 (97008)	Loss/tok 3.2188 (4.4540)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593830945101, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593830945102, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.579 (0.579)	Decoder iters 105.0 (105.0)	Tok/s 27956 (27956)
0: Running moses detokenizer
0: BLEU(score=20.26940217931967, counts=[34525, 15904, 8466, 4720], totals=[64166, 61163, 58161, 55165], precisions=[53.80575382601378, 26.002648660137666, 14.556145870944448, 8.556149732620321], bp=0.9920833683767939, sys_len=64166, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593830946682, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2027, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593830946682, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.4533	Test BLEU: 20.27
0: Performance: Epoch: 0	Training: 775990 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593830946683, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593830946683, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593830946683, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1677106809
0: TRAIN [1][0/1885]	Time 0.381 (0.381)	Data 2.08e-01 (2.08e-01)	Tok/s 38112 (38112)	Loss/tok 3.5346 (3.5346)	LR 2.800e-03
0: TRAIN [1][10/1885]	Time 0.194 (0.151)	Data 2.21e-04 (1.91e-02)	Tok/s 105539 (85725)	Loss/tok 3.5983 (3.4314)	LR 2.800e-03
0: TRAIN [1][20/1885]	Time 0.106 (0.142)	Data 2.20e-04 (1.01e-02)	Tok/s 86397 (88734)	Loss/tok 3.2783 (3.3925)	LR 2.800e-03
0: TRAIN [1][30/1885]	Time 0.150 (0.142)	Data 1.78e-04 (6.90e-03)	Tok/s 97320 (90187)	Loss/tok 3.5056 (3.4079)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][40/1885]	Time 0.197 (0.136)	Data 1.78e-04 (5.27e-03)	Tok/s 103401 (89776)	Loss/tok 3.5537 (3.3838)	LR 2.800e-03
0: TRAIN [1][50/1885]	Time 0.195 (0.139)	Data 2.19e-04 (4.28e-03)	Tok/s 105416 (91445)	Loss/tok 3.5198 (3.3956)	LR 2.800e-03
0: TRAIN [1][60/1885]	Time 0.102 (0.143)	Data 1.92e-04 (3.61e-03)	Tok/s 88737 (92107)	Loss/tok 3.2266 (3.4213)	LR 2.800e-03
0: TRAIN [1][70/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.13e-03)	Tok/s 87349 (92373)	Loss/tok 3.1569 (3.4147)	LR 2.800e-03
0: TRAIN [1][80/1885]	Time 0.103 (0.144)	Data 2.21e-04 (2.77e-03)	Tok/s 88818 (92607)	Loss/tok 3.1749 (3.4250)	LR 2.800e-03
0: TRAIN [1][90/1885]	Time 0.245 (0.144)	Data 2.19e-04 (2.49e-03)	Tok/s 106392 (93046)	Loss/tok 3.6965 (3.4270)	LR 2.800e-03
0: TRAIN [1][100/1885]	Time 0.150 (0.142)	Data 2.16e-04 (2.26e-03)	Tok/s 96834 (92931)	Loss/tok 3.4800 (3.4256)	LR 2.800e-03
0: TRAIN [1][110/1885]	Time 0.148 (0.142)	Data 2.14e-04 (2.08e-03)	Tok/s 100549 (93087)	Loss/tok 3.5669 (3.4291)	LR 2.800e-03
0: TRAIN [1][120/1885]	Time 0.148 (0.143)	Data 2.21e-04 (1.92e-03)	Tok/s 97705 (93217)	Loss/tok 3.4331 (3.4317)	LR 2.800e-03
0: TRAIN [1][130/1885]	Time 0.103 (0.144)	Data 2.17e-04 (1.79e-03)	Tok/s 87911 (93411)	Loss/tok 3.1699 (3.4411)	LR 2.800e-03
0: TRAIN [1][140/1885]	Time 0.148 (0.142)	Data 2.18e-04 (1.68e-03)	Tok/s 98280 (93223)	Loss/tok 3.4436 (3.4335)	LR 2.800e-03
0: TRAIN [1][150/1885]	Time 0.148 (0.144)	Data 1.66e-04 (1.58e-03)	Tok/s 98786 (93551)	Loss/tok 3.4758 (3.4416)	LR 2.800e-03
0: TRAIN [1][160/1885]	Time 0.147 (0.144)	Data 2.15e-04 (1.50e-03)	Tok/s 100363 (93704)	Loss/tok 3.3648 (3.4392)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][170/1885]	Time 0.105 (0.144)	Data 1.77e-04 (1.42e-03)	Tok/s 84717 (93926)	Loss/tok 3.0915 (3.4405)	LR 2.800e-03
0: TRAIN [1][180/1885]	Time 0.105 (0.144)	Data 2.18e-04 (1.35e-03)	Tok/s 84209 (93982)	Loss/tok 3.2154 (3.4371)	LR 2.800e-03
0: TRAIN [1][190/1885]	Time 0.104 (0.143)	Data 2.19e-04 (1.29e-03)	Tok/s 85995 (93849)	Loss/tok 3.2413 (3.4313)	LR 2.800e-03
0: TRAIN [1][200/1885]	Time 0.197 (0.143)	Data 1.45e-04 (1.24e-03)	Tok/s 105046 (93753)	Loss/tok 3.3068 (3.4270)	LR 2.800e-03
0: TRAIN [1][210/1885]	Time 0.104 (0.143)	Data 1.76e-04 (1.19e-03)	Tok/s 87406 (93766)	Loss/tok 3.1132 (3.4265)	LR 2.800e-03
0: TRAIN [1][220/1885]	Time 0.197 (0.143)	Data 2.20e-04 (1.14e-03)	Tok/s 104328 (93943)	Loss/tok 3.4978 (3.4251)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][230/1885]	Time 0.245 (0.144)	Data 2.18e-04 (1.10e-03)	Tok/s 106776 (93950)	Loss/tok 3.7420 (3.4292)	LR 2.800e-03
0: TRAIN [1][240/1885]	Time 0.247 (0.145)	Data 1.98e-04 (1.07e-03)	Tok/s 105836 (94194)	Loss/tok 3.6873 (3.4338)	LR 2.800e-03
0: TRAIN [1][250/1885]	Time 0.149 (0.145)	Data 1.97e-04 (1.03e-03)	Tok/s 99062 (94280)	Loss/tok 3.3924 (3.4342)	LR 2.800e-03
0: TRAIN [1][260/1885]	Time 0.150 (0.145)	Data 2.20e-04 (9.99e-04)	Tok/s 98613 (94291)	Loss/tok 3.2366 (3.4325)	LR 2.800e-03
0: TRAIN [1][270/1885]	Time 0.105 (0.145)	Data 2.21e-04 (9.70e-04)	Tok/s 85726 (94189)	Loss/tok 3.1279 (3.4307)	LR 2.800e-03
0: TRAIN [1][280/1885]	Time 0.149 (0.145)	Data 2.17e-04 (9.43e-04)	Tok/s 98389 (94257)	Loss/tok 3.3311 (3.4290)	LR 2.800e-03
0: TRAIN [1][290/1885]	Time 0.064 (0.145)	Data 2.19e-04 (9.18e-04)	Tok/s 73670 (94321)	Loss/tok 2.6445 (3.4278)	LR 2.800e-03
0: TRAIN [1][300/1885]	Time 0.148 (0.144)	Data 2.22e-04 (8.94e-04)	Tok/s 99868 (94224)	Loss/tok 3.3765 (3.4253)	LR 2.800e-03
0: TRAIN [1][310/1885]	Time 0.104 (0.145)	Data 2.18e-04 (8.72e-04)	Tok/s 87639 (94415)	Loss/tok 3.1871 (3.4296)	LR 2.800e-03
0: TRAIN [1][320/1885]	Time 0.148 (0.146)	Data 1.50e-04 (8.51e-04)	Tok/s 99379 (94602)	Loss/tok 3.3629 (3.4322)	LR 2.800e-03
0: TRAIN [1][330/1885]	Time 0.105 (0.146)	Data 2.31e-04 (8.31e-04)	Tok/s 87075 (94594)	Loss/tok 3.1784 (3.4324)	LR 2.800e-03
0: TRAIN [1][340/1885]	Time 0.104 (0.145)	Data 1.51e-04 (8.12e-04)	Tok/s 87900 (94481)	Loss/tok 3.1421 (3.4301)	LR 2.800e-03
0: TRAIN [1][350/1885]	Time 0.148 (0.145)	Data 2.19e-04 (7.95e-04)	Tok/s 99156 (94507)	Loss/tok 3.3435 (3.4285)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][360/1885]	Time 0.195 (0.146)	Data 2.20e-04 (7.79e-04)	Tok/s 103877 (94573)	Loss/tok 3.5716 (3.4284)	LR 2.800e-03
0: TRAIN [1][370/1885]	Time 0.105 (0.145)	Data 2.17e-04 (7.63e-04)	Tok/s 88516 (94560)	Loss/tok 3.0321 (3.4244)	LR 2.800e-03
0: TRAIN [1][380/1885]	Time 0.154 (0.145)	Data 1.53e-04 (7.48e-04)	Tok/s 95230 (94529)	Loss/tok 3.3412 (3.4214)	LR 2.800e-03
0: TRAIN [1][390/1885]	Time 0.150 (0.145)	Data 2.18e-04 (7.34e-04)	Tok/s 97992 (94595)	Loss/tok 3.4678 (3.4236)	LR 2.800e-03
0: TRAIN [1][400/1885]	Time 0.063 (0.145)	Data 2.17e-04 (7.21e-04)	Tok/s 72537 (94507)	Loss/tok 2.5739 (3.4213)	LR 2.800e-03
0: TRAIN [1][410/1885]	Time 0.149 (0.145)	Data 2.17e-04 (7.09e-04)	Tok/s 99340 (94505)	Loss/tok 3.3844 (3.4206)	LR 2.800e-03
0: TRAIN [1][420/1885]	Time 0.150 (0.144)	Data 2.19e-04 (6.97e-04)	Tok/s 99331 (94488)	Loss/tok 3.3176 (3.4181)	LR 2.800e-03
0: TRAIN [1][430/1885]	Time 0.063 (0.144)	Data 2.17e-04 (6.85e-04)	Tok/s 74460 (94472)	Loss/tok 2.5851 (3.4170)	LR 2.800e-03
0: TRAIN [1][440/1885]	Time 0.151 (0.144)	Data 2.16e-04 (6.74e-04)	Tok/s 97276 (94476)	Loss/tok 3.3340 (3.4155)	LR 2.800e-03
0: TRAIN [1][450/1885]	Time 0.105 (0.144)	Data 2.17e-04 (6.64e-04)	Tok/s 86681 (94466)	Loss/tok 3.1034 (3.4139)	LR 2.800e-03
0: TRAIN [1][460/1885]	Time 0.150 (0.144)	Data 2.19e-04 (6.54e-04)	Tok/s 97347 (94551)	Loss/tok 3.3576 (3.4148)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][470/1885]	Time 0.104 (0.145)	Data 2.03e-04 (6.44e-04)	Tok/s 87450 (94557)	Loss/tok 3.0914 (3.4159)	LR 2.800e-03
0: TRAIN [1][480/1885]	Time 0.196 (0.145)	Data 1.45e-04 (6.35e-04)	Tok/s 104105 (94564)	Loss/tok 3.7675 (3.4163)	LR 2.800e-03
0: TRAIN [1][490/1885]	Time 0.105 (0.144)	Data 2.23e-04 (6.26e-04)	Tok/s 87620 (94535)	Loss/tok 3.0684 (3.4155)	LR 2.800e-03
0: TRAIN [1][500/1885]	Time 0.105 (0.144)	Data 1.50e-04 (6.17e-04)	Tok/s 84996 (94480)	Loss/tok 3.1674 (3.4148)	LR 2.800e-03
0: TRAIN [1][510/1885]	Time 0.147 (0.144)	Data 2.20e-04 (6.10e-04)	Tok/s 98763 (94472)	Loss/tok 3.5174 (3.4136)	LR 2.800e-03
0: TRAIN [1][520/1885]	Time 0.147 (0.144)	Data 2.18e-04 (6.02e-04)	Tok/s 99858 (94410)	Loss/tok 3.3765 (3.4136)	LR 2.800e-03
0: TRAIN [1][530/1885]	Time 0.103 (0.144)	Data 2.16e-04 (5.94e-04)	Tok/s 87767 (94413)	Loss/tok 3.0981 (3.4138)	LR 2.800e-03
0: TRAIN [1][540/1885]	Time 0.195 (0.144)	Data 1.77e-04 (5.87e-04)	Tok/s 103927 (94439)	Loss/tok 3.6008 (3.4139)	LR 2.800e-03
0: TRAIN [1][550/1885]	Time 0.192 (0.144)	Data 1.91e-04 (5.80e-04)	Tok/s 107591 (94454)	Loss/tok 3.6248 (3.4147)	LR 2.800e-03
0: TRAIN [1][560/1885]	Time 0.148 (0.144)	Data 2.16e-04 (5.74e-04)	Tok/s 98929 (94384)	Loss/tok 3.3362 (3.4138)	LR 2.800e-03
0: TRAIN [1][570/1885]	Time 0.149 (0.143)	Data 2.18e-04 (5.67e-04)	Tok/s 99687 (94407)	Loss/tok 3.3500 (3.4126)	LR 2.800e-03
0: TRAIN [1][580/1885]	Time 0.149 (0.143)	Data 1.43e-04 (5.61e-04)	Tok/s 99479 (94344)	Loss/tok 3.3061 (3.4106)	LR 2.800e-03
0: TRAIN [1][590/1885]	Time 0.106 (0.143)	Data 1.92e-04 (5.55e-04)	Tok/s 86914 (94337)	Loss/tok 3.0336 (3.4088)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][600/1885]	Time 0.150 (0.143)	Data 2.14e-04 (5.49e-04)	Tok/s 99389 (94386)	Loss/tok 3.3681 (3.4090)	LR 2.800e-03
0: TRAIN [1][610/1885]	Time 0.147 (0.143)	Data 2.20e-04 (5.43e-04)	Tok/s 99996 (94420)	Loss/tok 3.2498 (3.4085)	LR 2.800e-03
0: TRAIN [1][620/1885]	Time 0.248 (0.144)	Data 1.65e-04 (5.38e-04)	Tok/s 104662 (94460)	Loss/tok 3.6500 (3.4105)	LR 2.800e-03
0: TRAIN [1][630/1885]	Time 0.104 (0.144)	Data 2.17e-04 (5.32e-04)	Tok/s 86194 (94478)	Loss/tok 3.1217 (3.4102)	LR 2.800e-03
0: TRAIN [1][640/1885]	Time 0.148 (0.143)	Data 2.16e-04 (5.27e-04)	Tok/s 98335 (94446)	Loss/tok 3.4086 (3.4082)	LR 2.800e-03
0: TRAIN [1][650/1885]	Time 0.147 (0.143)	Data 2.19e-04 (5.22e-04)	Tok/s 97930 (94449)	Loss/tok 3.4673 (3.4078)	LR 2.800e-03
0: TRAIN [1][660/1885]	Time 0.245 (0.143)	Data 2.18e-04 (5.17e-04)	Tok/s 107816 (94463)	Loss/tok 3.7523 (3.4069)	LR 2.800e-03
0: TRAIN [1][670/1885]	Time 0.103 (0.144)	Data 2.19e-04 (5.13e-04)	Tok/s 88486 (94475)	Loss/tok 3.0403 (3.4068)	LR 2.800e-03
0: TRAIN [1][680/1885]	Time 0.103 (0.144)	Data 2.18e-04 (5.08e-04)	Tok/s 86358 (94492)	Loss/tok 3.1617 (3.4068)	LR 2.800e-03
0: TRAIN [1][690/1885]	Time 0.150 (0.143)	Data 2.20e-04 (5.04e-04)	Tok/s 98204 (94450)	Loss/tok 3.4114 (3.4053)	LR 2.800e-03
0: TRAIN [1][700/1885]	Time 0.147 (0.143)	Data 2.21e-04 (4.99e-04)	Tok/s 99312 (94476)	Loss/tok 3.2370 (3.4057)	LR 2.800e-03
0: TRAIN [1][710/1885]	Time 0.248 (0.144)	Data 2.18e-04 (4.95e-04)	Tok/s 107058 (94542)	Loss/tok 3.5787 (3.4070)	LR 2.800e-03
0: TRAIN [1][720/1885]	Time 0.145 (0.143)	Data 2.21e-04 (4.91e-04)	Tok/s 99535 (94446)	Loss/tok 3.4772 (3.4052)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][730/1885]	Time 0.101 (0.143)	Data 2.17e-04 (4.87e-04)	Tok/s 88489 (94448)	Loss/tok 3.1060 (3.4048)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][740/1885]	Time 0.147 (0.143)	Data 2.19e-04 (4.84e-04)	Tok/s 100341 (94525)	Loss/tok 3.2368 (3.4042)	LR 2.800e-03
0: TRAIN [1][750/1885]	Time 0.061 (0.143)	Data 2.38e-04 (4.80e-04)	Tok/s 75807 (94508)	Loss/tok 2.6640 (3.4035)	LR 2.800e-03
0: TRAIN [1][760/1885]	Time 0.146 (0.143)	Data 2.29e-04 (4.77e-04)	Tok/s 100730 (94528)	Loss/tok 3.2446 (3.4021)	LR 2.800e-03
0: TRAIN [1][770/1885]	Time 0.146 (0.143)	Data 2.20e-04 (4.73e-04)	Tok/s 101058 (94506)	Loss/tok 3.2224 (3.4003)	LR 2.800e-03
0: TRAIN [1][780/1885]	Time 0.193 (0.143)	Data 2.20e-04 (4.70e-04)	Tok/s 105459 (94504)	Loss/tok 3.4384 (3.3994)	LR 2.800e-03
0: TRAIN [1][790/1885]	Time 0.193 (0.143)	Data 2.19e-04 (4.67e-04)	Tok/s 106432 (94544)	Loss/tok 3.4054 (3.3998)	LR 2.800e-03
0: TRAIN [1][800/1885]	Time 0.103 (0.143)	Data 2.17e-04 (4.64e-04)	Tok/s 87634 (94550)	Loss/tok 3.1514 (3.3993)	LR 2.800e-03
0: TRAIN [1][810/1885]	Time 0.193 (0.143)	Data 2.17e-04 (4.61e-04)	Tok/s 104770 (94609)	Loss/tok 3.6068 (3.4011)	LR 2.800e-03
0: TRAIN [1][820/1885]	Time 0.146 (0.143)	Data 1.89e-04 (4.58e-04)	Tok/s 101847 (94625)	Loss/tok 3.4002 (3.4008)	LR 2.800e-03
0: TRAIN [1][830/1885]	Time 0.147 (0.143)	Data 2.19e-04 (4.55e-04)	Tok/s 99259 (94669)	Loss/tok 3.4237 (3.4011)	LR 2.800e-03
0: TRAIN [1][840/1885]	Time 0.146 (0.143)	Data 2.16e-04 (4.52e-04)	Tok/s 100845 (94639)	Loss/tok 3.4005 (3.4002)	LR 2.800e-03
0: TRAIN [1][850/1885]	Time 0.146 (0.142)	Data 2.30e-04 (4.49e-04)	Tok/s 101077 (94642)	Loss/tok 3.3666 (3.3992)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][860/1885]	Time 0.194 (0.142)	Data 2.18e-04 (4.47e-04)	Tok/s 105787 (94633)	Loss/tok 3.5353 (3.3982)	LR 2.800e-03
0: TRAIN [1][870/1885]	Time 0.101 (0.142)	Data 2.14e-04 (4.44e-04)	Tok/s 90010 (94630)	Loss/tok 3.1697 (3.3978)	LR 2.800e-03
0: TRAIN [1][880/1885]	Time 0.146 (0.142)	Data 2.21e-04 (4.41e-04)	Tok/s 98857 (94640)	Loss/tok 3.4354 (3.3975)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][890/1885]	Time 0.194 (0.143)	Data 2.19e-04 (4.39e-04)	Tok/s 105633 (94731)	Loss/tok 3.4802 (3.3997)	LR 2.800e-03
0: TRAIN [1][900/1885]	Time 0.102 (0.142)	Data 2.31e-04 (4.36e-04)	Tok/s 89747 (94673)	Loss/tok 3.1114 (3.3984)	LR 2.800e-03
0: TRAIN [1][910/1885]	Time 0.102 (0.142)	Data 2.20e-04 (4.34e-04)	Tok/s 90777 (94665)	Loss/tok 3.2646 (3.3978)	LR 2.800e-03
0: TRAIN [1][920/1885]	Time 0.146 (0.142)	Data 2.21e-04 (4.32e-04)	Tok/s 98487 (94668)	Loss/tok 3.3572 (3.3977)	LR 2.800e-03
0: TRAIN [1][930/1885]	Time 0.102 (0.142)	Data 2.17e-04 (4.29e-04)	Tok/s 90035 (94734)	Loss/tok 3.1040 (3.3989)	LR 2.800e-03
0: TRAIN [1][940/1885]	Time 0.146 (0.142)	Data 2.17e-04 (4.27e-04)	Tok/s 100074 (94769)	Loss/tok 3.4718 (3.3993)	LR 2.800e-03
0: TRAIN [1][950/1885]	Time 0.194 (0.142)	Data 2.17e-04 (4.25e-04)	Tok/s 104623 (94795)	Loss/tok 3.5744 (3.3988)	LR 2.800e-03
0: TRAIN [1][960/1885]	Time 0.245 (0.143)	Data 2.21e-04 (4.23e-04)	Tok/s 106203 (94868)	Loss/tok 3.4680 (3.3995)	LR 2.800e-03
0: TRAIN [1][970/1885]	Time 0.147 (0.143)	Data 2.16e-04 (4.21e-04)	Tok/s 99964 (94911)	Loss/tok 3.3618 (3.4000)	LR 2.800e-03
0: TRAIN [1][980/1885]	Time 0.102 (0.143)	Data 2.21e-04 (4.19e-04)	Tok/s 87249 (94912)	Loss/tok 3.0857 (3.4000)	LR 2.800e-03
0: TRAIN [1][990/1885]	Time 0.147 (0.143)	Data 2.20e-04 (4.17e-04)	Tok/s 98742 (94930)	Loss/tok 3.2719 (3.3992)	LR 2.800e-03
0: TRAIN [1][1000/1885]	Time 0.193 (0.143)	Data 2.20e-04 (4.15e-04)	Tok/s 106754 (94975)	Loss/tok 3.4288 (3.3989)	LR 2.800e-03
0: TRAIN [1][1010/1885]	Time 0.146 (0.143)	Data 2.17e-04 (4.13e-04)	Tok/s 101012 (94996)	Loss/tok 3.2816 (3.3985)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1020/1885]	Time 0.146 (0.143)	Data 2.17e-04 (4.11e-04)	Tok/s 99803 (95008)	Loss/tok 3.4686 (3.3979)	LR 2.800e-03
0: TRAIN [1][1030/1885]	Time 0.147 (0.143)	Data 2.19e-04 (4.09e-04)	Tok/s 99388 (95068)	Loss/tok 3.3003 (3.3985)	LR 2.800e-03
0: TRAIN [1][1040/1885]	Time 0.245 (0.143)	Data 2.20e-04 (4.07e-04)	Tok/s 108624 (95090)	Loss/tok 3.5932 (3.3983)	LR 2.800e-03
0: TRAIN [1][1050/1885]	Time 0.192 (0.143)	Data 2.20e-04 (4.05e-04)	Tok/s 107030 (95119)	Loss/tok 3.5411 (3.3990)	LR 2.800e-03
0: TRAIN [1][1060/1885]	Time 0.193 (0.143)	Data 2.18e-04 (4.04e-04)	Tok/s 105605 (95172)	Loss/tok 3.4855 (3.3998)	LR 2.800e-03
0: TRAIN [1][1070/1885]	Time 0.146 (0.143)	Data 2.25e-04 (4.02e-04)	Tok/s 101028 (95226)	Loss/tok 3.3147 (3.4004)	LR 2.800e-03
0: TRAIN [1][1080/1885]	Time 0.145 (0.143)	Data 2.19e-04 (4.00e-04)	Tok/s 100805 (95239)	Loss/tok 3.3325 (3.3999)	LR 2.800e-03
0: TRAIN [1][1090/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.98e-04)	Tok/s 99822 (95224)	Loss/tok 3.2353 (3.3987)	LR 2.800e-03
0: TRAIN [1][1100/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.97e-04)	Tok/s 99604 (95249)	Loss/tok 3.4227 (3.3982)	LR 2.800e-03
0: TRAIN [1][1110/1885]	Time 0.245 (0.143)	Data 2.17e-04 (3.95e-04)	Tok/s 107147 (95280)	Loss/tok 3.6521 (3.3984)	LR 2.800e-03
0: TRAIN [1][1120/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.94e-04)	Tok/s 89677 (95222)	Loss/tok 3.0704 (3.3970)	LR 2.800e-03
0: TRAIN [1][1130/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.92e-04)	Tok/s 87228 (95241)	Loss/tok 3.1174 (3.3970)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1140/1885]	Time 0.147 (0.143)	Data 2.23e-04 (3.91e-04)	Tok/s 101502 (95282)	Loss/tok 3.2824 (3.3973)	LR 2.800e-03
0: TRAIN [1][1150/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.89e-04)	Tok/s 88802 (95271)	Loss/tok 3.1345 (3.3974)	LR 2.800e-03
0: TRAIN [1][1160/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.88e-04)	Tok/s 90496 (95251)	Loss/tok 3.0723 (3.3963)	LR 2.800e-03
0: TRAIN [1][1170/1885]	Time 0.145 (0.143)	Data 2.19e-04 (3.86e-04)	Tok/s 98964 (95229)	Loss/tok 3.3829 (3.3958)	LR 2.800e-03
0: TRAIN [1][1180/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.85e-04)	Tok/s 88635 (95249)	Loss/tok 3.1273 (3.3952)	LR 2.800e-03
0: TRAIN [1][1190/1885]	Time 0.145 (0.143)	Data 2.20e-04 (3.83e-04)	Tok/s 100168 (95262)	Loss/tok 3.3909 (3.3956)	LR 2.800e-03
0: TRAIN [1][1200/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.82e-04)	Tok/s 88909 (95285)	Loss/tok 3.0214 (3.3948)	LR 2.800e-03
0: TRAIN [1][1210/1885]	Time 0.101 (0.143)	Data 2.21e-04 (3.81e-04)	Tok/s 92891 (95299)	Loss/tok 3.0977 (3.3943)	LR 2.800e-03
0: TRAIN [1][1220/1885]	Time 0.194 (0.143)	Data 2.17e-04 (3.79e-04)	Tok/s 105198 (95314)	Loss/tok 3.5672 (3.3943)	LR 2.800e-03
0: TRAIN [1][1230/1885]	Time 0.193 (0.143)	Data 2.21e-04 (3.78e-04)	Tok/s 105157 (95327)	Loss/tok 3.4642 (3.3940)	LR 2.800e-03
0: TRAIN [1][1240/1885]	Time 0.194 (0.143)	Data 2.30e-04 (3.77e-04)	Tok/s 104230 (95335)	Loss/tok 3.5605 (3.3940)	LR 2.800e-03
0: TRAIN [1][1250/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.75e-04)	Tok/s 100349 (95363)	Loss/tok 3.2811 (3.3938)	LR 2.800e-03
0: TRAIN [1][1260/1885]	Time 0.192 (0.143)	Data 2.17e-04 (3.74e-04)	Tok/s 105025 (95394)	Loss/tok 3.4563 (3.3944)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1270/1885]	Time 0.246 (0.143)	Data 2.18e-04 (3.73e-04)	Tok/s 106056 (95456)	Loss/tok 3.6595 (3.3957)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1280/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.72e-04)	Tok/s 87840 (95488)	Loss/tok 3.1303 (3.3960)	LR 2.800e-03
0: TRAIN [1][1290/1885]	Time 0.101 (0.143)	Data 2.34e-04 (3.70e-04)	Tok/s 92238 (95469)	Loss/tok 3.0942 (3.3954)	LR 2.800e-03
0: TRAIN [1][1300/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.69e-04)	Tok/s 100447 (95485)	Loss/tok 3.4299 (3.3950)	LR 2.800e-03
0: TRAIN [1][1310/1885]	Time 0.146 (0.143)	Data 2.17e-04 (3.68e-04)	Tok/s 101666 (95499)	Loss/tok 3.2886 (3.3947)	LR 2.800e-03
0: TRAIN [1][1320/1885]	Time 0.193 (0.143)	Data 2.19e-04 (3.67e-04)	Tok/s 106027 (95495)	Loss/tok 3.4660 (3.3943)	LR 2.800e-03
0: TRAIN [1][1330/1885]	Time 0.194 (0.143)	Data 2.20e-04 (3.66e-04)	Tok/s 105986 (95490)	Loss/tok 3.4030 (3.3936)	LR 2.800e-03
0: TRAIN [1][1340/1885]	Time 0.100 (0.143)	Data 2.16e-04 (3.65e-04)	Tok/s 91979 (95504)	Loss/tok 2.9765 (3.3930)	LR 2.800e-03
0: TRAIN [1][1350/1885]	Time 0.102 (0.143)	Data 2.16e-04 (3.64e-04)	Tok/s 90172 (95500)	Loss/tok 2.9719 (3.3925)	LR 2.800e-03
0: TRAIN [1][1360/1885]	Time 0.243 (0.143)	Data 2.16e-04 (3.63e-04)	Tok/s 106853 (95514)	Loss/tok 3.5393 (3.3931)	LR 2.800e-03
0: TRAIN [1][1370/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.62e-04)	Tok/s 88862 (95508)	Loss/tok 3.0245 (3.3923)	LR 2.800e-03
0: TRAIN [1][1380/1885]	Time 0.145 (0.143)	Data 2.20e-04 (3.61e-04)	Tok/s 100340 (95511)	Loss/tok 3.3783 (3.3919)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1390/1885]	Time 0.146 (0.143)	Data 2.27e-04 (3.60e-04)	Tok/s 99067 (95515)	Loss/tok 3.3997 (3.3918)	LR 2.800e-03
0: TRAIN [1][1400/1885]	Time 0.194 (0.143)	Data 2.17e-04 (3.59e-04)	Tok/s 105159 (95513)	Loss/tok 3.5217 (3.3915)	LR 2.800e-03
0: TRAIN [1][1410/1885]	Time 0.145 (0.143)	Data 2.18e-04 (3.58e-04)	Tok/s 101581 (95528)	Loss/tok 3.2961 (3.3916)	LR 2.800e-03
0: TRAIN [1][1420/1885]	Time 0.192 (0.143)	Data 2.17e-04 (3.57e-04)	Tok/s 106223 (95549)	Loss/tok 3.4061 (3.3917)	LR 2.800e-03
0: TRAIN [1][1430/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.56e-04)	Tok/s 100314 (95555)	Loss/tok 3.2286 (3.3910)	LR 2.800e-03
0: TRAIN [1][1440/1885]	Time 0.102 (0.143)	Data 2.15e-04 (3.55e-04)	Tok/s 88690 (95569)	Loss/tok 3.1164 (3.3915)	LR 2.800e-03
0: TRAIN [1][1450/1885]	Time 0.147 (0.143)	Data 2.16e-04 (3.54e-04)	Tok/s 99819 (95578)	Loss/tok 3.4129 (3.3912)	LR 2.800e-03
0: TRAIN [1][1460/1885]	Time 0.102 (0.143)	Data 2.17e-04 (3.53e-04)	Tok/s 90386 (95583)	Loss/tok 3.2272 (3.3910)	LR 2.800e-03
0: TRAIN [1][1470/1885]	Time 0.193 (0.143)	Data 2.19e-04 (3.52e-04)	Tok/s 105114 (95582)	Loss/tok 3.4930 (3.3902)	LR 2.800e-03
0: TRAIN [1][1480/1885]	Time 0.243 (0.143)	Data 2.25e-04 (3.51e-04)	Tok/s 107651 (95621)	Loss/tok 3.6281 (3.3902)	LR 2.800e-03
0: TRAIN [1][1490/1885]	Time 0.193 (0.143)	Data 2.21e-04 (3.50e-04)	Tok/s 105779 (95659)	Loss/tok 3.3482 (3.3902)	LR 2.800e-03
0: TRAIN [1][1500/1885]	Time 0.100 (0.143)	Data 2.18e-04 (3.49e-04)	Tok/s 90129 (95668)	Loss/tok 3.0177 (3.3897)	LR 2.800e-03
0: TRAIN [1][1510/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.48e-04)	Tok/s 100372 (95683)	Loss/tok 3.4334 (3.3892)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1520/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.48e-04)	Tok/s 89518 (95684)	Loss/tok 2.9714 (3.3886)	LR 2.800e-03
0: TRAIN [1][1530/1885]	Time 0.145 (0.143)	Data 2.18e-04 (3.47e-04)	Tok/s 101187 (95710)	Loss/tok 3.3052 (3.3884)	LR 2.800e-03
0: TRAIN [1][1540/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.46e-04)	Tok/s 91496 (95726)	Loss/tok 3.0770 (3.3888)	LR 2.800e-03
0: TRAIN [1][1550/1885]	Time 0.146 (0.143)	Data 2.18e-04 (3.45e-04)	Tok/s 101076 (95723)	Loss/tok 3.2171 (3.3877)	LR 2.800e-03
0: TRAIN [1][1560/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.44e-04)	Tok/s 99940 (95733)	Loss/tok 3.3636 (3.3875)	LR 2.800e-03
0: TRAIN [1][1570/1885]	Time 0.059 (0.143)	Data 2.17e-04 (3.43e-04)	Tok/s 76098 (95731)	Loss/tok 2.5204 (3.3873)	LR 2.800e-03
0: TRAIN [1][1580/1885]	Time 0.193 (0.143)	Data 2.40e-04 (3.43e-04)	Tok/s 105312 (95732)	Loss/tok 3.5233 (3.3872)	LR 2.800e-03
0: TRAIN [1][1590/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.42e-04)	Tok/s 100339 (95757)	Loss/tok 3.2917 (3.3871)	LR 2.800e-03
0: TRAIN [1][1600/1885]	Time 0.059 (0.143)	Data 2.21e-04 (3.41e-04)	Tok/s 76730 (95770)	Loss/tok 2.6332 (3.3871)	LR 2.800e-03
0: TRAIN [1][1610/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.40e-04)	Tok/s 101248 (95780)	Loss/tok 3.4119 (3.3864)	LR 2.800e-03
0: TRAIN [1][1620/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.40e-04)	Tok/s 90074 (95795)	Loss/tok 3.1797 (3.3868)	LR 2.800e-03
0: TRAIN [1][1630/1885]	Time 0.146 (0.143)	Data 2.37e-04 (3.39e-04)	Tok/s 101461 (95819)	Loss/tok 3.3652 (3.3865)	LR 2.800e-03
0: TRAIN [1][1640/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.38e-04)	Tok/s 89180 (95818)	Loss/tok 2.9351 (3.3860)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1650/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.37e-04)	Tok/s 100687 (95815)	Loss/tok 3.2953 (3.3853)	LR 2.800e-03
0: TRAIN [1][1660/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.37e-04)	Tok/s 90821 (95818)	Loss/tok 3.1537 (3.3853)	LR 2.800e-03
0: TRAIN [1][1670/1885]	Time 0.245 (0.143)	Data 2.22e-04 (3.36e-04)	Tok/s 108030 (95832)	Loss/tok 3.6305 (3.3853)	LR 2.800e-03
0: TRAIN [1][1680/1885]	Time 0.145 (0.143)	Data 2.17e-04 (3.35e-04)	Tok/s 102176 (95849)	Loss/tok 3.3894 (3.3851)	LR 2.800e-03
0: TRAIN [1][1690/1885]	Time 0.244 (0.143)	Data 2.21e-04 (3.35e-04)	Tok/s 108973 (95827)	Loss/tok 3.5547 (3.3844)	LR 2.800e-03
0: TRAIN [1][1700/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.34e-04)	Tok/s 101635 (95844)	Loss/tok 3.2804 (3.3844)	LR 2.800e-03
0: TRAIN [1][1710/1885]	Time 0.101 (0.143)	Data 2.28e-04 (3.33e-04)	Tok/s 88724 (95839)	Loss/tok 3.1012 (3.3842)	LR 2.800e-03
0: TRAIN [1][1720/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.33e-04)	Tok/s 89838 (95829)	Loss/tok 3.1216 (3.3840)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1730/1885]	Time 0.195 (0.143)	Data 2.20e-04 (3.32e-04)	Tok/s 105446 (95819)	Loss/tok 3.4989 (3.3838)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1740/1885]	Time 0.145 (0.143)	Data 2.19e-04 (3.31e-04)	Tok/s 102905 (95836)	Loss/tok 3.1858 (3.3840)	LR 2.800e-03
0: TRAIN [1][1750/1885]	Time 0.193 (0.143)	Data 2.15e-04 (3.31e-04)	Tok/s 106390 (95866)	Loss/tok 3.3219 (3.3837)	LR 2.800e-03
0: TRAIN [1][1760/1885]	Time 0.194 (0.143)	Data 2.19e-04 (3.30e-04)	Tok/s 105805 (95887)	Loss/tok 3.4977 (3.3835)	LR 2.800e-03
0: TRAIN [1][1770/1885]	Time 0.100 (0.143)	Data 2.18e-04 (3.29e-04)	Tok/s 89004 (95913)	Loss/tok 3.1842 (3.3833)	LR 2.800e-03
0: TRAIN [1][1780/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.29e-04)	Tok/s 90511 (95899)	Loss/tok 3.1339 (3.3827)	LR 2.800e-03
0: TRAIN [1][1790/1885]	Time 0.146 (0.143)	Data 2.20e-04 (3.28e-04)	Tok/s 101121 (95920)	Loss/tok 3.2557 (3.3823)	LR 2.800e-03
0: TRAIN [1][1800/1885]	Time 0.194 (0.143)	Data 2.17e-04 (3.27e-04)	Tok/s 104967 (95938)	Loss/tok 3.4758 (3.3823)	LR 2.800e-03
0: TRAIN [1][1810/1885]	Time 0.193 (0.143)	Data 2.21e-04 (3.27e-04)	Tok/s 106705 (95953)	Loss/tok 3.3555 (3.3820)	LR 2.800e-03
0: TRAIN [1][1820/1885]	Time 0.193 (0.143)	Data 2.20e-04 (3.26e-04)	Tok/s 106590 (95957)	Loss/tok 3.3909 (3.3817)	LR 2.800e-03
0: TRAIN [1][1830/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.26e-04)	Tok/s 100455 (95967)	Loss/tok 3.3524 (3.3814)	LR 2.800e-03
0: TRAIN [1][1840/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.25e-04)	Tok/s 91754 (95985)	Loss/tok 2.9932 (3.3814)	LR 2.800e-03
0: TRAIN [1][1850/1885]	Time 0.193 (0.143)	Data 2.30e-04 (3.25e-04)	Tok/s 106149 (95989)	Loss/tok 3.4264 (3.3808)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1860/1885]	Time 0.102 (0.143)	Data 2.24e-04 (3.24e-04)	Tok/s 90795 (95995)	Loss/tok 2.9128 (3.3808)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1870/1885]	Time 0.101 (0.143)	Data 2.22e-04 (3.23e-04)	Tok/s 90387 (96000)	Loss/tok 3.1375 (3.3806)	LR 2.800e-03
0: TRAIN [1][1880/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.23e-04)	Tok/s 99810 (96008)	Loss/tok 3.3366 (3.3807)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593831217633, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593831217634, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.708 (0.708)	Decoder iters 149.0 (149.0)	Tok/s 24048 (24048)
0: Running moses detokenizer
0: BLEU(score=21.791568679993286, counts=[35830, 17234, 9459, 5452], totals=[65897, 62894, 59891, 56891], precisions=[54.37273320484999, 27.401659935764936, 15.79369187357032, 9.583238122022816], bp=1.0, sys_len=65897, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593831219365, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21789999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593831219365, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3831	Test BLEU: 21.79
0: Performance: Epoch: 1	Training: 767863 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593831219365, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593831219366, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593831219366, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 4131901174
0: TRAIN [2][0/1885]	Time 0.365 (0.365)	Data 2.10e-01 (2.10e-01)	Tok/s 25073 (25073)	Loss/tok 3.0238 (3.0238)	LR 2.800e-03
0: TRAIN [2][10/1885]	Time 0.146 (0.150)	Data 2.23e-04 (1.93e-02)	Tok/s 99219 (89386)	Loss/tok 3.2943 (3.1563)	LR 2.800e-03
0: TRAIN [2][20/1885]	Time 0.101 (0.140)	Data 2.19e-04 (1.02e-02)	Tok/s 90384 (92364)	Loss/tok 3.0083 (3.1676)	LR 2.800e-03
0: TRAIN [2][30/1885]	Time 0.102 (0.148)	Data 2.21e-04 (6.97e-03)	Tok/s 88860 (94938)	Loss/tok 3.0651 (3.2289)	LR 2.800e-03
0: TRAIN [2][40/1885]	Time 0.146 (0.152)	Data 2.20e-04 (5.32e-03)	Tok/s 101689 (96418)	Loss/tok 3.2012 (3.2433)	LR 2.800e-03
0: TRAIN [2][50/1885]	Time 0.101 (0.149)	Data 2.32e-04 (4.32e-03)	Tok/s 90010 (96269)	Loss/tok 3.0096 (3.2344)	LR 2.800e-03
0: TRAIN [2][60/1885]	Time 0.147 (0.146)	Data 2.24e-04 (3.65e-03)	Tok/s 100513 (95993)	Loss/tok 3.1521 (3.2274)	LR 2.800e-03
0: TRAIN [2][70/1885]	Time 0.148 (0.143)	Data 2.21e-04 (3.17e-03)	Tok/s 100410 (95815)	Loss/tok 3.1612 (3.2202)	LR 2.800e-03
0: TRAIN [2][80/1885]	Time 0.194 (0.148)	Data 2.18e-04 (2.80e-03)	Tok/s 105369 (96664)	Loss/tok 3.3788 (3.2462)	LR 2.800e-03
0: TRAIN [2][90/1885]	Time 0.100 (0.145)	Data 2.18e-04 (2.52e-03)	Tok/s 89571 (96023)	Loss/tok 3.0465 (3.2396)	LR 2.800e-03
0: TRAIN [2][100/1885]	Time 0.059 (0.142)	Data 2.16e-04 (2.29e-03)	Tok/s 77245 (95716)	Loss/tok 2.4533 (3.2292)	LR 2.800e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][110/1885]	Time 0.147 (0.143)	Data 2.21e-04 (2.10e-03)	Tok/s 100357 (96117)	Loss/tok 3.2331 (3.2392)	LR 2.800e-03
0: TRAIN [2][120/1885]	Time 0.195 (0.145)	Data 2.19e-04 (1.95e-03)	Tok/s 104636 (96507)	Loss/tok 3.4645 (3.2421)	LR 2.800e-03
0: TRAIN [2][130/1885]	Time 0.101 (0.145)	Data 2.17e-04 (1.82e-03)	Tok/s 87916 (96619)	Loss/tok 2.9448 (3.2433)	LR 2.800e-03
0: TRAIN [2][140/1885]	Time 0.245 (0.145)	Data 2.21e-04 (1.70e-03)	Tok/s 108940 (96494)	Loss/tok 3.4549 (3.2440)	LR 2.800e-03
0: TRAIN [2][150/1885]	Time 0.101 (0.145)	Data 2.18e-04 (1.60e-03)	Tok/s 89926 (96492)	Loss/tok 3.0868 (3.2455)	LR 2.800e-03
0: TRAIN [2][160/1885]	Time 0.100 (0.143)	Data 2.18e-04 (1.52e-03)	Tok/s 91030 (96232)	Loss/tok 3.0327 (3.2420)	LR 2.800e-03
0: TRAIN [2][170/1885]	Time 0.147 (0.142)	Data 2.22e-04 (1.44e-03)	Tok/s 98427 (96054)	Loss/tok 3.2467 (3.2350)	LR 2.800e-03
0: TRAIN [2][180/1885]	Time 0.194 (0.143)	Data 2.17e-04 (1.37e-03)	Tok/s 106497 (96288)	Loss/tok 3.2705 (3.2369)	LR 2.800e-03
0: TRAIN [2][190/1885]	Time 0.101 (0.142)	Data 2.17e-04 (1.31e-03)	Tok/s 88493 (96207)	Loss/tok 2.9698 (3.2355)	LR 2.800e-03
0: TRAIN [2][200/1885]	Time 0.193 (0.143)	Data 2.19e-04 (1.26e-03)	Tok/s 105888 (96332)	Loss/tok 3.3795 (3.2383)	LR 2.800e-03
0: TRAIN [2][210/1885]	Time 0.146 (0.143)	Data 2.16e-04 (1.21e-03)	Tok/s 100796 (96423)	Loss/tok 3.1852 (3.2399)	LR 2.800e-03
0: TRAIN [2][220/1885]	Time 0.102 (0.141)	Data 2.22e-04 (1.16e-03)	Tok/s 88403 (96092)	Loss/tok 2.8980 (3.2325)	LR 2.800e-03
0: TRAIN [2][230/1885]	Time 0.102 (0.142)	Data 2.19e-04 (1.12e-03)	Tok/s 90879 (96123)	Loss/tok 2.9815 (3.2346)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][240/1885]	Time 0.146 (0.142)	Data 2.21e-04 (1.09e-03)	Tok/s 100254 (96088)	Loss/tok 3.2132 (3.2349)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][250/1885]	Time 0.146 (0.142)	Data 2.18e-04 (1.05e-03)	Tok/s 99885 (96172)	Loss/tok 3.2439 (3.2382)	LR 2.800e-03
0: TRAIN [2][260/1885]	Time 0.101 (0.141)	Data 2.23e-04 (1.02e-03)	Tok/s 89122 (96009)	Loss/tok 2.8993 (3.2359)	LR 2.800e-03
0: TRAIN [2][270/1885]	Time 0.101 (0.141)	Data 2.16e-04 (9.90e-04)	Tok/s 89860 (95970)	Loss/tok 2.9404 (3.2362)	LR 2.800e-03
0: TRAIN [2][280/1885]	Time 0.102 (0.142)	Data 2.17e-04 (9.63e-04)	Tok/s 88173 (96097)	Loss/tok 2.9739 (3.2411)	LR 2.800e-03
0: TRAIN [2][290/1885]	Time 0.059 (0.141)	Data 2.20e-04 (9.37e-04)	Tok/s 76158 (96086)	Loss/tok 2.4808 (3.2398)	LR 2.800e-03
0: TRAIN [2][300/1885]	Time 0.244 (0.142)	Data 2.22e-04 (9.13e-04)	Tok/s 107298 (96121)	Loss/tok 3.5109 (3.2427)	LR 2.800e-03
0: TRAIN [2][310/1885]	Time 0.147 (0.143)	Data 2.23e-04 (8.91e-04)	Tok/s 99234 (96225)	Loss/tok 3.2594 (3.2468)	LR 2.800e-03
0: TRAIN [2][320/1885]	Time 0.101 (0.142)	Data 2.21e-04 (8.70e-04)	Tok/s 88276 (96132)	Loss/tok 2.9588 (3.2437)	LR 2.800e-03
0: TRAIN [2][330/1885]	Time 0.102 (0.142)	Data 1.90e-04 (8.50e-04)	Tok/s 88109 (96108)	Loss/tok 3.0195 (3.2431)	LR 2.800e-03
0: TRAIN [2][340/1885]	Time 0.100 (0.141)	Data 2.34e-04 (8.32e-04)	Tok/s 91284 (96031)	Loss/tok 3.0119 (3.2415)	LR 2.800e-03
0: TRAIN [2][350/1885]	Time 0.146 (0.141)	Data 2.24e-04 (8.14e-04)	Tok/s 100718 (95997)	Loss/tok 3.2909 (3.2419)	LR 2.800e-03
0: TRAIN [2][360/1885]	Time 0.246 (0.141)	Data 1.62e-04 (7.98e-04)	Tok/s 105715 (96009)	Loss/tok 3.5940 (3.2443)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][370/1885]	Time 0.149 (0.141)	Data 2.18e-04 (7.82e-04)	Tok/s 97174 (96012)	Loss/tok 3.3472 (3.2451)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][380/1885]	Time 0.147 (0.141)	Data 2.20e-04 (7.67e-04)	Tok/s 98704 (95990)	Loss/tok 3.2932 (3.2445)	LR 2.800e-03
0: TRAIN [2][390/1885]	Time 0.195 (0.141)	Data 2.25e-04 (7.53e-04)	Tok/s 104814 (95991)	Loss/tok 3.4046 (3.2442)	LR 2.800e-03
0: TRAIN [2][400/1885]	Time 0.102 (0.141)	Data 2.21e-04 (7.40e-04)	Tok/s 88923 (95925)	Loss/tok 2.9453 (3.2440)	LR 2.800e-03
0: TRAIN [2][410/1885]	Time 0.194 (0.141)	Data 2.23e-04 (7.27e-04)	Tok/s 105449 (95935)	Loss/tok 3.3313 (3.2429)	LR 2.800e-03
0: TRAIN [2][420/1885]	Time 0.103 (0.141)	Data 2.20e-04 (7.15e-04)	Tok/s 88517 (95950)	Loss/tok 3.0992 (3.2444)	LR 2.800e-03
0: TRAIN [2][430/1885]	Time 0.245 (0.142)	Data 2.30e-04 (7.03e-04)	Tok/s 107259 (95988)	Loss/tok 3.4810 (3.2468)	LR 2.800e-03
0: TRAIN [2][440/1885]	Time 0.102 (0.141)	Data 2.22e-04 (6.92e-04)	Tok/s 87434 (95926)	Loss/tok 2.9714 (3.2451)	LR 2.800e-03
0: TRAIN [2][450/1885]	Time 0.102 (0.141)	Data 2.31e-04 (6.81e-04)	Tok/s 90616 (95800)	Loss/tok 2.9700 (3.2426)	LR 2.800e-03
0: TRAIN [2][460/1885]	Time 0.104 (0.141)	Data 2.32e-04 (6.71e-04)	Tok/s 88090 (95883)	Loss/tok 3.0010 (3.2458)	LR 2.800e-03
0: TRAIN [2][470/1885]	Time 0.147 (0.142)	Data 2.22e-04 (6.62e-04)	Tok/s 100089 (95977)	Loss/tok 3.2584 (3.2487)	LR 2.800e-03
0: TRAIN [2][480/1885]	Time 0.102 (0.142)	Data 2.26e-04 (6.53e-04)	Tok/s 88724 (95999)	Loss/tok 3.1058 (3.2502)	LR 2.800e-03
0: TRAIN [2][490/1885]	Time 0.059 (0.142)	Data 2.22e-04 (6.44e-04)	Tok/s 77732 (95995)	Loss/tok 2.5406 (3.2509)	LR 2.800e-03
0: TRAIN [2][500/1885]	Time 0.194 (0.142)	Data 2.18e-04 (6.35e-04)	Tok/s 104349 (95994)	Loss/tok 3.3727 (3.2516)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][510/1885]	Time 0.148 (0.143)	Data 2.21e-04 (6.27e-04)	Tok/s 99394 (96024)	Loss/tok 3.3227 (3.2536)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][520/1885]	Time 0.195 (0.143)	Data 2.34e-04 (6.19e-04)	Tok/s 105190 (96044)	Loss/tok 3.3719 (3.2538)	LR 2.800e-03
0: TRAIN [2][530/1885]	Time 0.195 (0.143)	Data 2.06e-04 (6.11e-04)	Tok/s 104826 (96111)	Loss/tok 3.3208 (3.2555)	LR 2.800e-03
0: TRAIN [2][540/1885]	Time 0.195 (0.143)	Data 2.19e-04 (6.04e-04)	Tok/s 104593 (96131)	Loss/tok 3.3853 (3.2558)	LR 2.800e-03
0: TRAIN [2][550/1885]	Time 0.061 (0.143)	Data 2.17e-04 (5.97e-04)	Tok/s 75259 (96121)	Loss/tok 2.6179 (3.2551)	LR 2.800e-03
0: TRAIN [2][560/1885]	Time 0.194 (0.143)	Data 2.22e-04 (5.90e-04)	Tok/s 104025 (96160)	Loss/tok 3.3552 (3.2555)	LR 2.800e-03
0: TRAIN [2][570/1885]	Time 0.102 (0.143)	Data 2.06e-04 (5.83e-04)	Tok/s 89514 (96124)	Loss/tok 3.0282 (3.2542)	LR 2.800e-03
0: TRAIN [2][580/1885]	Time 0.102 (0.143)	Data 1.80e-04 (5.77e-04)	Tok/s 89450 (96116)	Loss/tok 2.9307 (3.2533)	LR 2.800e-03
0: TRAIN [2][590/1885]	Time 0.147 (0.143)	Data 2.23e-04 (5.71e-04)	Tok/s 98433 (96118)	Loss/tok 3.1535 (3.2524)	LR 2.800e-03
0: TRAIN [2][600/1885]	Time 0.193 (0.143)	Data 1.90e-04 (5.65e-04)	Tok/s 105460 (96085)	Loss/tok 3.4431 (3.2518)	LR 2.800e-03
0: TRAIN [2][610/1885]	Time 0.245 (0.143)	Data 1.71e-04 (5.59e-04)	Tok/s 107056 (96121)	Loss/tok 3.4479 (3.2528)	LR 2.800e-03
0: TRAIN [2][620/1885]	Time 0.102 (0.143)	Data 2.20e-04 (5.54e-04)	Tok/s 89705 (96173)	Loss/tok 3.0992 (3.2523)	LR 2.800e-03
0: TRAIN [2][630/1885]	Time 0.147 (0.143)	Data 2.22e-04 (5.48e-04)	Tok/s 99183 (96199)	Loss/tok 3.2257 (3.2532)	LR 2.800e-03
0: TRAIN [2][640/1885]	Time 0.147 (0.143)	Data 1.92e-04 (5.43e-04)	Tok/s 99016 (96179)	Loss/tok 3.2426 (3.2533)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][650/1885]	Time 0.245 (0.144)	Data 2.21e-04 (5.38e-04)	Tok/s 108070 (96266)	Loss/tok 3.5617 (3.2575)	LR 2.800e-03
0: TRAIN [2][660/1885]	Time 0.102 (0.144)	Data 2.26e-04 (5.33e-04)	Tok/s 91145 (96281)	Loss/tok 2.9695 (3.2571)	LR 2.800e-03
0: TRAIN [2][670/1885]	Time 0.146 (0.144)	Data 2.22e-04 (5.29e-04)	Tok/s 99710 (96240)	Loss/tok 3.1788 (3.2564)	LR 2.800e-03
0: TRAIN [2][680/1885]	Time 0.194 (0.144)	Data 2.15e-04 (5.24e-04)	Tok/s 104705 (96261)	Loss/tok 3.4636 (3.2574)	LR 2.800e-03
0: TRAIN [2][690/1885]	Time 0.193 (0.144)	Data 2.24e-04 (5.19e-04)	Tok/s 106241 (96239)	Loss/tok 3.4826 (3.2569)	LR 2.800e-03
0: TRAIN [2][700/1885]	Time 0.101 (0.144)	Data 2.21e-04 (5.15e-04)	Tok/s 90281 (96264)	Loss/tok 2.9965 (3.2573)	LR 2.800e-03
0: TRAIN [2][710/1885]	Time 0.059 (0.144)	Data 2.21e-04 (5.11e-04)	Tok/s 76601 (96202)	Loss/tok 2.4772 (3.2563)	LR 2.800e-03
0: TRAIN [2][720/1885]	Time 0.147 (0.143)	Data 2.23e-04 (5.07e-04)	Tok/s 99907 (96200)	Loss/tok 3.1993 (3.2558)	LR 2.800e-03
0: TRAIN [2][730/1885]	Time 0.193 (0.143)	Data 2.23e-04 (5.03e-04)	Tok/s 106685 (96192)	Loss/tok 3.4358 (3.2552)	LR 2.800e-03
0: TRAIN [2][740/1885]	Time 0.146 (0.143)	Data 2.28e-04 (4.99e-04)	Tok/s 101534 (96177)	Loss/tok 3.2032 (3.2549)	LR 2.800e-03
0: TRAIN [2][750/1885]	Time 0.193 (0.143)	Data 2.23e-04 (4.96e-04)	Tok/s 105992 (96186)	Loss/tok 3.4112 (3.2557)	LR 2.800e-03
0: TRAIN [2][760/1885]	Time 0.147 (0.143)	Data 2.23e-04 (4.92e-04)	Tok/s 100332 (96213)	Loss/tok 3.2908 (3.2560)	LR 2.800e-03
0: TRAIN [2][770/1885]	Time 0.101 (0.143)	Data 2.21e-04 (4.89e-04)	Tok/s 90827 (96203)	Loss/tok 3.0737 (3.2561)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][780/1885]	Time 0.146 (0.143)	Data 2.16e-04 (4.85e-04)	Tok/s 101825 (96197)	Loss/tok 3.3579 (3.2554)	LR 2.800e-03
0: TRAIN [2][790/1885]	Time 0.149 (0.143)	Data 2.18e-04 (4.82e-04)	Tok/s 99361 (96192)	Loss/tok 3.1791 (3.2546)	LR 2.800e-03
0: TRAIN [2][800/1885]	Time 0.148 (0.143)	Data 2.24e-04 (4.78e-04)	Tok/s 100195 (96225)	Loss/tok 3.2580 (3.2548)	LR 2.800e-03
0: TRAIN [2][810/1885]	Time 0.195 (0.143)	Data 2.08e-04 (4.75e-04)	Tok/s 103659 (96233)	Loss/tok 3.4198 (3.2544)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][820/1885]	Time 0.102 (0.143)	Data 2.26e-04 (4.72e-04)	Tok/s 89865 (96180)	Loss/tok 3.0902 (3.2537)	LR 2.800e-03
0: TRAIN [2][830/1885]	Time 0.101 (0.143)	Data 2.20e-04 (4.69e-04)	Tok/s 88879 (96186)	Loss/tok 2.9281 (3.2535)	LR 2.800e-03
0: TRAIN [2][840/1885]	Time 0.060 (0.143)	Data 2.22e-04 (4.66e-04)	Tok/s 74744 (96160)	Loss/tok 2.4776 (3.2535)	LR 2.800e-03
0: TRAIN [2][850/1885]	Time 0.103 (0.143)	Data 2.21e-04 (4.63e-04)	Tok/s 89374 (96173)	Loss/tok 2.9264 (3.2531)	LR 2.800e-03
0: TRAIN [2][860/1885]	Time 0.195 (0.143)	Data 2.20e-04 (4.60e-04)	Tok/s 104570 (96203)	Loss/tok 3.3079 (3.2541)	LR 2.800e-03
0: TRAIN [2][870/1885]	Time 0.103 (0.143)	Data 2.18e-04 (4.57e-04)	Tok/s 87638 (96208)	Loss/tok 3.0446 (3.2547)	LR 2.800e-03
0: TRAIN [2][880/1885]	Time 0.194 (0.143)	Data 2.16e-04 (4.55e-04)	Tok/s 103260 (96179)	Loss/tok 3.5703 (3.2547)	LR 2.800e-03
0: TRAIN [2][890/1885]	Time 0.195 (0.143)	Data 2.17e-04 (4.52e-04)	Tok/s 105504 (96212)	Loss/tok 3.3827 (3.2554)	LR 2.800e-03
0: TRAIN [2][900/1885]	Time 0.104 (0.143)	Data 2.19e-04 (4.49e-04)	Tok/s 87998 (96232)	Loss/tok 3.0564 (3.2559)	LR 2.800e-03
0: TRAIN [2][910/1885]	Time 0.147 (0.143)	Data 2.38e-04 (4.47e-04)	Tok/s 99563 (96238)	Loss/tok 3.1118 (3.2553)	LR 2.800e-03
0: TRAIN [2][920/1885]	Time 0.146 (0.143)	Data 2.18e-04 (4.44e-04)	Tok/s 99912 (96200)	Loss/tok 3.3186 (3.2546)	LR 2.800e-03
0: TRAIN [2][930/1885]	Time 0.147 (0.143)	Data 2.18e-04 (4.42e-04)	Tok/s 101626 (96243)	Loss/tok 3.2173 (3.2559)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][940/1885]	Time 0.062 (0.143)	Data 2.20e-04 (4.40e-04)	Tok/s 75096 (96208)	Loss/tok 2.5197 (3.2548)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][950/1885]	Time 0.103 (0.143)	Data 2.16e-04 (4.37e-04)	Tok/s 89540 (96224)	Loss/tok 3.0163 (3.2554)	LR 2.800e-03
0: TRAIN [2][960/1885]	Time 0.102 (0.143)	Data 2.15e-04 (4.35e-04)	Tok/s 89597 (96181)	Loss/tok 2.9902 (3.2549)	LR 2.800e-03
0: TRAIN [2][970/1885]	Time 0.195 (0.143)	Data 2.22e-04 (4.33e-04)	Tok/s 105397 (96182)	Loss/tok 3.3452 (3.2545)	LR 2.800e-03
0: TRAIN [2][980/1885]	Time 0.147 (0.143)	Data 2.24e-04 (4.30e-04)	Tok/s 99564 (96179)	Loss/tok 3.2279 (3.2541)	LR 2.800e-03
0: TRAIN [2][990/1885]	Time 0.101 (0.143)	Data 2.21e-04 (4.28e-04)	Tok/s 88517 (96138)	Loss/tok 3.0354 (3.2531)	LR 2.800e-03
0: TRAIN [2][1000/1885]	Time 0.194 (0.143)	Data 2.21e-04 (4.26e-04)	Tok/s 105767 (96150)	Loss/tok 3.3632 (3.2528)	LR 2.800e-03
0: TRAIN [2][1010/1885]	Time 0.102 (0.143)	Data 1.78e-04 (4.24e-04)	Tok/s 89523 (96117)	Loss/tok 3.0439 (3.2530)	LR 2.800e-03
0: TRAIN [2][1020/1885]	Time 0.102 (0.143)	Data 2.21e-04 (4.22e-04)	Tok/s 87735 (96141)	Loss/tok 3.0864 (3.2533)	LR 2.800e-03
0: TRAIN [2][1030/1885]	Time 0.102 (0.143)	Data 2.21e-04 (4.20e-04)	Tok/s 89763 (96179)	Loss/tok 3.0561 (3.2545)	LR 2.800e-03
0: TRAIN [2][1040/1885]	Time 0.145 (0.143)	Data 2.24e-04 (4.18e-04)	Tok/s 101238 (96142)	Loss/tok 3.2027 (3.2535)	LR 2.800e-03
0: TRAIN [2][1050/1885]	Time 0.196 (0.143)	Data 2.24e-04 (4.16e-04)	Tok/s 104501 (96144)	Loss/tok 3.4322 (3.2537)	LR 2.800e-03
0: TRAIN [2][1060/1885]	Time 0.147 (0.143)	Data 2.17e-04 (4.14e-04)	Tok/s 98702 (96130)	Loss/tok 3.2293 (3.2532)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1070/1885]	Time 0.147 (0.143)	Data 2.21e-04 (4.13e-04)	Tok/s 99495 (96107)	Loss/tok 3.2870 (3.2530)	LR 2.800e-03
0: TRAIN [2][1080/1885]	Time 0.101 (0.142)	Data 2.23e-04 (4.11e-04)	Tok/s 89071 (96069)	Loss/tok 2.9987 (3.2519)	LR 2.800e-03
0: TRAIN [2][1090/1885]	Time 0.193 (0.142)	Data 2.31e-04 (4.09e-04)	Tok/s 104316 (96041)	Loss/tok 3.5265 (3.2517)	LR 2.800e-03
0: TRAIN [2][1100/1885]	Time 0.102 (0.142)	Data 2.22e-04 (4.07e-04)	Tok/s 88684 (96051)	Loss/tok 3.0615 (3.2523)	LR 2.800e-03
0: TRAIN [2][1110/1885]	Time 0.195 (0.142)	Data 1.73e-04 (4.06e-04)	Tok/s 105809 (96052)	Loss/tok 3.4678 (3.2523)	LR 2.800e-03
0: TRAIN [2][1120/1885]	Time 0.148 (0.142)	Data 2.21e-04 (4.04e-04)	Tok/s 100593 (96076)	Loss/tok 3.2820 (3.2524)	LR 2.800e-03
0: TRAIN [2][1130/1885]	Time 0.060 (0.142)	Data 2.25e-04 (4.02e-04)	Tok/s 75510 (96037)	Loss/tok 2.4583 (3.2527)	LR 2.800e-03
0: TRAIN [2][1140/1885]	Time 0.147 (0.142)	Data 2.06e-04 (4.01e-04)	Tok/s 99666 (96000)	Loss/tok 3.2443 (3.2524)	LR 2.800e-03
0: TRAIN [2][1150/1885]	Time 0.245 (0.142)	Data 2.33e-04 (3.99e-04)	Tok/s 106512 (96043)	Loss/tok 3.5446 (3.2538)	LR 2.800e-03
0: TRAIN [2][1160/1885]	Time 0.148 (0.143)	Data 2.21e-04 (3.98e-04)	Tok/s 100416 (96056)	Loss/tok 3.1552 (3.2537)	LR 2.800e-03
0: TRAIN [2][1170/1885]	Time 0.103 (0.142)	Data 2.20e-04 (3.96e-04)	Tok/s 87902 (96042)	Loss/tok 2.9614 (3.2528)	LR 2.800e-03
0: TRAIN [2][1180/1885]	Time 0.195 (0.143)	Data 1.81e-04 (3.94e-04)	Tok/s 104489 (96060)	Loss/tok 3.5149 (3.2535)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1190/1885]	Time 0.147 (0.143)	Data 2.23e-04 (3.93e-04)	Tok/s 101808 (96112)	Loss/tok 3.2320 (3.2549)	LR 2.800e-03
0: TRAIN [2][1200/1885]	Time 0.194 (0.143)	Data 2.22e-04 (3.91e-04)	Tok/s 106227 (96117)	Loss/tok 3.4002 (3.2546)	LR 2.800e-03
0: TRAIN [2][1210/1885]	Time 0.246 (0.143)	Data 2.18e-04 (3.90e-04)	Tok/s 104977 (96119)	Loss/tok 3.5859 (3.2553)	LR 2.800e-03
0: TRAIN [2][1220/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.89e-04)	Tok/s 90270 (96135)	Loss/tok 2.9930 (3.2560)	LR 2.800e-03
0: TRAIN [2][1230/1885]	Time 0.102 (0.143)	Data 2.07e-04 (3.87e-04)	Tok/s 89228 (96153)	Loss/tok 3.0948 (3.2562)	LR 2.800e-03
0: TRAIN [2][1240/1885]	Time 0.195 (0.143)	Data 2.19e-04 (3.86e-04)	Tok/s 104633 (96179)	Loss/tok 3.3923 (3.2567)	LR 2.800e-03
0: TRAIN [2][1250/1885]	Time 0.246 (0.144)	Data 2.15e-04 (3.84e-04)	Tok/s 105966 (96208)	Loss/tok 3.6253 (3.2574)	LR 2.800e-03
0: TRAIN [2][1260/1885]	Time 0.103 (0.144)	Data 2.17e-04 (3.83e-04)	Tok/s 86937 (96222)	Loss/tok 3.0424 (3.2580)	LR 2.800e-03
0: TRAIN [2][1270/1885]	Time 0.197 (0.144)	Data 2.21e-04 (3.82e-04)	Tok/s 103389 (96224)	Loss/tok 3.4850 (3.2582)	LR 2.800e-03
0: TRAIN [2][1280/1885]	Time 0.102 (0.144)	Data 2.19e-04 (3.80e-04)	Tok/s 87500 (96219)	Loss/tok 2.9233 (3.2589)	LR 2.800e-03
0: TRAIN [2][1290/1885]	Time 0.193 (0.144)	Data 2.18e-04 (3.79e-04)	Tok/s 103783 (96235)	Loss/tok 3.4841 (3.2587)	LR 2.800e-03
0: TRAIN [2][1300/1885]	Time 0.101 (0.144)	Data 2.24e-04 (3.78e-04)	Tok/s 89849 (96249)	Loss/tok 2.9751 (3.2585)	LR 2.800e-03
0: TRAIN [2][1310/1885]	Time 0.102 (0.144)	Data 2.22e-04 (3.77e-04)	Tok/s 89786 (96227)	Loss/tok 2.9962 (3.2587)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1320/1885]	Time 0.146 (0.144)	Data 2.20e-04 (3.75e-04)	Tok/s 100337 (96253)	Loss/tok 3.2282 (3.2589)	LR 2.800e-03
0: TRAIN [2][1330/1885]	Time 0.147 (0.144)	Data 2.20e-04 (3.74e-04)	Tok/s 100764 (96228)	Loss/tok 3.2510 (3.2585)	LR 2.800e-03
0: TRAIN [2][1340/1885]	Time 0.145 (0.144)	Data 2.20e-04 (3.73e-04)	Tok/s 100333 (96232)	Loss/tok 3.3037 (3.2583)	LR 2.800e-03
0: TRAIN [2][1350/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.72e-04)	Tok/s 92144 (96225)	Loss/tok 3.1317 (3.2579)	LR 2.800e-03
0: TRAIN [2][1360/1885]	Time 0.147 (0.143)	Data 2.19e-04 (3.71e-04)	Tok/s 99862 (96185)	Loss/tok 3.2305 (3.2569)	LR 2.800e-03
0: TRAIN [2][1370/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.70e-04)	Tok/s 89001 (96204)	Loss/tok 3.0284 (3.2569)	LR 2.800e-03
0: TRAIN [2][1380/1885]	Time 0.147 (0.144)	Data 2.21e-04 (3.69e-04)	Tok/s 99819 (96249)	Loss/tok 3.1357 (3.2582)	LR 2.800e-03
0: TRAIN [2][1390/1885]	Time 0.102 (0.143)	Data 2.19e-04 (3.67e-04)	Tok/s 86500 (96210)	Loss/tok 2.9987 (3.2572)	LR 2.800e-03
0: TRAIN [2][1400/1885]	Time 0.193 (0.143)	Data 2.32e-04 (3.66e-04)	Tok/s 106834 (96221)	Loss/tok 3.3744 (3.2571)	LR 2.800e-03
0: TRAIN [2][1410/1885]	Time 0.102 (0.143)	Data 2.24e-04 (3.65e-04)	Tok/s 90560 (96208)	Loss/tok 3.0236 (3.2564)	LR 2.800e-03
0: TRAIN [2][1420/1885]	Time 0.102 (0.143)	Data 2.19e-04 (3.64e-04)	Tok/s 89493 (96227)	Loss/tok 3.0416 (3.2566)	LR 2.800e-03
0: TRAIN [2][1430/1885]	Time 0.147 (0.144)	Data 2.20e-04 (3.63e-04)	Tok/s 100414 (96263)	Loss/tok 3.0694 (3.2567)	LR 2.800e-03
0: TRAIN [2][1440/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.62e-04)	Tok/s 99769 (96256)	Loss/tok 3.3364 (3.2564)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1450/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.61e-04)	Tok/s 99916 (96245)	Loss/tok 3.2341 (3.2562)	LR 2.800e-03
0: TRAIN [2][1460/1885]	Time 0.101 (0.143)	Data 2.23e-04 (3.60e-04)	Tok/s 90966 (96239)	Loss/tok 2.9786 (3.2559)	LR 2.800e-03
0: TRAIN [2][1470/1885]	Time 0.247 (0.143)	Data 2.21e-04 (3.59e-04)	Tok/s 106110 (96248)	Loss/tok 3.6655 (3.2564)	LR 2.800e-03
0: TRAIN [2][1480/1885]	Time 0.148 (0.143)	Data 2.17e-04 (3.59e-04)	Tok/s 98976 (96248)	Loss/tok 3.1519 (3.2558)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1490/1885]	Time 0.193 (0.143)	Data 2.20e-04 (3.58e-04)	Tok/s 105845 (96232)	Loss/tok 3.3490 (3.2556)	LR 2.800e-03
0: TRAIN [2][1500/1885]	Time 0.247 (0.143)	Data 1.55e-04 (3.57e-04)	Tok/s 105496 (96255)	Loss/tok 3.4495 (3.2563)	LR 2.800e-03
0: TRAIN [2][1510/1885]	Time 0.060 (0.143)	Data 2.22e-04 (3.56e-04)	Tok/s 75501 (96237)	Loss/tok 2.4878 (3.2566)	LR 2.800e-03
0: TRAIN [2][1520/1885]	Time 0.193 (0.144)	Data 2.19e-04 (3.55e-04)	Tok/s 106049 (96235)	Loss/tok 3.3447 (3.2578)	LR 2.800e-03
0: TRAIN [2][1530/1885]	Time 0.147 (0.144)	Data 2.21e-04 (3.54e-04)	Tok/s 100055 (96236)	Loss/tok 3.2188 (3.2580)	LR 2.800e-03
0: TRAIN [2][1540/1885]	Time 0.247 (0.144)	Data 2.03e-04 (3.53e-04)	Tok/s 105446 (96251)	Loss/tok 3.4991 (3.2589)	LR 2.800e-03
0: TRAIN [2][1550/1885]	Time 0.245 (0.144)	Data 1.96e-04 (3.52e-04)	Tok/s 105759 (96224)	Loss/tok 3.4371 (3.2585)	LR 2.800e-03
0: TRAIN [2][1560/1885]	Time 0.147 (0.144)	Data 2.22e-04 (3.51e-04)	Tok/s 100749 (96220)	Loss/tok 3.2399 (3.2588)	LR 2.800e-03
0: TRAIN [2][1570/1885]	Time 0.246 (0.144)	Data 2.31e-04 (3.50e-04)	Tok/s 107682 (96209)	Loss/tok 3.4961 (3.2589)	LR 2.800e-03
0: TRAIN [2][1580/1885]	Time 0.147 (0.143)	Data 1.94e-04 (3.49e-04)	Tok/s 98840 (96178)	Loss/tok 3.2394 (3.2583)	LR 2.800e-03
0: TRAIN [2][1590/1885]	Time 0.102 (0.143)	Data 2.01e-04 (3.49e-04)	Tok/s 89437 (96175)	Loss/tok 2.8949 (3.2578)	LR 2.800e-03
0: TRAIN [2][1600/1885]	Time 0.102 (0.143)	Data 2.04e-04 (3.48e-04)	Tok/s 89652 (96162)	Loss/tok 3.0898 (3.2572)	LR 2.800e-03
0: TRAIN [2][1610/1885]	Time 0.147 (0.143)	Data 2.22e-04 (3.47e-04)	Tok/s 99522 (96169)	Loss/tok 3.2501 (3.2574)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1620/1885]	Time 0.194 (0.143)	Data 2.22e-04 (3.46e-04)	Tok/s 105230 (96185)	Loss/tok 3.4484 (3.2580)	LR 2.800e-03
0: TRAIN [2][1630/1885]	Time 0.147 (0.143)	Data 2.22e-04 (3.45e-04)	Tok/s 101642 (96181)	Loss/tok 3.1773 (3.2577)	LR 2.800e-03
0: TRAIN [2][1640/1885]	Time 0.104 (0.143)	Data 1.65e-04 (3.44e-04)	Tok/s 88401 (96170)	Loss/tok 3.0183 (3.2575)	LR 2.800e-03
0: TRAIN [2][1650/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.44e-04)	Tok/s 89214 (96146)	Loss/tok 3.0232 (3.2575)	LR 2.800e-03
0: TRAIN [2][1660/1885]	Time 0.102 (0.143)	Data 2.23e-04 (3.43e-04)	Tok/s 88600 (96134)	Loss/tok 2.9990 (3.2572)	LR 2.800e-03
0: TRAIN [2][1670/1885]	Time 0.103 (0.143)	Data 2.19e-04 (3.42e-04)	Tok/s 87846 (96128)	Loss/tok 2.9699 (3.2571)	LR 2.800e-03
0: TRAIN [2][1680/1885]	Time 0.194 (0.143)	Data 2.21e-04 (3.41e-04)	Tok/s 104228 (96120)	Loss/tok 3.3627 (3.2565)	LR 2.800e-03
0: TRAIN [2][1690/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.40e-04)	Tok/s 98967 (96135)	Loss/tok 3.1256 (3.2568)	LR 2.800e-03
0: TRAIN [2][1700/1885]	Time 0.147 (0.143)	Data 2.30e-04 (3.40e-04)	Tok/s 98585 (96127)	Loss/tok 3.3032 (3.2571)	LR 2.800e-03
0: TRAIN [2][1710/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.39e-04)	Tok/s 90464 (96146)	Loss/tok 3.0075 (3.2570)	LR 2.800e-03
0: TRAIN [2][1720/1885]	Time 0.104 (0.143)	Data 2.21e-04 (3.38e-04)	Tok/s 87627 (96136)	Loss/tok 3.0884 (3.2567)	LR 2.800e-03
0: TRAIN [2][1730/1885]	Time 0.148 (0.143)	Data 2.34e-04 (3.37e-04)	Tok/s 100069 (96130)	Loss/tok 3.2078 (3.2566)	LR 2.800e-03
0: TRAIN [2][1740/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.37e-04)	Tok/s 90108 (96116)	Loss/tok 2.9940 (3.2558)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1750/1885]	Time 0.148 (0.143)	Data 2.21e-04 (3.36e-04)	Tok/s 98178 (96137)	Loss/tok 3.1649 (3.2561)	LR 2.800e-03
0: TRAIN [2][1760/1885]	Time 0.103 (0.143)	Data 2.19e-04 (3.35e-04)	Tok/s 87135 (96127)	Loss/tok 2.9985 (3.2558)	LR 2.800e-03
0: TRAIN [2][1770/1885]	Time 0.146 (0.143)	Data 2.22e-04 (3.34e-04)	Tok/s 99806 (96124)	Loss/tok 3.2674 (3.2556)	LR 2.800e-03
0: TRAIN [2][1780/1885]	Time 0.244 (0.143)	Data 2.21e-04 (3.34e-04)	Tok/s 107794 (96153)	Loss/tok 3.4687 (3.2557)	LR 2.800e-03
0: TRAIN [2][1790/1885]	Time 0.147 (0.143)	Data 2.23e-04 (3.33e-04)	Tok/s 100524 (96135)	Loss/tok 3.1628 (3.2554)	LR 2.800e-03
0: TRAIN [2][1800/1885]	Time 0.148 (0.143)	Data 2.02e-04 (3.32e-04)	Tok/s 97908 (96153)	Loss/tok 3.2963 (3.2554)	LR 2.800e-03
0: TRAIN [2][1810/1885]	Time 0.194 (0.143)	Data 2.21e-04 (3.32e-04)	Tok/s 104233 (96168)	Loss/tok 3.5004 (3.2557)	LR 2.800e-03
0: TRAIN [2][1820/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.31e-04)	Tok/s 89977 (96147)	Loss/tok 2.9393 (3.2554)	LR 2.800e-03
0: TRAIN [2][1830/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.30e-04)	Tok/s 98777 (96151)	Loss/tok 3.2181 (3.2552)	LR 2.800e-03
0: TRAIN [2][1840/1885]	Time 0.104 (0.143)	Data 2.19e-04 (3.30e-04)	Tok/s 86670 (96143)	Loss/tok 3.0325 (3.2548)	LR 2.800e-03
0: TRAIN [2][1850/1885]	Time 0.193 (0.143)	Data 2.27e-04 (3.29e-04)	Tok/s 105249 (96160)	Loss/tok 3.4389 (3.2555)	LR 2.800e-03
0: TRAIN [2][1860/1885]	Time 0.104 (0.143)	Data 2.22e-04 (3.28e-04)	Tok/s 87564 (96139)	Loss/tok 2.9967 (3.2551)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1870/1885]	Time 0.103 (0.143)	Data 2.36e-04 (3.28e-04)	Tok/s 87993 (96136)	Loss/tok 2.9970 (3.2551)	LR 2.800e-03
0: TRAIN [2][1880/1885]	Time 0.148 (0.143)	Data 2.19e-04 (3.27e-04)	Tok/s 99575 (96147)	Loss/tok 3.2837 (3.2551)	LR 2.800e-03
:::MLLOG {"namespace": "", "time_ms": 1593831490136, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593831490136, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.669 (0.669)	Decoder iters 149.0 (149.0)	Tok/s 24402 (24402)
0: Running moses detokenizer
0: BLEU(score=22.941736165594687, counts=[36331, 17801, 9955, 5834], totals=[65277, 62274, 59272, 56274], precisions=[55.65666314322043, 28.584963227028936, 16.795451477932243, 10.367132245797348], bp=1.0, sys_len=65277, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593831491775, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22940000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593831491776, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2552	Test BLEU: 22.94
0: Performance: Epoch: 2	Training: 768858 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593831491776, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593831491776, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593831491776, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1520381951
0: TRAIN [3][0/1885]	Time 0.383 (0.383)	Data 2.12e-01 (2.12e-01)	Tok/s 38228 (38228)	Loss/tok 3.1276 (3.1276)	LR 2.800e-03
0: TRAIN [3][10/1885]	Time 0.146 (0.191)	Data 2.23e-04 (1.94e-02)	Tok/s 101158 (95166)	Loss/tok 3.1213 (3.2196)	LR 2.800e-03
0: TRAIN [3][20/1885]	Time 0.100 (0.166)	Data 2.22e-04 (1.03e-02)	Tok/s 90652 (95307)	Loss/tok 2.9569 (3.1908)	LR 2.800e-03
0: TRAIN [3][30/1885]	Time 0.243 (0.164)	Data 2.22e-04 (7.04e-03)	Tok/s 107635 (96347)	Loss/tok 3.3994 (3.1888)	LR 2.800e-03
0: TRAIN [3][40/1885]	Time 0.061 (0.155)	Data 2.20e-04 (5.38e-03)	Tok/s 74642 (96019)	Loss/tok 2.4473 (3.1683)	LR 2.800e-03
0: TRAIN [3][50/1885]	Time 0.146 (0.154)	Data 2.20e-04 (4.36e-03)	Tok/s 99053 (96209)	Loss/tok 3.1361 (3.1670)	LR 2.800e-03
0: TRAIN [3][60/1885]	Time 0.194 (0.151)	Data 2.22e-04 (3.68e-03)	Tok/s 105486 (96064)	Loss/tok 3.3573 (3.1659)	LR 2.800e-03
0: TRAIN [3][70/1885]	Time 0.195 (0.147)	Data 2.21e-04 (3.20e-03)	Tok/s 103823 (95517)	Loss/tok 3.3030 (3.1572)	LR 2.800e-03
0: TRAIN [3][80/1885]	Time 0.059 (0.148)	Data 2.19e-04 (2.83e-03)	Tok/s 76518 (95951)	Loss/tok 2.3697 (3.1581)	LR 2.800e-03
0: TRAIN [3][90/1885]	Time 0.246 (0.147)	Data 2.19e-04 (2.54e-03)	Tok/s 104645 (95805)	Loss/tok 3.4314 (3.1602)	LR 2.800e-03
0: TRAIN [3][100/1885]	Time 0.246 (0.149)	Data 2.24e-04 (2.31e-03)	Tok/s 105570 (96175)	Loss/tok 3.5017 (3.1706)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][110/1885]	Time 0.146 (0.150)	Data 2.16e-04 (2.12e-03)	Tok/s 101221 (96534)	Loss/tok 3.1000 (3.1782)	LR 2.800e-03
0: TRAIN [3][120/1885]	Time 0.102 (0.149)	Data 2.21e-04 (1.97e-03)	Tok/s 89688 (96530)	Loss/tok 3.0139 (3.1759)	LR 2.800e-03
0: TRAIN [3][130/1885]	Time 0.245 (0.149)	Data 2.21e-04 (1.83e-03)	Tok/s 106761 (96450)	Loss/tok 3.4211 (3.1762)	LR 2.800e-03
0: TRAIN [3][140/1885]	Time 0.102 (0.150)	Data 1.85e-04 (1.72e-03)	Tok/s 88194 (96716)	Loss/tok 2.8738 (3.1779)	LR 2.800e-03
0: TRAIN [3][150/1885]	Time 0.245 (0.150)	Data 2.20e-04 (1.62e-03)	Tok/s 107654 (96899)	Loss/tok 3.3916 (3.1800)	LR 2.800e-03
0: TRAIN [3][160/1885]	Time 0.147 (0.151)	Data 2.02e-04 (1.53e-03)	Tok/s 99622 (97144)	Loss/tok 3.2731 (3.1825)	LR 2.800e-03
0: TRAIN [3][170/1885]	Time 0.245 (0.151)	Data 2.21e-04 (1.45e-03)	Tok/s 106827 (97122)	Loss/tok 3.4309 (3.1857)	LR 2.800e-03
0: TRAIN [3][180/1885]	Time 0.194 (0.151)	Data 2.20e-04 (1.39e-03)	Tok/s 105740 (97250)	Loss/tok 3.2570 (3.1861)	LR 2.800e-03
0: TRAIN [3][190/1885]	Time 0.146 (0.150)	Data 2.19e-04 (1.33e-03)	Tok/s 100076 (97130)	Loss/tok 3.2903 (3.1840)	LR 2.800e-03
0: TRAIN [3][200/1885]	Time 0.060 (0.149)	Data 2.21e-04 (1.27e-03)	Tok/s 76453 (97005)	Loss/tok 2.4531 (3.1800)	LR 2.800e-03
0: TRAIN [3][210/1885]	Time 0.146 (0.148)	Data 2.20e-04 (1.22e-03)	Tok/s 100707 (96987)	Loss/tok 3.2218 (3.1782)	LR 2.800e-03
0: TRAIN [3][220/1885]	Time 0.246 (0.148)	Data 2.15e-04 (1.17e-03)	Tok/s 105466 (97001)	Loss/tok 3.5394 (3.1794)	LR 2.800e-03
0: TRAIN [3][230/1885]	Time 0.101 (0.147)	Data 2.18e-04 (1.13e-03)	Tok/s 90182 (96873)	Loss/tok 2.9036 (3.1760)	LR 2.800e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][240/1885]	Time 0.147 (0.147)	Data 2.20e-04 (1.10e-03)	Tok/s 99419 (96811)	Loss/tok 3.2305 (3.1741)	LR 2.800e-03
0: TRAIN [3][250/1885]	Time 0.102 (0.147)	Data 1.77e-04 (1.06e-03)	Tok/s 87894 (96787)	Loss/tok 2.8856 (3.1736)	LR 2.800e-03
0: TRAIN [3][260/1885]	Time 0.101 (0.147)	Data 2.03e-04 (1.03e-03)	Tok/s 91296 (96826)	Loss/tok 2.8965 (3.1756)	LR 2.800e-03
0: TRAIN [3][270/1885]	Time 0.101 (0.147)	Data 2.31e-04 (9.97e-04)	Tok/s 89356 (96828)	Loss/tok 2.9437 (3.1759)	LR 2.800e-03
0: TRAIN [3][280/1885]	Time 0.146 (0.147)	Data 2.22e-04 (9.70e-04)	Tok/s 101890 (96839)	Loss/tok 3.1350 (3.1770)	LR 2.800e-03
0: TRAIN [3][290/1885]	Time 0.147 (0.147)	Data 2.18e-04 (9.44e-04)	Tok/s 100274 (96893)	Loss/tok 3.1846 (3.1778)	LR 2.800e-03
0: TRAIN [3][300/1885]	Time 0.146 (0.147)	Data 2.18e-04 (9.20e-04)	Tok/s 100833 (96891)	Loss/tok 3.1233 (3.1774)	LR 2.800e-03
0: TRAIN [3][310/1885]	Time 0.102 (0.146)	Data 2.23e-04 (8.97e-04)	Tok/s 90221 (96858)	Loss/tok 2.9399 (3.1760)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][320/1885]	Time 0.146 (0.146)	Data 2.21e-04 (8.75e-04)	Tok/s 100101 (96771)	Loss/tok 3.1878 (3.1737)	LR 2.800e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][330/1885]	Time 0.101 (0.146)	Data 2.25e-04 (8.55e-04)	Tok/s 88069 (96775)	Loss/tok 3.0043 (3.1742)	LR 2.800e-03
0: TRAIN [3][340/1885]	Time 0.147 (0.146)	Data 1.57e-04 (8.36e-04)	Tok/s 99497 (96818)	Loss/tok 3.2429 (3.1749)	LR 2.800e-03
0: TRAIN [3][350/1885]	Time 0.101 (0.146)	Data 2.19e-04 (8.18e-04)	Tok/s 89252 (96815)	Loss/tok 2.9612 (3.1755)	LR 2.800e-03
0: TRAIN [3][360/1885]	Time 0.146 (0.145)	Data 2.20e-04 (8.00e-04)	Tok/s 101079 (96705)	Loss/tok 3.1689 (3.1727)	LR 2.800e-03
0: TRAIN [3][370/1885]	Time 0.193 (0.145)	Data 2.23e-04 (7.85e-04)	Tok/s 105238 (96741)	Loss/tok 3.3167 (3.1727)	LR 2.800e-03
0: TRAIN [3][380/1885]	Time 0.102 (0.145)	Data 2.03e-04 (7.70e-04)	Tok/s 90044 (96736)	Loss/tok 2.9672 (3.1731)	LR 2.800e-03
0: TRAIN [3][390/1885]	Time 0.101 (0.144)	Data 2.21e-04 (7.55e-04)	Tok/s 89475 (96706)	Loss/tok 2.9461 (3.1717)	LR 2.800e-03
0: TRAIN [3][400/1885]	Time 0.060 (0.144)	Data 2.04e-04 (7.41e-04)	Tok/s 77048 (96687)	Loss/tok 2.4025 (3.1707)	LR 2.800e-03
0: TRAIN [3][410/1885]	Time 0.102 (0.144)	Data 2.21e-04 (7.28e-04)	Tok/s 90027 (96634)	Loss/tok 2.9763 (3.1702)	LR 2.800e-03
0: TRAIN [3][420/1885]	Time 0.245 (0.143)	Data 1.90e-04 (7.16e-04)	Tok/s 106345 (96578)	Loss/tok 3.6057 (3.1701)	LR 2.800e-03
0: TRAIN [3][430/1885]	Time 0.060 (0.143)	Data 2.19e-04 (7.04e-04)	Tok/s 76365 (96536)	Loss/tok 2.5895 (3.1699)	LR 1.400e-03
0: TRAIN [3][440/1885]	Time 0.147 (0.143)	Data 2.19e-04 (6.93e-04)	Tok/s 99398 (96540)	Loss/tok 3.0962 (3.1700)	LR 1.400e-03
0: TRAIN [3][450/1885]	Time 0.102 (0.143)	Data 1.79e-04 (6.82e-04)	Tok/s 89785 (96532)	Loss/tok 2.9883 (3.1706)	LR 1.400e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][460/1885]	Time 0.102 (0.143)	Data 2.17e-04 (6.71e-04)	Tok/s 88904 (96498)	Loss/tok 2.9079 (3.1700)	LR 1.400e-03
0: TRAIN [3][470/1885]	Time 0.194 (0.143)	Data 2.05e-04 (6.61e-04)	Tok/s 104771 (96523)	Loss/tok 3.3232 (3.1706)	LR 1.400e-03
0: TRAIN [3][480/1885]	Time 0.147 (0.143)	Data 2.38e-04 (6.52e-04)	Tok/s 99186 (96481)	Loss/tok 3.2568 (3.1693)	LR 1.400e-03
0: TRAIN [3][490/1885]	Time 0.059 (0.142)	Data 2.17e-04 (6.43e-04)	Tok/s 78524 (96421)	Loss/tok 2.4377 (3.1690)	LR 1.400e-03
0: TRAIN [3][500/1885]	Time 0.146 (0.142)	Data 2.23e-04 (6.34e-04)	Tok/s 100921 (96434)	Loss/tok 3.0820 (3.1674)	LR 1.400e-03
0: TRAIN [3][510/1885]	Time 0.146 (0.143)	Data 1.89e-04 (6.26e-04)	Tok/s 100669 (96488)	Loss/tok 3.0285 (3.1678)	LR 1.400e-03
0: TRAIN [3][520/1885]	Time 0.146 (0.143)	Data 2.21e-04 (6.18e-04)	Tok/s 99852 (96523)	Loss/tok 3.1363 (3.1669)	LR 1.400e-03
0: TRAIN [3][530/1885]	Time 0.102 (0.143)	Data 1.92e-04 (6.10e-04)	Tok/s 88356 (96495)	Loss/tok 2.9132 (3.1660)	LR 1.400e-03
0: TRAIN [3][540/1885]	Time 0.102 (0.142)	Data 2.22e-04 (6.03e-04)	Tok/s 89936 (96460)	Loss/tok 2.9946 (3.1654)	LR 1.400e-03
0: TRAIN [3][550/1885]	Time 0.101 (0.142)	Data 1.89e-04 (5.95e-04)	Tok/s 89917 (96483)	Loss/tok 2.9735 (3.1650)	LR 1.400e-03
0: TRAIN [3][560/1885]	Time 0.146 (0.143)	Data 2.18e-04 (5.88e-04)	Tok/s 102135 (96522)	Loss/tok 3.0825 (3.1656)	LR 1.400e-03
0: TRAIN [3][570/1885]	Time 0.102 (0.143)	Data 2.18e-04 (5.82e-04)	Tok/s 90661 (96562)	Loss/tok 2.9706 (3.1666)	LR 1.400e-03
0: TRAIN [3][580/1885]	Time 0.100 (0.143)	Data 2.24e-04 (5.75e-04)	Tok/s 91624 (96600)	Loss/tok 3.0054 (3.1672)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][590/1885]	Time 0.102 (0.143)	Data 2.19e-04 (5.69e-04)	Tok/s 90296 (96561)	Loss/tok 2.9258 (3.1654)	LR 1.400e-03
0: TRAIN [3][600/1885]	Time 0.101 (0.143)	Data 2.20e-04 (5.63e-04)	Tok/s 89602 (96521)	Loss/tok 2.7909 (3.1630)	LR 1.400e-03
0: TRAIN [3][610/1885]	Time 0.102 (0.142)	Data 2.02e-04 (5.57e-04)	Tok/s 89959 (96534)	Loss/tok 2.9654 (3.1629)	LR 1.400e-03
0: TRAIN [3][620/1885]	Time 0.147 (0.143)	Data 1.59e-04 (5.52e-04)	Tok/s 99470 (96554)	Loss/tok 3.0965 (3.1633)	LR 1.400e-03
0: TRAIN [3][630/1885]	Time 0.146 (0.142)	Data 2.19e-04 (5.46e-04)	Tok/s 101786 (96475)	Loss/tok 3.0353 (3.1619)	LR 1.400e-03
0: TRAIN [3][640/1885]	Time 0.246 (0.142)	Data 2.07e-04 (5.41e-04)	Tok/s 107240 (96506)	Loss/tok 3.3630 (3.1620)	LR 1.400e-03
0: TRAIN [3][650/1885]	Time 0.146 (0.143)	Data 2.21e-04 (5.36e-04)	Tok/s 101165 (96549)	Loss/tok 3.0637 (3.1630)	LR 1.400e-03
0: TRAIN [3][660/1885]	Time 0.194 (0.143)	Data 2.21e-04 (5.31e-04)	Tok/s 105385 (96593)	Loss/tok 3.3629 (3.1646)	LR 1.400e-03
0: TRAIN [3][670/1885]	Time 0.244 (0.143)	Data 2.06e-04 (5.26e-04)	Tok/s 106306 (96552)	Loss/tok 3.4591 (3.1645)	LR 1.400e-03
0: TRAIN [3][680/1885]	Time 0.102 (0.143)	Data 1.39e-04 (5.20e-04)	Tok/s 88424 (96549)	Loss/tok 2.8785 (3.1652)	LR 1.400e-03
0: TRAIN [3][690/1885]	Time 0.194 (0.143)	Data 1.93e-04 (5.16e-04)	Tok/s 104359 (96541)	Loss/tok 3.1618 (3.1646)	LR 1.400e-03
0: TRAIN [3][700/1885]	Time 0.146 (0.143)	Data 1.90e-04 (5.11e-04)	Tok/s 100287 (96535)	Loss/tok 3.0787 (3.1632)	LR 1.400e-03
0: TRAIN [3][710/1885]	Time 0.101 (0.143)	Data 2.17e-04 (5.07e-04)	Tok/s 91155 (96574)	Loss/tok 2.9502 (3.1646)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][720/1885]	Time 0.146 (0.143)	Data 2.19e-04 (5.03e-04)	Tok/s 100149 (96586)	Loss/tok 3.1843 (3.1650)	LR 1.400e-03
0: TRAIN [3][730/1885]	Time 0.245 (0.143)	Data 2.21e-04 (4.99e-04)	Tok/s 106772 (96585)	Loss/tok 3.2799 (3.1647)	LR 1.400e-03
0: TRAIN [3][740/1885]	Time 0.147 (0.143)	Data 2.21e-04 (4.95e-04)	Tok/s 100105 (96616)	Loss/tok 3.0557 (3.1638)	LR 1.400e-03
0: TRAIN [3][750/1885]	Time 0.102 (0.143)	Data 1.06e-04 (4.91e-04)	Tok/s 86466 (96544)	Loss/tok 2.8971 (3.1622)	LR 1.400e-03
0: TRAIN [3][760/1885]	Time 0.146 (0.143)	Data 1.07e-04 (4.86e-04)	Tok/s 100445 (96563)	Loss/tok 3.0348 (3.1631)	LR 1.400e-03
0: TRAIN [3][770/1885]	Time 0.145 (0.143)	Data 1.08e-04 (4.81e-04)	Tok/s 99986 (96598)	Loss/tok 3.2038 (3.1625)	LR 1.400e-03
0: TRAIN [3][780/1885]	Time 0.102 (0.143)	Data 1.19e-04 (4.76e-04)	Tok/s 88246 (96597)	Loss/tok 2.8554 (3.1625)	LR 1.400e-03
0: TRAIN [3][790/1885]	Time 0.059 (0.143)	Data 1.08e-04 (4.72e-04)	Tok/s 75963 (96535)	Loss/tok 2.5465 (3.1626)	LR 1.400e-03
0: TRAIN [3][800/1885]	Time 0.148 (0.143)	Data 2.21e-04 (4.68e-04)	Tok/s 99835 (96526)	Loss/tok 3.1185 (3.1623)	LR 1.400e-03
0: TRAIN [3][810/1885]	Time 0.101 (0.143)	Data 2.22e-04 (4.65e-04)	Tok/s 88954 (96552)	Loss/tok 2.8725 (3.1620)	LR 1.400e-03
0: TRAIN [3][820/1885]	Time 0.146 (0.143)	Data 2.22e-04 (4.62e-04)	Tok/s 100604 (96517)	Loss/tok 3.0865 (3.1610)	LR 1.400e-03
0: TRAIN [3][830/1885]	Time 0.101 (0.143)	Data 2.05e-04 (4.59e-04)	Tok/s 90499 (96519)	Loss/tok 2.9758 (3.1605)	LR 1.400e-03
0: TRAIN [3][840/1885]	Time 0.193 (0.143)	Data 2.26e-04 (4.55e-04)	Tok/s 104272 (96543)	Loss/tok 3.4121 (3.1602)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][850/1885]	Time 0.060 (0.143)	Data 2.20e-04 (4.53e-04)	Tok/s 76327 (96529)	Loss/tok 2.5397 (3.1596)	LR 1.400e-03
0: TRAIN [3][860/1885]	Time 0.194 (0.143)	Data 2.22e-04 (4.50e-04)	Tok/s 104653 (96554)	Loss/tok 3.3350 (3.1608)	LR 1.400e-03
0: TRAIN [3][870/1885]	Time 0.195 (0.143)	Data 2.19e-04 (4.47e-04)	Tok/s 103127 (96548)	Loss/tok 3.2137 (3.1600)	LR 1.400e-03
0: TRAIN [3][880/1885]	Time 0.101 (0.143)	Data 2.21e-04 (4.44e-04)	Tok/s 89960 (96555)	Loss/tok 2.9976 (3.1590)	LR 1.400e-03
0: TRAIN [3][890/1885]	Time 0.194 (0.144)	Data 2.04e-04 (4.42e-04)	Tok/s 104637 (96642)	Loss/tok 3.1452 (3.1607)	LR 1.400e-03
0: TRAIN [3][900/1885]	Time 0.101 (0.143)	Data 2.22e-04 (4.39e-04)	Tok/s 91089 (96570)	Loss/tok 2.8165 (3.1595)	LR 1.400e-03
0: TRAIN [3][910/1885]	Time 0.102 (0.143)	Data 2.18e-04 (4.37e-04)	Tok/s 91043 (96538)	Loss/tok 2.8153 (3.1580)	LR 1.400e-03
0: TRAIN [3][920/1885]	Time 0.060 (0.143)	Data 2.20e-04 (4.34e-04)	Tok/s 76343 (96526)	Loss/tok 2.4553 (3.1570)	LR 1.400e-03
0: TRAIN [3][930/1885]	Time 0.194 (0.143)	Data 2.19e-04 (4.32e-04)	Tok/s 105057 (96502)	Loss/tok 3.1736 (3.1560)	LR 1.400e-03
0: TRAIN [3][940/1885]	Time 0.102 (0.143)	Data 2.21e-04 (4.30e-04)	Tok/s 87784 (96466)	Loss/tok 2.8813 (3.1551)	LR 1.400e-03
0: TRAIN [3][950/1885]	Time 0.147 (0.143)	Data 2.19e-04 (4.27e-04)	Tok/s 99451 (96499)	Loss/tok 3.0793 (3.1554)	LR 1.400e-03
0: TRAIN [3][960/1885]	Time 0.148 (0.143)	Data 1.97e-04 (4.25e-04)	Tok/s 99562 (96468)	Loss/tok 3.2316 (3.1546)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][970/1885]	Time 0.102 (0.142)	Data 1.83e-04 (4.23e-04)	Tok/s 89181 (96461)	Loss/tok 2.8232 (3.1537)	LR 1.400e-03
0: TRAIN [3][980/1885]	Time 0.102 (0.143)	Data 2.06e-04 (4.21e-04)	Tok/s 90478 (96480)	Loss/tok 2.9894 (3.1543)	LR 1.400e-03
0: TRAIN [3][990/1885]	Time 0.102 (0.142)	Data 2.04e-04 (4.19e-04)	Tok/s 89446 (96445)	Loss/tok 2.8311 (3.1536)	LR 1.400e-03
0: TRAIN [3][1000/1885]	Time 0.147 (0.143)	Data 2.20e-04 (4.16e-04)	Tok/s 100037 (96485)	Loss/tok 3.0730 (3.1541)	LR 1.400e-03
0: TRAIN [3][1010/1885]	Time 0.102 (0.143)	Data 1.80e-04 (4.14e-04)	Tok/s 88718 (96494)	Loss/tok 2.9029 (3.1543)	LR 1.400e-03
0: TRAIN [3][1020/1885]	Time 0.194 (0.143)	Data 1.13e-04 (4.12e-04)	Tok/s 104452 (96505)	Loss/tok 3.2001 (3.1540)	LR 1.400e-03
0: TRAIN [3][1030/1885]	Time 0.147 (0.143)	Data 1.07e-04 (4.09e-04)	Tok/s 99719 (96540)	Loss/tok 3.0441 (3.1547)	LR 1.400e-03
0: TRAIN [3][1040/1885]	Time 0.192 (0.143)	Data 1.08e-04 (4.06e-04)	Tok/s 105957 (96506)	Loss/tok 3.3333 (3.1545)	LR 1.400e-03
0: TRAIN [3][1050/1885]	Time 0.146 (0.143)	Data 2.17e-04 (4.04e-04)	Tok/s 100667 (96499)	Loss/tok 3.1011 (3.1542)	LR 1.400e-03
0: TRAIN [3][1060/1885]	Time 0.148 (0.143)	Data 1.80e-04 (4.02e-04)	Tok/s 100428 (96490)	Loss/tok 3.0228 (3.1536)	LR 1.400e-03
0: TRAIN [3][1070/1885]	Time 0.148 (0.143)	Data 1.21e-04 (4.00e-04)	Tok/s 98415 (96458)	Loss/tok 3.0915 (3.1532)	LR 1.400e-03
0: TRAIN [3][1080/1885]	Time 0.059 (0.143)	Data 2.21e-04 (3.98e-04)	Tok/s 77582 (96485)	Loss/tok 2.3916 (3.1545)	LR 1.400e-03
0: TRAIN [3][1090/1885]	Time 0.196 (0.143)	Data 1.90e-04 (3.96e-04)	Tok/s 103475 (96511)	Loss/tok 3.2093 (3.1549)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1100/1885]	Time 0.146 (0.143)	Data 1.91e-04 (3.94e-04)	Tok/s 100042 (96535)	Loss/tok 3.1221 (3.1546)	LR 1.400e-03
0: TRAIN [3][1110/1885]	Time 0.102 (0.143)	Data 1.07e-04 (3.92e-04)	Tok/s 89319 (96544)	Loss/tok 2.9606 (3.1546)	LR 1.400e-03
0: TRAIN [3][1120/1885]	Time 0.059 (0.143)	Data 1.04e-04 (3.89e-04)	Tok/s 77752 (96516)	Loss/tok 2.5546 (3.1537)	LR 1.400e-03
0: TRAIN [3][1130/1885]	Time 0.102 (0.143)	Data 1.53e-04 (3.87e-04)	Tok/s 90608 (96496)	Loss/tok 2.8614 (3.1532)	LR 1.400e-03
0: TRAIN [3][1140/1885]	Time 0.147 (0.143)	Data 2.17e-04 (3.86e-04)	Tok/s 100194 (96469)	Loss/tok 3.0916 (3.1527)	LR 1.400e-03
0: TRAIN [3][1150/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.84e-04)	Tok/s 87153 (96457)	Loss/tok 2.8789 (3.1518)	LR 1.400e-03
0: TRAIN [3][1160/1885]	Time 0.147 (0.143)	Data 2.20e-04 (3.83e-04)	Tok/s 100256 (96469)	Loss/tok 3.0317 (3.1515)	LR 1.400e-03
0: TRAIN [3][1170/1885]	Time 0.194 (0.143)	Data 2.21e-04 (3.81e-04)	Tok/s 105135 (96484)	Loss/tok 3.2381 (3.1522)	LR 1.400e-03
0: TRAIN [3][1180/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.80e-04)	Tok/s 100224 (96465)	Loss/tok 3.0454 (3.1516)	LR 1.400e-03
0: TRAIN [3][1190/1885]	Time 0.146 (0.143)	Data 2.18e-04 (3.79e-04)	Tok/s 99768 (96487)	Loss/tok 3.0654 (3.1518)	LR 1.400e-03
0: TRAIN [3][1200/1885]	Time 0.245 (0.143)	Data 2.29e-04 (3.77e-04)	Tok/s 106324 (96529)	Loss/tok 3.4054 (3.1518)	LR 1.400e-03
0: TRAIN [3][1210/1885]	Time 0.147 (0.143)	Data 2.18e-04 (3.76e-04)	Tok/s 101235 (96529)	Loss/tok 3.1142 (3.1517)	LR 1.400e-03
0: TRAIN [3][1220/1885]	Time 0.101 (0.143)	Data 2.27e-04 (3.75e-04)	Tok/s 90789 (96527)	Loss/tok 2.9377 (3.1518)	LR 1.400e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1230/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.73e-04)	Tok/s 100978 (96546)	Loss/tok 3.0819 (3.1521)	LR 1.400e-03
0: TRAIN [3][1240/1885]	Time 0.102 (0.143)	Data 2.05e-04 (3.72e-04)	Tok/s 90432 (96528)	Loss/tok 2.7504 (3.1513)	LR 1.400e-03
0: TRAIN [3][1250/1885]	Time 0.146 (0.143)	Data 2.19e-04 (3.71e-04)	Tok/s 98038 (96537)	Loss/tok 3.1154 (3.1511)	LR 1.400e-03
0: TRAIN [3][1260/1885]	Time 0.193 (0.143)	Data 2.24e-04 (3.70e-04)	Tok/s 105180 (96555)	Loss/tok 3.1930 (3.1510)	LR 1.400e-03
0: TRAIN [3][1270/1885]	Time 0.059 (0.143)	Data 2.19e-04 (3.69e-04)	Tok/s 77942 (96563)	Loss/tok 2.4699 (3.1519)	LR 1.400e-03
0: TRAIN [3][1280/1885]	Time 0.101 (0.144)	Data 2.33e-04 (3.67e-04)	Tok/s 88584 (96594)	Loss/tok 2.8659 (3.1531)	LR 1.400e-03
0: TRAIN [3][1290/1885]	Time 0.147 (0.144)	Data 2.35e-04 (3.66e-04)	Tok/s 100174 (96603)	Loss/tok 3.0558 (3.1529)	LR 7.000e-04
0: TRAIN [3][1300/1885]	Time 0.060 (0.144)	Data 2.19e-04 (3.65e-04)	Tok/s 76728 (96612)	Loss/tok 2.4281 (3.1526)	LR 7.000e-04
0: TRAIN [3][1310/1885]	Time 0.147 (0.144)	Data 2.18e-04 (3.64e-04)	Tok/s 100799 (96632)	Loss/tok 3.1220 (3.1529)	LR 7.000e-04
0: TRAIN [3][1320/1885]	Time 0.102 (0.144)	Data 2.22e-04 (3.63e-04)	Tok/s 89701 (96615)	Loss/tok 2.9395 (3.1526)	LR 7.000e-04
0: TRAIN [3][1330/1885]	Time 0.102 (0.144)	Data 2.21e-04 (3.62e-04)	Tok/s 90359 (96577)	Loss/tok 2.8432 (3.1522)	LR 7.000e-04
0: TRAIN [3][1340/1885]	Time 0.146 (0.144)	Data 2.22e-04 (3.61e-04)	Tok/s 99105 (96561)	Loss/tok 3.1613 (3.1517)	LR 7.000e-04
0: TRAIN [3][1350/1885]	Time 0.147 (0.144)	Data 2.19e-04 (3.60e-04)	Tok/s 100294 (96570)	Loss/tok 3.1371 (3.1513)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1360/1885]	Time 0.101 (0.144)	Data 2.16e-04 (3.59e-04)	Tok/s 90301 (96578)	Loss/tok 2.8578 (3.1512)	LR 7.000e-04
0: TRAIN [3][1370/1885]	Time 0.147 (0.144)	Data 2.15e-04 (3.58e-04)	Tok/s 99109 (96603)	Loss/tok 3.0623 (3.1513)	LR 7.000e-04
0: TRAIN [3][1380/1885]	Time 0.102 (0.143)	Data 2.22e-04 (3.57e-04)	Tok/s 89454 (96564)	Loss/tok 2.8949 (3.1503)	LR 7.000e-04
0: TRAIN [3][1390/1885]	Time 0.193 (0.143)	Data 2.18e-04 (3.55e-04)	Tok/s 105899 (96562)	Loss/tok 3.2308 (3.1499)	LR 7.000e-04
0: TRAIN [3][1400/1885]	Time 0.102 (0.143)	Data 2.19e-04 (3.54e-04)	Tok/s 91332 (96563)	Loss/tok 2.8953 (3.1496)	LR 7.000e-04
0: TRAIN [3][1410/1885]	Time 0.147 (0.143)	Data 2.17e-04 (3.54e-04)	Tok/s 100748 (96569)	Loss/tok 3.1210 (3.1496)	LR 7.000e-04
0: TRAIN [3][1420/1885]	Time 0.194 (0.143)	Data 2.17e-04 (3.53e-04)	Tok/s 105159 (96547)	Loss/tok 3.2078 (3.1490)	LR 7.000e-04
0: TRAIN [3][1430/1885]	Time 0.244 (0.144)	Data 2.18e-04 (3.52e-04)	Tok/s 107255 (96557)	Loss/tok 3.4288 (3.1496)	LR 7.000e-04
0: TRAIN [3][1440/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.51e-04)	Tok/s 99615 (96555)	Loss/tok 3.0027 (3.1492)	LR 7.000e-04
0: TRAIN [3][1450/1885]	Time 0.147 (0.144)	Data 2.16e-04 (3.50e-04)	Tok/s 100886 (96572)	Loss/tok 3.0680 (3.1492)	LR 7.000e-04
0: TRAIN [3][1460/1885]	Time 0.101 (0.144)	Data 2.21e-04 (3.49e-04)	Tok/s 87974 (96568)	Loss/tok 2.9760 (3.1489)	LR 7.000e-04
0: TRAIN [3][1470/1885]	Time 0.102 (0.143)	Data 2.24e-04 (3.48e-04)	Tok/s 89719 (96563)	Loss/tok 2.8106 (3.1487)	LR 7.000e-04
0: TRAIN [3][1480/1885]	Time 0.148 (0.144)	Data 2.18e-04 (3.47e-04)	Tok/s 98393 (96570)	Loss/tok 3.1047 (3.1483)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1490/1885]	Time 0.147 (0.144)	Data 2.21e-04 (3.46e-04)	Tok/s 99173 (96592)	Loss/tok 3.0939 (3.1479)	LR 7.000e-04
0: TRAIN [3][1500/1885]	Time 0.194 (0.144)	Data 2.19e-04 (3.45e-04)	Tok/s 105075 (96595)	Loss/tok 3.2243 (3.1478)	LR 7.000e-04
0: TRAIN [3][1510/1885]	Time 0.194 (0.144)	Data 2.19e-04 (3.45e-04)	Tok/s 105899 (96591)	Loss/tok 3.2878 (3.1473)	LR 7.000e-04
0: TRAIN [3][1520/1885]	Time 0.195 (0.144)	Data 2.16e-04 (3.44e-04)	Tok/s 106451 (96591)	Loss/tok 3.1555 (3.1469)	LR 7.000e-04
0: TRAIN [3][1530/1885]	Time 0.245 (0.144)	Data 2.17e-04 (3.43e-04)	Tok/s 106706 (96614)	Loss/tok 3.2924 (3.1469)	LR 7.000e-04
0: TRAIN [3][1540/1885]	Time 0.195 (0.144)	Data 2.23e-04 (3.42e-04)	Tok/s 104102 (96624)	Loss/tok 3.3061 (3.1467)	LR 7.000e-04
0: TRAIN [3][1550/1885]	Time 0.102 (0.144)	Data 2.21e-04 (3.41e-04)	Tok/s 89643 (96633)	Loss/tok 2.8820 (3.1467)	LR 7.000e-04
0: TRAIN [3][1560/1885]	Time 0.101 (0.144)	Data 2.18e-04 (3.41e-04)	Tok/s 90378 (96649)	Loss/tok 2.7721 (3.1469)	LR 7.000e-04
0: TRAIN [3][1570/1885]	Time 0.146 (0.144)	Data 2.31e-04 (3.40e-04)	Tok/s 101142 (96651)	Loss/tok 3.0559 (3.1464)	LR 7.000e-04
0: TRAIN [3][1580/1885]	Time 0.146 (0.144)	Data 2.35e-04 (3.39e-04)	Tok/s 100842 (96628)	Loss/tok 3.1028 (3.1457)	LR 7.000e-04
0: TRAIN [3][1590/1885]	Time 0.193 (0.144)	Data 2.18e-04 (3.38e-04)	Tok/s 105876 (96633)	Loss/tok 3.1711 (3.1453)	LR 7.000e-04
0: TRAIN [3][1600/1885]	Time 0.194 (0.144)	Data 2.16e-04 (3.37e-04)	Tok/s 102662 (96619)	Loss/tok 3.2059 (3.1446)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1610/1885]	Time 0.101 (0.143)	Data 2.34e-04 (3.37e-04)	Tok/s 89771 (96596)	Loss/tok 2.7570 (3.1438)	LR 7.000e-04
0: TRAIN [3][1620/1885]	Time 0.193 (0.144)	Data 2.22e-04 (3.36e-04)	Tok/s 105293 (96616)	Loss/tok 3.2110 (3.1439)	LR 7.000e-04
0: TRAIN [3][1630/1885]	Time 0.244 (0.144)	Data 2.18e-04 (3.35e-04)	Tok/s 108359 (96599)	Loss/tok 3.2420 (3.1435)	LR 7.000e-04
0: TRAIN [3][1640/1885]	Time 0.146 (0.143)	Data 2.18e-04 (3.35e-04)	Tok/s 100721 (96586)	Loss/tok 2.9697 (3.1430)	LR 7.000e-04
0: TRAIN [3][1650/1885]	Time 0.102 (0.144)	Data 2.18e-04 (3.34e-04)	Tok/s 88814 (96596)	Loss/tok 2.8484 (3.1428)	LR 7.000e-04
0: TRAIN [3][1660/1885]	Time 0.193 (0.144)	Data 2.21e-04 (3.33e-04)	Tok/s 105912 (96624)	Loss/tok 3.1805 (3.1434)	LR 7.000e-04
0: TRAIN [3][1670/1885]	Time 0.059 (0.144)	Data 2.19e-04 (3.33e-04)	Tok/s 77017 (96608)	Loss/tok 2.4367 (3.1427)	LR 7.000e-04
0: TRAIN [3][1680/1885]	Time 0.101 (0.143)	Data 2.19e-04 (3.32e-04)	Tok/s 87380 (96581)	Loss/tok 3.0580 (3.1424)	LR 7.000e-04
0: TRAIN [3][1690/1885]	Time 0.102 (0.144)	Data 2.17e-04 (3.31e-04)	Tok/s 89421 (96610)	Loss/tok 2.8974 (3.1429)	LR 7.000e-04
0: TRAIN [3][1700/1885]	Time 0.060 (0.144)	Data 2.22e-04 (3.30e-04)	Tok/s 76445 (96581)	Loss/tok 2.3878 (3.1424)	LR 7.000e-04
0: TRAIN [3][1710/1885]	Time 0.147 (0.143)	Data 2.21e-04 (3.30e-04)	Tok/s 101119 (96561)	Loss/tok 3.0447 (3.1421)	LR 7.000e-04
0: TRAIN [3][1720/1885]	Time 0.102 (0.143)	Data 2.19e-04 (3.29e-04)	Tok/s 87983 (96560)	Loss/tok 2.8489 (3.1413)	LR 7.000e-04
0: TRAIN [3][1730/1885]	Time 0.147 (0.143)	Data 2.22e-04 (3.29e-04)	Tok/s 100785 (96544)	Loss/tok 3.0909 (3.1409)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1740/1885]	Time 0.146 (0.143)	Data 2.22e-04 (3.28e-04)	Tok/s 100070 (96565)	Loss/tok 2.9967 (3.1407)	LR 7.000e-04
0: TRAIN [3][1750/1885]	Time 0.101 (0.143)	Data 2.20e-04 (3.27e-04)	Tok/s 88376 (96567)	Loss/tok 2.8318 (3.1405)	LR 7.000e-04
0: TRAIN [3][1760/1885]	Time 0.101 (0.143)	Data 2.17e-04 (3.27e-04)	Tok/s 88857 (96573)	Loss/tok 2.8306 (3.1407)	LR 7.000e-04
0: TRAIN [3][1770/1885]	Time 0.102 (0.143)	Data 2.18e-04 (3.26e-04)	Tok/s 89924 (96562)	Loss/tok 2.8527 (3.1403)	LR 7.000e-04
0: TRAIN [3][1780/1885]	Time 0.147 (0.143)	Data 2.33e-04 (3.25e-04)	Tok/s 100341 (96564)	Loss/tok 2.9912 (3.1398)	LR 7.000e-04
0: TRAIN [3][1790/1885]	Time 0.146 (0.143)	Data 2.24e-04 (3.25e-04)	Tok/s 99936 (96559)	Loss/tok 2.9918 (3.1397)	LR 7.000e-04
0: TRAIN [3][1800/1885]	Time 0.196 (0.143)	Data 2.17e-04 (3.24e-04)	Tok/s 103009 (96546)	Loss/tok 3.1798 (3.1392)	LR 7.000e-04
0: TRAIN [3][1810/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.24e-04)	Tok/s 88701 (96538)	Loss/tok 2.8727 (3.1390)	LR 7.000e-04
0: TRAIN [3][1820/1885]	Time 0.102 (0.143)	Data 2.19e-04 (3.23e-04)	Tok/s 88820 (96538)	Loss/tok 2.8581 (3.1386)	LR 7.000e-04
0: TRAIN [3][1830/1885]	Time 0.146 (0.143)	Data 2.21e-04 (3.23e-04)	Tok/s 100750 (96530)	Loss/tok 3.0327 (3.1384)	LR 7.000e-04
0: TRAIN [3][1840/1885]	Time 0.193 (0.143)	Data 2.23e-04 (3.22e-04)	Tok/s 104977 (96539)	Loss/tok 3.1571 (3.1383)	LR 7.000e-04
0: TRAIN [3][1850/1885]	Time 0.146 (0.143)	Data 2.36e-04 (3.22e-04)	Tok/s 100732 (96534)	Loss/tok 3.0857 (3.1377)	LR 7.000e-04
0: TRAIN [3][1860/1885]	Time 0.101 (0.143)	Data 2.18e-04 (3.21e-04)	Tok/s 91913 (96529)	Loss/tok 2.7567 (3.1373)	LR 7.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1870/1885]	Time 0.102 (0.143)	Data 2.20e-04 (3.20e-04)	Tok/s 85476 (96514)	Loss/tok 2.8951 (3.1368)	LR 7.000e-04
0: TRAIN [3][1880/1885]	Time 0.146 (0.143)	Data 2.28e-04 (3.20e-04)	Tok/s 99942 (96504)	Loss/tok 3.0903 (3.1363)	LR 7.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593831761678, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593831761678, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.566 (0.566)	Decoder iters 102.0 (102.0)	Tok/s 28593 (28593)
0: Running moses detokenizer
0: BLEU(score=24.10028679024574, counts=[36720, 18393, 10473, 6208], totals=[64474, 61471, 58469, 55471], precisions=[56.95319043335298, 29.921426363651154, 17.912055961278625, 11.191433361576319], bp=0.9968718569876424, sys_len=64474, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593831763249, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24100000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593831763249, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1356	Test BLEU: 24.10
0: Performance: Epoch: 3	Training: 771811 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593831763249, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593831763249, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-07-04 03:02:49 AM
RESULT,RNN_TRANSLATOR,,1108,Fujitsu,2020-07-04 02:44:21 AM
