# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Runs a reinforcement learning loop to train a Go playing model."""

import sys
sys.path.insert(0, '.')  # nopep8

import asyncio
import itertools
import logging
import os
import re
import tensorflow as tf
import time
from ml_perf.utils import *
import dual_net
import logging
from mlperf_logging import mllog

from absl import app, flags

flags.DEFINE_integer('iterations', 100, 'Number of iterations of the RL loop.')

flags.DEFINE_string('flags_dir', None,
                    'Directory in which to find the flag files for each stage '
                    'of the RL loop. The directory must contain the following '
                    'files: bootstrap.flags, selfplay.flags, eval.flags, '
                    'train.flags.')

flags.DEFINE_integer('window_size', 5,
                     'Maximum number of recent selfplay rounds to train on.')

flags.DEFINE_float('train_filter', 0.3,
                   'Fraction of selfplay games to pass to training.')

flags.DEFINE_integer('examples_per_generation', 131072,
                     'Number of examples use from each generation in the '
                     'training window.')

flags.DEFINE_boolean('validate', False, 'Run validation on holdout games')

flags.DEFINE_integer('min_games_per_iteration', 4096,
                     'Minimum number of games to play for each training '
                     'iteration.')

flags.DEFINE_string('golden_chunk_dir', None, 'Training example directory.')
flags.DEFINE_string('golden_chunk_local_dir', None, 'Local training example '
                    'directory.')
flags.DEFINE_string('golden_chunk_tmp_dir', None, 'Temporary training example '
                    'directory.')
flags.DEFINE_string('holdout_dir', None, 'Holdout example directory.')
flags.DEFINE_string('model_dir', None, 'Model directory.')
flags.DEFINE_string('selfplay_dir', None, 'Selfplay example directory.')
flags.DEFINE_string('pause', None, 'Pause file path directory.')
flags.DEFINE_boolean('quantization', True, 'Using Int8 if true.')
flags.DEFINE_integer('quantize_test_steps', 5, 'The steps to run for min&max log.')
flags.DEFINE_integer('quantize_test_batch_size', 16, 'The batch size for running inference for min&max log.')
flags.DEFINE_boolean('random_rotation', True, 'Do random rotation when running for min&max log.')

flags.DEFINE_string('precision', 'int8',
                    'Specify the model precision to use: fp32 or int8')

flags.DEFINE_string('hostlist', None, 'Comma seperated host list.')

flags.DEFINE_integer('physical_cores', None, 'The number of cores for each '
                     'node.')
flags.DEFINE_integer('virtual_cores', None, 'The number of SMT for each node.')
flags.DEFINE_integer('numa_cores', None, 'The number of core for each numa '
                     'node.')
flags.DEFINE_integer('train_instance_per_numa', 1, 'The number of instance for '
                     'each numa node.')

FLAGS = flags.FLAGS


# Training loop state.
class State:
    def __init__(self, model_num):
        self.start_time = time.time()
        self.start_iter_num = model_num
        self.iter_num = model_num

    def _model_name(self, it):
        return '%06d' % it

    @property
    def selfplay_model_name(self):
        return self._model_name(self.iter_num - 1)

    @property
    def selfplay_model_path(self):
        return '{}.pb'.format(
            os.path.join(FLAGS.model_dir, self.selfplay_model_name))

    @property
    def train_model_name(self):
        return self._model_name(self.iter_num)

    @property
    def train_model_path(self):
        return '{}.pb'.format(
            os.path.join(FLAGS.model_dir, self.train_model_name))


def wait_for_training_examples(state, num_games):
    """Wait for training examples to be generated by the latest model.

    Args:
        state: the RL loop State instance.
        num_games: number of games to wait for.
    """

    model_dir = os.path.join(FLAGS.selfplay_dir, state.selfplay_model_name)
    pattern = os.path.join(model_dir, '*', '*', '*.tfrecord.zz')
    for i in itertools.count():
        try:
            paths = sorted(tf.io.gfile.glob(pattern))
        except tf.errors.OpError:
            paths = []
        if len(paths) >= num_games:
            mllogger = mllog.get_mllogger()
            mllog.config(filename="train.log")

            mllogger.event(key='actual_selfplay_games_per_generation', value=len(paths))
            break
        if i % 30 == 0:
            logging.info('Waiting for %d games in %s (found %d)',
                         num_games, model_dir, len(paths))
        time.sleep(1)


def list_selfplay_dirs(base_dir):
    """Returns a sorted list of selfplay data directories.

    Training examples are written out to the following directory hierarchy:
      base_dir/device_id/model_name/timestamp/

    Args:
      base_dir: either selfplay_dir or holdout_dir.

    Returns:
      A list of model directories sorted so the most recent directory is first.
    """

    model_dirs = [os.path.join(base_dir, x)
                  for x in tf.io.gfile.listdir(base_dir)]
    return sorted(model_dirs, reverse=True)


def sample_training_examples(state, read_threads, write_threads,
      golden_chunk_dir):
    """Sample training examples from recent selfplay games.

    Args:
        state: the RL loop State instance.

    Returns:
        A (num_examples, record_paths) tuple:
         - num_examples : number of examples sampled.
         - record_paths : list of golden chunks up to window_size in length,
                          sorted by path.
    """

    # Read examples from the most recent `window_size` models.
    model_dirs = list_selfplay_dirs(FLAGS.selfplay_dir)[:FLAGS.window_size]
    src_patterns = [os.path.join(x, '*', '*', '*.tfrecord.zz')
                    for x in model_dirs]

    dst_path = os.path.join(golden_chunk_dir,
                            '{}.tfrecord.zz'.format(state.train_model_name))

    logging.info('Writing training chunks to %s', dst_path)
    output = wait(checked_run([
        'bazel-bin/cc/sample_records',
        '--num_read_threads={}'.format(read_threads),
        '--num_write_threads={}'.format(write_threads),
        '--files_per_pattern={}'.format(FLAGS.min_games_per_iteration),
        '--sample_frac={}'.format(FLAGS.train_filter),
        '--compression=1',
        '--shuffle=true',
        '--dst={}'.format(dst_path)] + src_patterns))

    m = re.search(r"sampled ([\d]+) records", output)
    assert m
    num_examples = int(m.group(1))

    chunk_pattern = os.path.join(
        golden_chunk_dir,
        '{}-*-of-*.tfrecord.zz'.format(state.train_model_name))
    chunk_paths = sorted(tf.io.gfile.glob(chunk_pattern))
    logging.info(chunk_pattern)
    logging.info(chunk_paths)
    assert len(chunk_paths) == write_threads

    return (num_examples, chunk_paths)


def append_timestamp(elapsed, model_name):
  # Append the time elapsed from when the RL was started to when this model
  # was trained. GCS files are immutable, so we have to do the append manually.
  timestamps_path = os.path.join(FLAGS.model_dir, 'train_times.txt')
  try:
    with tf.io.gfile.GFile(timestamps_path, 'r') as f:
      timestamps = f.read()
  except tf.errors.NotFoundError:
      timestamps = ''
  timestamps += '{:.3f} {}\n'.format(elapsed, model_name)
  with tf.io.gfile.GFile(timestamps_path, 'w') as f:
      f.write(timestamps)


def train(state):
    """Run training and write a new model to the model_dir.

    Args:
        state: the RL loop State instance.
    """

    wait_for_training_examples(state, FLAGS.min_games_per_iteration)

    mllogger = mllog.get_mllogger()
    mllogger.event(key='num_games', value=FLAGS.min_games_per_iteration)
    mllogger.event(key='min_selfplay_games_per_generation', value=FLAGS.min_games_per_iteration)

    # pause selfplay
    fd_pause = os.open(FLAGS.pause, os.O_CREAT)
    os.close(fd_pause)

    if (FLAGS.hostlist != None):
      dist_train = True
    else:
      dist_train = False

    if dist_train:
      intra_threads = FLAGS.numa_cores // FLAGS.train_instance_per_numa - 1
      inter_threads = 2
      numa_per_node = FLAGS.physical_cores // FLAGS.numa_cores
      instance_per_node = numa_per_node * FLAGS.train_instance_per_numa
      host_num = len(FLAGS.hostlist.split(","))
      train_node = FLAGS.hostlist.split(",")
      num_instances = instance_per_node * host_num

      mpi_async_progress = ''
      for i in range(numa_per_node):
        for j in range(FLAGS.train_instance_per_numa):
          if (not i == 0) or (not j == 0):
            mpi_async_progress += ','
          mpi_async_progress += '{}'.format(i * FLAGS.numa_cores + j)
      # golden_chunk_dir is usually supplied as a shared directory visible
      # to all instances.
      # golden_chunk_local_dir is usually a local directory only visible
      # by each node itself.
      # use a local dir could help improve data pipeline performance in
      # training.
      if FLAGS.golden_chunk_local_dir == None:
        num_examples, record_paths = sample_training_examples(state,
            FLAGS.numa_cores,
            num_instances, FLAGS.golden_chunk_dir)
      else:
        num_examples, record_paths = sample_training_examples(state,
            FLAGS.numa_cores,
            num_instances, FLAGS.golden_chunk_tmp_dir)
        # copy golden chunk records to each node
        scp_handles = [] * num_instances
        for i in range (0, host_num):
          for j in range (0, instance_per_node):
            scp_handles.append(asyncio.gather(
                checked_run(['scp',
                             record_paths[i * instance_per_node + j],
                             '{}:{}'.format(train_node[i],
                                            FLAGS.golden_chunk_local_dir)]),
                return_exceptions=True))
        for i in range (0, host_num):
          for j in range (0, instance_per_node):
            asyncio.get_event_loop().run_until_complete(
                scp_handles[i * instance_per_node + j])
        for i in range (0, len(record_paths)):
          record_paths[i] = record_paths[i].replace(FLAGS.golden_chunk_tmp_dir,
                                            FLAGS.golden_chunk_local_dir, 1)
      # for distributed training train each instance need to train on same
      # number of samples.  Make the total number of sample trained unbiased.
      num_examples = (num_examples + len(record_paths) // 2) // len(record_paths)
    else:
      intra_threads = FLAGS.physical_cores
      inter_threads = 2
      num_examples, record_paths = sample_training_examples(state,
          FLAGS.numa_cores, 8, FLAGS.golden_chunk_dir)

    mllogger.event(key='train_samples', value=num_examples)

    if(FLAGS.use_bfloat16):
        logging.info("Run bfloat16 training.")
    else:
        logging.info("Run fp32 training.")

    model_path = os.path.join(FLAGS.model_dir, state.train_model_name)
    commands = ['python3', 'train.py',
        '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'train.flags')),
        '--work_dir={}'.format(FLAGS.work_dir),
        '--export_path={}'.format(model_path),
        '--use_tpu={}'.format('true' if FLAGS.tpu_name else 'false'),
        '--tpu_name={}'.format(FLAGS.tpu_name),
        '--training_seed={}'.format(state.iter_num),
        '--num_examples={}'.format(num_examples),
        '--use_bfloat16={}'.format(FLAGS.use_bfloat16),
        '--selfplay_precision={}'.format(FLAGS.precision),
        '--num_intra_threads={}'.format(intra_threads),
        '--num_inter_threads={}'.format(inter_threads),
        '--freeze=true']

    if(dist_train):
      genvs = ['HOROVOD_FUSION_THRESHOLD=134217728',
               'KMP_BLOCKTIME=1',
               'KMP_HW_SUBSET=2T',
               'OMP_BIND_PROC=true',
               'I_MPI_PIN_DOMAIN=socket',
               'OMP_NUM_THREADS={}'.format(intra_threads)]
      hosts = []
      proclists = []
      numa_nodes = []
      num_node = len(train_node)
      for node in range(num_node):
        # add all instance to the list
        for numa in range(numa_per_node):
          for instance in range(FLAGS.train_instance_per_numa):
            hosts += [train_node[node]]
            proclist = numa * FLAGS.numa_cores + FLAGS.train_instance_per_numa + instance * intra_threads
            proclists += ['{}'.format(proclist)]
            numa_nodes += ['{}'.format(numa)]

      commands +=['--dist_train=True']
      result = wait(checked_run_distributed(genvs, 1, hosts, proclists, numa_nodes, None, FLAGS.log_dir, commands, record_paths))
    else:
      commands += record_paths
      result = wait(checked_run(commands))
    logging.info(result)

    # Append the time elapsed from when the RL was started to when this model
    # was trained.
    elapsed = time.time() - state.start_time
    append_timestamp(elapsed, state.train_model_name)

    if FLAGS.validate and state.iter_num - state.start_iter_num > 1:
        try:
            validate(state)
        except Exception as e:
            logging.error(e)

    if(FLAGS.precision == 'fp32'):
        logging.info('Run selfplay with fp32 precision.')
        #resume selfplay
        os.remove(FLAGS.pause)
    elif(FLAGS.precision == 'int8'):
        logging.info('Run selfplay with int8 precision.')
    else:
        raise Exception('Precision type {0} is not supported!'
                        .format(FLAGS.precision))


def validate(state):
    src_dirs = list_selfplay_dirs(FLAGS.holdout_dir)[:FLAGS.window_size]

    wait(checked_run([
        'python3', 'validate.py',
        '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'validate.flags')),
        '--work_dir={}'.format(FLAGS.work_dir),
        '--use_tpu={}'.format('true' if FLAGS.tpu_name else 'false'),
        '--tpu_name={}'.format(FLAGS.tpu_name),
        '--expand_validation_dirs'] + src_dirs))

def post_train(state):
    model_path = os.path.join(FLAGS.model_dir, state.train_model_name)
    selfplay_pb = model_path + '.pb'

    if (FLAGS.hostlist != None):
        golden_chunk_dir = FLAGS.golden_chunk_local_dir
    else:
        golden_chunk_dir = FLAGS.golden_chunk_dir

    dual_net.optimize_graph(model_path + '.pb',
        model_path,
        FLAGS.quantization,
        golden_chunk_dir + '/*.zz')

    dst_minigo_file = model_path + '.minigo'
    wait(checked_run([
         'python3', 'convert.py',
         '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'architecture.flags')),
         '--input_graph={}'.format(selfplay_pb),
         '--dst={}'.format(dst_minigo_file)]))

    #resume selfplay
    os.remove(FLAGS.pause)

def main(unused_argv):
    """Run the reinforcement learning loop."""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter('[%(asctime)s] %(message)s',
                                  '%Y-%m-%d %H:%M:%S')

    for handler in logger.handlers:
        handler.setFormatter(formatter)

    mllogger = mllog.get_mllogger()
    mllog.config(filename="train.log")

    mllog.config(
      default_namespace = "worker1",
      default_stack_offset = 1,
      default_clear_line = False)

    mllogger.event(key=mllog.constants.SUBMISSION_ORG, value="Intel")
    mllogger.event(key=mllog.constants.SUBMISSION_PLATFORM, value="8 nodes x 4s CPX")
    mllogger.event(key=mllog.constants.SUBMISSION_DIVISION, value="closed")
    mllogger.event(key=mllog.constants.SUBMISSION_STATUS, value="onprem")
    mllogger.event(key=mllog.constants.SUBMISSION_BENCHMARK, value="minigo")

    mllogger.event(key='cache_clear', value=True)

    mllogger.event(key="filter_amount", value=FLAGS.train_filter)
     
    # The training loop must be bootstrapped; either by running bootstrap.sh
    # to generate training data from random games, or by running
    # copy_checkpoint.sh to copy an already generated checkpoint.
    model_dirs = list_selfplay_dirs(FLAGS.selfplay_dir)
    if not model_dirs:
        raise RuntimeError(
            'Couldn\'t find any selfplay games under %s. Either bootstrap.sh '
            'or init_from_checkpoint.sh must be run before the train loop is '
            'started')
    model_num = int(os.path.basename(model_dirs[0]))


    mllogger.end(key=mllog.constants.INIT_STOP)
    mllogger.start(key=mllog.constants.RUN_START)
    with logged_timer('Total time'):
        try:
            state = State(model_num)
            wait(checked_run([
                'python3', 'parse_flags_train.py',
                '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'train.flags'))
                ]))
            wait(checked_run([
                'python3', 'parse_flags_selfplay.py',
                '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'selfplay.flags'))
                ]))

            mllogger.event(key="window_size", value=FLAGS.window_size)

            while state.iter_num <= FLAGS.iterations:
                mllogger.event(
                    key=mllog.constants.EPOCH_START,
                    value=None,
                    metadata={"epoch_num": state.iter_num})
                state.iter_num += 1
                train(state)
                mllogger.event(
                    key=mllog.constants.EPOCH_STOP,
                    value=None,
                    metadata={"epoch_num": state.iter_num})
                if(FLAGS.precision == 'int8'):
                    post_train(state)
        finally:
                asyncio.get_event_loop().close()

if __name__ == '__main__':
    app.run(main)
