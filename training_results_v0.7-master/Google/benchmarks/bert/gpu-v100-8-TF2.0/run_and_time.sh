python3 run_pretraing.py --all_reduce_alg=nccl --bert_config_file=/data/bert_mlperf_data/bert_config.json --device_warmup --do_eval --dtype=fp16 --eval_batch_size=48 --init_checkpoint=/data/bert_mlperf_data/model.ckpt-28252 '--input_files=gs://bert-data-europe/tfrecords3_500_parts/part-*' --learning_rate=0.0004 --loss_scale=dynamic --max_predictions_per_seq=76 --max_seq_length=512 --model_dir=/workspace/benchmarks/perfzero/workspace/output/2020-07-09-23-16-07-377743/benchmark_8_gpu --num_accumulation_steps=11 --num_gpus=8 --num_steps_per_epoch=8103 --num_train_epochs=1 --optimizer_type=lamb --scale_loss --steps_before_eval_start=6819 --steps_between_eval=1137 --steps_per_loop=1137 --stop_steps=8103 --train_batch_size=440 --verbosity=0 --warmup_steps=0