# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# pylint: disable=bad-indentation,broad-except,g-wrong-blank-lines,g-doc-args, missing-docstring, old-style-class, g-doc-return-or-yield
"""Runs a reinforcement learning loop to train a Go playing model."""

import gc
import logging
import os
import sys
import time

from absl import app
from absl import flags
import tensorflow.compat.v1 as tf

from REDACTED.minigo import train
from REDACTED.minigo import utils as minigo_utils
from REDACTED.minigo import validate as validate_file
from REDACTED.mlp_log import mlp_log

sys.path.insert(0, '.')  # nopep8

flags.DEFINE_integer('iterations', 100, 'Number of iterations of the RL loop.')
flags.DEFINE_string('abort_file_path', None,
                    'A file that signals selfplay to stop.')
flags.DEFINE_string('golden_chunk_dir', None, 'Training example directory.')
flags.DEFINE_string('holdout_dir', None, 'Holdout example directory.')
flags.DEFINE_string('model_dir', None, 'Model directory.')
flags.DEFINE_string('selfplay_dir', None, 'Selfplay example directory.')
flags.DEFINE_boolean('validate', False, 'Run validation on holdout games')
flags.DEFINE_integer('validate_batch_size', 64, 'Batch size for validation')
flags.DEFINE_integer('training_batch_size', 4096, 'Batch size for validation')
flags.DEFINE_integer('worker_reset_timeout_ms', 5000,
                     'Session timeout for resetting the worker.')

# ML Perf Logging related flags.
flags.DEFINE_integer(
    'mlperf_num_games', -1,
    'Total number of games to self play. 0 implies self '
    'play forever.')
flags.DEFINE_integer(
    'mlperf_num_readouts', -1,
    'Number of readouts to make during tree search for '
    'each move.')
flags.DEFINE_float(
    'mlperf_value_init_penalty', -1.0,
    'New children value initialization penalty. '
    'Child value = parent\'s value - penalty * color, '
    'clamped to [-1, 1].  Penalty should be in [0.0, 2.0]. '
    '0 is init-to-parent, 2.0 is init-to-loss [default]. '
    'This behaves similiarly to Leela\'s FPU '
    '"First Play Urgency".')
flags.DEFINE_float('mlperf_holdout_pct', -1.0,
                   'Fraction of games to hold out for validation.')
flags.DEFINE_float('mlperf_disable_resign_pct', -1.0,
                   'Fraction of games to disable resignation for.')
flags.DEFINE_multi_float(
    'mlperf_resign_threshold', [0.0, 0.0],
    'Each game\'s resign threshold is picked randomly '
    'from the range '
    '[min_resign_threshold, max_resign_threshold)')
flags.DEFINE_integer(
    'mlperf_parallel_games', -1,
    'Number of games to play concurrently on each selfplay '
    'thread. Inferences from a thread\'s concurrent games are '
    'batched up and evaluated together. Increasing '
    'concurrent_games_per_thread can help improve GPU or '
    'TPU utilization, especially for small models.')
flags.DEFINE_integer('mlperf_virtual_losses', -1,
                     'Number of virtual losses when running tree search')
flags.DEFINE_float(
    'mlperf_gating_win_rate', -1.0,
    'Win pct against the target model to define a converged '
    'model.')
flags.DEFINE_integer(
    'mlperf_eval_games', -1, 'Number of games to play against the target to '
    'when determining the win pct.')

flags.declare_key_flag('window_size')
flags.declare_key_flag('work_dir')
flags.declare_key_flag('tpu_name')
flags.declare_key_flag('train_batch_size')
flags.declare_key_flag('l2_strength')
flags.declare_key_flag('filter_amount')
flags.declare_key_flag('lr_boundaries')
flags.declare_key_flag('lr_rates')

FLAGS = flags.FLAGS


# Training loop state.
class State:

  def __init__(self, model_num):
    self.start_time = time.time()
    self.start_iter_num = model_num
    self.iter_num = model_num

  def _model_name(self, it):
    return '%06d' % it

  @property
  def selfplay_model_name(self):
    return self._model_name(self.iter_num - 1)

  @property
  def selfplay_model_path(self):
    return '{}.pb'.format(
        os.path.join(FLAGS.model_dir, self.selfplay_model_name))

  @property
  def train_model_name(self):
    return self._model_name(self.iter_num)

  @property
  def train_model_path(self):
    return os.path.join(FLAGS.model_dir, self.train_model_name)


def wait_for_training_examples(state):
  """Wait for training examples to be generated by the latest model.

    Args:
        state: the RL loop State instance.

    Returns:
      num_sampled_examples: number of examples sampled
      record_paths: a list of golden chunks, usually 8
  """
  logging.info('Start waiting for data for model %s', state.train_model_name)
  data_filename = os.path.join(
      FLAGS.golden_chunk_dir, state.train_model_name + '-golden_chunk_list.txt')
  logging.info('Data file: %s', data_filename)
  while True:
    if tf.io.gfile.exists(data_filename):
      with tf.io.gfile.GFile(data_filename, 'r') as data_list_file:
        tfrecords = [line.strip('\n') for line in data_list_file.readlines()]
      if tfrecords:
        break
    time.sleep(1)
  logging.info('Done waiting for data for %s', state.train_model_name)
  num_sampled_examples = tfrecords.pop()
  return num_sampled_examples, tfrecords


def list_selfplay_dirs(base_dir):
  """Returns a sorted list of selfplay data directories.

  Training examples are written out to the following directory hierarchy:
    base_dir/device_id/model_name/timestamp/

  Args:
    base_dir: either selfplay_dir or holdout_dir.

  Returns:
    A list of model directories sorted so the most recent directory is first.
  """

  model_dirs = [
      os.path.join(base_dir, x) for x in tf.io.gfile.listdir(base_dir)
  ]
  model_dirs = [x for x in model_dirs if tf.io.gfile.isdir(x)]
  return sorted(model_dirs, reverse=True)



def append_timestamp(elapsed, model_name):
  # Append the time elapsed from when the RL was started to when this model
  # was trained. GCS files are immutable, so we have to do the append manually.
  timestamps_path = os.path.join(FLAGS.model_dir, 'train_times.txt')
  try:
    with tf.gfile.Open(timestamps_path, 'r') as f:
      timestamps = f.read()
  except tf.errors.NotFoundError:
    timestamps = ''
  timestamps += '{:.3f} {}\n'.format(elapsed, model_name)
  with tf.gfile.Open(timestamps_path, 'w') as f:
    f.write(timestamps)


def train_once(state):
  """Run training and write a new model to the model_dir.

    Args:
        state: the RL loop State instance.
  """
  num_examples, record_paths = wait_for_training_examples(state)

  FLAGS.train_data_path = record_paths
  FLAGS.num_examples = int(num_examples)
  FLAGS.use_tpu = (True if FLAGS.tpu_name else False)
  FLAGS.export_path = state.train_model_path
  FLAGS.train_batch_size = FLAGS.training_batch_size
  FLAGS.freeze = True
  FLAGS.shuffle_buffer_size = 0
  FLAGS.shuffle_examples = False
  FLAGS.keep_checkpoint_max = 100

  # architecture flags
  FLAGS.l2_strength = 0.0001
  FLAGS.conv_width = 64
  FLAGS.fc_width = 64
  FLAGS.trunk_layers = 6
  FLAGS.value_cost_weight = 0.5
  FLAGS.summary_steps = 128
  FLAGS.bool_features = 1
  FLAGS.input_features = 'mlperf07'
  FLAGS.input_layout = 'nchw'

  train.main('unused')

  # Append the time elapsed from when the RL was started to when this model
  # was trained.
  elapsed = time.time() - state.start_time
  append_timestamp(elapsed, state.train_model_name)

  if FLAGS.validate and state.iter_num - state.start_iter_num > 1:
    try:
      validate()
    except Exception as e:
      logging.error(e)


def validate():
  src_dirs = list_selfplay_dirs(FLAGS.holdout_dir)[:FLAGS.window_size]
  FLAGS.validate_data_path = [
      os.path.join(FLAGS.holdout_dir, x, '*/*/*.zz') for x in src_dirs
  ]
  FLAGS.expand_validation_dirs = False
  FLAGS.use_tpu = (True if FLAGS.tpu_name else False)
  FLAGS.train_batch_size = FLAGS.validate_batch_size
  FLAGS.shuffle_buffer_size = 0
  FLAGS.shuffle_examples = False

  # architecture flags
  FLAGS.l2_strength = 0.0001
  FLAGS.conv_width = 64
  FLAGS.fc_width = 64
  FLAGS.trunk_layers = 6
  FLAGS.value_cost_weight = 0.5
  FLAGS.summary_steps = 128
  FLAGS.bool_features = 1
  FLAGS.input_features = 'mlperf07'
  FLAGS.input_layout = 'nchw'

  validate_file.main('unused')


def main(unused_argv):
  """Run the reinforcement learning loop."""
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  formatter = logging.Formatter('[%(asctime)s] %(message)s',
                                '%Y-%m-%d %H:%M:%S')

  # ML Perf Logging.

  mlp_log.mlperf_print('cache_clear', True)
  mlp_log.mlperf_print('init_start', None)

  mlp_log.mlperf_print(key='train_batch_size', value=FLAGS.training_batch_size)
  mlp_log.mlperf_print(key='filter_amount', value=FLAGS.filter_amount)
  mlp_log.mlperf_print(key='window_size', value=FLAGS.window_size)
  mlp_log.mlperf_print(
      key='lr_boundaries', value=str(FLAGS.lr_boundaries).strip('[]'))
  mlp_log.mlperf_print(key='lr_rates', value=str(FLAGS.lr_rates).strip('[]'))

  mlp_log.mlperf_print(key='opt_weight_decay', value=FLAGS.l2_strength)
  mlp_log.mlperf_print(
      key='min_selfplay_games_per_generation', value=FLAGS.mlperf_num_games)
  mlp_log.mlperf_print(key='train_samples', value=FLAGS.mlperf_num_games)
  mlp_log.mlperf_print(key='eval_samples', value=FLAGS.mlperf_num_games)
  mlp_log.mlperf_print(key='num_readouts', value=FLAGS.mlperf_num_readouts)
  mlp_log.mlperf_print(
      key='value_init_penalty', value=FLAGS.mlperf_value_init_penalty)
  mlp_log.mlperf_print(key='holdout_pct', value=FLAGS.mlperf_holdout_pct)
  mlp_log.mlperf_print(
      key='disable_resign_pct', value=FLAGS.mlperf_disable_resign_pct)
  mlp_log.mlperf_print(
      key='resign_threshold',
      value=(sum(FLAGS.mlperf_resign_threshold) /
             len(FLAGS.mlperf_resign_threshold)))
  mlp_log.mlperf_print(key='parallel_games', value=FLAGS.mlperf_parallel_games)
  mlp_log.mlperf_print(key='virtual_losses', value=FLAGS.mlperf_virtual_losses)
  mlp_log.mlperf_print(
      key='gating_win_rate', value=FLAGS.mlperf_gating_win_rate)
  mlp_log.mlperf_print(key='eval_games', value=FLAGS.mlperf_eval_games)

  for handler in logger.handlers:
    handler.setFormatter(formatter)

  # The training loop must be bootstrapped; either by running bootstrap.sh
  # to generate training data from random games, or by running
  # copy_checkpoint.sh to copy an already generated checkpoint.
  model_dirs = list_selfplay_dirs(FLAGS.selfplay_dir)

  iteration_model_names = []
  if not model_dirs:
    raise RuntimeError(
        'Couldn\'t find any selfplay games under %s. Either bootstrap.sh '
        'or init_from_checkpoint.sh must be run before the train loop is '
        'started')
  model_num = int(os.path.basename(model_dirs[0]))
  tpu_name = FLAGS.tpu_name.split(':')[0]
  session_config = tf.ConfigProto(
      allow_soft_placement=True, log_device_placement=True)
  timeout_run_options = tf.RunOptions(
      timeout_in_ms=FLAGS.worker_reset_timeout_ms)

  mlp_log.mlperf_print('init_stop', None)
  mlp_log.mlperf_print('run_start', None)
  with minigo_utils.logged_timer('Total time'):
    state = State(model_num)
    while state.iter_num < FLAGS.iterations:
      state.iter_num += 1
      iteration_model_names.append(state.train_model_name)
      mlp_log.mlperf_print(
          key='epoch_start', value=None, metadata={'epoch_num': state.iter_num})
      train_once(state)
      mlp_log.mlperf_print(
          key='epoch_stop', value=None, metadata={'epoch_num': state.iter_num})
      mlp_log.mlperf_print(
          key='save_model',
          value='{iteration_num: ' + str(state.iter_num) + ' }')

      # In the case where iterations are fast, TPUEstimator can deadlock
      # between iterations on TPU Init. We attempt to manually make sure
      # the worker can Init with deadlines so we don't get stuck.
      while True:
        try:
          tf.logging.info('Attempting to shutdown worker.')
          gc.collect()
          with tf.Graph().as_default():
            with tf.Session(tpu_name, config=session_config) as sess:
              sess.run(
                  tf.tpu.shutdown_system(job='tpu_worker'),
                  options=timeout_run_options)
          tf.logging.info('Attempting to initialize worker.')
          with tf.Graph().as_default():
            with tf.Session(tpu_name, config=session_config) as sess:
              init_result = sess.run(
                  tf.tpu.initialize_system(job='tpu_worker'),
                  options=timeout_run_options)
          if init_result:
            tf.logging.info('Worker reset.')
            break
        except tf.errors.DeadlineExceededError:
          pass
  with tf.gfile.GFile(FLAGS.abort_file_path, 'w') as f:
    f.write('abort')

  total_file_count = 0
  for iteration_model_name in iteration_model_names:
    total_file_count = total_file_count + len(
        tf.io.gfile.glob(FLAGS.selfplay_dir + '/' + iteration_model_name +
                         '/*/*/*'))

  mlp_log.mlperf_print(
      key='actual_selfplay_games_per_generation',
      value=int(total_file_count / len(iteration_model_names)))


if __name__ == '__main__':
  app.run(main)
