Beginning trial 1 of 10
:::MLLOG {"namespace": "", "time_ms": 1593141129300, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593141129347, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593141129347, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593141129347, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593141129347, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593141164209, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 03:12:44 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593141166862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141166862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141166862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141166864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141166869, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141166870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141166879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141166891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1620334342
:::MLLOG {"namespace": "", "time_ms": 1593141175433, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1620334342, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 436615894
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593141189159, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593141189162, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593141189162, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593141189162, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593141189162, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593141191166, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593141191166, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593141191166, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593141191490, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593141191491, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593141191491, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593141191492, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593141191492, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593141191492, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593141191493, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593141191493, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593141191493, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593141191493, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593141191493, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141191493, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3809747278
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.381 (0.381)	Data 2.75e-01 (2.75e-01)	Tok/s 64759 (64759)	Loss/tok 10.7044 (10.7044)	LR 2.942e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/1291]	Time 0.039 (0.124)	Data 1.09e-04 (2.85e-02)	Tok/s 203916 (178006)	Loss/tok 9.2116 (10.0630)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.066 (0.114)	Data 1.04e-04 (1.50e-02)	Tok/s 235550 (206221)	Loss/tok 9.1374 (9.7041)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.069 (0.106)	Data 1.08e-04 (1.21e-02)	Tok/s 221125 (214236)	Loss/tok 8.7949 (9.4810)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.067 (0.101)	Data 1.13e-04 (9.21e-03)	Tok/s 234740 (221622)	Loss/tok 8.5435 (9.3024)	LR 7.390e-05
0: TRAIN [0][50/1291]	Time 0.071 (0.096)	Data 1.14e-04 (7.43e-03)	Tok/s 214594 (225152)	Loss/tok 8.2821 (9.1521)	LR 9.303e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][60/1291]	Time 0.035 (0.094)	Data 1.18e-04 (6.23e-03)	Tok/s 228317 (227738)	Loss/tok 7.5973 (9.0109)	LR 1.119e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][70/1291]	Time 0.098 (0.093)	Data 1.07e-04 (5.37e-03)	Tok/s 256354 (230110)	Loss/tok 8.1829 (8.9178)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.066 (0.092)	Data 1.11e-04 (4.72e-03)	Tok/s 237261 (231780)	Loss/tok 7.9074 (8.8072)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.066 (0.092)	Data 1.13e-04 (4.21e-03)	Tok/s 235353 (233507)	Loss/tok 7.8031 (8.7100)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.066 (0.091)	Data 1.12e-04 (3.81e-03)	Tok/s 233262 (234391)	Loss/tok 7.8562 (8.6407)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.066 (0.089)	Data 1.29e-04 (3.47e-03)	Tok/s 235117 (234588)	Loss/tok 7.7124 (8.5790)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.098 (0.088)	Data 1.11e-04 (3.20e-03)	Tok/s 259354 (235562)	Loss/tok 8.0396 (8.5198)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.96e-03)	Tok/s 236313 (236572)	Loss/tok 7.5457 (8.4593)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.066 (0.089)	Data 1.08e-04 (2.76e-03)	Tok/s 229492 (237004)	Loss/tok 7.5993 (8.4063)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.098 (0.088)	Data 1.09e-04 (2.58e-03)	Tok/s 254699 (237436)	Loss/tok 7.6315 (8.3547)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.098 (0.089)	Data 1.09e-04 (2.43e-03)	Tok/s 258491 (238289)	Loss/tok 7.6236 (8.3020)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.30e-03)	Tok/s 235837 (238747)	Loss/tok 7.1845 (8.2460)	LR 1.376e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][180/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.17e-03)	Tok/s 235194 (238732)	Loss/tok 7.5629 (8.1940)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.066 (0.088)	Data 1.07e-04 (2.07e-03)	Tok/s 234268 (238779)	Loss/tok 6.8513 (8.1457)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.036 (0.088)	Data 1.07e-04 (1.97e-03)	Tok/s 222046 (238957)	Loss/tok 6.0511 (8.0903)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.066 (0.088)	Data 1.10e-04 (1.88e-03)	Tok/s 236449 (239335)	Loss/tok 6.6603 (8.0349)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.098 (0.088)	Data 1.12e-04 (1.80e-03)	Tok/s 259601 (239533)	Loss/tok 6.7452 (7.9734)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.098 (0.088)	Data 1.09e-04 (1.73e-03)	Tok/s 253804 (239686)	Loss/tok 6.5064 (7.9102)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.066 (0.088)	Data 1.11e-04 (1.66e-03)	Tok/s 236279 (239695)	Loss/tok 6.0537 (7.8523)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.098 (0.087)	Data 1.09e-04 (1.60e-03)	Tok/s 256871 (239660)	Loss/tok 6.1899 (7.7931)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.066 (0.087)	Data 1.09e-04 (1.54e-03)	Tok/s 235736 (239766)	Loss/tok 5.9748 (7.7315)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.173 (0.088)	Data 1.07e-04 (1.49e-03)	Tok/s 255065 (240214)	Loss/tok 6.2216 (7.6506)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.173 (0.089)	Data 1.09e-04 (1.44e-03)	Tok/s 254504 (240366)	Loss/tok 6.1358 (7.5749)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.035 (0.088)	Data 1.24e-04 (1.40e-03)	Tok/s 221542 (240362)	Loss/tok 4.6264 (7.5173)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.067 (0.088)	Data 1.28e-04 (1.35e-03)	Tok/s 230970 (240550)	Loss/tok 5.2752 (7.4539)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][310/1291]	Time 0.134 (0.088)	Data 1.06e-04 (1.31e-03)	Tok/s 257729 (240631)	Loss/tok 5.6701 (7.3917)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.067 (0.088)	Data 1.19e-04 (1.28e-03)	Tok/s 230073 (240750)	Loss/tok 5.0217 (7.3207)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.066 (0.089)	Data 1.14e-04 (1.24e-03)	Tok/s 235090 (240942)	Loss/tok 4.9867 (7.2536)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.066 (0.088)	Data 1.06e-04 (1.21e-03)	Tok/s 231689 (240830)	Loss/tok 4.8862 (7.2003)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.067 (0.088)	Data 1.07e-04 (1.18e-03)	Tok/s 232141 (240858)	Loss/tok 4.7104 (7.1384)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.098 (0.089)	Data 1.09e-04 (1.15e-03)	Tok/s 255753 (241223)	Loss/tok 4.9383 (7.0602)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.134 (0.089)	Data 1.09e-04 (1.12e-03)	Tok/s 259415 (241261)	Loss/tok 5.1623 (7.0011)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.099 (0.089)	Data 1.10e-04 (1.09e-03)	Tok/s 254628 (241435)	Loss/tok 4.6748 (6.9369)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.134 (0.089)	Data 1.12e-04 (1.07e-03)	Tok/s 261686 (241578)	Loss/tok 4.9870 (6.8717)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.173 (0.089)	Data 1.23e-04 (1.04e-03)	Tok/s 256087 (241521)	Loss/tok 5.0850 (6.8139)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.174 (0.090)	Data 1.11e-04 (1.02e-03)	Tok/s 254848 (241736)	Loss/tok 5.0075 (6.7465)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.100 (0.090)	Data 1.10e-04 (9.99e-04)	Tok/s 251911 (241896)	Loss/tok 4.4437 (6.6900)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.099 (0.090)	Data 1.09e-04 (9.79e-04)	Tok/s 252109 (241978)	Loss/tok 4.4596 (6.6367)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][440/1291]	Time 0.066 (0.090)	Data 1.09e-04 (9.59e-04)	Tok/s 230240 (241911)	Loss/tok 4.1862 (6.5896)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.099 (0.090)	Data 1.10e-04 (9.40e-04)	Tok/s 251858 (241842)	Loss/tok 4.3550 (6.5434)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.067 (0.090)	Data 1.09e-04 (9.22e-04)	Tok/s 231238 (241896)	Loss/tok 4.0581 (6.4965)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.099 (0.089)	Data 1.08e-04 (9.05e-04)	Tok/s 254003 (241755)	Loss/tok 4.3521 (6.4590)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.134 (0.089)	Data 1.10e-04 (8.89e-04)	Tok/s 261714 (241745)	Loss/tok 4.5078 (6.4132)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.098 (0.089)	Data 1.13e-04 (8.73e-04)	Tok/s 255196 (241719)	Loss/tok 4.5040 (6.3744)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.099 (0.088)	Data 1.11e-04 (8.58e-04)	Tok/s 254613 (241576)	Loss/tok 4.3522 (6.3405)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.099 (0.089)	Data 1.13e-04 (8.44e-04)	Tok/s 254827 (241624)	Loss/tok 4.1483 (6.2961)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.066 (0.089)	Data 1.11e-04 (8.30e-04)	Tok/s 229922 (241613)	Loss/tok 3.9969 (6.2572)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.099 (0.089)	Data 1.10e-04 (8.16e-04)	Tok/s 254299 (241734)	Loss/tok 4.1784 (6.2145)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.099 (0.088)	Data 1.06e-04 (8.03e-04)	Tok/s 253168 (241716)	Loss/tok 4.0938 (6.1792)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.066 (0.088)	Data 1.08e-04 (7.91e-04)	Tok/s 236324 (241691)	Loss/tok 3.8085 (6.1447)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.067 (0.088)	Data 1.08e-04 (7.79e-04)	Tok/s 231118 (241735)	Loss/tok 3.8898 (6.1063)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][570/1291]	Time 0.099 (0.088)	Data 1.09e-04 (7.67e-04)	Tok/s 253303 (241746)	Loss/tok 4.0755 (6.0706)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.099 (0.088)	Data 1.12e-04 (7.56e-04)	Tok/s 256861 (241762)	Loss/tok 3.9297 (6.0376)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.135 (0.088)	Data 1.11e-04 (7.45e-04)	Tok/s 260673 (241737)	Loss/tok 4.3209 (6.0032)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.100 (0.088)	Data 1.10e-04 (7.34e-04)	Tok/s 252323 (241695)	Loss/tok 4.0533 (5.9737)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.134 (0.088)	Data 1.10e-04 (7.24e-04)	Tok/s 258188 (241730)	Loss/tok 4.1595 (5.9401)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.134 (0.088)	Data 1.08e-04 (7.14e-04)	Tok/s 259899 (241685)	Loss/tok 4.3878 (5.9139)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.100 (0.088)	Data 1.10e-04 (7.05e-04)	Tok/s 255332 (241794)	Loss/tok 3.9319 (5.8793)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.173 (0.088)	Data 1.08e-04 (6.96e-04)	Tok/s 257572 (241770)	Loss/tok 4.3642 (5.8495)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.100 (0.088)	Data 1.12e-04 (6.87e-04)	Tok/s 252394 (241871)	Loss/tok 3.9699 (5.8167)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.067 (0.088)	Data 1.27e-04 (6.78e-04)	Tok/s 232054 (241877)	Loss/tok 3.7494 (5.7876)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.036 (0.088)	Data 1.13e-04 (6.70e-04)	Tok/s 220944 (241937)	Loss/tok 3.1050 (5.7579)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.134 (0.088)	Data 1.08e-04 (6.61e-04)	Tok/s 259286 (241938)	Loss/tok 4.1559 (5.7318)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.066 (0.088)	Data 1.10e-04 (6.53e-04)	Tok/s 233393 (241914)	Loss/tok 3.5998 (5.7076)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][700/1291]	Time 0.134 (0.088)	Data 1.12e-04 (6.46e-04)	Tok/s 260411 (242003)	Loss/tok 4.1022 (5.6776)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.135 (0.088)	Data 1.11e-04 (6.38e-04)	Tok/s 261618 (241975)	Loss/tok 4.1422 (5.6536)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.175 (0.088)	Data 1.34e-04 (6.31e-04)	Tok/s 253465 (242034)	Loss/tok 4.2712 (5.6252)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.067 (0.088)	Data 1.07e-04 (6.24e-04)	Tok/s 233610 (242010)	Loss/tok 3.6587 (5.6033)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.099 (0.088)	Data 1.09e-04 (6.17e-04)	Tok/s 253685 (242008)	Loss/tok 3.7956 (5.5811)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.067 (0.088)	Data 1.19e-04 (6.10e-04)	Tok/s 233918 (242068)	Loss/tok 3.5620 (5.5546)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.100 (0.088)	Data 1.11e-04 (6.04e-04)	Tok/s 252772 (242136)	Loss/tok 3.9614 (5.5307)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.067 (0.089)	Data 1.08e-04 (5.97e-04)	Tok/s 230208 (242203)	Loss/tok 3.5755 (5.5049)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.067 (0.089)	Data 1.25e-04 (5.91e-04)	Tok/s 238077 (242250)	Loss/tok 3.5113 (5.4819)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.036 (0.088)	Data 1.24e-04 (5.85e-04)	Tok/s 216097 (242149)	Loss/tok 3.0041 (5.4637)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.067 (0.088)	Data 1.30e-04 (5.79e-04)	Tok/s 230205 (242079)	Loss/tok 3.5371 (5.4458)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.067 (0.088)	Data 3.85e-04 (5.74e-04)	Tok/s 231113 (242060)	Loss/tok 3.4700 (5.4250)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][820/1291]	Time 0.066 (0.088)	Data 1.11e-04 (5.68e-04)	Tok/s 236759 (242005)	Loss/tok 3.5755 (5.4071)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.067 (0.088)	Data 1.09e-04 (5.63e-04)	Tok/s 234882 (241997)	Loss/tok 3.4571 (5.3885)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.100 (0.088)	Data 1.10e-04 (5.57e-04)	Tok/s 252667 (242056)	Loss/tok 3.7810 (5.3675)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.099 (0.088)	Data 1.10e-04 (5.52e-04)	Tok/s 252099 (242124)	Loss/tok 3.7791 (5.3463)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][860/1291]	Time 0.173 (0.089)	Data 1.11e-04 (5.47e-04)	Tok/s 257685 (242164)	Loss/tok 4.2371 (5.3254)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.135 (0.089)	Data 1.10e-04 (5.42e-04)	Tok/s 259897 (242249)	Loss/tok 3.9384 (5.3051)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.134 (0.089)	Data 1.09e-04 (5.37e-04)	Tok/s 261051 (242294)	Loss/tok 4.0362 (5.2857)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.100 (0.089)	Data 1.09e-04 (5.33e-04)	Tok/s 251264 (242270)	Loss/tok 3.9071 (5.2698)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.099 (0.089)	Data 1.09e-04 (5.28e-04)	Tok/s 256501 (242400)	Loss/tok 3.8456 (5.2497)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.100 (0.089)	Data 1.10e-04 (5.23e-04)	Tok/s 248055 (242482)	Loss/tok 3.8186 (5.2310)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.066 (0.089)	Data 1.09e-04 (5.19e-04)	Tok/s 233374 (242466)	Loss/tok 3.5391 (5.2153)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.099 (0.089)	Data 1.08e-04 (5.15e-04)	Tok/s 251829 (242448)	Loss/tok 3.6419 (5.1998)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.066 (0.089)	Data 1.31e-04 (5.10e-04)	Tok/s 234154 (242469)	Loss/tok 3.4441 (5.1829)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.175 (0.089)	Data 1.10e-04 (5.06e-04)	Tok/s 254165 (242551)	Loss/tok 4.0951 (5.1645)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.066 (0.089)	Data 1.09e-04 (5.02e-04)	Tok/s 232459 (242507)	Loss/tok 3.6170 (5.1504)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.099 (0.089)	Data 1.12e-04 (4.98e-04)	Tok/s 250270 (242509)	Loss/tok 3.7727 (5.1356)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.94e-04)	Tok/s 230780 (242482)	Loss/tok 3.3831 (5.1216)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][990/1291]	Time 0.099 (0.089)	Data 1.31e-04 (4.90e-04)	Tok/s 252995 (242489)	Loss/tok 3.6453 (5.1066)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.068 (0.089)	Data 1.07e-04 (4.87e-04)	Tok/s 230158 (242560)	Loss/tok 3.3942 (5.0895)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.101 (0.089)	Data 1.32e-04 (4.83e-04)	Tok/s 250002 (242552)	Loss/tok 3.6324 (5.0754)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.067 (0.090)	Data 1.18e-04 (4.79e-04)	Tok/s 234382 (242564)	Loss/tok 3.4890 (5.0611)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.066 (0.090)	Data 1.09e-04 (4.76e-04)	Tok/s 233649 (242584)	Loss/tok 3.5330 (5.0468)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.099 (0.090)	Data 1.11e-04 (4.72e-04)	Tok/s 256321 (242599)	Loss/tok 3.6574 (5.0330)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.066 (0.089)	Data 1.08e-04 (4.69e-04)	Tok/s 234421 (242564)	Loss/tok 3.4516 (5.0210)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1060/1291]	Time 0.174 (0.090)	Data 1.13e-04 (4.66e-04)	Tok/s 256858 (242580)	Loss/tok 4.0954 (5.0079)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.036 (0.090)	Data 1.11e-04 (4.62e-04)	Tok/s 222470 (242607)	Loss/tok 2.8607 (4.9948)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.098 (0.089)	Data 1.09e-04 (4.59e-04)	Tok/s 254592 (242536)	Loss/tok 3.7727 (4.9846)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.137 (0.089)	Data 1.10e-04 (4.56e-04)	Tok/s 254599 (242565)	Loss/tok 3.8451 (4.9716)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.099 (0.089)	Data 1.12e-04 (4.53e-04)	Tok/s 256102 (242580)	Loss/tok 3.6016 (4.9593)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.037 (0.089)	Data 1.08e-04 (4.50e-04)	Tok/s 211598 (242541)	Loss/tok 2.8573 (4.9480)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.036 (0.089)	Data 1.10e-04 (4.47e-04)	Tok/s 225597 (242510)	Loss/tok 3.0369 (4.9373)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.066 (0.089)	Data 1.12e-04 (4.44e-04)	Tok/s 234837 (242514)	Loss/tok 3.3552 (4.9248)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.135 (0.089)	Data 1.08e-04 (4.41e-04)	Tok/s 260627 (242571)	Loss/tok 3.8825 (4.9121)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.38e-04)	Tok/s 232092 (242553)	Loss/tok 3.3358 (4.9011)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.099 (0.089)	Data 1.07e-04 (4.35e-04)	Tok/s 254148 (242574)	Loss/tok 3.6480 (4.8895)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.099 (0.089)	Data 1.11e-04 (4.32e-04)	Tok/s 256585 (242556)	Loss/tok 3.5605 (4.8791)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.30e-04)	Tok/s 255538 (242578)	Loss/tok 3.5813 (4.8678)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1190/1291]	Time 0.099 (0.089)	Data 1.08e-04 (4.27e-04)	Tok/s 254682 (242598)	Loss/tok 3.6556 (4.8568)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.24e-04)	Tok/s 230590 (242606)	Loss/tok 3.4517 (4.8463)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.067 (0.089)	Data 1.08e-04 (4.22e-04)	Tok/s 230778 (242576)	Loss/tok 3.4001 (4.8366)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.067 (0.089)	Data 1.32e-04 (4.19e-04)	Tok/s 229600 (242507)	Loss/tok 3.3997 (4.8277)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.100 (0.089)	Data 1.10e-04 (4.17e-04)	Tok/s 251104 (242508)	Loss/tok 3.6478 (4.8175)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.14e-04)	Tok/s 253710 (242569)	Loss/tok 3.5535 (4.8061)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.067 (0.089)	Data 1.08e-04 (4.12e-04)	Tok/s 232793 (242587)	Loss/tok 3.1977 (4.7959)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.066 (0.089)	Data 1.20e-04 (4.10e-04)	Tok/s 227955 (242572)	Loss/tok 3.3458 (4.7868)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.100 (0.089)	Data 1.09e-04 (4.07e-04)	Tok/s 249351 (242531)	Loss/tok 3.5210 (4.7779)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.05e-04)	Tok/s 231716 (242540)	Loss/tok 3.3067 (4.7680)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.067 (0.089)	Data 4.20e-05 (4.06e-04)	Tok/s 235430 (242476)	Loss/tok 3.3069 (4.7599)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593141307067, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141307068, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.485 (0.485)	Decoder iters 149.0 (149.0)	Tok/s 32651 (32651)
0: Running moses detokenizer
0: BLEU(score=19.68056203233685, counts=[34113, 15488, 8223, 4488], totals=[64153, 61150, 58147, 55150], precisions=[53.174442348759996, 25.327882256745706, 14.141744200044714, 8.13780598368087], bp=0.9918807548739914, sys_len=64153, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593141309088, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1968, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141309088, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7598	Test BLEU: 19.68
0: Performance: Epoch: 0	Training: 1939176 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593141309088, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141309089, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141309089, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 4033951719
0: TRAIN [1][0/1291]	Time 0.294 (0.294)	Data 1.66e-01 (1.66e-01)	Tok/s 85211 (85211)	Loss/tok 3.4955 (3.4955)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.100 (0.106)	Data 1.16e-04 (1.52e-02)	Tok/s 254059 (227471)	Loss/tok 3.4971 (3.4456)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][20/1291]	Time 0.067 (0.094)	Data 1.10e-04 (8.03e-03)	Tok/s 232297 (234054)	Loss/tok 3.2002 (3.4212)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.068 (0.095)	Data 1.08e-04 (5.47e-03)	Tok/s 231460 (237888)	Loss/tok 3.2575 (3.4594)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.102 (0.089)	Data 1.11e-04 (4.17e-03)	Tok/s 250119 (236807)	Loss/tok 3.5224 (3.4341)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.100 (0.088)	Data 1.19e-04 (3.37e-03)	Tok/s 251987 (237983)	Loss/tok 3.4973 (3.4286)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.036 (0.086)	Data 1.04e-04 (2.84e-03)	Tok/s 221413 (237307)	Loss/tok 2.7574 (3.4331)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.066 (0.087)	Data 1.04e-04 (2.45e-03)	Tok/s 234853 (238832)	Loss/tok 3.2250 (3.4419)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.100 (0.087)	Data 1.09e-04 (2.17e-03)	Tok/s 251203 (239216)	Loss/tok 3.4693 (3.4443)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][90/1291]	Time 0.136 (0.090)	Data 1.08e-04 (1.94e-03)	Tok/s 258996 (240076)	Loss/tok 3.6826 (3.4782)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.100 (0.091)	Data 1.06e-04 (1.76e-03)	Tok/s 252780 (240545)	Loss/tok 3.5391 (3.4838)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.067 (0.092)	Data 1.06e-04 (1.61e-03)	Tok/s 226465 (241108)	Loss/tok 3.2795 (3.4935)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.067 (0.091)	Data 1.04e-04 (1.49e-03)	Tok/s 229465 (241264)	Loss/tok 3.2097 (3.4911)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.067 (0.091)	Data 1.12e-04 (1.38e-03)	Tok/s 233891 (241256)	Loss/tok 3.3416 (3.4948)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.134 (0.091)	Data 1.04e-04 (1.29e-03)	Tok/s 261006 (241064)	Loss/tok 3.7267 (3.4975)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.036 (0.092)	Data 1.05e-04 (1.21e-03)	Tok/s 220993 (241545)	Loss/tok 2.7963 (3.5023)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.099 (0.091)	Data 1.05e-04 (1.14e-03)	Tok/s 255382 (241641)	Loss/tok 3.4534 (3.4989)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.067 (0.091)	Data 1.10e-04 (1.08e-03)	Tok/s 228021 (241600)	Loss/tok 3.3359 (3.4957)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.066 (0.091)	Data 1.03e-04 (1.03e-03)	Tok/s 233210 (241652)	Loss/tok 3.2242 (3.4991)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.067 (0.091)	Data 1.04e-04 (9.80e-04)	Tok/s 230305 (241697)	Loss/tok 3.2354 (3.4991)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.100 (0.091)	Data 1.02e-04 (9.37e-04)	Tok/s 252002 (242068)	Loss/tok 3.3941 (3.4964)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.067 (0.091)	Data 1.08e-04 (8.97e-04)	Tok/s 228871 (242026)	Loss/tok 3.2594 (3.4927)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][220/1291]	Time 0.100 (0.091)	Data 1.02e-04 (8.62e-04)	Tok/s 253974 (242125)	Loss/tok 3.4787 (3.4930)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.100 (0.091)	Data 1.07e-04 (8.29e-04)	Tok/s 249884 (242158)	Loss/tok 3.5011 (3.4891)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][240/1291]	Time 0.067 (0.091)	Data 1.02e-04 (7.99e-04)	Tok/s 232923 (242177)	Loss/tok 3.2251 (3.4918)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.067 (0.090)	Data 1.05e-04 (7.72e-04)	Tok/s 228626 (241959)	Loss/tok 3.1899 (3.4873)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.134 (0.090)	Data 1.03e-04 (7.46e-04)	Tok/s 259517 (241936)	Loss/tok 3.7764 (3.4849)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.067 (0.089)	Data 1.05e-04 (7.23e-04)	Tok/s 229783 (241792)	Loss/tok 3.2817 (3.4799)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.099 (0.089)	Data 1.04e-04 (7.01e-04)	Tok/s 254494 (241837)	Loss/tok 3.4691 (3.4788)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.066 (0.089)	Data 1.07e-04 (6.80e-04)	Tok/s 233761 (241747)	Loss/tok 3.2564 (3.4768)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.067 (0.089)	Data 1.06e-04 (6.62e-04)	Tok/s 231189 (241741)	Loss/tok 3.3200 (3.4783)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.067 (0.089)	Data 5.13e-04 (6.45e-04)	Tok/s 231341 (241852)	Loss/tok 3.2253 (3.4775)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.066 (0.089)	Data 1.07e-04 (6.28e-04)	Tok/s 230254 (241763)	Loss/tok 3.3167 (3.4753)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.067 (0.089)	Data 1.08e-04 (6.12e-04)	Tok/s 228303 (241599)	Loss/tok 3.2140 (3.4739)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.036 (0.088)	Data 1.05e-04 (5.98e-04)	Tok/s 214210 (241420)	Loss/tok 2.7732 (3.4698)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.067 (0.088)	Data 1.18e-04 (5.84e-04)	Tok/s 231685 (241400)	Loss/tok 3.2200 (3.4682)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.067 (0.088)	Data 1.06e-04 (5.71e-04)	Tok/s 232030 (241314)	Loss/tok 3.0782 (3.4675)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][370/1291]	Time 0.099 (0.088)	Data 1.03e-04 (5.58e-04)	Tok/s 256280 (241449)	Loss/tok 3.4401 (3.4687)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.135 (0.088)	Data 1.05e-04 (5.46e-04)	Tok/s 259971 (241526)	Loss/tok 3.6487 (3.4687)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.067 (0.088)	Data 1.05e-04 (5.35e-04)	Tok/s 231145 (241518)	Loss/tok 3.3328 (3.4694)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.066 (0.088)	Data 1.04e-04 (5.24e-04)	Tok/s 229973 (241564)	Loss/tok 3.2581 (3.4692)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.099 (0.088)	Data 1.06e-04 (5.14e-04)	Tok/s 252182 (241522)	Loss/tok 3.3566 (3.4664)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.036 (0.088)	Data 1.04e-04 (5.04e-04)	Tok/s 219590 (241541)	Loss/tok 2.7635 (3.4654)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][430/1291]	Time 0.067 (0.088)	Data 1.03e-04 (4.95e-04)	Tok/s 230569 (241569)	Loss/tok 3.2401 (3.4681)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.134 (0.088)	Data 1.08e-04 (4.86e-04)	Tok/s 259675 (241626)	Loss/tok 3.6150 (3.4679)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.067 (0.088)	Data 1.03e-04 (4.78e-04)	Tok/s 228999 (241545)	Loss/tok 3.3171 (3.4660)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.099 (0.088)	Data 1.06e-04 (4.70e-04)	Tok/s 253521 (241502)	Loss/tok 3.4260 (3.4651)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.136 (0.088)	Data 1.04e-04 (4.62e-04)	Tok/s 259403 (241410)	Loss/tok 3.5440 (3.4645)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.067 (0.088)	Data 1.10e-04 (4.55e-04)	Tok/s 227833 (241375)	Loss/tok 3.2625 (3.4641)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.136 (0.088)	Data 1.04e-04 (4.48e-04)	Tok/s 259877 (241402)	Loss/tok 3.5763 (3.4634)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.067 (0.088)	Data 1.05e-04 (4.41e-04)	Tok/s 234617 (241269)	Loss/tok 3.1625 (3.4619)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.067 (0.088)	Data 1.05e-04 (4.34e-04)	Tok/s 233373 (241323)	Loss/tok 3.1609 (3.4604)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.099 (0.087)	Data 1.07e-04 (4.28e-04)	Tok/s 254591 (241348)	Loss/tok 3.4877 (3.4585)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.136 (0.088)	Data 1.07e-04 (4.22e-04)	Tok/s 256876 (241529)	Loss/tok 3.6128 (3.4606)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.067 (0.088)	Data 1.03e-04 (4.16e-04)	Tok/s 229580 (241473)	Loss/tok 3.2841 (3.4616)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.100 (0.088)	Data 1.07e-04 (4.11e-04)	Tok/s 253613 (241477)	Loss/tok 3.4610 (3.4613)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][560/1291]	Time 0.134 (0.088)	Data 1.05e-04 (4.05e-04)	Tok/s 264924 (241584)	Loss/tok 3.6629 (3.4625)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][570/1291]	Time 0.173 (0.089)	Data 1.07e-04 (4.00e-04)	Tok/s 257826 (241717)	Loss/tok 3.8087 (3.4644)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.036 (0.089)	Data 1.26e-04 (3.95e-04)	Tok/s 218578 (241704)	Loss/tok 2.7947 (3.4628)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.100 (0.088)	Data 1.06e-04 (3.90e-04)	Tok/s 252926 (241666)	Loss/tok 3.4588 (3.4607)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.099 (0.088)	Data 1.07e-04 (3.86e-04)	Tok/s 256280 (241679)	Loss/tok 3.4601 (3.4630)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.099 (0.088)	Data 1.08e-04 (3.81e-04)	Tok/s 253360 (241695)	Loss/tok 3.4014 (3.4615)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.100 (0.088)	Data 1.10e-04 (3.77e-04)	Tok/s 253076 (241664)	Loss/tok 3.4562 (3.4603)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.099 (0.088)	Data 1.24e-04 (3.72e-04)	Tok/s 254331 (241760)	Loss/tok 3.3340 (3.4601)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.067 (0.088)	Data 1.04e-04 (3.68e-04)	Tok/s 228888 (241704)	Loss/tok 3.1833 (3.4594)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.035 (0.088)	Data 1.05e-04 (3.64e-04)	Tok/s 222053 (241679)	Loss/tok 2.8113 (3.4573)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.100 (0.088)	Data 1.04e-04 (3.60e-04)	Tok/s 252373 (241735)	Loss/tok 3.3332 (3.4565)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.099 (0.088)	Data 1.07e-04 (3.57e-04)	Tok/s 252911 (241819)	Loss/tok 3.5053 (3.4559)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.067 (0.088)	Data 1.37e-04 (3.53e-04)	Tok/s 232869 (241845)	Loss/tok 3.1867 (3.4555)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.134 (0.088)	Data 1.06e-04 (3.49e-04)	Tok/s 260912 (241879)	Loss/tok 3.6042 (3.4553)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][700/1291]	Time 0.099 (0.088)	Data 1.03e-04 (3.46e-04)	Tok/s 252409 (241804)	Loss/tok 3.3674 (3.4539)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.100 (0.088)	Data 1.04e-04 (3.43e-04)	Tok/s 253303 (241898)	Loss/tok 3.4516 (3.4536)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.100 (0.088)	Data 1.06e-04 (3.40e-04)	Tok/s 251872 (241853)	Loss/tok 3.4231 (3.4528)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][730/1291]	Time 0.134 (0.088)	Data 1.02e-04 (3.36e-04)	Tok/s 262651 (241806)	Loss/tok 3.6747 (3.4524)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.067 (0.088)	Data 1.17e-04 (3.33e-04)	Tok/s 227051 (241794)	Loss/tok 3.2002 (3.4510)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.099 (0.088)	Data 1.06e-04 (3.30e-04)	Tok/s 254947 (241877)	Loss/tok 3.3986 (3.4503)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.036 (0.088)	Data 1.07e-04 (3.27e-04)	Tok/s 220768 (241885)	Loss/tok 2.6957 (3.4502)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.036 (0.088)	Data 1.14e-04 (3.24e-04)	Tok/s 215321 (241781)	Loss/tok 2.7638 (3.4488)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.135 (0.088)	Data 1.05e-04 (3.22e-04)	Tok/s 257756 (241802)	Loss/tok 3.6309 (3.4482)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.135 (0.088)	Data 1.09e-04 (3.19e-04)	Tok/s 257847 (241848)	Loss/tok 3.5918 (3.4484)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.17e-04)	Tok/s 231014 (241882)	Loss/tok 3.2724 (3.4472)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.067 (0.088)	Data 1.02e-04 (3.14e-04)	Tok/s 230758 (241803)	Loss/tok 3.1388 (3.4457)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.067 (0.088)	Data 1.18e-04 (3.11e-04)	Tok/s 231128 (241741)	Loss/tok 3.2324 (3.4450)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.067 (0.088)	Data 1.04e-04 (3.09e-04)	Tok/s 227838 (241736)	Loss/tok 3.1813 (3.4437)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.100 (0.088)	Data 1.07e-04 (3.07e-04)	Tok/s 251067 (241757)	Loss/tok 3.3869 (3.4432)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.099 (0.088)	Data 1.29e-04 (3.04e-04)	Tok/s 252561 (241820)	Loss/tok 3.3988 (3.4430)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][860/1291]	Time 0.100 (0.088)	Data 1.04e-04 (3.02e-04)	Tok/s 249428 (241840)	Loss/tok 3.4505 (3.4429)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.067 (0.088)	Data 1.05e-04 (3.00e-04)	Tok/s 230604 (241858)	Loss/tok 3.1646 (3.4427)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.067 (0.088)	Data 1.04e-04 (2.98e-04)	Tok/s 237436 (241886)	Loss/tok 3.1418 (3.4424)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.067 (0.088)	Data 1.05e-04 (2.96e-04)	Tok/s 231052 (241835)	Loss/tok 3.0539 (3.4413)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.100 (0.088)	Data 1.05e-04 (2.94e-04)	Tok/s 249640 (241874)	Loss/tok 3.4198 (3.4411)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.173 (0.088)	Data 1.05e-04 (2.92e-04)	Tok/s 258763 (241927)	Loss/tok 3.8324 (3.4412)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.135 (0.088)	Data 1.03e-04 (2.90e-04)	Tok/s 258224 (241925)	Loss/tok 3.6180 (3.4410)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.100 (0.088)	Data 1.12e-04 (2.88e-04)	Tok/s 251818 (241946)	Loss/tok 3.3982 (3.4400)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.067 (0.088)	Data 1.03e-04 (2.86e-04)	Tok/s 230770 (241957)	Loss/tok 3.2152 (3.4392)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.135 (0.088)	Data 1.08e-04 (2.84e-04)	Tok/s 259342 (242032)	Loss/tok 3.5256 (3.4391)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.067 (0.088)	Data 1.06e-04 (2.82e-04)	Tok/s 231336 (242006)	Loss/tok 3.2644 (3.4379)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.100 (0.088)	Data 1.14e-04 (2.80e-04)	Tok/s 253068 (242002)	Loss/tok 3.3280 (3.4365)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.138 (0.089)	Data 1.06e-04 (2.79e-04)	Tok/s 254578 (242075)	Loss/tok 3.5921 (3.4390)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][990/1291]	Time 0.136 (0.089)	Data 1.07e-04 (2.77e-04)	Tok/s 257276 (242092)	Loss/tok 3.5712 (3.4388)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.135 (0.089)	Data 1.17e-04 (2.75e-04)	Tok/s 258873 (242107)	Loss/tok 3.6511 (3.4382)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.100 (0.089)	Data 1.05e-04 (2.73e-04)	Tok/s 249733 (242081)	Loss/tok 3.4406 (3.4377)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1020/1291]	Time 0.136 (0.089)	Data 1.18e-04 (2.72e-04)	Tok/s 257823 (242069)	Loss/tok 3.6095 (3.4376)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.135 (0.089)	Data 1.03e-04 (2.70e-04)	Tok/s 258487 (242049)	Loss/tok 3.5662 (3.4365)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.101 (0.089)	Data 1.26e-04 (2.69e-04)	Tok/s 250193 (242046)	Loss/tok 3.4770 (3.4357)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.067 (0.089)	Data 1.27e-04 (2.67e-04)	Tok/s 231245 (242030)	Loss/tok 3.2978 (3.4352)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.66e-04)	Tok/s 234070 (242100)	Loss/tok 3.1701 (3.4366)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.067 (0.089)	Data 1.05e-04 (2.64e-04)	Tok/s 229293 (242126)	Loss/tok 3.1769 (3.4372)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.099 (0.089)	Data 1.05e-04 (2.63e-04)	Tok/s 255777 (242093)	Loss/tok 3.4156 (3.4364)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.135 (0.089)	Data 1.05e-04 (2.61e-04)	Tok/s 256530 (242046)	Loss/tok 3.6025 (3.4353)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.100 (0.089)	Data 1.05e-04 (2.60e-04)	Tok/s 251116 (242065)	Loss/tok 3.4849 (3.4359)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.59e-04)	Tok/s 232131 (242049)	Loss/tok 3.1672 (3.4359)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1120/1291]	Time 0.100 (0.089)	Data 1.24e-04 (2.57e-04)	Tok/s 250635 (242089)	Loss/tok 3.3093 (3.4358)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.067 (0.089)	Data 1.02e-04 (2.56e-04)	Tok/s 229912 (242012)	Loss/tok 3.1712 (3.4342)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.134 (0.089)	Data 1.08e-04 (2.55e-04)	Tok/s 262183 (242004)	Loss/tok 3.5824 (3.4345)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.067 (0.089)	Data 1.25e-04 (2.53e-04)	Tok/s 229828 (241988)	Loss/tok 3.2055 (3.4341)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.134 (0.089)	Data 1.08e-04 (2.52e-04)	Tok/s 260933 (242011)	Loss/tok 3.5099 (3.4345)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.067 (0.089)	Data 1.04e-04 (2.51e-04)	Tok/s 234332 (242005)	Loss/tok 3.2409 (3.4340)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.067 (0.089)	Data 1.03e-04 (2.50e-04)	Tok/s 230175 (242008)	Loss/tok 3.1668 (3.4333)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.49e-04)	Tok/s 237281 (242029)	Loss/tok 3.3218 (3.4331)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.136 (0.089)	Data 1.40e-04 (2.47e-04)	Tok/s 258243 (242051)	Loss/tok 3.6118 (3.4330)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.101 (0.089)	Data 1.03e-04 (2.46e-04)	Tok/s 249605 (242053)	Loss/tok 3.4069 (3.4322)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.066 (0.089)	Data 1.04e-04 (2.45e-04)	Tok/s 237155 (242024)	Loss/tok 3.0994 (3.4313)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.067 (0.089)	Data 1.06e-04 (2.44e-04)	Tok/s 227944 (241982)	Loss/tok 3.1927 (3.4301)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.099 (0.089)	Data 1.04e-04 (2.43e-04)	Tok/s 254140 (242026)	Loss/tok 3.2904 (3.4302)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1250/1291]	Time 0.067 (0.089)	Data 1.02e-04 (2.42e-04)	Tok/s 224285 (241953)	Loss/tok 3.1449 (3.4287)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.175 (0.089)	Data 1.13e-04 (2.41e-04)	Tok/s 255188 (241922)	Loss/tok 3.7070 (3.4282)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.100 (0.089)	Data 1.06e-04 (2.40e-04)	Tok/s 248077 (241957)	Loss/tok 3.3404 (3.4282)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.067 (0.089)	Data 1.23e-04 (2.39e-04)	Tok/s 229346 (241941)	Loss/tok 3.1807 (3.4274)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1290/1291]	Time 0.067 (0.089)	Data 4.46e-05 (2.41e-04)	Tok/s 232446 (241965)	Loss/tok 3.1780 (3.4273)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593141424781, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593141424781, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.403 (0.403)	Decoder iters 110.0 (110.0)	Tok/s 40347 (40347)
0: Running moses detokenizer
0: BLEU(score=21.904420792419423, counts=[35857, 17250, 9447, 5359], totals=[65326, 62323, 59320, 56322], precisions=[54.88932431191256, 27.678385186849155, 15.925488873904248, 9.514931998153475], bp=1.0, sys_len=65326, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593141426741, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21899999999999997, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593141426742, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4262	Test BLEU: 21.90
0: Performance: Epoch: 1	Training: 1935360 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593141426742, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593141426742, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141426742, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2333176532
0: TRAIN [2][0/1291]	Time 0.337 (0.337)	Data 1.92e-01 (1.92e-01)	Tok/s 104018 (104018)	Loss/tok 3.4400 (3.4400)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.067 (0.103)	Data 1.31e-04 (1.75e-02)	Tok/s 229409 (227205)	Loss/tok 3.0326 (3.2278)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.067 (0.094)	Data 1.18e-04 (9.25e-03)	Tok/s 228888 (233832)	Loss/tok 3.0746 (3.2331)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.135 (0.094)	Data 1.31e-04 (6.30e-03)	Tok/s 259010 (236757)	Loss/tok 3.4550 (3.2801)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.136 (0.091)	Data 1.13e-04 (4.79e-03)	Tok/s 256121 (237425)	Loss/tok 3.3900 (3.2606)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.067 (0.088)	Data 1.15e-04 (3.88e-03)	Tok/s 232818 (237108)	Loss/tok 3.1117 (3.2504)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.138 (0.088)	Data 1.37e-04 (3.26e-03)	Tok/s 255517 (237613)	Loss/tok 3.4358 (3.2543)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.100 (0.090)	Data 1.16e-04 (2.82e-03)	Tok/s 251088 (238729)	Loss/tok 3.2569 (3.2614)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.067 (0.090)	Data 1.14e-04 (2.49e-03)	Tok/s 226979 (239221)	Loss/tok 3.0283 (3.2665)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.23e-03)	Tok/s 229687 (239517)	Loss/tok 3.0281 (3.2651)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.135 (0.091)	Data 1.27e-04 (2.02e-03)	Tok/s 258716 (240562)	Loss/tok 3.3533 (3.2680)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.099 (0.091)	Data 1.10e-04 (1.85e-03)	Tok/s 259574 (241075)	Loss/tok 3.2313 (3.2651)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.067 (0.090)	Data 1.11e-04 (1.70e-03)	Tok/s 228714 (240803)	Loss/tok 3.1073 (3.2576)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][130/1291]	Time 0.174 (0.091)	Data 1.17e-04 (1.58e-03)	Tok/s 257842 (241075)	Loss/tok 3.5701 (3.2699)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.067 (0.091)	Data 1.15e-04 (1.48e-03)	Tok/s 232059 (241476)	Loss/tok 3.0739 (3.2712)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.135 (0.091)	Data 1.13e-04 (1.39e-03)	Tok/s 260155 (241748)	Loss/tok 3.4344 (3.2733)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][160/1291]	Time 0.067 (0.091)	Data 1.14e-04 (1.31e-03)	Tok/s 226804 (241542)	Loss/tok 3.1007 (3.2762)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.100 (0.092)	Data 1.11e-04 (1.24e-03)	Tok/s 254170 (241826)	Loss/tok 3.3753 (3.2824)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.067 (0.091)	Data 1.26e-04 (1.18e-03)	Tok/s 230775 (241492)	Loss/tok 3.0703 (3.2773)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.100 (0.091)	Data 1.14e-04 (1.12e-03)	Tok/s 252796 (241592)	Loss/tok 3.2538 (3.2817)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.067 (0.090)	Data 1.17e-04 (1.07e-03)	Tok/s 233803 (241252)	Loss/tok 3.1403 (3.2768)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.099 (0.090)	Data 1.15e-04 (1.03e-03)	Tok/s 253559 (241254)	Loss/tok 3.3017 (3.2762)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.135 (0.089)	Data 1.15e-04 (9.86e-04)	Tok/s 261418 (241174)	Loss/tok 3.4762 (3.2773)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.067 (0.090)	Data 1.15e-04 (9.49e-04)	Tok/s 232906 (241487)	Loss/tok 3.1557 (3.2808)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.100 (0.090)	Data 1.13e-04 (9.14e-04)	Tok/s 252209 (241717)	Loss/tok 3.2389 (3.2818)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.035 (0.089)	Data 1.14e-04 (8.82e-04)	Tok/s 223835 (241258)	Loss/tok 2.6840 (3.2755)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.067 (0.090)	Data 1.27e-04 (8.53e-04)	Tok/s 227864 (241473)	Loss/tok 3.0955 (3.2839)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.100 (0.089)	Data 1.14e-04 (8.26e-04)	Tok/s 250475 (241160)	Loss/tok 3.3274 (3.2793)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.067 (0.089)	Data 1.33e-04 (8.01e-04)	Tok/s 226881 (241397)	Loss/tok 3.0549 (3.2839)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][290/1291]	Time 0.067 (0.090)	Data 1.14e-04 (7.77e-04)	Tok/s 233643 (241512)	Loss/tok 3.0787 (3.2888)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.174 (0.090)	Data 1.13e-04 (7.55e-04)	Tok/s 258571 (241652)	Loss/tok 3.6325 (3.2884)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.135 (0.090)	Data 1.36e-04 (7.35e-04)	Tok/s 259498 (241730)	Loss/tok 3.4882 (3.2888)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.067 (0.090)	Data 1.13e-04 (7.16e-04)	Tok/s 231243 (241738)	Loss/tok 3.0638 (3.2864)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.067 (0.089)	Data 1.17e-04 (6.97e-04)	Tok/s 233024 (241633)	Loss/tok 2.9399 (3.2833)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.037 (0.089)	Data 1.15e-04 (6.80e-04)	Tok/s 217656 (241659)	Loss/tok 2.5442 (3.2834)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.135 (0.090)	Data 1.27e-04 (6.64e-04)	Tok/s 256461 (241766)	Loss/tok 3.5755 (3.2872)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.066 (0.090)	Data 1.17e-04 (6.49e-04)	Tok/s 233772 (241824)	Loss/tok 3.1120 (3.2869)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.135 (0.090)	Data 1.12e-04 (6.35e-04)	Tok/s 259547 (241995)	Loss/tok 3.4526 (3.2938)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.067 (0.090)	Data 1.14e-04 (6.21e-04)	Tok/s 234059 (241977)	Loss/tok 3.0637 (3.2929)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.099 (0.090)	Data 1.15e-04 (6.08e-04)	Tok/s 252788 (242036)	Loss/tok 3.2762 (3.2941)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.099 (0.091)	Data 1.13e-04 (5.96e-04)	Tok/s 255826 (242084)	Loss/tok 3.2532 (3.2957)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.100 (0.091)	Data 1.13e-04 (5.84e-04)	Tok/s 252834 (242165)	Loss/tok 3.3117 (3.2961)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][420/1291]	Time 0.067 (0.090)	Data 1.15e-04 (5.73e-04)	Tok/s 233347 (242022)	Loss/tok 3.0689 (3.2932)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.100 (0.090)	Data 1.15e-04 (5.63e-04)	Tok/s 255262 (242035)	Loss/tok 3.2232 (3.2929)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.135 (0.090)	Data 1.15e-04 (5.53e-04)	Tok/s 256848 (242113)	Loss/tok 3.5931 (3.2927)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.135 (0.090)	Data 1.13e-04 (5.43e-04)	Tok/s 258742 (241981)	Loss/tok 3.4581 (3.2918)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.099 (0.090)	Data 1.18e-04 (5.34e-04)	Tok/s 253675 (241949)	Loss/tok 3.3124 (3.2899)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.100 (0.090)	Data 1.16e-04 (5.25e-04)	Tok/s 251177 (242010)	Loss/tok 3.3469 (3.2912)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.069 (0.090)	Data 1.14e-04 (5.17e-04)	Tok/s 221922 (242001)	Loss/tok 3.0785 (3.2914)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.066 (0.090)	Data 1.12e-04 (5.08e-04)	Tok/s 232529 (242005)	Loss/tok 3.0868 (3.2903)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.036 (0.090)	Data 1.13e-04 (5.01e-04)	Tok/s 215252 (242016)	Loss/tok 2.6154 (3.2883)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.135 (0.090)	Data 1.16e-04 (4.93e-04)	Tok/s 257148 (241975)	Loss/tok 3.4480 (3.2871)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.066 (0.089)	Data 1.24e-04 (4.86e-04)	Tok/s 233680 (241881)	Loss/tok 3.1094 (3.2860)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.067 (0.089)	Data 1.17e-04 (4.79e-04)	Tok/s 234823 (241950)	Loss/tok 3.0338 (3.2865)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][540/1291]	Time 0.036 (0.090)	Data 1.15e-04 (4.72e-04)	Tok/s 225813 (242056)	Loss/tok 2.6306 (3.2897)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.100 (0.090)	Data 1.33e-04 (4.66e-04)	Tok/s 255516 (242003)	Loss/tok 3.3724 (3.2881)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.099 (0.090)	Data 1.16e-04 (4.60e-04)	Tok/s 251126 (242051)	Loss/tok 3.2505 (3.2877)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.100 (0.089)	Data 1.14e-04 (4.54e-04)	Tok/s 253876 (241984)	Loss/tok 3.2380 (3.2862)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][580/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.48e-04)	Tok/s 228538 (242026)	Loss/tok 3.1355 (3.2865)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][590/1291]	Time 0.099 (0.090)	Data 1.12e-04 (4.42e-04)	Tok/s 255780 (242066)	Loss/tok 3.2403 (3.2881)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.099 (0.090)	Data 1.16e-04 (4.37e-04)	Tok/s 252853 (242118)	Loss/tok 3.4040 (3.2892)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.067 (0.090)	Data 1.32e-04 (4.32e-04)	Tok/s 231572 (242063)	Loss/tok 3.1071 (3.2878)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.099 (0.090)	Data 1.37e-04 (4.27e-04)	Tok/s 253692 (242125)	Loss/tok 3.3171 (3.2898)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.067 (0.090)	Data 1.20e-04 (4.22e-04)	Tok/s 232649 (242073)	Loss/tok 3.0348 (3.2891)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.135 (0.090)	Data 1.19e-04 (4.17e-04)	Tok/s 257063 (242104)	Loss/tok 3.4802 (3.2895)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.102 (0.090)	Data 1.14e-04 (4.12e-04)	Tok/s 247041 (242135)	Loss/tok 3.2599 (3.2895)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.100 (0.090)	Data 1.13e-04 (4.08e-04)	Tok/s 254824 (242055)	Loss/tok 3.2750 (3.2885)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.035 (0.090)	Data 1.35e-04 (4.03e-04)	Tok/s 228221 (242089)	Loss/tok 2.6863 (3.2889)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.136 (0.090)	Data 1.15e-04 (3.99e-04)	Tok/s 261652 (242116)	Loss/tok 3.3224 (3.2877)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.95e-04)	Tok/s 231943 (242061)	Loss/tok 3.1354 (3.2870)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.035 (0.089)	Data 1.19e-04 (3.91e-04)	Tok/s 221413 (241951)	Loss/tok 2.6937 (3.2853)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][710/1291]	Time 0.100 (0.089)	Data 1.22e-04 (3.87e-04)	Tok/s 248191 (241943)	Loss/tok 3.3454 (3.2849)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.135 (0.090)	Data 1.17e-04 (3.84e-04)	Tok/s 259604 (241961)	Loss/tok 3.4331 (3.2851)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.035 (0.089)	Data 1.13e-04 (3.80e-04)	Tok/s 224364 (241908)	Loss/tok 2.6401 (3.2844)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.100 (0.089)	Data 1.18e-04 (3.76e-04)	Tok/s 253816 (241898)	Loss/tok 3.2038 (3.2832)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.73e-04)	Tok/s 252650 (241928)	Loss/tok 3.2864 (3.2845)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.099 (0.090)	Data 1.35e-04 (3.70e-04)	Tok/s 252672 (241991)	Loss/tok 3.2689 (3.2875)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.066 (0.090)	Data 1.15e-04 (3.66e-04)	Tok/s 232571 (242006)	Loss/tok 3.0404 (3.2883)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.100 (0.090)	Data 1.17e-04 (3.63e-04)	Tok/s 251440 (242089)	Loss/tok 3.4009 (3.2896)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.60e-04)	Tok/s 230501 (242035)	Loss/tok 3.1387 (3.2895)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.067 (0.090)	Data 1.34e-04 (3.57e-04)	Tok/s 232842 (242007)	Loss/tok 3.0224 (3.2886)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.54e-04)	Tok/s 235489 (241948)	Loss/tok 3.0312 (3.2876)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.51e-04)	Tok/s 233072 (241905)	Loss/tok 3.1857 (3.2868)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.135 (0.090)	Data 1.11e-04 (3.48e-04)	Tok/s 259414 (241961)	Loss/tok 3.4020 (3.2868)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][840/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.46e-04)	Tok/s 230884 (241945)	Loss/tok 3.2259 (3.2863)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.43e-04)	Tok/s 229063 (241917)	Loss/tok 3.0587 (3.2854)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.40e-04)	Tok/s 231878 (241960)	Loss/tok 3.0894 (3.2864)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][870/1291]	Time 0.099 (0.090)	Data 1.15e-04 (3.38e-04)	Tok/s 254034 (241926)	Loss/tok 3.3435 (3.2871)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.134 (0.090)	Data 1.19e-04 (3.35e-04)	Tok/s 260732 (241950)	Loss/tok 3.3830 (3.2866)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][890/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.33e-04)	Tok/s 230771 (241984)	Loss/tok 2.9460 (3.2876)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.135 (0.090)	Data 1.12e-04 (3.30e-04)	Tok/s 257613 (241961)	Loss/tok 3.4963 (3.2876)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.28e-04)	Tok/s 233075 (242009)	Loss/tok 3.0932 (3.2879)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.26e-04)	Tok/s 228933 (241977)	Loss/tok 3.0056 (3.2886)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.173 (0.090)	Data 1.37e-04 (3.24e-04)	Tok/s 258248 (242032)	Loss/tok 3.6350 (3.2903)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.100 (0.090)	Data 1.11e-04 (3.21e-04)	Tok/s 253455 (241971)	Loss/tok 3.3007 (3.2892)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.100 (0.090)	Data 1.37e-04 (3.19e-04)	Tok/s 252327 (241976)	Loss/tok 3.3650 (3.2890)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.101 (0.090)	Data 1.21e-04 (3.17e-04)	Tok/s 247468 (241986)	Loss/tok 3.4321 (3.2894)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.135 (0.090)	Data 1.20e-04 (3.15e-04)	Tok/s 262199 (242011)	Loss/tok 3.5052 (3.2901)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.13e-04)	Tok/s 230594 (242014)	Loss/tok 3.0779 (3.2895)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.036 (0.090)	Data 1.33e-04 (3.11e-04)	Tok/s 223508 (242005)	Loss/tok 2.6978 (3.2894)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.09e-04)	Tok/s 231856 (241962)	Loss/tok 3.0182 (3.2889)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.134 (0.090)	Data 1.12e-04 (3.07e-04)	Tok/s 258381 (242010)	Loss/tok 3.5143 (3.2897)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1020/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.06e-04)	Tok/s 234378 (241974)	Loss/tok 3.0932 (3.2889)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.04e-04)	Tok/s 228354 (242042)	Loss/tok 2.9511 (3.2898)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.100 (0.090)	Data 1.12e-04 (3.02e-04)	Tok/s 249656 (241988)	Loss/tok 3.3694 (3.2888)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.067 (0.090)	Data 1.21e-04 (3.00e-04)	Tok/s 229988 (241977)	Loss/tok 2.9400 (3.2880)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.067 (0.090)	Data 1.15e-04 (2.99e-04)	Tok/s 232899 (241966)	Loss/tok 2.9825 (3.2882)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.134 (0.090)	Data 1.16e-04 (2.97e-04)	Tok/s 260966 (241967)	Loss/tok 3.4072 (3.2876)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.135 (0.090)	Data 1.15e-04 (2.95e-04)	Tok/s 260289 (241948)	Loss/tok 3.3875 (3.2867)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.101 (0.090)	Data 1.13e-04 (2.93e-04)	Tok/s 250124 (241890)	Loss/tok 3.2969 (3.2854)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.100 (0.090)	Data 1.16e-04 (2.92e-04)	Tok/s 251386 (241847)	Loss/tok 3.2731 (3.2842)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.036 (0.089)	Data 1.14e-04 (2.90e-04)	Tok/s 219065 (241823)	Loss/tok 2.7126 (3.2837)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.89e-04)	Tok/s 252877 (241810)	Loss/tok 3.2629 (3.2838)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.87e-04)	Tok/s 234372 (241761)	Loss/tok 3.0893 (3.2827)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.86e-04)	Tok/s 250600 (241802)	Loss/tok 3.2495 (3.2832)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1150/1291]	Time 0.100 (0.089)	Data 1.17e-04 (2.84e-04)	Tok/s 254723 (241776)	Loss/tok 3.2365 (3.2832)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.83e-04)	Tok/s 252896 (241769)	Loss/tok 3.2880 (3.2832)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.137 (0.089)	Data 1.14e-04 (2.81e-04)	Tok/s 254362 (241793)	Loss/tok 3.5596 (3.2837)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.80e-04)	Tok/s 251441 (241775)	Loss/tok 3.2624 (3.2832)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.136 (0.089)	Data 1.33e-04 (2.79e-04)	Tok/s 256336 (241772)	Loss/tok 3.4565 (3.2828)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.135 (0.089)	Data 1.15e-04 (2.77e-04)	Tok/s 258963 (241759)	Loss/tok 3.4168 (3.2824)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.067 (0.089)	Data 1.27e-04 (2.76e-04)	Tok/s 234810 (241752)	Loss/tok 3.0561 (3.2824)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.067 (0.089)	Data 1.28e-04 (2.75e-04)	Tok/s 231515 (241764)	Loss/tok 3.0980 (3.2821)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.135 (0.089)	Data 1.18e-04 (2.73e-04)	Tok/s 258957 (241754)	Loss/tok 3.5013 (3.2825)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.72e-04)	Tok/s 254260 (241718)	Loss/tok 3.2671 (3.2817)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.101 (0.089)	Data 1.16e-04 (2.71e-04)	Tok/s 250680 (241745)	Loss/tok 3.3228 (3.2816)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.068 (0.089)	Data 1.22e-04 (2.70e-04)	Tok/s 230593 (241721)	Loss/tok 3.0131 (3.2814)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.068 (0.089)	Data 1.17e-04 (2.69e-04)	Tok/s 226987 (241678)	Loss/tok 3.1434 (3.2815)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1280/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.67e-04)	Tok/s 229726 (241690)	Loss/tok 2.9986 (3.2812)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.035 (0.089)	Data 4.29e-05 (2.69e-04)	Tok/s 219412 (241602)	Loss/tok 2.6691 (3.2808)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593141542564, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593141542565, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.353 (0.353)	Decoder iters 89.0 (89.0)	Tok/s 45065 (45065)
0: Running moses detokenizer
0: BLEU(score=22.70800092695512, counts=[35689, 17460, 9738, 5677], totals=[63365, 60362, 57359, 54362], precisions=[56.32289118598595, 28.9254829197177, 16.97728342544326, 10.442956476950812], bp=0.9795229072712712, sys_len=63365, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593141544404, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2271, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593141544404, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2818	Test BLEU: 22.71
0: Performance: Epoch: 2	Training: 1933538 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593141544405, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593141544405, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141544405, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1557583708
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][0/1291]	Time 0.297 (0.297)	Data 1.85e-01 (1.85e-01)	Tok/s 85249 (85249)	Loss/tok 3.1802 (3.1802)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.066 (0.101)	Data 1.25e-04 (1.70e-02)	Tok/s 236153 (223876)	Loss/tok 2.9586 (3.1801)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.134 (0.091)	Data 1.16e-04 (8.94e-03)	Tok/s 262046 (232250)	Loss/tok 3.3442 (3.1523)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.067 (0.090)	Data 1.17e-04 (6.10e-03)	Tok/s 234562 (235320)	Loss/tok 3.0328 (3.1605)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.036 (0.086)	Data 1.16e-04 (4.64e-03)	Tok/s 221567 (235868)	Loss/tok 2.6170 (3.1416)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.100 (0.086)	Data 1.37e-04 (3.75e-03)	Tok/s 254538 (237068)	Loss/tok 3.1643 (3.1421)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.099 (0.093)	Data 1.13e-04 (3.16e-03)	Tok/s 255360 (239851)	Loss/tok 3.1403 (3.2005)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.067 (0.094)	Data 1.13e-04 (2.73e-03)	Tok/s 230808 (240613)	Loss/tok 3.0433 (3.2047)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.036 (0.092)	Data 1.31e-04 (2.41e-03)	Tok/s 220664 (240089)	Loss/tok 2.5704 (3.2003)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.173 (0.093)	Data 1.19e-04 (2.16e-03)	Tok/s 256022 (240836)	Loss/tok 3.5163 (3.2027)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.037 (0.092)	Data 1.16e-04 (1.96e-03)	Tok/s 211732 (240368)	Loss/tok 2.6382 (3.2081)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.100 (0.094)	Data 1.14e-04 (1.79e-03)	Tok/s 253771 (241355)	Loss/tok 3.2707 (3.2185)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.135 (0.094)	Data 1.40e-04 (1.65e-03)	Tok/s 260873 (241375)	Loss/tok 3.3442 (3.2179)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][130/1291]	Time 0.100 (0.094)	Data 1.20e-04 (1.53e-03)	Tok/s 252480 (241596)	Loss/tok 3.2013 (3.2161)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.100 (0.093)	Data 1.14e-04 (1.43e-03)	Tok/s 255192 (241573)	Loss/tok 3.2410 (3.2097)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.099 (0.092)	Data 1.17e-04 (1.35e-03)	Tok/s 252664 (241712)	Loss/tok 3.1300 (3.2079)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.067 (0.091)	Data 1.24e-04 (1.27e-03)	Tok/s 228022 (241498)	Loss/tok 2.9966 (3.2042)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.036 (0.091)	Data 1.16e-04 (1.20e-03)	Tok/s 222911 (241602)	Loss/tok 2.4885 (3.2007)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.067 (0.092)	Data 1.17e-04 (1.14e-03)	Tok/s 225464 (242007)	Loss/tok 2.8917 (3.2040)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.067 (0.092)	Data 1.15e-04 (1.09e-03)	Tok/s 231053 (242036)	Loss/tok 3.0061 (3.2024)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.067 (0.091)	Data 1.14e-04 (1.04e-03)	Tok/s 230369 (242003)	Loss/tok 2.9938 (3.1995)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.100 (0.091)	Data 1.16e-04 (9.98e-04)	Tok/s 250374 (241893)	Loss/tok 3.2168 (3.2005)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.067 (0.091)	Data 1.12e-04 (9.58e-04)	Tok/s 232290 (241728)	Loss/tok 3.0265 (3.1989)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.100 (0.091)	Data 1.13e-04 (9.22e-04)	Tok/s 253872 (241745)	Loss/tok 3.2195 (3.1981)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.066 (0.089)	Data 1.21e-04 (8.88e-04)	Tok/s 234986 (241300)	Loss/tok 3.0066 (3.1916)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.067 (0.090)	Data 1.19e-04 (8.58e-04)	Tok/s 230676 (241511)	Loss/tok 2.9618 (3.1922)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][260/1291]	Time 0.134 (0.090)	Data 1.21e-04 (8.29e-04)	Tok/s 262208 (241711)	Loss/tok 3.3031 (3.1924)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.067 (0.090)	Data 1.29e-04 (8.03e-04)	Tok/s 229684 (241676)	Loss/tok 2.9965 (3.1896)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.174 (0.090)	Data 1.23e-04 (7.79e-04)	Tok/s 255581 (241873)	Loss/tok 3.4986 (3.1930)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.067 (0.090)	Data 1.42e-04 (7.56e-04)	Tok/s 232543 (241832)	Loss/tok 2.9817 (3.1893)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.100 (0.090)	Data 1.16e-04 (7.35e-04)	Tok/s 251581 (241924)	Loss/tok 3.1818 (3.1883)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.036 (0.090)	Data 1.16e-04 (7.15e-04)	Tok/s 218645 (241945)	Loss/tok 2.6038 (3.1870)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.100 (0.090)	Data 1.17e-04 (6.97e-04)	Tok/s 250426 (241826)	Loss/tok 3.1641 (3.1851)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.134 (0.089)	Data 1.18e-04 (6.79e-04)	Tok/s 261926 (241667)	Loss/tok 3.3412 (3.1824)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.067 (0.089)	Data 1.19e-04 (6.63e-04)	Tok/s 234310 (241645)	Loss/tok 2.9769 (3.1826)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.136 (0.090)	Data 1.18e-04 (6.47e-04)	Tok/s 258553 (241748)	Loss/tok 3.2891 (3.1839)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.100 (0.089)	Data 1.19e-04 (6.33e-04)	Tok/s 249635 (241756)	Loss/tok 3.1632 (3.1813)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.099 (0.090)	Data 1.26e-04 (6.19e-04)	Tok/s 254612 (241933)	Loss/tok 3.2444 (3.1825)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.099 (0.089)	Data 1.16e-04 (6.06e-04)	Tok/s 253649 (241763)	Loss/tok 3.2037 (3.1791)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][390/1291]	Time 0.100 (0.089)	Data 1.18e-04 (5.93e-04)	Tok/s 253762 (241750)	Loss/tok 3.0735 (3.1772)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.067 (0.089)	Data 1.24e-04 (5.81e-04)	Tok/s 230411 (241702)	Loss/tok 2.9137 (3.1758)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.067 (0.089)	Data 1.17e-04 (5.70e-04)	Tok/s 235400 (241772)	Loss/tok 2.9229 (3.1749)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.067 (0.089)	Data 1.15e-04 (5.59e-04)	Tok/s 230628 (241780)	Loss/tok 2.9744 (3.1744)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.066 (0.089)	Data 1.33e-04 (5.49e-04)	Tok/s 232458 (241748)	Loss/tok 2.9531 (3.1723)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.099 (0.089)	Data 1.33e-04 (5.39e-04)	Tok/s 254666 (241843)	Loss/tok 3.1588 (3.1749)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.099 (0.089)	Data 1.16e-04 (5.30e-04)	Tok/s 256390 (241874)	Loss/tok 3.2082 (3.1748)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.135 (0.089)	Data 1.21e-04 (5.21e-04)	Tok/s 258983 (242010)	Loss/tok 3.3636 (3.1750)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.089)	Data 1.15e-04 (5.13e-04)	Tok/s 229441 (241982)	Loss/tok 2.9450 (3.1766)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.067 (0.089)	Data 1.11e-04 (5.04e-04)	Tok/s 229040 (241972)	Loss/tok 2.9230 (3.1755)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.099 (0.089)	Data 1.37e-04 (4.97e-04)	Tok/s 254453 (242005)	Loss/tok 3.2050 (3.1751)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.066 (0.089)	Data 1.15e-04 (4.90e-04)	Tok/s 232944 (241965)	Loss/tok 2.9136 (3.1733)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.174 (0.089)	Data 1.18e-04 (4.82e-04)	Tok/s 256349 (241992)	Loss/tok 3.5275 (3.1748)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][520/1291]	Time 0.100 (0.089)	Data 1.15e-04 (4.75e-04)	Tok/s 254883 (242041)	Loss/tok 3.0983 (3.1740)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.100 (0.089)	Data 1.15e-04 (4.69e-04)	Tok/s 253575 (242050)	Loss/tok 3.1246 (3.1724)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.066 (0.089)	Data 1.17e-04 (4.62e-04)	Tok/s 234879 (241962)	Loss/tok 2.9379 (3.1704)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.036 (0.089)	Data 1.38e-04 (4.56e-04)	Tok/s 223301 (242001)	Loss/tok 2.5966 (3.1697)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.135 (0.089)	Data 1.17e-04 (4.50e-04)	Tok/s 260451 (241992)	Loss/tok 3.2997 (3.1685)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.173 (0.089)	Data 1.16e-04 (4.44e-04)	Tok/s 258917 (241996)	Loss/tok 3.4912 (3.1689)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.099 (0.089)	Data 1.17e-04 (4.38e-04)	Tok/s 255305 (241993)	Loss/tok 3.1829 (3.1694)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.066 (0.089)	Data 1.18e-04 (4.33e-04)	Tok/s 235301 (242071)	Loss/tok 3.0038 (3.1700)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.067 (0.089)	Data 1.35e-04 (4.28e-04)	Tok/s 232326 (242085)	Loss/tok 2.9823 (3.1700)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.036 (0.089)	Data 1.18e-04 (4.23e-04)	Tok/s 223222 (242115)	Loss/tok 2.6019 (3.1691)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.099 (0.089)	Data 1.33e-04 (4.18e-04)	Tok/s 252402 (242166)	Loss/tok 3.1359 (3.1694)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.099 (0.089)	Data 1.20e-04 (4.13e-04)	Tok/s 253796 (242254)	Loss/tok 3.2385 (3.1706)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][640/1291]	Time 0.067 (0.090)	Data 1.21e-04 (4.09e-04)	Tok/s 226451 (242289)	Loss/tok 2.9691 (3.1719)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.066 (0.089)	Data 1.19e-04 (4.04e-04)	Tok/s 235396 (242279)	Loss/tok 3.0106 (3.1708)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.173 (0.089)	Data 1.17e-04 (4.00e-04)	Tok/s 259592 (242201)	Loss/tok 3.4469 (3.1703)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.96e-04)	Tok/s 233173 (242200)	Loss/tok 2.9021 (3.1701)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.174 (0.090)	Data 1.24e-04 (3.92e-04)	Tok/s 256215 (242254)	Loss/tok 3.4487 (3.1711)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.88e-04)	Tok/s 228350 (242186)	Loss/tok 2.8982 (3.1696)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.84e-04)	Tok/s 232745 (242129)	Loss/tok 2.9748 (3.1696)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.100 (0.089)	Data 1.35e-04 (3.80e-04)	Tok/s 249205 (242083)	Loss/tok 3.2394 (3.1686)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.76e-04)	Tok/s 251983 (242072)	Loss/tok 3.2177 (3.1676)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.067 (0.089)	Data 1.39e-04 (3.73e-04)	Tok/s 232202 (242051)	Loss/tok 2.9910 (3.1667)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.036 (0.089)	Data 1.15e-04 (3.69e-04)	Tok/s 222257 (242063)	Loss/tok 2.5101 (3.1659)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.174 (0.089)	Data 1.38e-04 (3.66e-04)	Tok/s 258622 (242020)	Loss/tok 3.4469 (3.1656)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.63e-04)	Tok/s 251358 (242109)	Loss/tok 3.2270 (3.1655)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][770/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.60e-04)	Tok/s 255243 (242114)	Loss/tok 3.1333 (3.1644)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.067 (0.089)	Data 1.43e-04 (3.57e-04)	Tok/s 231698 (242103)	Loss/tok 2.9134 (3.1647)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.173 (0.089)	Data 1.16e-04 (3.54e-04)	Tok/s 258500 (242152)	Loss/tok 3.4928 (3.1659)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.51e-04)	Tok/s 232926 (242119)	Loss/tok 2.8907 (3.1653)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.036 (0.089)	Data 1.15e-04 (3.48e-04)	Tok/s 219624 (242127)	Loss/tok 2.5706 (3.1651)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.036 (0.089)	Data 1.18e-04 (3.45e-04)	Tok/s 220495 (242042)	Loss/tok 2.5136 (3.1634)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.066 (0.089)	Data 1.15e-04 (3.42e-04)	Tok/s 231693 (242000)	Loss/tok 2.9762 (3.1636)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.40e-04)	Tok/s 234624 (242037)	Loss/tok 2.9080 (3.1629)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.134 (0.089)	Data 1.40e-04 (3.37e-04)	Tok/s 263553 (242080)	Loss/tok 3.1822 (3.1621)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.067 (0.089)	Data 1.17e-04 (3.35e-04)	Tok/s 230297 (242101)	Loss/tok 2.9818 (3.1629)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.134 (0.089)	Data 1.17e-04 (3.32e-04)	Tok/s 264006 (242126)	Loss/tok 3.3024 (3.1621)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.30e-04)	Tok/s 254764 (242230)	Loss/tok 3.0957 (3.1628)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.27e-04)	Tok/s 235236 (242114)	Loss/tok 2.9446 (3.1618)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][900/1291]	Time 0.067 (0.089)	Data 1.29e-04 (3.25e-04)	Tok/s 231877 (242157)	Loss/tok 2.9302 (3.1613)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.23e-04)	Tok/s 248310 (242151)	Loss/tok 3.1705 (3.1610)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.100 (0.089)	Data 1.16e-04 (3.20e-04)	Tok/s 249837 (242158)	Loss/tok 3.0860 (3.1604)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.067 (0.089)	Data 1.17e-04 (3.18e-04)	Tok/s 234278 (242088)	Loss/tok 2.8197 (3.1591)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.099 (0.089)	Data 1.19e-04 (3.16e-04)	Tok/s 252002 (242150)	Loss/tok 3.1622 (3.1601)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.036 (0.089)	Data 1.17e-04 (3.14e-04)	Tok/s 222786 (242130)	Loss/tok 2.5118 (3.1592)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.12e-04)	Tok/s 232143 (242084)	Loss/tok 2.9468 (3.1579)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.10e-04)	Tok/s 231937 (242081)	Loss/tok 2.8304 (3.1569)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.174 (0.089)	Data 1.17e-04 (3.08e-04)	Tok/s 254019 (242162)	Loss/tok 3.4583 (3.1580)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.06e-04)	Tok/s 256335 (242175)	Loss/tok 3.4370 (3.1589)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.135 (0.089)	Data 1.39e-04 (3.05e-04)	Tok/s 258692 (242157)	Loss/tok 3.2492 (3.1583)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.066 (0.089)	Data 1.19e-04 (3.03e-04)	Tok/s 234449 (242135)	Loss/tok 2.9582 (3.1572)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.135 (0.089)	Data 1.19e-04 (3.01e-04)	Tok/s 259564 (242138)	Loss/tok 3.3021 (3.1566)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1030/1291]	Time 0.174 (0.089)	Data 1.35e-04 (2.99e-04)	Tok/s 256792 (242196)	Loss/tok 3.4259 (3.1574)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.136 (0.089)	Data 1.17e-04 (2.98e-04)	Tok/s 257869 (242237)	Loss/tok 3.3440 (3.1577)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.135 (0.089)	Data 1.17e-04 (2.96e-04)	Tok/s 258521 (242259)	Loss/tok 3.1898 (3.1572)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.067 (0.089)	Data 1.18e-04 (2.94e-04)	Tok/s 228447 (242261)	Loss/tok 2.9820 (3.1568)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.136 (0.089)	Data 1.15e-04 (2.93e-04)	Tok/s 256956 (242262)	Loss/tok 3.2951 (3.1573)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.099 (0.089)	Data 1.18e-04 (2.91e-04)	Tok/s 252754 (242283)	Loss/tok 3.1673 (3.1565)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.89e-04)	Tok/s 234971 (242335)	Loss/tok 2.9770 (3.1568)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.88e-04)	Tok/s 233405 (242339)	Loss/tok 2.9858 (3.1561)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.86e-04)	Tok/s 227579 (242280)	Loss/tok 2.9837 (3.1553)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.135 (0.089)	Data 1.19e-04 (2.85e-04)	Tok/s 260755 (242327)	Loss/tok 3.1994 (3.1552)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.174 (0.089)	Data 1.15e-04 (2.83e-04)	Tok/s 256769 (242380)	Loss/tok 3.4809 (3.1563)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.100 (0.089)	Data 1.21e-04 (2.82e-04)	Tok/s 255565 (242400)	Loss/tok 3.0615 (3.1558)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.099 (0.089)	Data 1.16e-04 (2.81e-04)	Tok/s 252958 (242385)	Loss/tok 3.1591 (3.1548)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1160/1291]	Time 0.136 (0.089)	Data 1.31e-04 (2.79e-04)	Tok/s 258845 (242394)	Loss/tok 3.2695 (3.1544)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.099 (0.089)	Data 1.19e-04 (2.78e-04)	Tok/s 253619 (242353)	Loss/tok 3.0027 (3.1536)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.100 (0.089)	Data 1.32e-04 (2.76e-04)	Tok/s 250511 (242340)	Loss/tok 3.1545 (3.1530)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.75e-04)	Tok/s 250848 (242347)	Loss/tok 3.1183 (3.1529)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.066 (0.089)	Data 1.17e-04 (2.74e-04)	Tok/s 237125 (242286)	Loss/tok 2.9325 (3.1519)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.067 (0.089)	Data 1.42e-04 (2.73e-04)	Tok/s 233045 (242269)	Loss/tok 2.9487 (3.1519)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.173 (0.089)	Data 1.18e-04 (2.71e-04)	Tok/s 256278 (242275)	Loss/tok 3.4121 (3.1518)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.70e-04)	Tok/s 253161 (242258)	Loss/tok 3.1090 (3.1509)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.067 (0.089)	Data 1.19e-04 (2.69e-04)	Tok/s 230144 (242264)	Loss/tok 2.9147 (3.1512)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.099 (0.089)	Data 1.14e-04 (2.68e-04)	Tok/s 254323 (242272)	Loss/tok 3.0751 (3.1506)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.135 (0.089)	Data 1.16e-04 (2.67e-04)	Tok/s 259968 (242253)	Loss/tok 3.2660 (3.1502)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.099 (0.089)	Data 1.21e-04 (2.65e-04)	Tok/s 254890 (242293)	Loss/tok 3.0661 (3.1503)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1280/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.64e-04)	Tok/s 231238 (242275)	Loss/tok 2.9311 (3.1503)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.099 (0.089)	Data 4.94e-05 (2.66e-04)	Tok/s 253150 (242233)	Loss/tok 3.1595 (3.1494)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593141659997, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593141659998, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.388 (0.388)	Decoder iters 103.0 (103.0)	Tok/s 42043 (42043)
0: Running moses detokenizer
0: BLEU(score=24.120158181503346, counts=[37010, 18571, 10556, 6251], totals=[65099, 62096, 59093, 56093], precisions=[56.85187176454323, 29.906918320020612, 17.863367911596974, 11.143993011605726], bp=1.0, sys_len=65099, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593141661930, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2412, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593141661930, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1492	Test BLEU: 24.12
0: Performance: Epoch: 3	Training: 1937369 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593141661930, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593141661931, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 03:21:09 AM
RESULT,RNN_TRANSLATOR,,505,nvidia,2020-06-26 03:12:44 AM
