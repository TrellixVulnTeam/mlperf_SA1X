Beginning trial 9 of 10
:::MLLOG {"namespace": "", "time_ms": 1593145233665, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593145233713, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593145233713, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593145233713, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593145233713, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593145236078, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 04:20:36 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593145238744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145238744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145238747, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145238760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145238761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145238766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145238765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145238769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1143451339
:::MLLOG {"namespace": "", "time_ms": 1593145247516, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1143451339, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3465118597
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593145261202, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593145261204, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593145261205, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593145261205, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593145261205, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593145263188, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593145263189, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593145263189, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593145263524, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593145263525, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593145263525, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593145263526, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593145263526, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593145263526, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593145263527, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593145263527, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593145263527, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593145263527, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593145263527, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145263527, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 994001946
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.342 (0.342)	Data 2.67e-01 (2.67e-01)	Tok/s 45775 (45775)	Loss/tok 10.5488 (10.5488)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.126 (0.124)	Data 6.57e-03 (3.35e-02)	Tok/s 203099 (178669)	Loss/tok 9.7009 (9.9882)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.099 (0.109)	Data 1.13e-04 (1.76e-02)	Tok/s 257038 (205021)	Loss/tok 9.2176 (9.6774)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.070 (0.101)	Data 1.13e-04 (1.29e-02)	Tok/s 222096 (209081)	Loss/tok 8.8260 (9.4682)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1291]	Time 0.174 (0.100)	Data 1.38e-04 (9.80e-03)	Tok/s 256367 (215958)	Loss/tok 8.8432 (9.2940)	LR 7.222e-05
0: TRAIN [0][50/1291]	Time 0.067 (0.095)	Data 1.12e-04 (8.01e-03)	Tok/s 229536 (219076)	Loss/tok 8.3213 (9.1582)	LR 9.092e-05
0: TRAIN [0][60/1291]	Time 0.069 (0.094)	Data 1.26e-04 (6.72e-03)	Tok/s 223015 (221966)	Loss/tok 8.4596 (9.0259)	LR 1.145e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][70/1291]	Time 0.066 (0.091)	Data 1.14e-04 (5.79e-03)	Tok/s 232771 (223960)	Loss/tok 7.9396 (8.9224)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.09e-03)	Tok/s 226243 (225801)	Loss/tok 7.8232 (8.8107)	LR 1.773e-04
0: TRAIN [0][90/1291]	Time 0.099 (0.089)	Data 1.15e-04 (4.54e-03)	Tok/s 250376 (226951)	Loss/tok 8.0173 (8.7172)	LR 2.232e-04
0: TRAIN [0][100/1291]	Time 0.174 (0.091)	Data 1.11e-04 (4.10e-03)	Tok/s 254880 (229073)	Loss/tok 8.3345 (8.6294)	LR 2.810e-04
0: TRAIN [0][110/1291]	Time 0.067 (0.090)	Data 1.26e-04 (3.74e-03)	Tok/s 230724 (230452)	Loss/tok 7.6777 (8.5603)	LR 3.537e-04
0: TRAIN [0][120/1291]	Time 0.098 (0.090)	Data 1.10e-04 (3.44e-03)	Tok/s 255210 (231598)	Loss/tok 7.8289 (8.4976)	LR 4.453e-04
0: TRAIN [0][130/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.19e-03)	Tok/s 230511 (231972)	Loss/tok 7.4980 (8.4441)	LR 5.606e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][140/1291]	Time 0.134 (0.090)	Data 1.11e-04 (2.97e-03)	Tok/s 261461 (233365)	Loss/tok 7.8511 (8.3892)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.78e-03)	Tok/s 237388 (233691)	Loss/tok 7.4280 (8.3404)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.62e-03)	Tok/s 229948 (234128)	Loss/tok 7.2292 (8.2889)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.47e-03)	Tok/s 233553 (234896)	Loss/tok 7.2334 (8.2310)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.099 (0.090)	Data 1.11e-04 (2.34e-03)	Tok/s 258576 (235935)	Loss/tok 7.0831 (8.1588)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.22e-03)	Tok/s 235456 (236182)	Loss/tok 6.6396 (8.1002)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.12e-03)	Tok/s 236247 (236568)	Loss/tok 6.5419 (8.0358)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.02e-03)	Tok/s 230265 (236818)	Loss/tok 6.4067 (7.9729)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.099 (0.089)	Data 1.12e-04 (1.94e-03)	Tok/s 256959 (237269)	Loss/tok 6.4631 (7.9018)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.066 (0.089)	Data 1.10e-04 (1.86e-03)	Tok/s 234029 (237497)	Loss/tok 6.0184 (7.8354)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.066 (0.089)	Data 1.17e-04 (1.79e-03)	Tok/s 233393 (237558)	Loss/tok 5.9363 (7.7698)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.098 (0.090)	Data 1.28e-04 (1.72e-03)	Tok/s 258821 (238155)	Loss/tok 6.0395 (7.6890)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][260/1291]	Time 0.036 (0.089)	Data 1.10e-04 (1.66e-03)	Tok/s 215669 (238207)	Loss/tok 4.8947 (7.6253)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.135 (0.090)	Data 1.12e-04 (1.60e-03)	Tok/s 259384 (238625)	Loss/tok 5.9633 (7.5477)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.036 (0.090)	Data 1.08e-04 (1.55e-03)	Tok/s 220605 (238588)	Loss/tok 4.5699 (7.4878)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.135 (0.090)	Data 1.10e-04 (1.50e-03)	Tok/s 261252 (238695)	Loss/tok 5.8086 (7.4233)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.099 (0.090)	Data 1.07e-04 (1.45e-03)	Tok/s 251858 (238970)	Loss/tok 5.4648 (7.3460)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.066 (0.090)	Data 1.09e-04 (1.41e-03)	Tok/s 237877 (239205)	Loss/tok 4.8843 (7.2795)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.100 (0.090)	Data 1.08e-04 (1.37e-03)	Tok/s 252809 (239455)	Loss/tok 5.1642 (7.2108)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.174 (0.090)	Data 1.35e-04 (1.33e-03)	Tok/s 257056 (239489)	Loss/tok 5.4654 (7.1511)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.067 (0.090)	Data 1.16e-04 (1.30e-03)	Tok/s 229598 (239601)	Loss/tok 4.6834 (7.0896)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.067 (0.090)	Data 1.09e-04 (1.26e-03)	Tok/s 234753 (239693)	Loss/tok 4.5849 (7.0322)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.172 (0.089)	Data 1.09e-04 (1.23e-03)	Tok/s 259677 (239773)	Loss/tok 5.6172 (6.9769)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.099 (0.089)	Data 1.33e-04 (1.20e-03)	Tok/s 251106 (240010)	Loss/tok 4.7806 (6.9129)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.099 (0.089)	Data 1.09e-04 (1.17e-03)	Tok/s 254664 (240043)	Loss/tok 4.6871 (6.8572)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][390/1291]	Time 0.066 (0.089)	Data 1.14e-04 (1.14e-03)	Tok/s 236075 (240076)	Loss/tok 4.2044 (6.7990)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.036 (0.089)	Data 1.08e-04 (1.12e-03)	Tok/s 219045 (240055)	Loss/tok 3.5571 (6.7411)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.099 (0.089)	Data 1.07e-04 (1.09e-03)	Tok/s 253170 (240075)	Loss/tok 4.5348 (6.6894)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.066 (0.089)	Data 1.09e-04 (1.07e-03)	Tok/s 238779 (240147)	Loss/tok 4.1248 (6.6372)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.067 (0.089)	Data 1.08e-04 (1.05e-03)	Tok/s 232998 (240009)	Loss/tok 4.1672 (6.5942)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.100 (0.088)	Data 1.15e-04 (1.03e-03)	Tok/s 257213 (240014)	Loss/tok 4.2366 (6.5465)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.066 (0.089)	Data 1.10e-04 (1.01e-03)	Tok/s 231954 (240186)	Loss/tok 4.0875 (6.4900)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.100 (0.089)	Data 1.13e-04 (9.87e-04)	Tok/s 252643 (240287)	Loss/tok 4.3186 (6.4395)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.066 (0.089)	Data 1.09e-04 (9.69e-04)	Tok/s 235107 (240355)	Loss/tok 3.9117 (6.3926)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.134 (0.089)	Data 1.10e-04 (9.51e-04)	Tok/s 261008 (240587)	Loss/tok 4.3679 (6.3360)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.099 (0.090)	Data 1.10e-04 (9.34e-04)	Tok/s 255358 (240757)	Loss/tok 4.2931 (6.2852)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.135 (0.090)	Data 1.25e-04 (9.18e-04)	Tok/s 259162 (240779)	Loss/tok 4.4363 (6.2454)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.067 (0.090)	Data 1.08e-04 (9.02e-04)	Tok/s 233273 (240827)	Loss/tok 3.8311 (6.2035)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][520/1291]	Time 0.036 (0.090)	Data 1.06e-04 (8.87e-04)	Tok/s 220776 (240926)	Loss/tok 3.3116 (6.1576)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.135 (0.090)	Data 1.08e-04 (8.72e-04)	Tok/s 257186 (240913)	Loss/tok 4.3740 (6.1213)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.100 (0.090)	Data 1.06e-04 (8.58e-04)	Tok/s 251072 (241132)	Loss/tok 4.1358 (6.0764)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][550/1291]	Time 0.067 (0.091)	Data 1.09e-04 (8.44e-04)	Tok/s 235047 (241184)	Loss/tok 3.8692 (6.0357)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.136 (0.091)	Data 1.10e-04 (8.31e-04)	Tok/s 259020 (241255)	Loss/tok 4.3050 (5.9992)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.099 (0.091)	Data 1.10e-04 (8.19e-04)	Tok/s 256488 (241405)	Loss/tok 4.0893 (5.9607)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.067 (0.091)	Data 1.08e-04 (8.07e-04)	Tok/s 228644 (241469)	Loss/tok 3.7673 (5.9256)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.135 (0.091)	Data 1.07e-04 (7.95e-04)	Tok/s 261666 (241379)	Loss/tok 4.2042 (5.8986)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.067 (0.091)	Data 1.10e-04 (7.84e-04)	Tok/s 227088 (241434)	Loss/tok 3.6129 (5.8654)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.067 (0.090)	Data 1.12e-04 (7.73e-04)	Tok/s 232567 (241443)	Loss/tok 3.6627 (5.8358)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.066 (0.090)	Data 1.05e-04 (7.62e-04)	Tok/s 229965 (241427)	Loss/tok 3.5931 (5.8060)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.099 (0.090)	Data 1.09e-04 (7.52e-04)	Tok/s 253920 (241410)	Loss/tok 3.9004 (5.7791)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.134 (0.090)	Data 1.07e-04 (7.42e-04)	Tok/s 261499 (241450)	Loss/tok 4.1882 (5.7499)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.100 (0.091)	Data 1.08e-04 (7.32e-04)	Tok/s 252506 (241581)	Loss/tok 3.9426 (5.7158)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.100 (0.091)	Data 1.10e-04 (7.23e-04)	Tok/s 253010 (241679)	Loss/tok 3.9669 (5.6852)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.067 (0.091)	Data 1.13e-04 (7.14e-04)	Tok/s 235784 (241714)	Loss/tok 3.6852 (5.6567)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][680/1291]	Time 0.099 (0.091)	Data 1.08e-04 (7.05e-04)	Tok/s 254635 (241716)	Loss/tok 3.8185 (5.6323)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.036 (0.091)	Data 1.11e-04 (6.96e-04)	Tok/s 220383 (241689)	Loss/tok 3.1423 (5.6094)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.099 (0.091)	Data 1.09e-04 (6.88e-04)	Tok/s 254732 (241743)	Loss/tok 3.8900 (5.5842)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.099 (0.091)	Data 1.31e-04 (6.80e-04)	Tok/s 252589 (241847)	Loss/tok 3.9015 (5.5577)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.067 (0.091)	Data 1.11e-04 (6.72e-04)	Tok/s 232591 (241871)	Loss/tok 3.5829 (5.5343)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.067 (0.091)	Data 1.08e-04 (6.64e-04)	Tok/s 232687 (241834)	Loss/tok 3.6023 (5.5131)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.134 (0.091)	Data 1.08e-04 (6.57e-04)	Tok/s 261729 (241817)	Loss/tok 3.9354 (5.4921)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.067 (0.091)	Data 1.08e-04 (6.50e-04)	Tok/s 229087 (241867)	Loss/tok 3.6142 (5.4682)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.175 (0.091)	Data 1.23e-04 (6.43e-04)	Tok/s 250810 (241936)	Loss/tok 4.3245 (5.4440)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.100 (0.091)	Data 1.11e-04 (6.36e-04)	Tok/s 252933 (241896)	Loss/tok 3.7966 (5.4243)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.100 (0.090)	Data 1.10e-04 (6.29e-04)	Tok/s 254046 (241872)	Loss/tok 3.7697 (5.4049)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.135 (0.090)	Data 1.08e-04 (6.23e-04)	Tok/s 258126 (241905)	Loss/tok 3.9739 (5.3840)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.135 (0.091)	Data 1.11e-04 (6.16e-04)	Tok/s 261092 (241957)	Loss/tok 3.9967 (5.3630)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][810/1291]	Time 0.067 (0.091)	Data 1.11e-04 (6.10e-04)	Tok/s 229076 (241978)	Loss/tok 3.5495 (5.3428)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.067 (0.090)	Data 1.09e-04 (6.04e-04)	Tok/s 232196 (241927)	Loss/tok 3.5189 (5.3259)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.099 (0.091)	Data 1.10e-04 (5.98e-04)	Tok/s 255877 (242035)	Loss/tok 3.6330 (5.3045)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.067 (0.090)	Data 1.31e-04 (5.92e-04)	Tok/s 235782 (241955)	Loss/tok 3.5211 (5.2884)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.87e-04)	Tok/s 234684 (241919)	Loss/tok 3.4804 (5.2721)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.100 (0.090)	Data 1.42e-04 (5.81e-04)	Tok/s 251515 (241878)	Loss/tok 3.7010 (5.2555)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.067 (0.090)	Data 1.37e-04 (5.76e-04)	Tok/s 236503 (241866)	Loss/tok 3.5392 (5.2387)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.067 (0.090)	Data 1.24e-04 (5.71e-04)	Tok/s 233784 (241899)	Loss/tok 3.4959 (5.2208)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.100 (0.090)	Data 1.24e-04 (5.66e-04)	Tok/s 250487 (241882)	Loss/tok 3.6677 (5.2049)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.067 (0.090)	Data 1.07e-04 (5.61e-04)	Tok/s 236104 (241891)	Loss/tok 3.4273 (5.1880)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.100 (0.090)	Data 1.08e-04 (5.56e-04)	Tok/s 255016 (241871)	Loss/tok 3.7382 (5.1725)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.135 (0.090)	Data 1.11e-04 (5.51e-04)	Tok/s 257483 (241866)	Loss/tok 4.0391 (5.1573)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.099 (0.090)	Data 1.23e-04 (5.46e-04)	Tok/s 257184 (241850)	Loss/tok 3.7403 (5.1428)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][940/1291]	Time 0.068 (0.090)	Data 1.13e-04 (5.42e-04)	Tok/s 231505 (241918)	Loss/tok 3.4615 (5.1256)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.37e-04)	Tok/s 231590 (241847)	Loss/tok 3.3313 (5.1127)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.036 (0.090)	Data 1.29e-04 (5.33e-04)	Tok/s 228262 (241873)	Loss/tok 2.9319 (5.0978)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.066 (0.090)	Data 1.15e-04 (5.28e-04)	Tok/s 232676 (241859)	Loss/tok 3.4031 (5.0831)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.135 (0.090)	Data 1.09e-04 (5.24e-04)	Tok/s 260985 (241848)	Loss/tok 3.7522 (5.0691)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.174 (0.090)	Data 1.10e-04 (5.20e-04)	Tok/s 254213 (241790)	Loss/tok 4.1050 (5.0560)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.067 (0.090)	Data 1.17e-04 (5.16e-04)	Tok/s 231213 (241773)	Loss/tok 3.4340 (5.0428)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.100 (0.090)	Data 1.09e-04 (5.12e-04)	Tok/s 251425 (241809)	Loss/tok 3.6225 (5.0286)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.100 (0.090)	Data 1.09e-04 (5.08e-04)	Tok/s 252322 (241760)	Loss/tok 3.6413 (5.0164)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.134 (0.090)	Data 1.09e-04 (5.04e-04)	Tok/s 262178 (241809)	Loss/tok 3.8521 (5.0023)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.01e-04)	Tok/s 229048 (241823)	Loss/tok 3.4563 (4.9887)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.97e-04)	Tok/s 233035 (241804)	Loss/tok 3.3680 (4.9765)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.93e-04)	Tok/s 232156 (241750)	Loss/tok 3.3566 (4.9647)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1070/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.90e-04)	Tok/s 234829 (241813)	Loss/tok 3.4843 (4.9510)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.175 (0.090)	Data 1.13e-04 (4.86e-04)	Tok/s 257564 (241831)	Loss/tok 4.0277 (4.9383)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.067 (0.090)	Data 1.10e-04 (4.83e-04)	Tok/s 232387 (241839)	Loss/tok 3.3600 (4.9263)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.137 (0.090)	Data 1.12e-04 (4.79e-04)	Tok/s 255233 (241798)	Loss/tok 3.8921 (4.9155)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.76e-04)	Tok/s 230270 (241828)	Loss/tok 3.3621 (4.9024)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.067 (0.090)	Data 1.23e-04 (4.73e-04)	Tok/s 231199 (241858)	Loss/tok 3.3812 (4.8905)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.100 (0.090)	Data 1.08e-04 (4.70e-04)	Tok/s 250144 (241807)	Loss/tok 3.6833 (4.8807)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.100 (0.089)	Data 1.07e-04 (4.67e-04)	Tok/s 254159 (241787)	Loss/tok 3.5539 (4.8704)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.63e-04)	Tok/s 229197 (241753)	Loss/tok 3.4006 (4.8599)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.136 (0.090)	Data 1.10e-04 (4.60e-04)	Tok/s 255916 (241741)	Loss/tok 3.8470 (4.8489)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.57e-04)	Tok/s 230805 (241755)	Loss/tok 3.2791 (4.8379)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.036 (0.089)	Data 1.12e-04 (4.55e-04)	Tok/s 217976 (241698)	Loss/tok 2.9131 (4.8287)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.52e-04)	Tok/s 251794 (241661)	Loss/tok 3.7297 (4.8195)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1200/1291]	Time 0.036 (0.089)	Data 1.09e-04 (4.49e-04)	Tok/s 218438 (241643)	Loss/tok 2.8455 (4.8094)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.135 (0.089)	Data 1.09e-04 (4.46e-04)	Tok/s 257318 (241612)	Loss/tok 3.8633 (4.7997)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.136 (0.089)	Data 1.13e-04 (4.43e-04)	Tok/s 256549 (241636)	Loss/tok 3.8295 (4.7894)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.099 (0.089)	Data 1.08e-04 (4.41e-04)	Tok/s 251196 (241627)	Loss/tok 3.6249 (4.7797)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.099 (0.089)	Data 1.08e-04 (4.38e-04)	Tok/s 252444 (241626)	Loss/tok 3.6413 (4.7703)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.35e-04)	Tok/s 232589 (241624)	Loss/tok 3.3617 (4.7609)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.066 (0.089)	Data 1.26e-04 (4.33e-04)	Tok/s 237842 (241628)	Loss/tok 3.4086 (4.7516)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.099 (0.089)	Data 1.08e-04 (4.30e-04)	Tok/s 257895 (241636)	Loss/tok 3.6792 (4.7424)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.100 (0.089)	Data 1.07e-04 (4.28e-04)	Tok/s 253417 (241690)	Loss/tok 3.5657 (4.7320)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.066 (0.089)	Data 4.89e-05 (4.29e-04)	Tok/s 231343 (241660)	Loss/tok 3.4114 (4.7236)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593145379467, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145379467, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.486 (0.486)	Decoder iters 149.0 (149.0)	Tok/s 33313 (33313)
0: Running moses detokenizer
0: BLEU(score=19.711737993917595, counts=[33965, 15496, 8203, 4538], totals=[63775, 60772, 57771, 54775], precisions=[53.25754606036848, 25.498584874613307, 14.199165671357601, 8.28480146052031], bp=0.9859715359290188, sys_len=63775, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593145381460, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1971, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145381461, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7243	Test BLEU: 19.71
0: Performance: Epoch: 0	Training: 1932971 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593145381461, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145381461, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145381461, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2882148618
0: TRAIN [1][0/1291]	Time 0.303 (0.303)	Data 1.78e-01 (1.78e-01)	Tok/s 81412 (81412)	Loss/tok 3.4815 (3.4815)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.175 (0.134)	Data 1.13e-04 (1.62e-02)	Tok/s 254406 (238684)	Loss/tok 3.8960 (3.6195)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.067 (0.113)	Data 1.11e-04 (8.56e-03)	Tok/s 232058 (242347)	Loss/tok 3.1715 (3.5576)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][30/1291]	Time 0.134 (0.106)	Data 1.13e-04 (5.84e-03)	Tok/s 261162 (241923)	Loss/tok 3.6766 (3.5530)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.100 (0.101)	Data 1.13e-04 (4.44e-03)	Tok/s 255689 (242140)	Loss/tok 3.4672 (3.5325)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][50/1291]	Time 0.135 (0.097)	Data 1.21e-04 (3.59e-03)	Tok/s 256205 (241683)	Loss/tok 3.8411 (3.5165)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.067 (0.096)	Data 1.10e-04 (3.02e-03)	Tok/s 233364 (241161)	Loss/tok 3.2188 (3.5260)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][70/1291]	Time 0.135 (0.094)	Data 1.10e-04 (2.62e-03)	Tok/s 260284 (240830)	Loss/tok 3.7211 (3.5281)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.067 (0.094)	Data 1.34e-04 (2.31e-03)	Tok/s 228612 (240840)	Loss/tok 3.3364 (3.5330)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.099 (0.094)	Data 1.15e-04 (2.07e-03)	Tok/s 254511 (241422)	Loss/tok 3.5666 (3.5317)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.036 (0.092)	Data 1.11e-04 (1.87e-03)	Tok/s 223097 (240826)	Loss/tok 2.8241 (3.5203)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.099 (0.092)	Data 1.08e-04 (1.71e-03)	Tok/s 257355 (241332)	Loss/tok 3.3769 (3.5164)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.068 (0.092)	Data 1.12e-04 (1.58e-03)	Tok/s 229734 (241271)	Loss/tok 3.1766 (3.5123)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.067 (0.092)	Data 1.46e-04 (1.47e-03)	Tok/s 231806 (241112)	Loss/tok 3.2656 (3.5040)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.174 (0.092)	Data 1.12e-04 (1.37e-03)	Tok/s 256824 (241397)	Loss/tok 3.8547 (3.5059)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.066 (0.093)	Data 1.09e-04 (1.29e-03)	Tok/s 233018 (241906)	Loss/tok 3.3212 (3.5087)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.136 (0.092)	Data 1.10e-04 (1.22e-03)	Tok/s 258356 (241908)	Loss/tok 3.6941 (3.5073)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.066 (0.092)	Data 1.09e-04 (1.16e-03)	Tok/s 239929 (241771)	Loss/tok 3.2169 (3.5054)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.100 (0.092)	Data 1.13e-04 (1.10e-03)	Tok/s 252844 (241964)	Loss/tok 3.4675 (3.5024)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.099 (0.092)	Data 1.09e-04 (1.05e-03)	Tok/s 256632 (242233)	Loss/tok 3.5191 (3.5023)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][200/1291]	Time 0.101 (0.092)	Data 1.25e-04 (1.00e-03)	Tok/s 251835 (242420)	Loss/tok 3.3744 (3.5010)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.099 (0.091)	Data 1.12e-04 (9.59e-04)	Tok/s 255146 (242340)	Loss/tok 3.4806 (3.4971)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.100 (0.091)	Data 1.12e-04 (9.20e-04)	Tok/s 252132 (242213)	Loss/tok 3.5152 (3.4924)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.066 (0.091)	Data 1.14e-04 (8.86e-04)	Tok/s 234500 (242194)	Loss/tok 3.2550 (3.4888)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.067 (0.091)	Data 1.31e-04 (8.54e-04)	Tok/s 233493 (242195)	Loss/tok 3.1999 (3.4911)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.066 (0.091)	Data 1.29e-04 (8.24e-04)	Tok/s 231648 (242313)	Loss/tok 3.3186 (3.4975)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.036 (0.091)	Data 1.18e-04 (7.97e-04)	Tok/s 220685 (242263)	Loss/tok 2.8004 (3.4957)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.099 (0.091)	Data 1.12e-04 (7.72e-04)	Tok/s 256813 (242363)	Loss/tok 3.5380 (3.4933)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.036 (0.091)	Data 1.11e-04 (7.49e-04)	Tok/s 217658 (242416)	Loss/tok 2.8146 (3.4919)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.066 (0.090)	Data 1.09e-04 (7.27e-04)	Tok/s 231760 (242345)	Loss/tok 3.1539 (3.4916)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.175 (0.091)	Data 1.24e-04 (7.06e-04)	Tok/s 251393 (242403)	Loss/tok 3.9432 (3.4945)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.066 (0.091)	Data 1.11e-04 (6.87e-04)	Tok/s 233736 (242316)	Loss/tok 3.2472 (3.4937)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.099 (0.091)	Data 1.10e-04 (6.69e-04)	Tok/s 253693 (242410)	Loss/tok 3.5555 (3.4937)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][330/1291]	Time 0.134 (0.091)	Data 1.35e-04 (6.53e-04)	Tok/s 263705 (242672)	Loss/tok 3.6101 (3.4943)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][340/1291]	Time 0.067 (0.091)	Data 1.10e-04 (6.37e-04)	Tok/s 230885 (242596)	Loss/tok 3.3089 (3.4931)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][350/1291]	Time 0.066 (0.090)	Data 1.17e-04 (6.22e-04)	Tok/s 231844 (242533)	Loss/tok 3.2441 (3.4922)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.100 (0.090)	Data 1.13e-04 (6.08e-04)	Tok/s 252357 (242611)	Loss/tok 3.4957 (3.4898)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.95e-04)	Tok/s 234379 (242616)	Loss/tok 3.2458 (3.4900)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.134 (0.091)	Data 1.10e-04 (5.82e-04)	Tok/s 257925 (242727)	Loss/tok 3.6589 (3.4903)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.135 (0.091)	Data 1.11e-04 (5.70e-04)	Tok/s 258835 (242798)	Loss/tok 3.6637 (3.4917)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.135 (0.091)	Data 1.17e-04 (5.59e-04)	Tok/s 255867 (243070)	Loss/tok 3.6762 (3.4931)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.066 (0.091)	Data 1.42e-04 (5.48e-04)	Tok/s 233782 (242992)	Loss/tok 3.2080 (3.4912)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.091)	Data 1.11e-04 (5.38e-04)	Tok/s 231282 (242961)	Loss/tok 3.2317 (3.4901)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.100 (0.091)	Data 1.30e-04 (5.28e-04)	Tok/s 252833 (242995)	Loss/tok 3.4758 (3.4883)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.099 (0.091)	Data 1.09e-04 (5.18e-04)	Tok/s 252597 (242906)	Loss/tok 3.4151 (3.4849)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.066 (0.090)	Data 1.12e-04 (5.10e-04)	Tok/s 233018 (242809)	Loss/tok 3.2752 (3.4822)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.036 (0.090)	Data 1.22e-04 (5.01e-04)	Tok/s 224600 (242691)	Loss/tok 2.7447 (3.4788)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.067 (0.090)	Data 1.31e-04 (4.93e-04)	Tok/s 235954 (242691)	Loss/tok 3.1492 (3.4782)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][480/1291]	Time 0.100 (0.090)	Data 1.25e-04 (4.85e-04)	Tok/s 254044 (242707)	Loss/tok 3.3913 (3.4781)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.174 (0.090)	Data 1.15e-04 (4.77e-04)	Tok/s 258756 (242761)	Loss/tok 3.7258 (3.4785)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.067 (0.090)	Data 1.16e-04 (4.70e-04)	Tok/s 231489 (242697)	Loss/tok 3.2161 (3.4760)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.63e-04)	Tok/s 250636 (242835)	Loss/tok 3.4029 (3.4778)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.100 (0.090)	Data 1.14e-04 (4.56e-04)	Tok/s 248924 (242710)	Loss/tok 3.4009 (3.4751)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.099 (0.090)	Data 1.11e-04 (4.50e-04)	Tok/s 253493 (242653)	Loss/tok 3.5210 (3.4748)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.099 (0.090)	Data 1.32e-04 (4.44e-04)	Tok/s 255688 (242735)	Loss/tok 3.4172 (3.4762)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.066 (0.090)	Data 1.35e-04 (4.38e-04)	Tok/s 232811 (242622)	Loss/tok 3.1558 (3.4741)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.136 (0.090)	Data 1.10e-04 (4.32e-04)	Tok/s 254246 (242674)	Loss/tok 3.5792 (3.4731)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.100 (0.090)	Data 1.14e-04 (4.27e-04)	Tok/s 256523 (242610)	Loss/tok 3.3977 (3.4707)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.21e-04)	Tok/s 234073 (242596)	Loss/tok 3.1398 (3.4694)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.067 (0.090)	Data 1.36e-04 (4.16e-04)	Tok/s 230494 (242668)	Loss/tok 3.2057 (3.4711)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.067 (0.090)	Data 1.33e-04 (4.11e-04)	Tok/s 235159 (242769)	Loss/tok 3.1319 (3.4717)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][610/1291]	Time 0.099 (0.090)	Data 1.17e-04 (4.06e-04)	Tok/s 253683 (242810)	Loss/tok 3.4339 (3.4726)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.066 (0.090)	Data 3.09e-04 (4.02e-04)	Tok/s 232843 (242749)	Loss/tok 3.2157 (3.4700)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.036 (0.090)	Data 1.10e-04 (3.97e-04)	Tok/s 223580 (242709)	Loss/tok 2.8131 (3.4675)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.100 (0.090)	Data 1.10e-04 (3.93e-04)	Tok/s 252026 (242701)	Loss/tok 3.4387 (3.4663)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.036 (0.090)	Data 1.14e-04 (3.89e-04)	Tok/s 222321 (242592)	Loss/tok 2.7438 (3.4650)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.066 (0.089)	Data 1.30e-04 (3.85e-04)	Tok/s 233626 (242578)	Loss/tok 3.2121 (3.4629)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.80e-04)	Tok/s 224000 (242568)	Loss/tok 3.2706 (3.4618)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.77e-04)	Tok/s 232595 (242445)	Loss/tok 3.1497 (3.4600)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.134 (0.089)	Data 1.12e-04 (3.73e-04)	Tok/s 260619 (242482)	Loss/tok 3.6049 (3.4594)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.69e-04)	Tok/s 255351 (242557)	Loss/tok 3.4020 (3.4593)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.65e-04)	Tok/s 235376 (242534)	Loss/tok 3.2357 (3.4584)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.036 (0.089)	Data 1.15e-04 (3.62e-04)	Tok/s 224142 (242545)	Loss/tok 2.7504 (3.4578)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.067 (0.089)	Data 1.26e-04 (3.59e-04)	Tok/s 231314 (242508)	Loss/tok 3.1996 (3.4563)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][740/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.56e-04)	Tok/s 232611 (242517)	Loss/tok 3.2345 (3.4560)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][750/1291]	Time 0.100 (0.089)	Data 1.26e-04 (3.52e-04)	Tok/s 249841 (242414)	Loss/tok 3.3916 (3.4549)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.49e-04)	Tok/s 221409 (242292)	Loss/tok 2.7526 (3.4530)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.46e-04)	Tok/s 235358 (242265)	Loss/tok 3.2776 (3.4515)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.43e-04)	Tok/s 253656 (242296)	Loss/tok 3.3961 (3.4512)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.41e-04)	Tok/s 233602 (242264)	Loss/tok 3.2112 (3.4490)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.174 (0.089)	Data 1.12e-04 (3.38e-04)	Tok/s 254314 (242269)	Loss/tok 3.8080 (3.4490)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.35e-04)	Tok/s 234162 (242222)	Loss/tok 3.2229 (3.4473)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.134 (0.089)	Data 1.11e-04 (3.32e-04)	Tok/s 262055 (242252)	Loss/tok 3.5780 (3.4469)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.30e-04)	Tok/s 253298 (242270)	Loss/tok 3.4366 (3.4458)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.27e-04)	Tok/s 229211 (242339)	Loss/tok 3.2848 (3.4474)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.25e-04)	Tok/s 250237 (242374)	Loss/tok 3.4589 (3.4471)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.22e-04)	Tok/s 252539 (242420)	Loss/tok 3.3256 (3.4459)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.20e-04)	Tok/s 252970 (242348)	Loss/tok 3.3820 (3.4442)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][880/1291]	Time 0.100 (0.089)	Data 1.16e-04 (3.18e-04)	Tok/s 251777 (242362)	Loss/tok 3.4723 (3.4452)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.135 (0.089)	Data 1.09e-04 (3.15e-04)	Tok/s 260914 (242327)	Loss/tok 3.5209 (3.4456)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.174 (0.089)	Data 1.34e-04 (3.13e-04)	Tok/s 257497 (242307)	Loss/tok 3.7484 (3.4447)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.11e-04)	Tok/s 254017 (242383)	Loss/tok 3.3572 (3.4454)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.09e-04)	Tok/s 235005 (242367)	Loss/tok 3.1445 (3.4440)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.07e-04)	Tok/s 256301 (242384)	Loss/tok 3.4748 (3.4430)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.05e-04)	Tok/s 231302 (242348)	Loss/tok 3.1830 (3.4425)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.03e-04)	Tok/s 232919 (242293)	Loss/tok 3.1758 (3.4417)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.01e-04)	Tok/s 234166 (242238)	Loss/tok 3.1703 (3.4413)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.99e-04)	Tok/s 253276 (242279)	Loss/tok 3.4455 (3.4416)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.97e-04)	Tok/s 230206 (242303)	Loss/tok 3.1590 (3.4415)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.95e-04)	Tok/s 232938 (242329)	Loss/tok 3.1211 (3.4408)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.93e-04)	Tok/s 229748 (242372)	Loss/tok 3.2375 (3.4398)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1010/1291]	Time 0.099 (0.089)	Data 1.19e-04 (2.91e-04)	Tok/s 251590 (242324)	Loss/tok 3.4688 (3.4385)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.90e-04)	Tok/s 230940 (242409)	Loss/tok 3.1396 (3.4385)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.136 (0.089)	Data 1.14e-04 (2.88e-04)	Tok/s 257134 (242423)	Loss/tok 3.6045 (3.4376)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.100 (0.089)	Data 1.28e-04 (2.86e-04)	Tok/s 248660 (242439)	Loss/tok 3.3969 (3.4364)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.85e-04)	Tok/s 223006 (242446)	Loss/tok 2.7273 (3.4358)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.136 (0.089)	Data 1.12e-04 (2.83e-04)	Tok/s 262064 (242370)	Loss/tok 3.5500 (3.4344)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.82e-04)	Tok/s 248931 (242413)	Loss/tok 3.4642 (3.4344)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.101 (0.089)	Data 1.11e-04 (2.80e-04)	Tok/s 252959 (242416)	Loss/tok 3.2374 (3.4337)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.79e-04)	Tok/s 230828 (242362)	Loss/tok 3.0974 (3.4320)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.067 (0.089)	Data 1.21e-04 (2.77e-04)	Tok/s 230090 (242420)	Loss/tok 3.2129 (3.4330)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.066 (0.089)	Data 1.20e-04 (2.76e-04)	Tok/s 233439 (242374)	Loss/tok 3.1810 (3.4322)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.174 (0.089)	Data 1.11e-04 (2.74e-04)	Tok/s 256418 (242402)	Loss/tok 3.7380 (3.4328)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.099 (0.089)	Data 1.14e-04 (2.73e-04)	Tok/s 252985 (242342)	Loss/tok 3.4121 (3.4314)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1140/1291]	Time 0.066 (0.089)	Data 1.25e-04 (2.71e-04)	Tok/s 228737 (242388)	Loss/tok 3.1898 (3.4317)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.70e-04)	Tok/s 236777 (242353)	Loss/tok 3.2098 (3.4309)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.69e-04)	Tok/s 234761 (242331)	Loss/tok 3.0915 (3.4296)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.174 (0.089)	Data 1.12e-04 (2.68e-04)	Tok/s 251221 (242384)	Loss/tok 3.7344 (3.4292)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.66e-04)	Tok/s 236215 (242392)	Loss/tok 3.1542 (3.4297)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.035 (0.089)	Data 1.16e-04 (2.65e-04)	Tok/s 225083 (242477)	Loss/tok 2.7358 (3.4302)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.067 (0.089)	Data 1.35e-04 (2.64e-04)	Tok/s 230574 (242561)	Loss/tok 3.2247 (3.4314)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.63e-04)	Tok/s 251068 (242581)	Loss/tok 3.3513 (3.4308)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.61e-04)	Tok/s 235913 (242512)	Loss/tok 3.1166 (3.4294)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.135 (0.089)	Data 1.09e-04 (2.60e-04)	Tok/s 257251 (242459)	Loss/tok 3.5131 (3.4280)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.59e-04)	Tok/s 232045 (242458)	Loss/tok 3.0907 (3.4271)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.58e-04)	Tok/s 231546 (242437)	Loss/tok 3.1441 (3.4266)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.067 (0.089)	Data 1.32e-04 (2.57e-04)	Tok/s 230511 (242419)	Loss/tok 3.1359 (3.4254)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1270/1291]	Time 0.174 (0.089)	Data 1.10e-04 (2.56e-04)	Tok/s 256817 (242450)	Loss/tok 3.7527 (3.4251)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.135 (0.089)	Data 1.23e-04 (2.55e-04)	Tok/s 258181 (242452)	Loss/tok 3.5336 (3.4249)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.067 (0.089)	Data 4.43e-05 (2.56e-04)	Tok/s 236986 (242417)	Loss/tok 3.1354 (3.4241)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593145496955, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593145496956, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.395 (0.395)	Decoder iters 109.0 (109.0)	Tok/s 40952 (40952)
0: Running moses detokenizer
0: BLEU(score=21.85595348867764, counts=[35749, 17071, 9399, 5371], totals=[65213, 62210, 59208, 56209], precisions=[54.81882446751415, 27.440925896158173, 15.87454398054317, 9.555409276094576], bp=1.0, sys_len=65213, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593145498882, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2186, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593145498882, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4236	Test BLEU: 21.86
0: Performance: Epoch: 1	Training: 1939358 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593145498882, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593145498882, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145498882, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2181348374
0: TRAIN [2][0/1291]	Time 0.303 (0.303)	Data 1.90e-01 (1.90e-01)	Tok/s 84256 (84256)	Loss/tok 3.2718 (3.2718)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.068 (0.113)	Data 1.27e-04 (1.74e-02)	Tok/s 229899 (229320)	Loss/tok 3.0267 (3.2323)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.099 (0.101)	Data 1.12e-04 (9.16e-03)	Tok/s 252888 (235757)	Loss/tok 3.2196 (3.2431)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.067 (0.099)	Data 1.35e-04 (6.24e-03)	Tok/s 234028 (238378)	Loss/tok 3.0978 (3.2718)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.101 (0.098)	Data 1.06e-04 (4.75e-03)	Tok/s 251560 (239723)	Loss/tok 3.3625 (3.2795)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][50/1291]	Time 0.067 (0.100)	Data 1.27e-04 (3.85e-03)	Tok/s 235371 (241330)	Loss/tok 3.1021 (3.3020)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.067 (0.097)	Data 1.04e-04 (3.24e-03)	Tok/s 232063 (241095)	Loss/tok 3.1383 (3.2959)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.067 (0.094)	Data 1.22e-04 (2.80e-03)	Tok/s 229803 (240116)	Loss/tok 3.0925 (3.2787)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.036 (0.090)	Data 1.10e-04 (2.47e-03)	Tok/s 221218 (239153)	Loss/tok 2.7015 (3.2603)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.100 (0.090)	Data 1.17e-04 (2.21e-03)	Tok/s 252971 (239360)	Loss/tok 3.2888 (3.2564)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.00e-03)	Tok/s 251676 (239302)	Loss/tok 3.2960 (3.2566)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.136 (0.089)	Data 1.31e-04 (1.83e-03)	Tok/s 255778 (239380)	Loss/tok 3.5406 (3.2624)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.100 (0.089)	Data 1.10e-04 (1.69e-03)	Tok/s 250188 (239861)	Loss/tok 3.2365 (3.2581)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.036 (0.089)	Data 1.16e-04 (1.57e-03)	Tok/s 222304 (240011)	Loss/tok 2.6386 (3.2611)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.067 (0.090)	Data 1.10e-04 (1.47e-03)	Tok/s 236117 (240271)	Loss/tok 3.0790 (3.2634)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.067 (0.090)	Data 1.17e-04 (1.38e-03)	Tok/s 231457 (240279)	Loss/tok 3.0439 (3.2673)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.135 (0.090)	Data 1.12e-04 (1.30e-03)	Tok/s 258956 (240529)	Loss/tok 3.4799 (3.2735)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][170/1291]	Time 0.036 (0.090)	Data 1.12e-04 (1.23e-03)	Tok/s 222647 (240652)	Loss/tok 2.6472 (3.2750)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.036 (0.089)	Data 1.34e-04 (1.17e-03)	Tok/s 220117 (240266)	Loss/tok 2.5857 (3.2682)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.067 (0.089)	Data 1.18e-04 (1.11e-03)	Tok/s 231720 (240373)	Loss/tok 3.0253 (3.2654)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.136 (0.090)	Data 1.11e-04 (1.06e-03)	Tok/s 258946 (241008)	Loss/tok 3.4174 (3.2766)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][210/1291]	Time 0.066 (0.091)	Data 1.14e-04 (1.02e-03)	Tok/s 232169 (240998)	Loss/tok 3.0964 (3.2792)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.036 (0.090)	Data 1.12e-04 (9.79e-04)	Tok/s 222748 (240835)	Loss/tok 2.6822 (3.2777)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.173 (0.090)	Data 1.14e-04 (9.42e-04)	Tok/s 256072 (240834)	Loss/tok 3.7728 (3.2803)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][240/1291]	Time 0.066 (0.090)	Data 1.39e-04 (9.08e-04)	Tok/s 231320 (240719)	Loss/tok 3.1746 (3.2813)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.100 (0.090)	Data 2.73e-04 (8.77e-04)	Tok/s 252596 (240863)	Loss/tok 3.2424 (3.2801)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.099 (0.089)	Data 1.17e-04 (8.48e-04)	Tok/s 255265 (240648)	Loss/tok 3.3194 (3.2751)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.066 (0.089)	Data 1.14e-04 (8.21e-04)	Tok/s 236700 (240655)	Loss/tok 3.0545 (3.2762)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.066 (0.088)	Data 1.11e-04 (7.95e-04)	Tok/s 234962 (240351)	Loss/tok 3.1080 (3.2743)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.067 (0.088)	Data 1.16e-04 (7.72e-04)	Tok/s 231798 (240572)	Loss/tok 3.0523 (3.2780)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.099 (0.088)	Data 1.16e-04 (7.50e-04)	Tok/s 252611 (240822)	Loss/tok 3.2078 (3.2784)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.067 (0.088)	Data 1.10e-04 (7.30e-04)	Tok/s 232866 (240891)	Loss/tok 3.1182 (3.2783)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.135 (0.088)	Data 1.14e-04 (7.11e-04)	Tok/s 257642 (240876)	Loss/tok 3.4491 (3.2770)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.067 (0.088)	Data 1.12e-04 (6.93e-04)	Tok/s 232476 (240915)	Loss/tok 3.0685 (3.2789)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.100 (0.089)	Data 1.18e-04 (6.76e-04)	Tok/s 249897 (241002)	Loss/tok 3.3746 (3.2803)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.100 (0.089)	Data 1.08e-04 (6.60e-04)	Tok/s 249291 (241115)	Loss/tok 3.3484 (3.2809)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.174 (0.088)	Data 1.11e-04 (6.45e-04)	Tok/s 256738 (241050)	Loss/tok 3.7446 (3.2812)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][370/1291]	Time 0.036 (0.089)	Data 1.11e-04 (6.31e-04)	Tok/s 219679 (241169)	Loss/tok 2.6297 (3.2837)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.067 (0.088)	Data 1.11e-04 (6.17e-04)	Tok/s 232963 (241239)	Loss/tok 3.0895 (3.2822)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.135 (0.088)	Data 1.24e-04 (6.04e-04)	Tok/s 258341 (241091)	Loss/tok 3.5175 (3.2800)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.175 (0.088)	Data 1.13e-04 (5.92e-04)	Tok/s 257568 (241247)	Loss/tok 3.6041 (3.2828)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.067 (0.088)	Data 1.12e-04 (5.80e-04)	Tok/s 236320 (241215)	Loss/tok 3.1458 (3.2809)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.099 (0.088)	Data 1.09e-04 (5.69e-04)	Tok/s 248766 (241359)	Loss/tok 3.3342 (3.2814)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.067 (0.088)	Data 1.05e-04 (5.59e-04)	Tok/s 231746 (241458)	Loss/tok 3.0327 (3.2809)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.066 (0.088)	Data 1.08e-04 (5.49e-04)	Tok/s 232275 (241539)	Loss/tok 3.0562 (3.2803)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.067 (0.088)	Data 1.33e-04 (5.39e-04)	Tok/s 234302 (241555)	Loss/tok 3.1106 (3.2790)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.099 (0.088)	Data 1.09e-04 (5.30e-04)	Tok/s 254718 (241601)	Loss/tok 3.2689 (3.2792)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.066 (0.088)	Data 1.08e-04 (5.21e-04)	Tok/s 232142 (241566)	Loss/tok 3.0234 (3.2791)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.100 (0.088)	Data 1.36e-04 (5.13e-04)	Tok/s 252198 (241572)	Loss/tok 3.2062 (3.2780)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.067 (0.088)	Data 1.08e-04 (5.05e-04)	Tok/s 229985 (241584)	Loss/tok 3.0764 (3.2784)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][500/1291]	Time 0.099 (0.088)	Data 1.08e-04 (4.97e-04)	Tok/s 252712 (241571)	Loss/tok 3.2442 (3.2763)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.036 (0.088)	Data 1.07e-04 (4.89e-04)	Tok/s 221481 (241460)	Loss/tok 2.6735 (3.2747)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.036 (0.088)	Data 1.12e-04 (4.82e-04)	Tok/s 222643 (241403)	Loss/tok 2.6448 (3.2749)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][530/1291]	Time 0.067 (0.088)	Data 1.16e-04 (4.75e-04)	Tok/s 237476 (241471)	Loss/tok 3.0216 (3.2772)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.067 (0.088)	Data 1.13e-04 (4.69e-04)	Tok/s 234998 (241418)	Loss/tok 3.1604 (3.2760)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.100 (0.088)	Data 1.29e-04 (4.62e-04)	Tok/s 252928 (241468)	Loss/tok 3.2322 (3.2769)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.067 (0.088)	Data 1.08e-04 (4.56e-04)	Tok/s 226288 (241525)	Loss/tok 3.1331 (3.2767)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.174 (0.088)	Data 1.11e-04 (4.50e-04)	Tok/s 256325 (241508)	Loss/tok 3.6548 (3.2773)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.067 (0.088)	Data 1.08e-04 (4.44e-04)	Tok/s 226688 (241534)	Loss/tok 3.0848 (3.2760)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.067 (0.088)	Data 1.10e-04 (4.39e-04)	Tok/s 232575 (241478)	Loss/tok 3.1012 (3.2750)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.136 (0.088)	Data 1.33e-04 (4.33e-04)	Tok/s 257569 (241456)	Loss/tok 3.4966 (3.2746)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.135 (0.088)	Data 1.10e-04 (4.28e-04)	Tok/s 263018 (241554)	Loss/tok 3.4285 (3.2779)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.067 (0.088)	Data 1.10e-04 (4.23e-04)	Tok/s 229431 (241507)	Loss/tok 3.0811 (3.2766)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.035 (0.087)	Data 1.08e-04 (4.18e-04)	Tok/s 224627 (241343)	Loss/tok 2.6022 (3.2744)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.067 (0.088)	Data 1.33e-04 (4.13e-04)	Tok/s 231138 (241413)	Loss/tok 3.1358 (3.2764)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.138 (0.087)	Data 1.10e-04 (4.09e-04)	Tok/s 253693 (241415)	Loss/tok 3.4463 (3.2759)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][660/1291]	Time 0.036 (0.088)	Data 1.10e-04 (4.04e-04)	Tok/s 220592 (241447)	Loss/tok 2.6353 (3.2789)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.100 (0.088)	Data 1.09e-04 (4.00e-04)	Tok/s 253988 (241480)	Loss/tok 3.3096 (3.2781)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.036 (0.088)	Data 1.12e-04 (3.96e-04)	Tok/s 222232 (241478)	Loss/tok 2.6829 (3.2782)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.92e-04)	Tok/s 228752 (241515)	Loss/tok 3.0837 (3.2785)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.136 (0.088)	Data 1.08e-04 (3.88e-04)	Tok/s 257500 (241654)	Loss/tok 3.4231 (3.2807)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][710/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.84e-04)	Tok/s 229694 (241630)	Loss/tok 3.0115 (3.2811)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.036 (0.088)	Data 1.13e-04 (3.80e-04)	Tok/s 219507 (241550)	Loss/tok 2.5925 (3.2800)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.100 (0.088)	Data 1.12e-04 (3.76e-04)	Tok/s 248826 (241575)	Loss/tok 3.2906 (3.2796)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.067 (0.088)	Data 1.11e-04 (3.73e-04)	Tok/s 232153 (241542)	Loss/tok 3.2040 (3.2786)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.69e-04)	Tok/s 251246 (241652)	Loss/tok 3.2356 (3.2801)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.100 (0.088)	Data 1.10e-04 (3.66e-04)	Tok/s 252519 (241671)	Loss/tok 3.3303 (3.2803)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.068 (0.088)	Data 1.09e-04 (3.63e-04)	Tok/s 227998 (241604)	Loss/tok 3.1465 (3.2790)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.100 (0.088)	Data 1.09e-04 (3.60e-04)	Tok/s 248292 (241637)	Loss/tok 3.3812 (3.2792)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.100 (0.088)	Data 1.29e-04 (3.57e-04)	Tok/s 252386 (241738)	Loss/tok 3.2634 (3.2798)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.067 (0.088)	Data 1.15e-04 (3.54e-04)	Tok/s 231977 (241749)	Loss/tok 3.0220 (3.2807)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.100 (0.088)	Data 1.14e-04 (3.51e-04)	Tok/s 250610 (241798)	Loss/tok 3.3006 (3.2806)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.48e-04)	Tok/s 233878 (241788)	Loss/tok 3.1568 (3.2810)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.134 (0.089)	Data 1.09e-04 (3.45e-04)	Tok/s 262197 (241858)	Loss/tok 3.4870 (3.2828)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][840/1291]	Time 0.067 (0.089)	Data 1.34e-04 (3.43e-04)	Tok/s 230195 (241888)	Loss/tok 2.9670 (3.2834)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.40e-04)	Tok/s 255196 (241948)	Loss/tok 3.2873 (3.2838)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.136 (0.089)	Data 1.11e-04 (3.37e-04)	Tok/s 257742 (241912)	Loss/tok 3.5432 (3.2829)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][870/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.35e-04)	Tok/s 257385 (241969)	Loss/tok 3.4944 (3.2838)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.32e-04)	Tok/s 233815 (241961)	Loss/tok 3.1118 (3.2833)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.30e-04)	Tok/s 221864 (241905)	Loss/tok 2.6537 (3.2825)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.27e-04)	Tok/s 251524 (242001)	Loss/tok 3.2789 (3.2835)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.25e-04)	Tok/s 232969 (241950)	Loss/tok 3.0992 (3.2824)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.136 (0.089)	Data 1.11e-04 (3.23e-04)	Tok/s 256198 (242040)	Loss/tok 3.4599 (3.2838)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.21e-04)	Tok/s 258397 (242130)	Loss/tok 3.4633 (3.2851)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.18e-04)	Tok/s 228844 (242074)	Loss/tok 3.0974 (3.2847)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.16e-04)	Tok/s 233538 (242077)	Loss/tok 3.0517 (3.2846)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.14e-04)	Tok/s 227488 (242128)	Loss/tok 3.0962 (3.2852)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.067 (0.089)	Data 1.30e-04 (3.13e-04)	Tok/s 232684 (242090)	Loss/tok 3.1252 (3.2844)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.135 (0.089)	Data 3.20e-04 (3.11e-04)	Tok/s 257602 (242072)	Loss/tok 3.4395 (3.2846)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.036 (0.089)	Data 1.09e-04 (3.09e-04)	Tok/s 221892 (242038)	Loss/tok 2.6682 (3.2839)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1000/1291]	Time 0.174 (0.089)	Data 1.10e-04 (3.07e-04)	Tok/s 254986 (242058)	Loss/tok 3.6346 (3.2845)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.067 (0.089)	Data 1.27e-04 (3.05e-04)	Tok/s 232288 (242057)	Loss/tok 3.0162 (3.2860)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.036 (0.089)	Data 1.14e-04 (3.03e-04)	Tok/s 214318 (242043)	Loss/tok 2.6402 (3.2854)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1030/1291]	Time 0.099 (0.089)	Data 1.27e-04 (3.01e-04)	Tok/s 254523 (242100)	Loss/tok 3.4137 (3.2871)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.99e-04)	Tok/s 251427 (242060)	Loss/tok 3.2767 (3.2860)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.98e-04)	Tok/s 234386 (242067)	Loss/tok 3.0953 (3.2854)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.96e-04)	Tok/s 254298 (242101)	Loss/tok 3.2410 (3.2861)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.134 (0.089)	Data 1.15e-04 (2.94e-04)	Tok/s 261671 (242127)	Loss/tok 3.4401 (3.2861)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.135 (0.089)	Data 1.17e-04 (2.93e-04)	Tok/s 258885 (242166)	Loss/tok 3.4604 (3.2865)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.91e-04)	Tok/s 233090 (242149)	Loss/tok 3.0540 (3.2863)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.89e-04)	Tok/s 235111 (242152)	Loss/tok 3.0877 (3.2863)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.88e-04)	Tok/s 233920 (242131)	Loss/tok 3.1451 (3.2854)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.067 (0.089)	Data 1.30e-04 (2.86e-04)	Tok/s 230444 (242077)	Loss/tok 3.1011 (3.2847)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.85e-04)	Tok/s 232475 (242043)	Loss/tok 2.9812 (3.2838)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.83e-04)	Tok/s 254289 (241976)	Loss/tok 3.3290 (3.2825)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.067 (0.089)	Data 1.33e-04 (2.82e-04)	Tok/s 228721 (241961)	Loss/tok 3.0660 (3.2827)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1160/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.80e-04)	Tok/s 220947 (241986)	Loss/tok 2.5895 (3.2829)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.068 (0.089)	Data 1.20e-04 (2.79e-04)	Tok/s 227913 (242022)	Loss/tok 3.1286 (3.2828)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.175 (0.089)	Data 1.13e-04 (2.78e-04)	Tok/s 255106 (242041)	Loss/tok 3.6379 (3.2830)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.77e-04)	Tok/s 229031 (242082)	Loss/tok 3.0320 (3.2826)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.067 (0.089)	Data 1.19e-04 (2.75e-04)	Tok/s 230446 (242113)	Loss/tok 3.1068 (3.2836)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.74e-04)	Tok/s 230716 (242115)	Loss/tok 3.0860 (3.2835)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.135 (0.089)	Data 1.11e-04 (2.73e-04)	Tok/s 257538 (242123)	Loss/tok 3.4886 (3.2836)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.71e-04)	Tok/s 251064 (242112)	Loss/tok 3.3069 (3.2833)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.70e-04)	Tok/s 248672 (242128)	Loss/tok 3.2613 (3.2830)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.69e-04)	Tok/s 233434 (242108)	Loss/tok 3.0301 (3.2826)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.68e-04)	Tok/s 216773 (242084)	Loss/tok 2.6987 (3.2820)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.66e-04)	Tok/s 234788 (242077)	Loss/tok 3.1292 (3.2821)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.136 (0.089)	Data 1.10e-04 (2.65e-04)	Tok/s 256816 (242124)	Loss/tok 3.5038 (3.2825)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1290/1291]	Time 0.066 (0.089)	Data 4.58e-05 (2.67e-04)	Tok/s 235078 (242101)	Loss/tok 3.0195 (3.2822)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593145614511, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593145614511, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.459 (0.459)	Decoder iters 128.0 (128.0)	Tok/s 36839 (36839)
0: Running moses detokenizer
0: BLEU(score=21.83332268735733, counts=[36678, 17793, 9886, 5675], totals=[67948, 64945, 61942, 58946], precisions=[53.97951374580562, 27.397028254677036, 15.960091698685867, 9.62745563736301], bp=1.0, sys_len=67948, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593145616541, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2183, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593145616541, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2787	Test BLEU: 21.83
0: Performance: Epoch: 2	Training: 1936830 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593145616541, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593145616541, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145616542, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2554384573
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][0/1291]	Time 0.377 (0.377)	Data 1.96e-01 (1.96e-01)	Tok/s 118509 (118509)	Loss/tok 3.5361 (3.5361)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.099 (0.136)	Data 1.38e-04 (1.80e-02)	Tok/s 251546 (237071)	Loss/tok 3.1892 (3.3307)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.035 (0.116)	Data 1.24e-04 (9.47e-03)	Tok/s 225443 (240200)	Loss/tok 2.5155 (3.2927)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.099 (0.106)	Data 1.46e-04 (6.46e-03)	Tok/s 249887 (240489)	Loss/tok 3.2457 (3.2589)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.100 (0.100)	Data 1.24e-04 (4.91e-03)	Tok/s 252057 (240722)	Loss/tok 3.2238 (3.2261)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.136 (0.099)	Data 1.17e-04 (3.97e-03)	Tok/s 252841 (241277)	Loss/tok 3.4489 (3.2256)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.067 (0.098)	Data 1.15e-04 (3.34e-03)	Tok/s 232960 (241723)	Loss/tok 3.0050 (3.2214)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.036 (0.096)	Data 1.36e-04 (2.89e-03)	Tok/s 222368 (241017)	Loss/tok 2.6104 (3.2131)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.067 (0.096)	Data 1.16e-04 (2.55e-03)	Tok/s 234437 (241407)	Loss/tok 2.9566 (3.2141)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.099 (0.092)	Data 1.18e-04 (2.28e-03)	Tok/s 257098 (240616)	Loss/tok 3.1373 (3.1970)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.099 (0.094)	Data 1.18e-04 (2.07e-03)	Tok/s 255096 (241242)	Loss/tok 3.1268 (3.2086)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.099 (0.094)	Data 1.16e-04 (1.89e-03)	Tok/s 253228 (241675)	Loss/tok 3.2703 (3.2140)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.067 (0.095)	Data 1.13e-04 (1.74e-03)	Tok/s 233522 (241844)	Loss/tok 2.9954 (3.2191)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][130/1291]	Time 0.100 (0.096)	Data 1.28e-04 (1.62e-03)	Tok/s 249077 (242186)	Loss/tok 3.2448 (3.2311)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.067 (0.095)	Data 1.12e-04 (1.51e-03)	Tok/s 228057 (242046)	Loss/tok 2.9714 (3.2246)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.100 (0.095)	Data 1.13e-04 (1.42e-03)	Tok/s 251510 (242225)	Loss/tok 3.1974 (3.2237)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.036 (0.095)	Data 1.24e-04 (1.34e-03)	Tok/s 218232 (242301)	Loss/tok 2.5924 (3.2237)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.067 (0.095)	Data 1.15e-04 (1.27e-03)	Tok/s 230033 (242456)	Loss/tok 2.9757 (3.2180)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.100 (0.094)	Data 1.13e-04 (1.21e-03)	Tok/s 254914 (242584)	Loss/tok 3.2730 (3.2149)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.100 (0.094)	Data 1.38e-04 (1.15e-03)	Tok/s 251595 (242527)	Loss/tok 3.1967 (3.2095)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.100 (0.094)	Data 1.15e-04 (1.10e-03)	Tok/s 248202 (242613)	Loss/tok 3.2098 (3.2093)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.100 (0.094)	Data 1.15e-04 (1.05e-03)	Tok/s 251657 (242813)	Loss/tok 3.1699 (3.2137)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.134 (0.093)	Data 1.13e-04 (1.01e-03)	Tok/s 256476 (242584)	Loss/tok 3.4045 (3.2108)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.036 (0.093)	Data 1.13e-04 (9.70e-04)	Tok/s 224883 (242441)	Loss/tok 2.6518 (3.2088)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.100 (0.093)	Data 1.36e-04 (9.35e-04)	Tok/s 254274 (242330)	Loss/tok 3.1688 (3.2067)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.100 (0.093)	Data 1.11e-04 (9.02e-04)	Tok/s 249256 (242458)	Loss/tok 3.2448 (3.2051)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][260/1291]	Time 0.067 (0.093)	Data 1.34e-04 (8.72e-04)	Tok/s 231052 (242675)	Loss/tok 2.9837 (3.2059)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.066 (0.092)	Data 1.15e-04 (8.45e-04)	Tok/s 234591 (242517)	Loss/tok 2.9689 (3.2016)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.067 (0.092)	Data 1.15e-04 (8.19e-04)	Tok/s 228777 (242445)	Loss/tok 2.8958 (3.1995)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.067 (0.092)	Data 1.13e-04 (7.95e-04)	Tok/s 232862 (242478)	Loss/tok 3.0413 (3.1995)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.067 (0.092)	Data 1.16e-04 (7.72e-04)	Tok/s 230565 (242346)	Loss/tok 2.9543 (3.1966)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.136 (0.092)	Data 1.15e-04 (7.51e-04)	Tok/s 258760 (242303)	Loss/tok 3.3911 (3.1970)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.100 (0.092)	Data 1.14e-04 (7.31e-04)	Tok/s 249999 (242199)	Loss/tok 3.2669 (3.1949)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.134 (0.092)	Data 1.12e-04 (7.13e-04)	Tok/s 260628 (242302)	Loss/tok 3.3326 (3.1947)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.066 (0.092)	Data 1.13e-04 (6.95e-04)	Tok/s 231125 (242233)	Loss/tok 2.9166 (3.1937)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.175 (0.092)	Data 1.16e-04 (6.79e-04)	Tok/s 255615 (242389)	Loss/tok 3.4149 (3.1967)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.099 (0.092)	Data 1.17e-04 (6.63e-04)	Tok/s 250473 (242319)	Loss/tok 3.1908 (3.1937)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.100 (0.092)	Data 1.12e-04 (6.49e-04)	Tok/s 251567 (242358)	Loss/tok 3.1527 (3.1925)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.099 (0.092)	Data 1.15e-04 (6.35e-04)	Tok/s 253540 (242432)	Loss/tok 3.2113 (3.1939)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][390/1291]	Time 0.067 (0.092)	Data 1.18e-04 (6.21e-04)	Tok/s 231450 (242263)	Loss/tok 2.9666 (3.1918)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.175 (0.092)	Data 1.13e-04 (6.09e-04)	Tok/s 255686 (242405)	Loss/tok 3.4269 (3.1921)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.067 (0.092)	Data 1.14e-04 (5.97e-04)	Tok/s 230473 (242512)	Loss/tok 2.9452 (3.1950)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.136 (0.092)	Data 1.16e-04 (5.86e-04)	Tok/s 259390 (242598)	Loss/tok 3.2394 (3.1930)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.067 (0.093)	Data 1.20e-04 (5.75e-04)	Tok/s 231373 (242591)	Loss/tok 2.9839 (3.1932)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.067 (0.092)	Data 1.15e-04 (5.64e-04)	Tok/s 228450 (242297)	Loss/tok 3.0328 (3.1898)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.067 (0.091)	Data 1.15e-04 (5.54e-04)	Tok/s 234638 (242102)	Loss/tok 2.8746 (3.1864)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.100 (0.091)	Data 1.33e-04 (5.45e-04)	Tok/s 248485 (242126)	Loss/tok 3.0898 (3.1864)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.091)	Data 1.20e-04 (5.36e-04)	Tok/s 233533 (242038)	Loss/tok 2.8738 (3.1851)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.100 (0.091)	Data 1.13e-04 (5.27e-04)	Tok/s 253561 (242082)	Loss/tok 3.1491 (3.1845)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.100 (0.091)	Data 1.13e-04 (5.19e-04)	Tok/s 251311 (242033)	Loss/tok 3.1965 (3.1829)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.100 (0.091)	Data 1.19e-04 (5.11e-04)	Tok/s 254459 (241940)	Loss/tok 3.1702 (3.1807)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.03e-04)	Tok/s 228331 (241829)	Loss/tok 2.9525 (3.1790)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][520/1291]	Time 0.037 (0.090)	Data 1.19e-04 (4.96e-04)	Tok/s 211896 (241808)	Loss/tok 2.6427 (3.1795)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.068 (0.090)	Data 1.12e-04 (4.89e-04)	Tok/s 223636 (241657)	Loss/tok 2.9569 (3.1783)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.100 (0.090)	Data 1.13e-04 (4.82e-04)	Tok/s 248109 (241596)	Loss/tok 3.2216 (3.1771)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.036 (0.090)	Data 1.17e-04 (4.75e-04)	Tok/s 223047 (241588)	Loss/tok 2.4800 (3.1767)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.036 (0.090)	Data 1.15e-04 (4.69e-04)	Tok/s 214339 (241577)	Loss/tok 2.5900 (3.1760)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.068 (0.090)	Data 1.37e-04 (4.63e-04)	Tok/s 230122 (241464)	Loss/tok 2.8830 (3.1742)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.57e-04)	Tok/s 235612 (241537)	Loss/tok 2.9957 (3.1754)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.100 (0.090)	Data 1.48e-04 (4.51e-04)	Tok/s 255473 (241611)	Loss/tok 3.1282 (3.1759)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.067 (0.090)	Data 1.33e-04 (4.46e-04)	Tok/s 232058 (241563)	Loss/tok 2.9585 (3.1753)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.100 (0.090)	Data 1.13e-04 (4.40e-04)	Tok/s 251134 (241563)	Loss/tok 3.1975 (3.1749)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][620/1291]	Time 0.175 (0.090)	Data 1.40e-04 (4.35e-04)	Tok/s 252626 (241614)	Loss/tok 3.5679 (3.1764)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.100 (0.090)	Data 1.14e-04 (4.30e-04)	Tok/s 254699 (241645)	Loss/tok 3.2394 (3.1760)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.100 (0.090)	Data 1.15e-04 (4.25e-04)	Tok/s 249894 (241631)	Loss/tok 3.1668 (3.1747)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.100 (0.090)	Data 1.17e-04 (4.21e-04)	Tok/s 252783 (241558)	Loss/tok 3.1877 (3.1729)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.066 (0.090)	Data 1.42e-04 (4.16e-04)	Tok/s 234647 (241583)	Loss/tok 3.0229 (3.1727)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.100 (0.090)	Data 1.15e-04 (4.12e-04)	Tok/s 249569 (241674)	Loss/tok 3.1646 (3.1731)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.07e-04)	Tok/s 230049 (241661)	Loss/tok 2.9507 (3.1724)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.099 (0.090)	Data 1.34e-04 (4.03e-04)	Tok/s 253801 (241713)	Loss/tok 3.0845 (3.1724)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.100 (0.090)	Data 1.16e-04 (3.99e-04)	Tok/s 253198 (241790)	Loss/tok 3.1185 (3.1718)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.134 (0.090)	Data 1.11e-04 (3.95e-04)	Tok/s 262708 (241926)	Loss/tok 3.2926 (3.1724)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.067 (0.090)	Data 1.37e-04 (3.91e-04)	Tok/s 232542 (241828)	Loss/tok 2.9192 (3.1704)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.87e-04)	Tok/s 232674 (241870)	Loss/tok 2.8721 (3.1715)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.066 (0.090)	Data 1.18e-04 (3.84e-04)	Tok/s 235053 (241910)	Loss/tok 2.9746 (3.1711)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][750/1291]	Time 0.099 (0.090)	Data 1.16e-04 (3.80e-04)	Tok/s 250765 (241909)	Loss/tok 3.2034 (3.1702)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.101 (0.090)	Data 1.17e-04 (3.77e-04)	Tok/s 251749 (241876)	Loss/tok 3.1641 (3.1691)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.067 (0.090)	Data 1.18e-04 (3.74e-04)	Tok/s 225784 (241858)	Loss/tok 2.9889 (3.1685)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.70e-04)	Tok/s 227988 (241831)	Loss/tok 2.8995 (3.1676)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.067 (0.090)	Data 1.39e-04 (3.67e-04)	Tok/s 234278 (241798)	Loss/tok 2.9128 (3.1658)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.036 (0.090)	Data 1.13e-04 (3.64e-04)	Tok/s 223766 (241772)	Loss/tok 2.5130 (3.1655)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.61e-04)	Tok/s 231766 (241784)	Loss/tok 2.9419 (3.1647)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.099 (0.090)	Data 1.13e-04 (3.58e-04)	Tok/s 251545 (241844)	Loss/tok 3.1602 (3.1656)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.135 (0.090)	Data 1.22e-04 (3.55e-04)	Tok/s 256722 (241779)	Loss/tok 3.2917 (3.1643)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.100 (0.089)	Data 1.34e-04 (3.52e-04)	Tok/s 253392 (241728)	Loss/tok 3.1636 (3.1632)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.49e-04)	Tok/s 233935 (241694)	Loss/tok 3.0030 (3.1623)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.135 (0.089)	Data 1.13e-04 (3.47e-04)	Tok/s 258209 (241755)	Loss/tok 3.2968 (3.1629)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.44e-04)	Tok/s 231366 (241692)	Loss/tok 2.9121 (3.1619)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][880/1291]	Time 0.135 (0.090)	Data 1.15e-04 (3.41e-04)	Tok/s 261944 (241764)	Loss/tok 3.1671 (3.1628)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.100 (0.090)	Data 1.15e-04 (3.39e-04)	Tok/s 250443 (241748)	Loss/tok 3.1950 (3.1629)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.36e-04)	Tok/s 233245 (241728)	Loss/tok 2.9616 (3.1618)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.175 (0.090)	Data 1.17e-04 (3.34e-04)	Tok/s 256873 (241728)	Loss/tok 3.3892 (3.1617)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.32e-04)	Tok/s 250250 (241742)	Loss/tok 3.0928 (3.1605)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.30e-04)	Tok/s 253275 (241700)	Loss/tok 3.1173 (3.1593)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.067 (0.089)	Data 1.37e-04 (3.27e-04)	Tok/s 228992 (241694)	Loss/tok 2.9344 (3.1588)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.100 (0.089)	Data 1.28e-04 (3.25e-04)	Tok/s 251306 (241641)	Loss/tok 3.1796 (3.1576)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.23e-04)	Tok/s 231860 (241599)	Loss/tok 2.8994 (3.1564)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.21e-04)	Tok/s 254455 (241636)	Loss/tok 3.0565 (3.1562)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.19e-04)	Tok/s 232137 (241627)	Loss/tok 2.9214 (3.1556)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.036 (0.089)	Data 1.16e-04 (3.17e-04)	Tok/s 221192 (241590)	Loss/tok 2.5566 (3.1541)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.15e-04)	Tok/s 252786 (241579)	Loss/tok 3.0931 (3.1539)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1010/1291]	Time 0.067 (0.089)	Data 1.34e-04 (3.13e-04)	Tok/s 235239 (241554)	Loss/tok 2.9763 (3.1538)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.11e-04)	Tok/s 252864 (241581)	Loss/tok 3.1087 (3.1532)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.099 (0.089)	Data 1.18e-04 (3.09e-04)	Tok/s 256179 (241531)	Loss/tok 3.1156 (3.1519)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.07e-04)	Tok/s 251042 (241500)	Loss/tok 3.0549 (3.1506)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.05e-04)	Tok/s 232000 (241569)	Loss/tok 2.9680 (3.1508)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.066 (0.088)	Data 1.26e-04 (3.04e-04)	Tok/s 231922 (241492)	Loss/tok 2.9957 (3.1498)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.02e-04)	Tok/s 249691 (241587)	Loss/tok 3.0597 (3.1518)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.036 (0.089)	Data 1.17e-04 (3.00e-04)	Tok/s 221365 (241607)	Loss/tok 2.5723 (3.1517)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.135 (0.089)	Data 1.15e-04 (2.99e-04)	Tok/s 259540 (241625)	Loss/tok 3.2400 (3.1515)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.97e-04)	Tok/s 254102 (241643)	Loss/tok 3.1230 (3.1512)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.95e-04)	Tok/s 234577 (241650)	Loss/tok 2.9846 (3.1511)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.94e-04)	Tok/s 262907 (241728)	Loss/tok 3.2160 (3.1518)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.134 (0.089)	Data 1.15e-04 (2.92e-04)	Tok/s 261340 (241718)	Loss/tok 3.2059 (3.1516)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1140/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.91e-04)	Tok/s 252202 (241711)	Loss/tok 3.1376 (3.1509)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.89e-04)	Tok/s 231487 (241699)	Loss/tok 2.9803 (3.1505)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.101 (0.089)	Data 1.10e-04 (2.88e-04)	Tok/s 249312 (241676)	Loss/tok 3.0951 (3.1495)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.86e-04)	Tok/s 228658 (241621)	Loss/tok 2.9203 (3.1492)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.036 (0.089)	Data 1.15e-04 (2.85e-04)	Tok/s 222292 (241612)	Loss/tok 2.5705 (3.1489)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.135 (0.089)	Data 1.37e-04 (2.83e-04)	Tok/s 258221 (241634)	Loss/tok 3.2240 (3.1488)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.82e-04)	Tok/s 233561 (241670)	Loss/tok 2.8589 (3.1494)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.036 (0.089)	Data 1.17e-04 (2.81e-04)	Tok/s 224251 (241665)	Loss/tok 2.5941 (3.1493)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.79e-04)	Tok/s 250141 (241713)	Loss/tok 3.1362 (3.1489)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.78e-04)	Tok/s 232154 (241727)	Loss/tok 2.8768 (3.1483)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.036 (0.089)	Data 1.36e-04 (2.77e-04)	Tok/s 219013 (241681)	Loss/tok 2.5128 (3.1474)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.067 (0.089)	Data 1.27e-04 (2.76e-04)	Tok/s 227307 (241710)	Loss/tok 2.9485 (3.1469)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1260/1291]	Time 0.137 (0.089)	Data 1.29e-04 (2.74e-04)	Tok/s 254763 (241761)	Loss/tok 3.3259 (3.1471)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.73e-04)	Tok/s 236100 (241776)	Loss/tok 3.0339 (3.1474)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.175 (0.089)	Data 1.13e-04 (2.72e-04)	Tok/s 254701 (241797)	Loss/tok 3.4825 (3.1475)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.067 (0.089)	Data 4.32e-05 (2.73e-04)	Tok/s 235559 (241769)	Loss/tok 2.8809 (3.1473)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593145732351, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593145732351, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.387 (0.387)	Decoder iters 104.0 (104.0)	Tok/s 42578 (42578)
0: Running moses detokenizer
0: BLEU(score=24.272965621925895, counts=[37187, 18689, 10747, 6391], totals=[65491, 62488, 59485, 56488], precisions=[56.78184788749599, 29.90814236333376, 18.066739514163235, 11.313907378558278], bp=1.0, sys_len=65491, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593145734253, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2427, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593145734254, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1471	Test BLEU: 24.27
0: Performance: Epoch: 3	Training: 1934398 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593145734254, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593145734254, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 04:29:01 AM
RESULT,RNN_TRANSLATOR,,505,nvidia,2020-06-26 04:20:36 AM
