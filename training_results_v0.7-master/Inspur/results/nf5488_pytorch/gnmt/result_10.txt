Beginning trial 10 of 10
:::MLLOG {"namespace": "", "time_ms": 1593145742350, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593145742397, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593145742397, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593145742397, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593145742397, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593145744714, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 04:29:05 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593145747292, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145747292, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145747295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145747299, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145747321, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145747333, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145747335, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593145747336, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3652179874
:::MLLOG {"namespace": "", "time_ms": 1593145755902, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3652179874, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 702339652
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593145769582, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593145769584, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593145769584, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593145769584, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593145769584, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593145771552, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593145771552, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593145771553, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593145771888, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593145771888, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593145771889, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593145771889, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593145771890, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593145771890, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593145771890, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593145771890, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593145771890, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593145771890, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593145771890, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145771890, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2816736566
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.349 (0.349)	Data 2.73e-01 (2.73e-01)	Tok/s 44422 (44422)	Loss/tok 10.6013 (10.6013)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.103 (0.121)	Data 1.25e-02 (3.60e-02)	Tok/s 151881 (189669)	Loss/tok 9.4137 (10.0009)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.067 (0.110)	Data 1.08e-04 (1.89e-02)	Tok/s 230254 (208354)	Loss/tok 9.0701 (9.6611)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.099 (0.110)	Data 1.63e-04 (1.35e-02)	Tok/s 255820 (209796)	Loss/tok 8.8849 (9.4321)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.133 (0.112)	Data 1.08e-04 (1.02e-02)	Tok/s 263232 (221098)	Loss/tok 8.5894 (9.2190)	LR 7.390e-05
0: TRAIN [0][50/1291]	Time 0.099 (0.110)	Data 1.27e-04 (8.23e-03)	Tok/s 253796 (227209)	Loss/tok 8.3003 (9.0560)	LR 9.303e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][60/1291]	Time 0.099 (0.108)	Data 1.05e-04 (6.90e-03)	Tok/s 254649 (230143)	Loss/tok 8.1926 (8.9272)	LR 1.145e-04
0: TRAIN [0][70/1291]	Time 0.133 (0.104)	Data 1.06e-04 (5.94e-03)	Tok/s 265349 (231540)	Loss/tok 8.1669 (8.8277)	LR 1.441e-04
0: TRAIN [0][80/1291]	Time 0.067 (0.101)	Data 1.11e-04 (5.22e-03)	Tok/s 229663 (232587)	Loss/tok 7.8754 (8.7393)	LR 1.814e-04
0: TRAIN [0][90/1291]	Time 0.068 (0.098)	Data 1.06e-04 (4.66e-03)	Tok/s 221496 (232793)	Loss/tok 7.7142 (8.6648)	LR 2.284e-04
0: TRAIN [0][100/1291]	Time 0.100 (0.096)	Data 1.08e-04 (4.21e-03)	Tok/s 249282 (233181)	Loss/tok 7.9689 (8.5947)	LR 2.875e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][110/1291]	Time 0.099 (0.095)	Data 1.03e-04 (3.84e-03)	Tok/s 253139 (234046)	Loss/tok 8.2126 (8.5776)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.134 (0.095)	Data 1.50e-04 (3.53e-03)	Tok/s 256480 (235013)	Loss/tok 8.0723 (8.5214)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.067 (0.096)	Data 1.11e-04 (3.27e-03)	Tok/s 231992 (236053)	Loss/tok 7.6620 (8.4644)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.098 (0.096)	Data 1.04e-04 (3.05e-03)	Tok/s 257883 (236850)	Loss/tok 7.7247 (8.4135)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.099 (0.095)	Data 1.12e-04 (2.85e-03)	Tok/s 256127 (237697)	Loss/tok 7.5776 (8.3625)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.099 (0.096)	Data 1.09e-04 (2.68e-03)	Tok/s 251349 (238289)	Loss/tok 7.6697 (8.3176)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.067 (0.095)	Data 1.26e-04 (2.53e-03)	Tok/s 230063 (238509)	Loss/tok 7.1577 (8.2676)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.036 (0.094)	Data 1.16e-04 (2.40e-03)	Tok/s 213447 (238585)	Loss/tok 6.1901 (8.2147)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.036 (0.093)	Data 1.07e-04 (2.28e-03)	Tok/s 216787 (238760)	Loss/tok 6.2170 (8.1578)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.099 (0.094)	Data 1.07e-04 (2.17e-03)	Tok/s 253244 (239427)	Loss/tok 6.8515 (8.0868)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.066 (0.093)	Data 1.28e-04 (2.07e-03)	Tok/s 232512 (239541)	Loss/tok 6.4765 (8.0310)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.066 (0.093)	Data 1.22e-04 (1.99e-03)	Tok/s 234375 (239537)	Loss/tok 6.2824 (7.9738)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.066 (0.092)	Data 1.16e-04 (1.90e-03)	Tok/s 231144 (239525)	Loss/tok 6.0621 (7.9129)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][240/1291]	Time 0.067 (0.092)	Data 1.08e-04 (1.83e-03)	Tok/s 229174 (239645)	Loss/tok 5.9395 (7.8469)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.067 (0.091)	Data 1.05e-04 (1.76e-03)	Tok/s 233645 (239718)	Loss/tok 5.8075 (7.7831)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.066 (0.091)	Data 1.07e-04 (1.70e-03)	Tok/s 236689 (239884)	Loss/tok 5.6816 (7.7136)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.066 (0.091)	Data 1.07e-04 (1.64e-03)	Tok/s 233410 (239906)	Loss/tok 5.5208 (7.6454)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.067 (0.091)	Data 1.21e-04 (1.59e-03)	Tok/s 231763 (239851)	Loss/tok 5.3658 (7.5855)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.134 (0.091)	Data 1.08e-04 (1.53e-03)	Tok/s 260135 (239975)	Loss/tok 5.8162 (7.5144)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.100 (0.091)	Data 1.23e-04 (1.49e-03)	Tok/s 254803 (239988)	Loss/tok 5.5133 (7.4504)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.134 (0.091)	Data 1.16e-04 (1.44e-03)	Tok/s 261143 (240213)	Loss/tok 5.6195 (7.3788)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.100 (0.091)	Data 1.07e-04 (1.40e-03)	Tok/s 248904 (240482)	Loss/tok 5.3619 (7.3055)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.036 (0.091)	Data 1.21e-04 (1.36e-03)	Tok/s 220484 (240536)	Loss/tok 4.0744 (7.2428)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.099 (0.091)	Data 1.08e-04 (1.33e-03)	Tok/s 257757 (240776)	Loss/tok 4.9589 (7.1716)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.036 (0.091)	Data 1.05e-04 (1.29e-03)	Tok/s 220895 (240695)	Loss/tok 3.8307 (7.1163)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.067 (0.091)	Data 1.06e-04 (1.26e-03)	Tok/s 233208 (240810)	Loss/tok 4.4937 (7.0508)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][370/1291]	Time 0.036 (0.090)	Data 1.04e-04 (1.23e-03)	Tok/s 226600 (240890)	Loss/tok 3.6384 (6.9920)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.067 (0.091)	Data 1.07e-04 (1.20e-03)	Tok/s 230342 (241176)	Loss/tok 4.3244 (6.9191)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.067 (0.090)	Data 1.13e-04 (1.17e-03)	Tok/s 231205 (240997)	Loss/tok 4.2329 (6.8713)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.099 (0.091)	Data 1.17e-04 (1.14e-03)	Tok/s 256084 (241044)	Loss/tok 4.5556 (6.8074)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.134 (0.091)	Data 1.12e-04 (1.12e-03)	Tok/s 260859 (241077)	Loss/tok 4.8497 (6.7514)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.174 (0.091)	Data 1.12e-04 (1.10e-03)	Tok/s 256457 (241063)	Loss/tok 4.8941 (6.6983)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.134 (0.090)	Data 1.54e-04 (1.07e-03)	Tok/s 260712 (240990)	Loss/tok 4.7068 (6.6503)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.067 (0.091)	Data 1.10e-04 (1.05e-03)	Tok/s 231898 (241099)	Loss/tok 4.0545 (6.5927)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.134 (0.091)	Data 1.12e-04 (1.03e-03)	Tok/s 259978 (241275)	Loss/tok 4.4528 (6.5365)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.067 (0.091)	Data 1.25e-04 (1.01e-03)	Tok/s 232037 (241360)	Loss/tok 4.1161 (6.4866)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.036 (0.090)	Data 1.11e-04 (9.92e-04)	Tok/s 221519 (241152)	Loss/tok 3.3645 (6.4492)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.099 (0.090)	Data 1.11e-04 (9.73e-04)	Tok/s 256537 (241231)	Loss/tok 4.2606 (6.4030)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][490/1291]	Time 0.099 (0.090)	Data 1.12e-04 (9.56e-04)	Tok/s 255549 (241247)	Loss/tok 4.1411 (6.3609)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.067 (0.090)	Data 1.12e-04 (9.39e-04)	Tok/s 233671 (241359)	Loss/tok 3.8681 (6.3137)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.067 (0.090)	Data 1.09e-04 (9.23e-04)	Tok/s 231193 (241385)	Loss/tok 3.9107 (6.2722)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.067 (0.090)	Data 1.12e-04 (9.08e-04)	Tok/s 233286 (241466)	Loss/tok 3.7886 (6.2291)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.066 (0.090)	Data 1.15e-04 (8.93e-04)	Tok/s 226697 (241540)	Loss/tok 3.9408 (6.1887)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][540/1291]	Time 0.067 (0.091)	Data 1.12e-04 (8.78e-04)	Tok/s 231836 (241698)	Loss/tok 3.7535 (6.1433)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.066 (0.090)	Data 1.10e-04 (8.64e-04)	Tok/s 233081 (241641)	Loss/tok 3.7632 (6.1103)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.066 (0.090)	Data 1.09e-04 (8.51e-04)	Tok/s 232832 (241716)	Loss/tok 3.7059 (6.0732)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.135 (0.090)	Data 1.12e-04 (8.38e-04)	Tok/s 259846 (241716)	Loss/tok 4.2542 (6.0383)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.099 (0.090)	Data 1.13e-04 (8.26e-04)	Tok/s 255935 (241841)	Loss/tok 3.9311 (6.0010)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.067 (0.090)	Data 1.17e-04 (8.14e-04)	Tok/s 231112 (241845)	Loss/tok 3.7267 (5.9685)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.099 (0.090)	Data 1.13e-04 (8.02e-04)	Tok/s 259280 (241970)	Loss/tok 3.8822 (5.9317)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.066 (0.090)	Data 1.10e-04 (7.91e-04)	Tok/s 232406 (241838)	Loss/tok 3.6331 (5.9062)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.099 (0.090)	Data 1.21e-04 (7.80e-04)	Tok/s 253878 (241838)	Loss/tok 3.9764 (5.8756)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.067 (0.090)	Data 1.11e-04 (7.69e-04)	Tok/s 232012 (241857)	Loss/tok 3.6497 (5.8426)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.067 (0.090)	Data 1.07e-04 (7.59e-04)	Tok/s 230336 (241852)	Loss/tok 3.7483 (5.8141)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.066 (0.090)	Data 1.11e-04 (7.49e-04)	Tok/s 236010 (241934)	Loss/tok 3.6607 (5.7833)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.099 (0.090)	Data 1.14e-04 (7.40e-04)	Tok/s 258472 (241924)	Loss/tok 3.9624 (5.7572)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][670/1291]	Time 0.099 (0.090)	Data 1.30e-04 (7.30e-04)	Tok/s 252650 (241902)	Loss/tok 3.8988 (5.7314)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.067 (0.090)	Data 1.12e-04 (7.21e-04)	Tok/s 231290 (241903)	Loss/tok 3.6258 (5.7054)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.100 (0.090)	Data 1.35e-04 (7.13e-04)	Tok/s 252282 (241883)	Loss/tok 3.8896 (5.6796)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.099 (0.090)	Data 1.34e-04 (7.04e-04)	Tok/s 252654 (241876)	Loss/tok 3.9522 (5.6555)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.067 (0.089)	Data 1.32e-04 (6.96e-04)	Tok/s 229800 (241752)	Loss/tok 3.5938 (5.6351)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.066 (0.089)	Data 1.12e-04 (6.88e-04)	Tok/s 230827 (241732)	Loss/tok 3.5342 (5.6114)	LR 2.875e-03
^[[B^[[B^[[B^[[B^[[B^[[B0: TRAIN [0][730/1291]	Time 0.172 (0.089)	Data 1.35e-04 (6.80e-04)	Tok/s 257008 (241719)	Loss/tok 4.2763 (5.5871)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][740/1291]	Time 0.099 (0.090)	Data 1.12e-04 (6.73e-04)	Tok/s 255166 (241788)	Loss/tok 3.9031 (5.5614)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.135 (0.090)	Data 1.34e-04 (6.65e-04)	Tok/s 260462 (241903)	Loss/tok 3.9132 (5.5346)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.066 (0.090)	Data 1.13e-04 (6.58e-04)	Tok/s 235673 (241890)	Loss/tok 3.5049 (5.5129)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.036 (0.090)	Data 1.10e-04 (6.51e-04)	Tok/s 218921 (241977)	Loss/tok 3.0025 (5.4882)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.099 (0.090)	Data 1.19e-04 (6.44e-04)	Tok/s 255500 (242043)	Loss/tok 3.8043 (5.4646)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.099 (0.090)	Data 1.37e-04 (6.37e-04)	Tok/s 254166 (242031)	Loss/tok 3.8910 (5.4453)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.099 (0.090)	Data 1.14e-04 (6.31e-04)	Tok/s 252823 (242047)	Loss/tok 3.9218 (5.4252)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.099 (0.090)	Data 4.65e-04 (6.25e-04)	Tok/s 257113 (242197)	Loss/tok 3.8078 (5.4007)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.066 (0.090)	Data 1.34e-04 (6.19e-04)	Tok/s 231781 (242146)	Loss/tok 3.5640 (5.3836)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.067 (0.090)	Data 1.11e-04 (6.13e-04)	Tok/s 234687 (242057)	Loss/tok 3.3757 (5.3673)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.066 (0.090)	Data 1.09e-04 (6.07e-04)	Tok/s 238322 (242088)	Loss/tok 3.5358 (5.3476)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.100 (0.090)	Data 1.10e-04 (6.01e-04)	Tok/s 255543 (242125)	Loss/tok 3.7767 (5.3282)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][860/1291]	Time 0.067 (0.089)	Data 1.24e-04 (5.95e-04)	Tok/s 230372 (242087)	Loss/tok 3.4045 (5.3114)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.099 (0.089)	Data 1.12e-04 (5.90e-04)	Tok/s 254025 (242033)	Loss/tok 3.6938 (5.2960)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.099 (0.089)	Data 1.14e-04 (5.84e-04)	Tok/s 254095 (242027)	Loss/tok 3.7157 (5.2781)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.100 (0.090)	Data 1.12e-04 (5.79e-04)	Tok/s 254597 (242147)	Loss/tok 3.7158 (5.2574)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.173 (0.090)	Data 1.10e-04 (5.74e-04)	Tok/s 259004 (242158)	Loss/tok 4.1932 (5.2400)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.099 (0.090)	Data 1.18e-04 (5.69e-04)	Tok/s 251314 (242189)	Loss/tok 3.6827 (5.2230)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.067 (0.089)	Data 1.28e-04 (5.64e-04)	Tok/s 226912 (242193)	Loss/tok 3.4779 (5.2073)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.099 (0.089)	Data 1.13e-04 (5.59e-04)	Tok/s 254399 (242157)	Loss/tok 3.8023 (5.1932)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.066 (0.089)	Data 1.12e-04 (5.55e-04)	Tok/s 232852 (242131)	Loss/tok 3.4132 (5.1784)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.067 (0.089)	Data 1.11e-04 (5.50e-04)	Tok/s 226734 (242154)	Loss/tok 3.3376 (5.1622)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.135 (0.089)	Data 1.09e-04 (5.45e-04)	Tok/s 259351 (242251)	Loss/tok 3.8735 (5.1442)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.134 (0.089)	Data 1.12e-04 (5.41e-04)	Tok/s 260364 (242239)	Loss/tok 4.0002 (5.1303)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.035 (0.089)	Data 1.12e-04 (5.37e-04)	Tok/s 217026 (242258)	Loss/tok 2.9502 (5.1149)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][990/1291]	Time 0.135 (0.090)	Data 1.13e-04 (5.32e-04)	Tok/s 258370 (242372)	Loss/tok 3.8586 (5.0970)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.067 (0.089)	Data 1.11e-04 (5.28e-04)	Tok/s 229900 (242279)	Loss/tok 3.3476 (5.0854)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.067 (0.089)	Data 1.11e-04 (5.24e-04)	Tok/s 230812 (242296)	Loss/tok 3.3948 (5.0714)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.20e-04)	Tok/s 253652 (242247)	Loss/tok 3.7026 (5.0597)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.067 (0.089)	Data 1.13e-04 (5.16e-04)	Tok/s 232437 (242225)	Loss/tok 3.3604 (5.0465)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.12e-04)	Tok/s 256189 (242170)	Loss/tok 3.7290 (5.0348)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.036 (0.089)	Data 1.15e-04 (5.09e-04)	Tok/s 222292 (242128)	Loss/tok 2.9340 (5.0232)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.066 (0.089)	Data 1.11e-04 (5.05e-04)	Tok/s 236071 (242097)	Loss/tok 3.3375 (5.0110)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.036 (0.089)	Data 1.10e-04 (5.01e-04)	Tok/s 224779 (242058)	Loss/tok 2.8641 (4.9995)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1080/1291]	Time 0.066 (0.089)	Data 1.18e-04 (4.98e-04)	Tok/s 237009 (242082)	Loss/tok 3.3926 (4.9866)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.94e-04)	Tok/s 253532 (242164)	Loss/tok 3.6279 (4.9722)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.066 (0.089)	Data 1.15e-04 (4.91e-04)	Tok/s 229837 (242143)	Loss/tok 3.3402 (4.9609)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.066 (0.089)	Data 1.10e-04 (4.87e-04)	Tok/s 233702 (242210)	Loss/tok 3.3072 (4.9468)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.135 (0.089)	Data 1.13e-04 (4.84e-04)	Tok/s 258448 (242247)	Loss/tok 3.7469 (4.9340)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.100 (0.089)	Data 1.17e-04 (4.81e-04)	Tok/s 251864 (242325)	Loss/tok 3.6037 (4.9203)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.099 (0.089)	Data 1.13e-04 (4.78e-04)	Tok/s 254903 (242335)	Loss/tok 3.6609 (4.9089)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.067 (0.089)	Data 1.15e-04 (4.74e-04)	Tok/s 229579 (242317)	Loss/tok 3.3480 (4.8981)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.71e-04)	Tok/s 231050 (242292)	Loss/tok 3.4331 (4.8876)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.066 (0.089)	Data 1.15e-04 (4.68e-04)	Tok/s 235390 (242290)	Loss/tok 3.4102 (4.8768)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.135 (0.089)	Data 1.13e-04 (4.65e-04)	Tok/s 260306 (242326)	Loss/tok 3.7736 (4.8653)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.066 (0.089)	Data 1.13e-04 (4.62e-04)	Tok/s 234465 (242283)	Loss/tok 3.4237 (4.8554)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.036 (0.089)	Data 1.13e-04 (4.59e-04)	Tok/s 220949 (242234)	Loss/tok 2.8248 (4.8461)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1210/1291]	Time 0.175 (0.089)	Data 1.13e-04 (4.57e-04)	Tok/s 253457 (242285)	Loss/tok 3.8994 (4.8339)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.067 (0.089)	Data 1.26e-04 (4.54e-04)	Tok/s 233720 (242267)	Loss/tok 3.3221 (4.8240)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.099 (0.089)	Data 1.26e-04 (4.51e-04)	Tok/s 254511 (242283)	Loss/tok 3.4905 (4.8135)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.174 (0.089)	Data 1.12e-04 (4.48e-04)	Tok/s 254804 (242304)	Loss/tok 3.9621 (4.8030)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.100 (0.089)	Data 1.08e-04 (4.46e-04)	Tok/s 254039 (242303)	Loss/tok 3.6244 (4.7935)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.43e-04)	Tok/s 233624 (242298)	Loss/tok 3.3796 (4.7839)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.067 (0.089)	Data 1.15e-04 (4.40e-04)	Tok/s 232257 (242266)	Loss/tok 3.3821 (4.7751)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.100 (0.089)	Data 1.09e-04 (4.38e-04)	Tok/s 251538 (242225)	Loss/tok 3.4946 (4.7668)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.100 (0.089)	Data 4.94e-05 (4.38e-04)	Tok/s 250811 (242266)	Loss/tok 3.4241 (4.7565)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593145887488, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145887488, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.465 (0.465)	Decoder iters 134.0 (134.0)	Tok/s 34264 (34264)
0: Running moses detokenizer
0: BLEU(score=19.702346892525693, counts=[34049, 15538, 8194, 4517], totals=[64118, 61115, 58112, 55114], precisions=[53.10365264044418, 25.42420027816412, 14.100357929515418, 8.195739739449142], bp=0.9913350545287655, sys_len=64118, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593145889481, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.19699999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145889481, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7581	Test BLEU: 19.70
0: Performance: Epoch: 0	Training: 1938103 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593145889482, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145889482, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145889482, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3513124564
0: TRAIN [1][0/1291]	Time 0.275 (0.275)	Data 1.94e-01 (1.94e-01)	Tok/s 56144 (56144)	Loss/tok 3.3271 (3.3271)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.066 (0.089)	Data 1.30e-04 (1.77e-02)	Tok/s 234764 (219177)	Loss/tok 3.2428 (3.3321)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.134 (0.092)	Data 1.32e-04 (9.33e-03)	Tok/s 263004 (233869)	Loss/tok 3.7552 (3.4433)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.098 (0.093)	Data 1.14e-04 (6.36e-03)	Tok/s 258321 (237410)	Loss/tok 3.5822 (3.4681)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][40/1291]	Time 0.067 (0.092)	Data 1.34e-04 (4.84e-03)	Tok/s 232025 (239660)	Loss/tok 3.3261 (3.4604)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.91e-03)	Tok/s 229363 (239719)	Loss/tok 3.3152 (3.4496)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.066 (0.089)	Data 1.15e-04 (3.29e-03)	Tok/s 233406 (240239)	Loss/tok 3.1861 (3.4512)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.099 (0.087)	Data 1.11e-04 (2.84e-03)	Tok/s 249618 (239882)	Loss/tok 3.5296 (3.4374)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.067 (0.088)	Data 1.19e-04 (2.51e-03)	Tok/s 232692 (240723)	Loss/tok 3.2083 (3.4544)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][90/1291]	Time 0.098 (0.090)	Data 1.14e-04 (2.24e-03)	Tok/s 256254 (241737)	Loss/tok 3.5617 (3.4789)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.067 (0.090)	Data 1.12e-04 (2.03e-03)	Tok/s 229907 (241795)	Loss/tok 3.2754 (3.4754)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.099 (0.088)	Data 1.36e-04 (1.86e-03)	Tok/s 254357 (241438)	Loss/tok 3.4174 (3.4640)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.100 (0.089)	Data 1.27e-04 (1.72e-03)	Tok/s 252292 (241761)	Loss/tok 3.4151 (3.4725)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.067 (0.090)	Data 1.15e-04 (1.59e-03)	Tok/s 236436 (242257)	Loss/tok 3.3198 (3.4744)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.067 (0.088)	Data 1.28e-04 (1.49e-03)	Tok/s 231757 (241711)	Loss/tok 3.2179 (3.4668)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.099 (0.088)	Data 1.16e-04 (1.40e-03)	Tok/s 251760 (241851)	Loss/tok 3.4922 (3.4680)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.099 (0.089)	Data 1.11e-04 (1.32e-03)	Tok/s 254107 (242171)	Loss/tok 3.5007 (3.4757)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.099 (0.089)	Data 1.13e-04 (1.25e-03)	Tok/s 254877 (242296)	Loss/tok 3.3660 (3.4774)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.067 (0.089)	Data 1.16e-04 (1.19e-03)	Tok/s 231678 (242103)	Loss/tok 3.2018 (3.4724)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.099 (0.089)	Data 1.10e-04 (1.13e-03)	Tok/s 254579 (242131)	Loss/tok 3.4917 (3.4707)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.067 (0.089)	Data 1.26e-04 (1.08e-03)	Tok/s 232994 (242245)	Loss/tok 3.1355 (3.4690)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][210/1291]	Time 0.036 (0.088)	Data 1.15e-04 (1.03e-03)	Tok/s 219574 (241975)	Loss/tok 2.7039 (3.4661)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.067 (0.089)	Data 1.12e-04 (9.92e-04)	Tok/s 232862 (242164)	Loss/tok 3.2669 (3.4713)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.036 (0.088)	Data 1.14e-04 (9.54e-04)	Tok/s 222086 (241882)	Loss/tok 2.8063 (3.4675)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.135 (0.088)	Data 1.13e-04 (9.20e-04)	Tok/s 258870 (241907)	Loss/tok 3.6369 (3.4690)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.067 (0.088)	Data 1.08e-04 (8.88e-04)	Tok/s 234431 (241672)	Loss/tok 3.1340 (3.4662)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.100 (0.087)	Data 1.14e-04 (8.58e-04)	Tok/s 255374 (241679)	Loss/tok 3.4296 (3.4634)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.067 (0.088)	Data 1.11e-04 (8.31e-04)	Tok/s 233767 (241715)	Loss/tok 3.3056 (3.4643)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.135 (0.087)	Data 1.33e-04 (8.05e-04)	Tok/s 260820 (241655)	Loss/tok 3.6012 (3.4622)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.036 (0.087)	Data 1.12e-04 (7.82e-04)	Tok/s 220294 (241710)	Loss/tok 2.7626 (3.4632)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.134 (0.087)	Data 1.34e-04 (7.59e-04)	Tok/s 261322 (241871)	Loss/tok 3.7328 (3.4637)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.036 (0.087)	Data 1.17e-04 (7.39e-04)	Tok/s 225040 (241743)	Loss/tok 2.7810 (3.4599)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.099 (0.087)	Data 1.16e-04 (7.19e-04)	Tok/s 255288 (241571)	Loss/tok 3.3892 (3.4585)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.067 (0.087)	Data 1.12e-04 (7.01e-04)	Tok/s 228127 (241520)	Loss/tok 3.2493 (3.4574)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][340/1291]	Time 0.036 (0.087)	Data 1.35e-04 (6.84e-04)	Tok/s 223934 (241716)	Loss/tok 2.7448 (3.4602)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][350/1291]	Time 0.136 (0.088)	Data 1.13e-04 (6.68e-04)	Tok/s 256736 (241940)	Loss/tok 3.6718 (3.4622)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.067 (0.088)	Data 1.10e-04 (6.52e-04)	Tok/s 232691 (241863)	Loss/tok 3.2195 (3.4627)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.067 (0.087)	Data 1.14e-04 (6.38e-04)	Tok/s 232367 (241862)	Loss/tok 3.2205 (3.4599)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.100 (0.087)	Data 1.16e-04 (6.24e-04)	Tok/s 250912 (241831)	Loss/tok 3.5568 (3.4581)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.066 (0.087)	Data 1.12e-04 (6.11e-04)	Tok/s 236337 (241824)	Loss/tok 3.2433 (3.4558)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.099 (0.087)	Data 1.28e-04 (5.99e-04)	Tok/s 254928 (241946)	Loss/tok 3.4428 (3.4582)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.100 (0.087)	Data 1.38e-04 (5.87e-04)	Tok/s 252477 (242031)	Loss/tok 3.4721 (3.4577)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.088)	Data 1.14e-04 (5.76e-04)	Tok/s 228781 (242040)	Loss/tok 3.2342 (3.4599)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.066 (0.088)	Data 1.30e-04 (5.65e-04)	Tok/s 232359 (242051)	Loss/tok 3.1022 (3.4597)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.067 (0.088)	Data 1.23e-04 (5.55e-04)	Tok/s 231850 (242116)	Loss/tok 3.2629 (3.4614)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.174 (0.088)	Data 1.14e-04 (5.46e-04)	Tok/s 256008 (242227)	Loss/tok 3.7782 (3.4651)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.036 (0.088)	Data 1.27e-04 (5.36e-04)	Tok/s 219212 (242166)	Loss/tok 2.8210 (3.4644)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.067 (0.088)	Data 1.17e-04 (5.27e-04)	Tok/s 227771 (242114)	Loss/tok 3.1157 (3.4649)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][480/1291]	Time 0.066 (0.088)	Data 1.11e-04 (5.19e-04)	Tok/s 237535 (242012)	Loss/tok 3.2357 (3.4626)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.067 (0.088)	Data 1.12e-04 (5.11e-04)	Tok/s 229437 (242057)	Loss/tok 3.2739 (3.4622)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.067 (0.088)	Data 1.33e-04 (5.03e-04)	Tok/s 230777 (241871)	Loss/tok 3.2419 (3.4595)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.035 (0.087)	Data 1.12e-04 (4.95e-04)	Tok/s 222042 (241770)	Loss/tok 2.8101 (3.4569)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.099 (0.087)	Data 1.13e-04 (4.88e-04)	Tok/s 255215 (241762)	Loss/tok 3.4545 (3.4561)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][530/1291]	Time 0.134 (0.088)	Data 1.11e-04 (4.81e-04)	Tok/s 261543 (241878)	Loss/tok 3.6777 (3.4575)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.099 (0.088)	Data 1.11e-04 (4.74e-04)	Tok/s 253480 (241898)	Loss/tok 3.5760 (3.4577)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.099 (0.088)	Data 1.11e-04 (4.68e-04)	Tok/s 253570 (241957)	Loss/tok 3.4252 (3.4588)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.099 (0.088)	Data 1.14e-04 (4.62e-04)	Tok/s 251120 (242082)	Loss/tok 3.4210 (3.4580)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.099 (0.088)	Data 1.12e-04 (4.55e-04)	Tok/s 252669 (242162)	Loss/tok 3.5110 (3.4587)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.068 (0.089)	Data 1.13e-04 (4.50e-04)	Tok/s 228260 (242264)	Loss/tok 3.1788 (3.4587)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.134 (0.089)	Data 1.12e-04 (4.44e-04)	Tok/s 257884 (242295)	Loss/tok 3.6186 (3.4597)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.135 (0.089)	Data 1.17e-04 (4.39e-04)	Tok/s 258864 (242375)	Loss/tok 3.6650 (3.4604)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.33e-04)	Tok/s 232466 (242339)	Loss/tok 3.2279 (3.4592)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.099 (0.088)	Data 1.16e-04 (4.28e-04)	Tok/s 253904 (242209)	Loss/tok 3.4105 (3.4566)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.099 (0.088)	Data 1.14e-04 (4.23e-04)	Tok/s 257051 (242172)	Loss/tok 3.3920 (3.4549)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.099 (0.088)	Data 1.15e-04 (4.18e-04)	Tok/s 253278 (242159)	Loss/tok 3.5458 (3.4542)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.066 (0.088)	Data 1.13e-04 (4.14e-04)	Tok/s 233668 (242180)	Loss/tok 3.1808 (3.4534)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][660/1291]	Time 0.099 (0.088)	Data 1.13e-04 (4.09e-04)	Tok/s 255366 (242178)	Loss/tok 3.4523 (3.4524)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.099 (0.088)	Data 1.14e-04 (4.05e-04)	Tok/s 251922 (242164)	Loss/tok 3.3869 (3.4506)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.099 (0.088)	Data 1.14e-04 (4.01e-04)	Tok/s 253231 (242089)	Loss/tok 3.3296 (3.4484)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.035 (0.088)	Data 1.18e-04 (3.96e-04)	Tok/s 225578 (242060)	Loss/tok 2.7406 (3.4479)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.067 (0.088)	Data 1.15e-04 (3.92e-04)	Tok/s 232157 (241969)	Loss/tok 3.2246 (3.4463)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.066 (0.088)	Data 1.33e-04 (3.89e-04)	Tok/s 238500 (242037)	Loss/tok 3.2567 (3.4460)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.134 (0.088)	Data 1.14e-04 (3.85e-04)	Tok/s 263056 (242017)	Loss/tok 3.5722 (3.4445)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.81e-04)	Tok/s 252653 (242001)	Loss/tok 3.4613 (3.4428)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.037 (0.087)	Data 1.10e-04 (3.77e-04)	Tok/s 220242 (241954)	Loss/tok 2.7909 (3.4410)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.099 (0.087)	Data 1.17e-04 (3.74e-04)	Tok/s 254558 (241960)	Loss/tok 3.5072 (3.4408)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.71e-04)	Tok/s 235625 (241985)	Loss/tok 3.2362 (3.4412)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.67e-04)	Tok/s 233623 (241991)	Loss/tok 3.1057 (3.4413)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][780/1291]	Time 0.036 (0.088)	Data 1.16e-04 (3.64e-04)	Tok/s 219307 (242009)	Loss/tok 2.7037 (3.4409)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][790/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.61e-04)	Tok/s 228703 (241940)	Loss/tok 3.2551 (3.4399)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.58e-04)	Tok/s 230707 (241939)	Loss/tok 3.2037 (3.4401)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.55e-04)	Tok/s 230658 (241997)	Loss/tok 3.0896 (3.4412)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.036 (0.088)	Data 1.12e-04 (3.52e-04)	Tok/s 224677 (242036)	Loss/tok 2.8656 (3.4418)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.099 (0.088)	Data 1.18e-04 (3.49e-04)	Tok/s 252493 (242119)	Loss/tok 3.4713 (3.4432)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.067 (0.088)	Data 1.36e-04 (3.47e-04)	Tok/s 229540 (242059)	Loss/tok 3.1625 (3.4421)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.099 (0.088)	Data 1.36e-04 (3.44e-04)	Tok/s 255353 (242053)	Loss/tok 3.3742 (3.4408)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.067 (0.088)	Data 1.23e-04 (3.41e-04)	Tok/s 230845 (242052)	Loss/tok 3.2139 (3.4404)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.39e-04)	Tok/s 230612 (242061)	Loss/tok 3.1916 (3.4397)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.036 (0.088)	Data 1.18e-04 (3.36e-04)	Tok/s 222955 (242039)	Loss/tok 2.7566 (3.4395)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.135 (0.088)	Data 1.17e-04 (3.34e-04)	Tok/s 261376 (242131)	Loss/tok 3.6212 (3.4400)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.31e-04)	Tok/s 233381 (242048)	Loss/tok 3.2256 (3.4394)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.29e-04)	Tok/s 233485 (242031)	Loss/tok 3.1678 (3.4398)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][920/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.27e-04)	Tok/s 237776 (242030)	Loss/tok 3.1210 (3.4389)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.099 (0.088)	Data 1.14e-04 (3.24e-04)	Tok/s 255235 (242067)	Loss/tok 3.4337 (3.4390)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.22e-04)	Tok/s 233024 (242081)	Loss/tok 3.1667 (3.4384)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.20e-04)	Tok/s 254054 (242121)	Loss/tok 3.4082 (3.4377)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.134 (0.088)	Data 1.13e-04 (3.18e-04)	Tok/s 259924 (242140)	Loss/tok 3.6253 (3.4373)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.16e-04)	Tok/s 232378 (242132)	Loss/tok 3.1592 (3.4366)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.066 (0.088)	Data 1.31e-04 (3.14e-04)	Tok/s 231480 (242129)	Loss/tok 3.2440 (3.4359)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.12e-04)	Tok/s 252798 (242159)	Loss/tok 3.5261 (3.4367)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.036 (0.088)	Data 1.13e-04 (3.10e-04)	Tok/s 218581 (242096)	Loss/tok 2.7575 (3.4353)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.067 (0.088)	Data 1.15e-04 (3.08e-04)	Tok/s 231290 (242038)	Loss/tok 3.2560 (3.4361)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.100 (0.088)	Data 1.15e-04 (3.06e-04)	Tok/s 253199 (242076)	Loss/tok 3.3421 (3.4370)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.134 (0.089)	Data 1.23e-04 (3.04e-04)	Tok/s 259560 (242094)	Loss/tok 3.5009 (3.4366)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1040/1291]	Time 0.173 (0.089)	Data 1.28e-04 (3.02e-04)	Tok/s 257197 (242162)	Loss/tok 3.7609 (3.4368)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.100 (0.089)	Data 1.28e-04 (3.01e-04)	Tok/s 251558 (242194)	Loss/tok 3.3287 (3.4369)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.99e-04)	Tok/s 255614 (242255)	Loss/tok 3.3115 (3.4373)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1070/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.97e-04)	Tok/s 231054 (242255)	Loss/tok 3.2003 (3.4373)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.135 (0.089)	Data 1.34e-04 (2.95e-04)	Tok/s 260752 (242244)	Loss/tok 3.6548 (3.4371)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.94e-04)	Tok/s 228382 (242210)	Loss/tok 3.2288 (3.4360)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.92e-04)	Tok/s 227683 (242197)	Loss/tok 3.1342 (3.4352)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.91e-04)	Tok/s 258975 (242199)	Loss/tok 3.2792 (3.4341)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.89e-04)	Tok/s 229846 (242186)	Loss/tok 3.0298 (3.4334)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.136 (0.089)	Data 1.12e-04 (2.88e-04)	Tok/s 257394 (242208)	Loss/tok 3.5496 (3.4333)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.036 (0.089)	Data 1.14e-04 (2.86e-04)	Tok/s 224083 (242172)	Loss/tok 2.7392 (3.4321)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.85e-04)	Tok/s 250390 (242197)	Loss/tok 3.3018 (3.4325)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.067 (0.089)	Data 1.38e-04 (2.83e-04)	Tok/s 234342 (242199)	Loss/tok 3.2580 (3.4332)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.82e-04)	Tok/s 235628 (242186)	Loss/tok 3.1693 (3.4321)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.067 (0.089)	Data 1.38e-04 (2.80e-04)	Tok/s 227822 (242134)	Loss/tok 3.1239 (3.4308)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.134 (0.089)	Data 1.35e-04 (2.79e-04)	Tok/s 260428 (242144)	Loss/tok 3.5654 (3.4303)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1200/1291]	Time 0.036 (0.089)	Data 1.23e-04 (2.78e-04)	Tok/s 222015 (242164)	Loss/tok 2.7216 (3.4295)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.76e-04)	Tok/s 233264 (242153)	Loss/tok 3.2267 (3.4286)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.067 (0.089)	Data 1.35e-04 (2.75e-04)	Tok/s 230409 (242206)	Loss/tok 3.1885 (3.4280)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1230/1291]	Time 0.173 (0.089)	Data 1.13e-04 (2.74e-04)	Tok/s 257033 (242209)	Loss/tok 3.6889 (3.4284)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.73e-04)	Tok/s 252530 (242192)	Loss/tok 3.3400 (3.4279)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.71e-04)	Tok/s 257044 (242176)	Loss/tok 3.3157 (3.4274)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.135 (0.089)	Data 1.12e-04 (2.70e-04)	Tok/s 261170 (242207)	Loss/tok 3.5321 (3.4268)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.69e-04)	Tok/s 232070 (242239)	Loss/tok 3.1628 (3.4263)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.68e-04)	Tok/s 235716 (242252)	Loss/tok 3.1330 (3.4257)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.099 (0.089)	Data 4.55e-05 (2.69e-04)	Tok/s 253928 (242275)	Loss/tok 3.3556 (3.4260)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593146004993, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593146004993, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.479 (0.479)	Decoder iters 149.0 (149.0)	Tok/s 33957 (33957)
0: Running moses detokenizer
0: BLEU(score=22.183766221029103, counts=[35706, 17210, 9502, 5469], totals=[64856, 61853, 58850, 55853], precisions=[55.05427408412483, 27.824034404151778, 16.146134239592183, 9.791774837519919], bp=1.0, sys_len=64856, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593146006977, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2218, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593146006977, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4260	Test BLEU: 22.18
0: Performance: Epoch: 1	Training: 1938281 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593146006977, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593146006977, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593146006978, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 858620448
0: TRAIN [2][0/1291]	Time 0.289 (0.289)	Data 1.91e-01 (1.91e-01)	Tok/s 54049 (54049)	Loss/tok 3.1226 (3.1226)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.036 (0.125)	Data 1.19e-04 (1.75e-02)	Tok/s 216120 (229090)	Loss/tok 2.7137 (3.3663)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.067 (0.107)	Data 1.14e-04 (9.20e-03)	Tok/s 231216 (235750)	Loss/tok 3.0705 (3.3150)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.067 (0.101)	Data 1.14e-04 (6.27e-03)	Tok/s 229895 (237207)	Loss/tok 2.9811 (3.3037)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.067 (0.095)	Data 1.15e-04 (4.77e-03)	Tok/s 230717 (236752)	Loss/tok 3.0710 (3.2883)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.036 (0.093)	Data 1.29e-04 (3.86e-03)	Tok/s 219961 (237809)	Loss/tok 2.6056 (3.2808)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.177 (0.094)	Data 1.18e-04 (3.25e-03)	Tok/s 252286 (239204)	Loss/tok 3.6544 (3.2939)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][70/1291]	Time 0.036 (0.092)	Data 1.14e-04 (2.81e-03)	Tok/s 219200 (238617)	Loss/tok 2.6762 (3.2842)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.067 (0.090)	Data 1.24e-04 (2.47e-03)	Tok/s 232654 (238689)	Loss/tok 3.0652 (3.2802)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.067 (0.090)	Data 1.13e-04 (2.22e-03)	Tok/s 231896 (238610)	Loss/tok 3.0005 (3.2878)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.066 (0.090)	Data 1.13e-04 (2.01e-03)	Tok/s 230589 (239185)	Loss/tok 3.0171 (3.2872)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.068 (0.089)	Data 1.31e-04 (1.84e-03)	Tok/s 230831 (239012)	Loss/tok 3.0222 (3.2752)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.067 (0.088)	Data 1.13e-04 (1.70e-03)	Tok/s 232704 (238753)	Loss/tok 2.9947 (3.2656)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.036 (0.087)	Data 1.14e-04 (1.57e-03)	Tok/s 220296 (238791)	Loss/tok 2.6245 (3.2615)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.135 (0.086)	Data 1.13e-04 (1.47e-03)	Tok/s 258914 (238714)	Loss/tok 3.4299 (3.2570)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.100 (0.086)	Data 1.13e-04 (1.38e-03)	Tok/s 253230 (239036)	Loss/tok 3.2546 (3.2569)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.100 (0.086)	Data 1.17e-04 (1.30e-03)	Tok/s 253852 (239124)	Loss/tok 3.2997 (3.2556)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.134 (0.086)	Data 1.12e-04 (1.23e-03)	Tok/s 260116 (239325)	Loss/tok 3.4098 (3.2563)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.067 (0.086)	Data 1.11e-04 (1.17e-03)	Tok/s 229414 (239434)	Loss/tok 3.0405 (3.2563)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.036 (0.086)	Data 1.14e-04 (1.12e-03)	Tok/s 223186 (239342)	Loss/tok 2.6301 (3.2561)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][200/1291]	Time 0.099 (0.087)	Data 1.14e-04 (1.07e-03)	Tok/s 253777 (239758)	Loss/tok 3.2491 (3.2598)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][210/1291]	Time 0.100 (0.087)	Data 1.17e-04 (1.02e-03)	Tok/s 252016 (240043)	Loss/tok 3.3542 (3.2658)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.100 (0.088)	Data 1.16e-04 (9.81e-04)	Tok/s 251955 (240295)	Loss/tok 3.1966 (3.2656)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.067 (0.088)	Data 1.12e-04 (9.43e-04)	Tok/s 231058 (240601)	Loss/tok 3.1325 (3.2707)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.036 (0.088)	Data 1.11e-04 (9.09e-04)	Tok/s 221157 (240843)	Loss/tok 2.6823 (3.2729)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.036 (0.088)	Data 1.15e-04 (8.78e-04)	Tok/s 218330 (240709)	Loss/tok 2.6151 (3.2734)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.067 (0.088)	Data 1.08e-04 (8.48e-04)	Tok/s 232994 (240759)	Loss/tok 3.0236 (3.2751)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.036 (0.089)	Data 1.21e-04 (8.22e-04)	Tok/s 221282 (240901)	Loss/tok 2.6874 (3.2799)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.068 (0.089)	Data 1.12e-04 (7.96e-04)	Tok/s 229027 (240919)	Loss/tok 3.0873 (3.2815)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.174 (0.089)	Data 1.13e-04 (7.73e-04)	Tok/s 259642 (241171)	Loss/tok 3.5784 (3.2844)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.066 (0.089)	Data 1.16e-04 (7.51e-04)	Tok/s 230144 (241015)	Loss/tok 3.1626 (3.2829)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.066 (0.089)	Data 1.13e-04 (7.31e-04)	Tok/s 233939 (240973)	Loss/tok 3.1224 (3.2811)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.100 (0.089)	Data 1.19e-04 (7.12e-04)	Tok/s 251252 (241139)	Loss/tok 3.3568 (3.2819)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.173 (0.089)	Data 1.14e-04 (6.94e-04)	Tok/s 256731 (241368)	Loss/tok 3.7291 (3.2863)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][340/1291]	Time 0.099 (0.089)	Data 1.23e-04 (6.77e-04)	Tok/s 255806 (241091)	Loss/tok 3.1889 (3.2826)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.036 (0.089)	Data 1.12e-04 (6.61e-04)	Tok/s 223278 (241174)	Loss/tok 2.6493 (3.2852)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.099 (0.089)	Data 1.13e-04 (6.46e-04)	Tok/s 253186 (241136)	Loss/tok 3.2799 (3.2854)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][370/1291]	Time 0.067 (0.089)	Data 1.14e-04 (6.31e-04)	Tok/s 233871 (241364)	Loss/tok 3.0565 (3.2888)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.035 (0.089)	Data 1.13e-04 (6.18e-04)	Tok/s 217524 (241275)	Loss/tok 2.6447 (3.2870)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.174 (0.089)	Data 1.12e-04 (6.05e-04)	Tok/s 254431 (241450)	Loss/tok 3.6853 (3.2891)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.066 (0.089)	Data 1.10e-04 (5.93e-04)	Tok/s 231118 (241456)	Loss/tok 3.1521 (3.2888)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.036 (0.089)	Data 1.13e-04 (5.81e-04)	Tok/s 223634 (241487)	Loss/tok 2.7206 (3.2882)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][420/1291]	Time 0.099 (0.090)	Data 1.13e-04 (5.70e-04)	Tok/s 252945 (241607)	Loss/tok 3.3655 (3.2957)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.099 (0.090)	Data 1.11e-04 (5.60e-04)	Tok/s 254190 (241549)	Loss/tok 3.2966 (3.2950)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.50e-04)	Tok/s 236048 (241586)	Loss/tok 3.0370 (3.2947)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.067 (0.090)	Data 1.13e-04 (5.40e-04)	Tok/s 234280 (241732)	Loss/tok 3.0650 (3.2955)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.067 (0.090)	Data 1.13e-04 (5.31e-04)	Tok/s 235141 (241880)	Loss/tok 3.0793 (3.2961)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.22e-04)	Tok/s 229826 (242027)	Loss/tok 3.1115 (3.2982)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.036 (0.090)	Data 1.13e-04 (5.14e-04)	Tok/s 218063 (241990)	Loss/tok 2.6612 (3.2979)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.036 (0.090)	Data 1.13e-04 (5.05e-04)	Tok/s 218772 (242002)	Loss/tok 2.6634 (3.2979)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.134 (0.090)	Data 1.12e-04 (4.98e-04)	Tok/s 261767 (241972)	Loss/tok 3.5216 (3.2968)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.135 (0.090)	Data 1.11e-04 (4.90e-04)	Tok/s 262165 (242034)	Loss/tok 3.3549 (3.2961)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.099 (0.090)	Data 1.37e-04 (4.83e-04)	Tok/s 255648 (242078)	Loss/tok 3.2655 (3.2960)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.135 (0.090)	Data 1.15e-04 (4.76e-04)	Tok/s 259801 (242162)	Loss/tok 3.5003 (3.2978)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.134 (0.091)	Data 1.17e-04 (4.70e-04)	Tok/s 258367 (242255)	Loss/tok 3.5459 (3.2984)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][550/1291]	Time 0.100 (0.091)	Data 1.12e-04 (4.63e-04)	Tok/s 250542 (242305)	Loss/tok 3.2787 (3.2985)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.57e-04)	Tok/s 254885 (242209)	Loss/tok 3.2947 (3.2971)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.036 (0.090)	Data 1.14e-04 (4.51e-04)	Tok/s 216547 (242117)	Loss/tok 2.6669 (3.2965)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.066 (0.090)	Data 1.33e-04 (4.45e-04)	Tok/s 230330 (242224)	Loss/tok 3.0740 (3.2966)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.100 (0.090)	Data 1.12e-04 (4.40e-04)	Tok/s 252433 (242118)	Loss/tok 3.2388 (3.2947)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.134 (0.090)	Data 1.15e-04 (4.34e-04)	Tok/s 261957 (242149)	Loss/tok 3.4937 (3.2952)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.067 (0.090)	Data 1.16e-04 (4.29e-04)	Tok/s 234315 (242237)	Loss/tok 3.0811 (3.2948)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.036 (0.090)	Data 1.13e-04 (4.24e-04)	Tok/s 220695 (242170)	Loss/tok 2.6062 (3.2934)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.100 (0.090)	Data 1.16e-04 (4.19e-04)	Tok/s 252130 (242100)	Loss/tok 3.3065 (3.2920)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.15e-04)	Tok/s 227807 (242041)	Loss/tok 3.0528 (3.2917)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][650/1291]	Time 0.102 (0.090)	Data 1.13e-04 (4.10e-04)	Tok/s 248832 (242119)	Loss/tok 3.2610 (3.2925)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.067 (0.090)	Data 1.10e-04 (4.06e-04)	Tok/s 230891 (242035)	Loss/tok 3.0529 (3.2922)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.099 (0.090)	Data 1.34e-04 (4.02e-04)	Tok/s 253102 (242045)	Loss/tok 3.2916 (3.2916)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.98e-04)	Tok/s 235581 (242093)	Loss/tok 3.0542 (3.2927)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.099 (0.090)	Data 1.13e-04 (3.93e-04)	Tok/s 250505 (242083)	Loss/tok 3.2635 (3.2916)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.175 (0.090)	Data 1.13e-04 (3.89e-04)	Tok/s 253056 (242122)	Loss/tok 3.6184 (3.2929)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.099 (0.090)	Data 1.17e-04 (3.86e-04)	Tok/s 256002 (242126)	Loss/tok 3.2362 (3.2931)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.099 (0.090)	Data 1.35e-04 (3.82e-04)	Tok/s 253723 (242147)	Loss/tok 3.2633 (3.2921)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.78e-04)	Tok/s 231902 (242211)	Loss/tok 3.1323 (3.2929)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.100 (0.090)	Data 1.16e-04 (3.75e-04)	Tok/s 251243 (242320)	Loss/tok 3.3482 (3.2933)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.174 (0.090)	Data 1.13e-04 (3.71e-04)	Tok/s 257955 (242426)	Loss/tok 3.5154 (3.2955)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.136 (0.090)	Data 1.14e-04 (3.68e-04)	Tok/s 259274 (242421)	Loss/tok 3.4924 (3.2954)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.65e-04)	Tok/s 234066 (242432)	Loss/tok 2.9989 (3.2951)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][780/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.61e-04)	Tok/s 229939 (242377)	Loss/tok 3.1081 (3.2939)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.066 (0.090)	Data 1.24e-04 (3.58e-04)	Tok/s 235185 (242370)	Loss/tok 3.0258 (3.2942)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.100 (0.090)	Data 1.25e-04 (3.55e-04)	Tok/s 252908 (242402)	Loss/tok 3.2423 (3.2935)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.035 (0.090)	Data 1.13e-04 (3.52e-04)	Tok/s 225334 (242324)	Loss/tok 2.7508 (3.2926)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.100 (0.090)	Data 1.18e-04 (3.50e-04)	Tok/s 254538 (242385)	Loss/tok 3.1760 (3.2932)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.066 (0.090)	Data 1.11e-04 (3.47e-04)	Tok/s 234754 (242293)	Loss/tok 3.1048 (3.2917)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.44e-04)	Tok/s 234619 (242301)	Loss/tok 3.0593 (3.2911)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.099 (0.090)	Data 1.14e-04 (3.41e-04)	Tok/s 254572 (242246)	Loss/tok 3.3388 (3.2899)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.135 (0.090)	Data 1.16e-04 (3.39e-04)	Tok/s 260823 (242209)	Loss/tok 3.3803 (3.2887)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.100 (0.090)	Data 1.23e-04 (3.36e-04)	Tok/s 253896 (242234)	Loss/tok 3.2161 (3.2883)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.34e-04)	Tok/s 227310 (242206)	Loss/tok 3.0453 (3.2880)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.099 (0.090)	Data 1.13e-04 (3.31e-04)	Tok/s 257561 (242332)	Loss/tok 3.3197 (3.2900)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.101 (0.090)	Data 1.34e-04 (3.29e-04)	Tok/s 250251 (242296)	Loss/tok 3.2917 (3.2893)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][910/1291]	Time 0.100 (0.090)	Data 1.41e-04 (3.27e-04)	Tok/s 252207 (242249)	Loss/tok 3.3525 (3.2883)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.036 (0.090)	Data 1.14e-04 (3.24e-04)	Tok/s 221814 (242175)	Loss/tok 2.6161 (3.2884)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.100 (0.089)	Data 1.16e-04 (3.22e-04)	Tok/s 255174 (242131)	Loss/tok 3.1882 (3.2874)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.068 (0.089)	Data 1.15e-04 (3.20e-04)	Tok/s 229147 (242163)	Loss/tok 3.0744 (3.2874)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.18e-04)	Tok/s 231645 (242174)	Loss/tok 3.0880 (3.2876)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.16e-04)	Tok/s 229681 (242143)	Loss/tok 3.0352 (3.2864)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.14e-04)	Tok/s 233808 (242165)	Loss/tok 3.1301 (3.2864)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.12e-04)	Tok/s 250516 (242159)	Loss/tok 3.2149 (3.2857)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.10e-04)	Tok/s 254572 (242101)	Loss/tok 3.2888 (3.2849)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.036 (0.089)	Data 1.13e-04 (3.08e-04)	Tok/s 221293 (242061)	Loss/tok 2.7097 (3.2840)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.135 (0.089)	Data 1.30e-04 (3.06e-04)	Tok/s 256405 (242131)	Loss/tok 3.4527 (3.2847)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.04e-04)	Tok/s 229548 (242080)	Loss/tok 3.1057 (3.2835)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1030/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.02e-04)	Tok/s 229581 (242050)	Loss/tok 3.1746 (3.2830)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.036 (0.089)	Data 1.16e-04 (3.00e-04)	Tok/s 223707 (242027)	Loss/tok 2.6852 (3.2828)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.036 (0.089)	Data 1.21e-04 (2.99e-04)	Tok/s 223053 (241978)	Loss/tok 2.6784 (3.2814)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.97e-04)	Tok/s 231010 (241959)	Loss/tok 3.0310 (3.2807)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.174 (0.089)	Data 1.12e-04 (2.95e-04)	Tok/s 255037 (241987)	Loss/tok 3.5512 (3.2810)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1080/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.94e-04)	Tok/s 231608 (241989)	Loss/tok 3.1556 (3.2810)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.067 (0.089)	Data 1.34e-04 (2.92e-04)	Tok/s 233208 (241991)	Loss/tok 2.9881 (3.2808)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.036 (0.089)	Data 1.22e-04 (2.90e-04)	Tok/s 221570 (241990)	Loss/tok 2.6256 (3.2803)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.135 (0.089)	Data 1.18e-04 (2.89e-04)	Tok/s 258328 (242048)	Loss/tok 3.4521 (3.2808)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.101 (0.089)	Data 1.14e-04 (2.87e-04)	Tok/s 249869 (242050)	Loss/tok 3.2810 (3.2804)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.036 (0.089)	Data 1.14e-04 (2.86e-04)	Tok/s 224600 (242017)	Loss/tok 2.7048 (3.2800)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.84e-04)	Tok/s 229554 (241958)	Loss/tok 3.1256 (3.2802)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.83e-04)	Tok/s 230789 (241943)	Loss/tok 3.0431 (3.2806)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.101 (0.089)	Data 1.13e-04 (2.81e-04)	Tok/s 251090 (241965)	Loss/tok 3.2710 (3.2809)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.80e-04)	Tok/s 230642 (241982)	Loss/tok 2.9676 (3.2801)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.101 (0.089)	Data 1.15e-04 (2.79e-04)	Tok/s 249844 (242029)	Loss/tok 3.2639 (3.2802)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.066 (0.089)	Data 1.36e-04 (2.77e-04)	Tok/s 228417 (242049)	Loss/tok 2.9283 (3.2801)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1200/1291]	Time 0.175 (0.089)	Data 1.13e-04 (2.76e-04)	Tok/s 255878 (242107)	Loss/tok 3.6113 (3.2810)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.75e-04)	Tok/s 234207 (242101)	Loss/tok 3.1372 (3.2810)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.101 (0.089)	Data 1.15e-04 (2.73e-04)	Tok/s 247544 (242048)	Loss/tok 3.2960 (3.2806)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1230/1291]	Time 0.136 (0.089)	Data 1.18e-04 (2.72e-04)	Tok/s 258063 (242013)	Loss/tok 3.3891 (3.2804)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.71e-04)	Tok/s 231512 (241979)	Loss/tok 3.0075 (3.2795)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.70e-04)	Tok/s 250387 (241946)	Loss/tok 3.3328 (3.2788)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.099 (0.089)	Data 1.27e-04 (2.68e-04)	Tok/s 253473 (241943)	Loss/tok 3.2350 (3.2791)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.174 (0.089)	Data 1.16e-04 (2.67e-04)	Tok/s 254211 (241946)	Loss/tok 3.6923 (3.2796)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.66e-04)	Tok/s 251929 (241994)	Loss/tok 3.2540 (3.2799)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.136 (0.089)	Data 5.29e-05 (2.68e-04)	Tok/s 259756 (241980)	Loss/tok 3.3587 (3.2799)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593146122621, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593146122621, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.494 (0.494)	Decoder iters 149.0 (149.0)	Tok/s 34067 (34067)
0: Running moses detokenizer
0: BLEU(score=22.261202629672155, counts=[36613, 17876, 9965, 5799], totals=[67238, 64235, 61232, 58234], precisions=[54.452839168327436, 27.829065151397213, 16.274170368434806, 9.958100078991654], bp=1.0, sys_len=67238, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593146124735, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22260000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593146124735, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2812	Test BLEU: 22.26
0: Performance: Epoch: 2	Training: 1936514 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593146124736, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593146124736, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593146124736, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2290567188
0: TRAIN [3][0/1291]	Time 0.333 (0.333)	Data 1.95e-01 (1.95e-01)	Tok/s 105991 (105991)	Loss/tok 3.2869 (3.2869)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.067 (0.103)	Data 1.10e-04 (1.79e-02)	Tok/s 234128 (226626)	Loss/tok 2.9878 (3.1479)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.099 (0.097)	Data 1.13e-04 (9.41e-03)	Tok/s 253392 (235024)	Loss/tok 3.1766 (3.1825)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.100 (0.096)	Data 1.10e-04 (6.41e-03)	Tok/s 253606 (239330)	Loss/tok 3.1613 (3.1634)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.100 (0.092)	Data 1.06e-04 (4.87e-03)	Tok/s 253947 (239365)	Loss/tok 3.1949 (3.1546)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.036 (0.089)	Data 1.12e-04 (3.94e-03)	Tok/s 219469 (238244)	Loss/tok 2.6119 (3.1538)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][60/1291]	Time 0.039 (0.090)	Data 1.06e-04 (3.31e-03)	Tok/s 195744 (238883)	Loss/tok 2.5851 (3.1718)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.86e-03)	Tok/s 252908 (238969)	Loss/tok 3.1542 (3.1665)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.067 (0.088)	Data 1.08e-04 (2.52e-03)	Tok/s 230778 (238980)	Loss/tok 2.9940 (3.1617)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.101 (0.087)	Data 1.08e-04 (2.26e-03)	Tok/s 250124 (239125)	Loss/tok 3.2437 (3.1588)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.036 (0.087)	Data 1.06e-04 (2.04e-03)	Tok/s 216595 (239109)	Loss/tok 2.5949 (3.1637)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.100 (0.087)	Data 1.08e-04 (1.87e-03)	Tok/s 251994 (239683)	Loss/tok 3.1688 (3.1643)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][120/1291]	Time 0.135 (0.090)	Data 1.06e-04 (1.72e-03)	Tok/s 258628 (240669)	Loss/tok 3.3596 (3.1795)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.067 (0.091)	Data 1.17e-04 (1.60e-03)	Tok/s 229521 (241183)	Loss/tok 3.0679 (3.1914)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.175 (0.091)	Data 1.09e-04 (1.50e-03)	Tok/s 257013 (241085)	Loss/tok 3.4813 (3.1918)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.067 (0.091)	Data 1.07e-04 (1.40e-03)	Tok/s 231979 (241140)	Loss/tok 2.9530 (3.1924)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.100 (0.092)	Data 1.30e-04 (1.32e-03)	Tok/s 254459 (241708)	Loss/tok 3.1855 (3.1946)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.100 (0.091)	Data 1.09e-04 (1.25e-03)	Tok/s 251605 (241646)	Loss/tok 3.1602 (3.1907)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.067 (0.091)	Data 1.08e-04 (1.19e-03)	Tok/s 229116 (241540)	Loss/tok 2.9685 (3.1903)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.067 (0.091)	Data 1.07e-04 (1.13e-03)	Tok/s 228029 (241660)	Loss/tok 2.9074 (3.1891)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.100 (0.091)	Data 1.08e-04 (1.08e-03)	Tok/s 253893 (241643)	Loss/tok 3.1891 (3.1881)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.099 (0.090)	Data 1.06e-04 (1.04e-03)	Tok/s 255683 (241727)	Loss/tok 3.1452 (3.1855)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.036 (0.090)	Data 1.08e-04 (9.95e-04)	Tok/s 217972 (241535)	Loss/tok 2.5765 (3.1810)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.100 (0.090)	Data 1.05e-04 (9.57e-04)	Tok/s 254533 (241673)	Loss/tok 3.1834 (3.1812)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.101 (0.091)	Data 1.11e-04 (9.22e-04)	Tok/s 252873 (242060)	Loss/tok 3.1044 (3.1817)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][250/1291]	Time 0.100 (0.091)	Data 1.06e-04 (8.89e-04)	Tok/s 252382 (242073)	Loss/tok 3.1470 (3.1806)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.067 (0.090)	Data 1.06e-04 (8.59e-04)	Tok/s 229780 (241873)	Loss/tok 2.9772 (3.1772)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.175 (0.090)	Data 1.07e-04 (8.32e-04)	Tok/s 255044 (241812)	Loss/tok 3.6400 (3.1789)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.137 (0.090)	Data 1.08e-04 (8.06e-04)	Tok/s 255482 (241576)	Loss/tok 3.2319 (3.1751)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.067 (0.089)	Data 1.11e-04 (7.82e-04)	Tok/s 231461 (241525)	Loss/tok 2.9648 (3.1725)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.067 (0.090)	Data 1.10e-04 (7.60e-04)	Tok/s 234520 (241515)	Loss/tok 3.0538 (3.1730)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.067 (0.090)	Data 1.09e-04 (7.39e-04)	Tok/s 232490 (241567)	Loss/tok 2.9998 (3.1730)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.101 (0.090)	Data 1.18e-04 (7.19e-04)	Tok/s 251742 (241514)	Loss/tok 3.2050 (3.1748)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.067 (0.089)	Data 1.07e-04 (7.01e-04)	Tok/s 231688 (241403)	Loss/tok 2.9713 (3.1722)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.067 (0.089)	Data 1.09e-04 (6.84e-04)	Tok/s 231658 (241404)	Loss/tok 2.8862 (3.1701)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.067 (0.089)	Data 1.07e-04 (6.67e-04)	Tok/s 234414 (241323)	Loss/tok 2.9425 (3.1684)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.067 (0.089)	Data 1.08e-04 (6.52e-04)	Tok/s 233135 (241217)	Loss/tok 2.9554 (3.1672)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.100 (0.088)	Data 1.11e-04 (6.37e-04)	Tok/s 250981 (241161)	Loss/tok 3.1695 (3.1654)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][380/1291]	Time 0.067 (0.088)	Data 1.09e-04 (6.24e-04)	Tok/s 228915 (241123)	Loss/tok 3.0100 (3.1662)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.067 (0.089)	Data 1.14e-04 (6.11e-04)	Tok/s 232490 (241238)	Loss/tok 3.0383 (3.1677)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.068 (0.089)	Data 1.10e-04 (5.98e-04)	Tok/s 224362 (241344)	Loss/tok 2.9241 (3.1691)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.067 (0.089)	Data 1.09e-04 (5.86e-04)	Tok/s 233454 (241299)	Loss/tok 2.9356 (3.1669)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.100 (0.088)	Data 1.07e-04 (5.75e-04)	Tok/s 253972 (241305)	Loss/tok 3.1115 (3.1641)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.64e-04)	Tok/s 254150 (241506)	Loss/tok 3.1303 (3.1657)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.068 (0.089)	Data 1.06e-04 (5.54e-04)	Tok/s 226526 (241552)	Loss/tok 2.9466 (3.1656)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.100 (0.089)	Data 1.08e-04 (5.44e-04)	Tok/s 249943 (241536)	Loss/tok 3.0740 (3.1639)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.068 (0.089)	Data 1.07e-04 (5.35e-04)	Tok/s 231130 (241511)	Loss/tok 2.9634 (3.1631)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.26e-04)	Tok/s 226042 (241589)	Loss/tok 2.8624 (3.1639)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.136 (0.089)	Data 1.12e-04 (5.17e-04)	Tok/s 257252 (241542)	Loss/tok 3.3435 (3.1623)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.067 (0.088)	Data 1.11e-04 (5.09e-04)	Tok/s 231440 (241395)	Loss/tok 2.9738 (3.1609)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.067 (0.088)	Data 1.32e-04 (5.01e-04)	Tok/s 229811 (241416)	Loss/tok 2.9307 (3.1606)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][510/1291]	Time 0.100 (0.089)	Data 1.08e-04 (4.93e-04)	Tok/s 253491 (241454)	Loss/tok 3.1704 (3.1608)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.101 (0.088)	Data 1.15e-04 (4.86e-04)	Tok/s 249799 (241454)	Loss/tok 3.0981 (3.1596)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][530/1291]	Time 0.100 (0.088)	Data 1.10e-04 (4.79e-04)	Tok/s 252757 (241453)	Loss/tok 3.0930 (3.1585)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.067 (0.089)	Data 1.06e-04 (4.72e-04)	Tok/s 231654 (241502)	Loss/tok 3.0451 (3.1593)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.099 (0.088)	Data 1.06e-04 (4.66e-04)	Tok/s 254739 (241514)	Loss/tok 3.1566 (3.1584)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.067 (0.088)	Data 1.12e-04 (4.59e-04)	Tok/s 231033 (241481)	Loss/tok 2.8862 (3.1578)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.099 (0.088)	Data 1.07e-04 (4.53e-04)	Tok/s 253530 (241459)	Loss/tok 3.1809 (3.1569)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.099 (0.088)	Data 1.08e-04 (4.47e-04)	Tok/s 254908 (241461)	Loss/tok 3.0914 (3.1574)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.135 (0.088)	Data 1.08e-04 (4.42e-04)	Tok/s 259301 (241467)	Loss/tok 3.2964 (3.1567)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.036 (0.089)	Data 1.07e-04 (4.36e-04)	Tok/s 221846 (241530)	Loss/tok 2.6341 (3.1580)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.067 (0.088)	Data 1.09e-04 (4.31e-04)	Tok/s 231910 (241442)	Loss/tok 2.9357 (3.1560)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.067 (0.088)	Data 1.09e-04 (4.26e-04)	Tok/s 230324 (241488)	Loss/tok 2.9401 (3.1574)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.21e-04)	Tok/s 231154 (241548)	Loss/tok 2.8976 (3.1583)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.036 (0.089)	Data 1.09e-04 (4.16e-04)	Tok/s 218563 (241622)	Loss/tok 2.5049 (3.1588)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][650/1291]	Time 0.068 (0.089)	Data 1.18e-04 (4.11e-04)	Tok/s 223509 (241643)	Loss/tok 2.8989 (3.1591)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.067 (0.089)	Data 1.04e-04 (4.07e-04)	Tok/s 231809 (241581)	Loss/tok 2.9432 (3.1582)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.067 (0.088)	Data 1.07e-04 (4.02e-04)	Tok/s 229602 (241501)	Loss/tok 2.9436 (3.1567)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.98e-04)	Tok/s 256349 (241603)	Loss/tok 3.1622 (3.1568)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.135 (0.089)	Data 1.14e-04 (3.94e-04)	Tok/s 259355 (241714)	Loss/tok 3.3379 (3.1585)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.036 (0.089)	Data 1.10e-04 (3.90e-04)	Tok/s 221858 (241687)	Loss/tok 2.5810 (3.1597)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.099 (0.089)	Data 1.33e-04 (3.86e-04)	Tok/s 253844 (241733)	Loss/tok 3.0253 (3.1589)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.82e-04)	Tok/s 229597 (241713)	Loss/tok 2.9444 (3.1585)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.79e-04)	Tok/s 233336 (241753)	Loss/tok 3.0493 (3.1605)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.75e-04)	Tok/s 251307 (241724)	Loss/tok 3.1820 (3.1597)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.71e-04)	Tok/s 230507 (241745)	Loss/tok 2.9923 (3.1601)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.136 (0.089)	Data 1.09e-04 (3.68e-04)	Tok/s 259851 (241804)	Loss/tok 3.2378 (3.1596)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.65e-04)	Tok/s 234884 (241796)	Loss/tok 2.8690 (3.1598)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][780/1291]	Time 0.100 (0.089)	Data 1.07e-04 (3.61e-04)	Tok/s 252243 (241811)	Loss/tok 3.2229 (3.1608)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.135 (0.089)	Data 1.08e-04 (3.58e-04)	Tok/s 260334 (241750)	Loss/tok 3.2859 (3.1599)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.036 (0.089)	Data 1.10e-04 (3.55e-04)	Tok/s 216570 (241745)	Loss/tok 2.4506 (3.1608)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][810/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.52e-04)	Tok/s 234647 (241729)	Loss/tok 2.9721 (3.1616)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.067 (0.089)	Data 1.07e-04 (3.49e-04)	Tok/s 233920 (241732)	Loss/tok 2.9581 (3.1617)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.100 (0.089)	Data 1.08e-04 (3.46e-04)	Tok/s 253782 (241741)	Loss/tok 3.0320 (3.1619)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.136 (0.090)	Data 1.16e-04 (3.44e-04)	Tok/s 259787 (241755)	Loss/tok 3.2680 (3.1619)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.067 (0.089)	Data 1.31e-04 (3.41e-04)	Tok/s 233672 (241698)	Loss/tok 2.9412 (3.1605)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.38e-04)	Tok/s 253330 (241660)	Loss/tok 3.1917 (3.1605)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.176 (0.089)	Data 1.11e-04 (3.36e-04)	Tok/s 255010 (241729)	Loss/tok 3.4203 (3.1612)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.135 (0.090)	Data 1.10e-04 (3.33e-04)	Tok/s 259122 (241725)	Loss/tok 3.3164 (3.1614)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.31e-04)	Tok/s 226649 (241645)	Loss/tok 2.9268 (3.1606)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.28e-04)	Tok/s 231800 (241660)	Loss/tok 2.9431 (3.1606)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.26e-04)	Tok/s 257071 (241603)	Loss/tok 3.1080 (3.1597)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.176 (0.089)	Data 1.05e-04 (3.23e-04)	Tok/s 254496 (241617)	Loss/tok 3.4385 (3.1604)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.21e-04)	Tok/s 230241 (241564)	Loss/tok 2.8664 (3.1591)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][940/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.19e-04)	Tok/s 231464 (241592)	Loss/tok 2.9495 (3.1593)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.17e-04)	Tok/s 230586 (241615)	Loss/tok 2.9586 (3.1590)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.099 (0.089)	Data 1.06e-04 (3.15e-04)	Tok/s 257874 (241584)	Loss/tok 3.1184 (3.1582)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.100 (0.089)	Data 1.32e-04 (3.13e-04)	Tok/s 250727 (241615)	Loss/tok 3.1302 (3.1584)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.10e-04)	Tok/s 236209 (241618)	Loss/tok 2.9482 (3.1573)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.099 (0.089)	Data 1.08e-04 (3.08e-04)	Tok/s 257763 (241664)	Loss/tok 3.1528 (3.1575)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.07e-04)	Tok/s 254374 (241627)	Loss/tok 3.1518 (3.1563)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.05e-04)	Tok/s 226363 (241653)	Loss/tok 2.8592 (3.1558)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.03e-04)	Tok/s 251375 (241601)	Loss/tok 3.0620 (3.1546)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.067 (0.089)	Data 1.29e-04 (3.01e-04)	Tok/s 230438 (241579)	Loss/tok 2.9103 (3.1542)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.99e-04)	Tok/s 226582 (241598)	Loss/tok 2.9481 (3.1539)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.037 (0.089)	Data 1.08e-04 (2.97e-04)	Tok/s 216497 (241532)	Loss/tok 2.5555 (3.1530)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.135 (0.089)	Data 1.07e-04 (2.95e-04)	Tok/s 260985 (241516)	Loss/tok 3.3368 (3.1526)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1070/1291]	Time 0.099 (0.089)	Data 1.07e-04 (2.94e-04)	Tok/s 253196 (241512)	Loss/tok 3.1252 (3.1519)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.92e-04)	Tok/s 256439 (241475)	Loss/tok 3.0957 (3.1507)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.135 (0.089)	Data 1.10e-04 (2.90e-04)	Tok/s 256699 (241486)	Loss/tok 3.3651 (3.1518)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.099 (0.089)	Data 1.19e-04 (2.89e-04)	Tok/s 253388 (241542)	Loss/tok 3.1500 (3.1525)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.036 (0.089)	Data 1.06e-04 (2.87e-04)	Tok/s 222479 (241499)	Loss/tok 2.5874 (3.1516)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.86e-04)	Tok/s 230529 (241510)	Loss/tok 2.9883 (3.1514)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.100 (0.089)	Data 1.07e-04 (2.84e-04)	Tok/s 251100 (241560)	Loss/tok 3.1045 (3.1521)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.82e-04)	Tok/s 232998 (241572)	Loss/tok 2.8591 (3.1522)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.81e-04)	Tok/s 232245 (241498)	Loss/tok 2.8990 (3.1511)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.80e-04)	Tok/s 233190 (241448)	Loss/tok 2.9166 (3.1502)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.036 (0.089)	Data 1.09e-04 (2.78e-04)	Tok/s 220098 (241401)	Loss/tok 2.5664 (3.1496)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.77e-04)	Tok/s 255324 (241447)	Loss/tok 3.1175 (3.1497)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.101 (0.089)	Data 1.13e-04 (2.75e-04)	Tok/s 248911 (241463)	Loss/tok 3.0907 (3.1498)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1200/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.74e-04)	Tok/s 234631 (241430)	Loss/tok 2.8876 (3.1490)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.101 (0.089)	Data 1.11e-04 (2.73e-04)	Tok/s 252521 (241423)	Loss/tok 3.0655 (3.1478)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.099 (0.089)	Data 1.07e-04 (2.71e-04)	Tok/s 251827 (241414)	Loss/tok 3.1518 (3.1478)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.100 (0.089)	Data 1.18e-04 (2.70e-04)	Tok/s 251759 (241465)	Loss/tok 3.2465 (3.1480)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.100 (0.089)	Data 1.28e-04 (2.69e-04)	Tok/s 250901 (241481)	Loss/tok 3.1804 (3.1480)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.135 (0.089)	Data 1.16e-04 (2.67e-04)	Tok/s 257840 (241508)	Loss/tok 3.3055 (3.1483)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.135 (0.089)	Data 1.07e-04 (2.66e-04)	Tok/s 259445 (241516)	Loss/tok 3.2228 (3.1483)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.067 (0.089)	Data 1.19e-04 (2.65e-04)	Tok/s 230697 (241513)	Loss/tok 2.9696 (3.1487)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.099 (0.089)	Data 1.07e-04 (2.64e-04)	Tok/s 253719 (241534)	Loss/tok 3.0724 (3.1488)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.099 (0.089)	Data 4.79e-05 (2.66e-04)	Tok/s 256255 (241543)	Loss/tok 3.1988 (3.1484)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593146240604, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593146240604, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.402 (0.402)	Decoder iters 108.0 (108.0)	Tok/s 41006 (41006)
0: Running moses detokenizer
0: BLEU(score=24.20491445461652, counts=[37087, 18634, 10661, 6331], totals=[65311, 62308, 59305, 56307], precisions=[56.78522760331338, 29.90627206779226, 17.976561841328724, 11.24371747740068], bp=1.0, sys_len=65311, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593146242542, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.242, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593146242542, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1482	Test BLEU: 24.20
0: Performance: Epoch: 3	Training: 1932774 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593146242542, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593146242542, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 04:37:29 AM
RESULT,RNN_TRANSLATOR,,504,nvidia,2020-06-26 04:29:05 AM
