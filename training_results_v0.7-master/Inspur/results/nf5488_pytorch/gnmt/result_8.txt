Beginning trial 8 of 10
:::MLLOG {"namespace": "", "time_ms": 1593144725028, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593144725075, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593144725075, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593144725075, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593144725075, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593144727465, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 04:12:07 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593144730096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144730102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144730107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144730107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144730109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144730111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144730120, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144730123, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 282238359
:::MLLOG {"namespace": "", "time_ms": 1593144739225, "event_type": "POINT_IN_TIME", "key": "seed", "value": 282238359, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1871150617
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593144753008, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593144753010, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593144753010, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593144753011, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593144753011, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593144754958, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593144754959, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593144754959, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593144755315, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593144755316, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593144755316, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593144755317, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593144755317, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593144755317, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593144755317, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593144755317, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593144755317, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593144755317, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593144755318, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144755318, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 633226497
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.413 (0.413)	Data 2.61e-01 (2.61e-01)	Tok/s 84286 (84286)	Loss/tok 10.7448 (10.7448)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.068 (0.119)	Data 1.11e-04 (2.94e-02)	Tok/s 222909 (205639)	Loss/tok 9.4636 (10.0708)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.067 (0.105)	Data 1.12e-04 (1.69e-02)	Tok/s 233881 (215690)	Loss/tok 9.0577 (9.7183)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.067 (0.099)	Data 1.11e-04 (1.21e-02)	Tok/s 229266 (219667)	Loss/tok 8.7964 (9.4936)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.100 (0.097)	Data 1.21e-04 (9.16e-03)	Tok/s 254707 (225768)	Loss/tok 8.5768 (9.2962)	LR 7.390e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][50/1291]	Time 0.099 (0.094)	Data 1.10e-04 (7.38e-03)	Tok/s 254296 (229399)	Loss/tok 8.6552 (9.1386)	LR 9.092e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][60/1291]	Time 0.067 (0.094)	Data 1.09e-04 (6.19e-03)	Tok/s 231285 (231394)	Loss/tok 8.2274 (9.0166)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.099 (0.095)	Data 1.33e-04 (5.34e-03)	Tok/s 253285 (233723)	Loss/tok 8.1476 (8.9010)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.098 (0.095)	Data 1.11e-04 (4.69e-03)	Tok/s 255264 (235618)	Loss/tok 8.0137 (8.7882)	LR 1.773e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][90/1291]	Time 0.133 (0.096)	Data 1.10e-04 (4.19e-03)	Tok/s 263473 (237079)	Loss/tok 8.0868 (8.6918)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.066 (0.094)	Data 1.27e-04 (3.78e-03)	Tok/s 234506 (237431)	Loss/tok 7.7639 (8.6220)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.134 (0.094)	Data 1.10e-04 (3.45e-03)	Tok/s 260531 (238316)	Loss/tok 7.9766 (8.5546)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.099 (0.093)	Data 1.07e-04 (3.18e-03)	Tok/s 251747 (238768)	Loss/tok 7.9349 (8.4982)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.066 (0.093)	Data 1.10e-04 (2.94e-03)	Tok/s 236636 (239181)	Loss/tok 8.1048 (8.4485)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.099 (0.092)	Data 1.13e-04 (2.74e-03)	Tok/s 252054 (239506)	Loss/tok 7.8904 (8.4114)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.098 (0.093)	Data 1.08e-04 (2.57e-03)	Tok/s 254400 (240095)	Loss/tok 7.6617 (8.3605)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.066 (0.093)	Data 1.10e-04 (2.42e-03)	Tok/s 230817 (240314)	Loss/tok 7.4418 (8.3095)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.067 (0.092)	Data 1.09e-04 (2.28e-03)	Tok/s 230553 (240512)	Loss/tok 7.0646 (8.2558)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.099 (0.092)	Data 1.13e-04 (2.16e-03)	Tok/s 253772 (240671)	Loss/tok 7.2637 (8.2049)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.099 (0.092)	Data 3.00e-04 (2.05e-03)	Tok/s 256483 (240956)	Loss/tok 7.0433 (8.1454)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.099 (0.092)	Data 1.08e-04 (1.96e-03)	Tok/s 258500 (241020)	Loss/tok 6.9495 (8.0907)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.099 (0.092)	Data 1.09e-04 (1.87e-03)	Tok/s 254441 (241445)	Loss/tok 6.7389 (8.0215)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][220/1291]	Time 0.134 (0.093)	Data 1.11e-04 (1.79e-03)	Tok/s 259774 (241915)	Loss/tok 6.6005 (7.9446)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.066 (0.093)	Data 1.07e-04 (1.72e-03)	Tok/s 234200 (242077)	Loss/tok 6.1210 (7.8727)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.036 (0.093)	Data 1.17e-04 (1.65e-03)	Tok/s 226875 (242102)	Loss/tok 5.4757 (7.8012)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.036 (0.093)	Data 1.07e-04 (1.59e-03)	Tok/s 221826 (242082)	Loss/tok 5.1476 (7.7399)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.135 (0.093)	Data 1.28e-04 (1.53e-03)	Tok/s 259451 (242277)	Loss/tok 6.1147 (7.6732)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.036 (0.093)	Data 1.30e-04 (1.48e-03)	Tok/s 220553 (242316)	Loss/tok 4.8284 (7.6102)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.066 (0.093)	Data 1.06e-04 (1.43e-03)	Tok/s 233088 (242500)	Loss/tok 5.4313 (7.5404)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.066 (0.093)	Data 1.07e-04 (1.39e-03)	Tok/s 235572 (242470)	Loss/tok 5.3493 (7.4746)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.099 (0.092)	Data 1.06e-04 (1.34e-03)	Tok/s 253622 (242173)	Loss/tok 5.5026 (7.4250)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.067 (0.092)	Data 1.09e-04 (1.31e-03)	Tok/s 229068 (242145)	Loss/tok 5.1513 (7.3642)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.174 (0.092)	Data 1.10e-04 (1.27e-03)	Tok/s 258118 (242327)	Loss/tok 5.6236 (7.2947)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.100 (0.092)	Data 1.08e-04 (1.23e-03)	Tok/s 251298 (242314)	Loss/tok 5.1853 (7.2367)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.066 (0.092)	Data 1.09e-04 (1.20e-03)	Tok/s 234885 (242448)	Loss/tok 4.6631 (7.1706)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][350/1291]	Time 0.100 (0.092)	Data 1.08e-04 (1.17e-03)	Tok/s 254395 (242483)	Loss/tok 4.8750 (7.1116)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.100 (0.091)	Data 1.07e-04 (1.14e-03)	Tok/s 248488 (242487)	Loss/tok 4.9074 (7.0548)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.134 (0.091)	Data 1.10e-04 (1.11e-03)	Tok/s 262201 (242540)	Loss/tok 4.9376 (6.9952)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.067 (0.091)	Data 1.27e-04 (1.09e-03)	Tok/s 235466 (242416)	Loss/tok 4.4306 (6.9386)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.099 (0.091)	Data 1.09e-04 (1.06e-03)	Tok/s 255845 (242481)	Loss/tok 4.7416 (6.8806)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.066 (0.091)	Data 1.33e-04 (1.04e-03)	Tok/s 231910 (242453)	Loss/tok 4.3378 (6.8234)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.067 (0.091)	Data 1.08e-04 (1.02e-03)	Tok/s 229136 (242465)	Loss/tok 4.2722 (6.7704)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.099 (0.091)	Data 1.08e-04 (9.94e-04)	Tok/s 255407 (242690)	Loss/tok 4.3495 (6.7074)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.036 (0.091)	Data 1.09e-04 (9.74e-04)	Tok/s 219038 (242630)	Loss/tok 3.4535 (6.6571)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.036 (0.091)	Data 1.09e-04 (9.54e-04)	Tok/s 216671 (242444)	Loss/tok 3.4982 (6.6158)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.036 (0.091)	Data 1.09e-04 (9.35e-04)	Tok/s 220231 (242605)	Loss/tok 3.4697 (6.5594)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.174 (0.091)	Data 1.08e-04 (9.18e-04)	Tok/s 256275 (242601)	Loss/tok 4.7589 (6.5132)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.100 (0.091)	Data 1.09e-04 (9.01e-04)	Tok/s 253555 (242579)	Loss/tok 4.2779 (6.4684)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][480/1291]	Time 0.035 (0.091)	Data 1.08e-04 (8.85e-04)	Tok/s 222553 (242559)	Loss/tok 3.3370 (6.4263)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.099 (0.090)	Data 1.08e-04 (8.69e-04)	Tok/s 251464 (242497)	Loss/tok 4.1890 (6.3865)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.100 (0.090)	Data 1.07e-04 (8.54e-04)	Tok/s 251421 (242568)	Loss/tok 4.1548 (6.3399)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.067 (0.091)	Data 1.08e-04 (8.39e-04)	Tok/s 232750 (242645)	Loss/tok 3.9174 (6.2938)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.135 (0.091)	Data 1.06e-04 (8.25e-04)	Tok/s 258225 (242650)	Loss/tok 4.3701 (6.2530)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.067 (0.091)	Data 1.13e-04 (8.12e-04)	Tok/s 232657 (242635)	Loss/tok 3.7166 (6.2149)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.099 (0.091)	Data 1.08e-04 (7.99e-04)	Tok/s 254169 (242696)	Loss/tok 4.2035 (6.1751)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.067 (0.091)	Data 1.11e-04 (7.86e-04)	Tok/s 232020 (242733)	Loss/tok 3.7926 (6.1358)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.100 (0.091)	Data 1.06e-04 (7.74e-04)	Tok/s 251355 (242808)	Loss/tok 3.9852 (6.0975)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.100 (0.091)	Data 1.10e-04 (7.63e-04)	Tok/s 253677 (242896)	Loss/tok 3.9893 (6.0604)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][580/1291]	Time 0.036 (0.090)	Data 1.11e-04 (7.51e-04)	Tok/s 218901 (242762)	Loss/tok 3.1715 (6.0308)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.136 (0.091)	Data 1.30e-04 (7.41e-04)	Tok/s 258501 (242881)	Loss/tok 4.2884 (5.9924)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.099 (0.091)	Data 1.12e-04 (7.30e-04)	Tok/s 254002 (242894)	Loss/tok 4.0531 (5.9586)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.067 (0.091)	Data 1.07e-04 (7.20e-04)	Tok/s 234407 (242914)	Loss/tok 3.7683 (5.9260)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.135 (0.091)	Data 1.07e-04 (7.10e-04)	Tok/s 259427 (242946)	Loss/tok 4.1205 (5.8933)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.135 (0.091)	Data 1.09e-04 (7.01e-04)	Tok/s 260586 (242885)	Loss/tok 4.2077 (5.8662)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.099 (0.091)	Data 1.24e-04 (6.92e-04)	Tok/s 254540 (242902)	Loss/tok 3.9147 (5.8355)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.099 (0.091)	Data 1.10e-04 (6.83e-04)	Tok/s 253812 (242957)	Loss/tok 4.0568 (5.8034)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.066 (0.091)	Data 1.08e-04 (6.74e-04)	Tok/s 230226 (242940)	Loss/tok 3.6697 (5.7760)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.066 (0.091)	Data 1.09e-04 (6.66e-04)	Tok/s 238957 (242843)	Loss/tok 3.5601 (5.7531)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.036 (0.090)	Data 1.10e-04 (6.58e-04)	Tok/s 221064 (242814)	Loss/tok 3.0343 (5.7279)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.100 (0.091)	Data 1.11e-04 (6.50e-04)	Tok/s 253809 (242786)	Loss/tok 3.9524 (5.7015)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][700/1291]	Time 0.099 (0.091)	Data 1.08e-04 (6.42e-04)	Tok/s 254429 (242818)	Loss/tok 3.8181 (5.6731)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.067 (0.091)	Data 1.13e-04 (6.35e-04)	Tok/s 228029 (242813)	Loss/tok 3.5493 (5.6480)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.135 (0.091)	Data 1.13e-04 (6.27e-04)	Tok/s 260629 (242876)	Loss/tok 4.0009 (5.6215)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.067 (0.091)	Data 1.10e-04 (6.20e-04)	Tok/s 229582 (242888)	Loss/tok 3.5857 (5.5967)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.099 (0.090)	Data 1.09e-04 (6.13e-04)	Tok/s 256674 (242831)	Loss/tok 3.8038 (5.5755)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.099 (0.091)	Data 1.07e-04 (6.07e-04)	Tok/s 251628 (242876)	Loss/tok 3.8082 (5.5500)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.066 (0.091)	Data 1.10e-04 (6.00e-04)	Tok/s 232596 (242927)	Loss/tok 3.5150 (5.5258)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.173 (0.091)	Data 1.07e-04 (5.94e-04)	Tok/s 253728 (242866)	Loss/tok 4.3984 (5.5061)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.036 (0.090)	Data 1.07e-04 (5.88e-04)	Tok/s 218878 (242796)	Loss/tok 2.9426 (5.4878)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.175 (0.090)	Data 1.09e-04 (5.82e-04)	Tok/s 256021 (242829)	Loss/tok 4.1307 (5.4649)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.76e-04)	Tok/s 227799 (242813)	Loss/tok 3.4217 (5.4443)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.099 (0.091)	Data 1.16e-04 (5.70e-04)	Tok/s 254010 (242918)	Loss/tok 3.7777 (5.4187)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.099 (0.091)	Data 1.08e-04 (5.65e-04)	Tok/s 256001 (242987)	Loss/tok 3.7842 (5.3968)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][830/1291]	Time 0.067 (0.091)	Data 1.29e-04 (5.59e-04)	Tok/s 234931 (242943)	Loss/tok 3.5758 (5.3793)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.067 (0.090)	Data 1.12e-04 (5.54e-04)	Tok/s 232205 (242887)	Loss/tok 3.4637 (5.3614)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.099 (0.090)	Data 1.08e-04 (5.49e-04)	Tok/s 251631 (242854)	Loss/tok 3.6710 (5.3435)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.036 (0.091)	Data 1.08e-04 (5.44e-04)	Tok/s 223417 (242891)	Loss/tok 3.0008 (5.3232)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.067 (0.091)	Data 1.10e-04 (5.39e-04)	Tok/s 237338 (242899)	Loss/tok 3.4437 (5.3058)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.134 (0.090)	Data 1.08e-04 (5.34e-04)	Tok/s 262028 (242865)	Loss/tok 3.9784 (5.2891)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.29e-04)	Tok/s 232171 (242870)	Loss/tok 3.4839 (5.2716)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][900/1291]	Time 0.067 (0.091)	Data 1.09e-04 (5.25e-04)	Tok/s 234002 (242908)	Loss/tok 3.4998 (5.2529)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.133 (0.091)	Data 1.11e-04 (5.20e-04)	Tok/s 262833 (242921)	Loss/tok 3.9328 (5.2362)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.066 (0.091)	Data 1.09e-04 (5.16e-04)	Tok/s 234238 (242940)	Loss/tok 3.4472 (5.2186)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.174 (0.091)	Data 1.10e-04 (5.11e-04)	Tok/s 257970 (242964)	Loss/tok 4.0330 (5.2012)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.036 (0.091)	Data 1.24e-04 (5.07e-04)	Tok/s 223729 (242915)	Loss/tok 2.9191 (5.1872)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.067 (0.091)	Data 1.07e-04 (5.03e-04)	Tok/s 234530 (242950)	Loss/tok 3.4335 (5.1718)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.067 (0.090)	Data 1.06e-04 (4.99e-04)	Tok/s 236825 (242927)	Loss/tok 3.3707 (5.1574)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.067 (0.090)	Data 1.10e-04 (4.95e-04)	Tok/s 232921 (242883)	Loss/tok 3.4782 (5.1441)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.066 (0.090)	Data 1.08e-04 (4.91e-04)	Tok/s 234161 (242849)	Loss/tok 3.4182 (5.1306)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.099 (0.090)	Data 1.09e-04 (4.87e-04)	Tok/s 253899 (242832)	Loss/tok 3.6195 (5.1161)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.066 (0.090)	Data 1.10e-04 (4.83e-04)	Tok/s 235466 (242837)	Loss/tok 3.3436 (5.1017)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.066 (0.090)	Data 1.09e-04 (4.80e-04)	Tok/s 234960 (242856)	Loss/tok 3.2712 (5.0869)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1020/1291]	Time 0.066 (0.090)	Data 1.15e-04 (4.76e-04)	Tok/s 234427 (242867)	Loss/tok 3.3806 (5.0723)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.066 (0.090)	Data 1.10e-04 (4.73e-04)	Tok/s 231808 (242857)	Loss/tok 3.4054 (5.0589)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.067 (0.090)	Data 1.10e-04 (4.69e-04)	Tok/s 231766 (242751)	Loss/tok 3.3619 (5.0485)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.099 (0.090)	Data 1.08e-04 (4.66e-04)	Tok/s 257744 (242743)	Loss/tok 3.6977 (5.0361)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.067 (0.090)	Data 1.08e-04 (4.62e-04)	Tok/s 234958 (242738)	Loss/tok 3.3415 (5.0234)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.066 (0.090)	Data 1.10e-04 (4.59e-04)	Tok/s 230278 (242735)	Loss/tok 3.5178 (5.0103)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.067 (0.090)	Data 1.09e-04 (4.56e-04)	Tok/s 228559 (242704)	Loss/tok 3.4353 (4.9991)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.173 (0.090)	Data 1.17e-04 (4.53e-04)	Tok/s 256787 (242677)	Loss/tok 4.0895 (4.9874)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.066 (0.090)	Data 1.07e-04 (4.50e-04)	Tok/s 232958 (242627)	Loss/tok 3.4344 (4.9770)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.066 (0.090)	Data 1.22e-04 (4.47e-04)	Tok/s 231854 (242622)	Loss/tok 3.4424 (4.9651)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.099 (0.090)	Data 1.08e-04 (4.44e-04)	Tok/s 252088 (242636)	Loss/tok 3.6722 (4.9530)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.067 (0.090)	Data 1.29e-04 (4.41e-04)	Tok/s 231950 (242641)	Loss/tok 3.4136 (4.9417)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.136 (0.089)	Data 1.08e-04 (4.38e-04)	Tok/s 255976 (242608)	Loss/tok 3.8771 (4.9316)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1150/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.35e-04)	Tok/s 256553 (242585)	Loss/tok 3.5953 (4.9207)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1160/1291]	Time 0.067 (0.089)	Data 1.07e-04 (4.32e-04)	Tok/s 229020 (242563)	Loss/tok 3.4392 (4.9101)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.134 (0.089)	Data 1.07e-04 (4.29e-04)	Tok/s 257336 (242539)	Loss/tok 3.8833 (4.8998)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.174 (0.089)	Data 1.08e-04 (4.27e-04)	Tok/s 256092 (242599)	Loss/tok 3.9632 (4.8871)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.24e-04)	Tok/s 232331 (242620)	Loss/tok 3.3913 (4.8758)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.100 (0.089)	Data 1.07e-04 (4.22e-04)	Tok/s 254032 (242597)	Loss/tok 3.6148 (4.8660)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.099 (0.090)	Data 1.10e-04 (4.19e-04)	Tok/s 254161 (242653)	Loss/tok 3.5592 (4.8543)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.135 (0.089)	Data 1.07e-04 (4.17e-04)	Tok/s 255274 (242640)	Loss/tok 3.7322 (4.8442)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.135 (0.089)	Data 1.11e-04 (4.14e-04)	Tok/s 262103 (242664)	Loss/tok 3.6553 (4.8334)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.067 (0.089)	Data 1.08e-04 (4.12e-04)	Tok/s 234010 (242621)	Loss/tok 3.3236 (4.8244)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.134 (0.089)	Data 1.07e-04 (4.09e-04)	Tok/s 259109 (242612)	Loss/tok 3.7830 (4.8151)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.067 (0.089)	Data 1.07e-04 (4.07e-04)	Tok/s 232954 (242596)	Loss/tok 3.3261 (4.8059)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.036 (0.089)	Data 1.12e-04 (4.05e-04)	Tok/s 224668 (242534)	Loss/tok 2.8351 (4.7979)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1280/1291]	Time 0.067 (0.089)	Data 1.26e-04 (4.02e-04)	Tok/s 234927 (242481)	Loss/tok 3.2601 (4.7895)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.173 (0.089)	Data 4.36e-05 (4.03e-04)	Tok/s 258469 (242479)	Loss/tok 3.9271 (4.7796)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593144870819, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144870819, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.509 (0.509)	Decoder iters 149.0 (149.0)	Tok/s 33699 (33699)
0: Running moses detokenizer
0: BLEU(score=18.77683461351911, counts=[35129, 15918, 8402, 4619], totals=[69230, 66227, 63224, 60226], precisions=[50.74245269391882, 24.03551421625621, 13.289257244084524, 7.669445090160396], bp=1.0, sys_len=69230, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593144872965, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.18780000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144872966, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7798	Test BLEU: 18.78
0: Performance: Epoch: 0	Training: 1939589 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593144872966, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144872966, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144872966, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 931737552
0: TRAIN [1][0/1291]	Time 0.252 (0.252)	Data 1.94e-01 (1.94e-01)	Tok/s 32015 (32015)	Loss/tok 2.9696 (2.9696)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.100 (0.115)	Data 1.05e-04 (1.78e-02)	Tok/s 253682 (225032)	Loss/tok 3.5861 (3.6318)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.066 (0.094)	Data 1.20e-04 (9.35e-03)	Tok/s 236385 (229820)	Loss/tok 3.2595 (3.5264)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.099 (0.097)	Data 1.09e-04 (6.37e-03)	Tok/s 254651 (236507)	Loss/tok 3.4727 (3.5475)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.099 (0.095)	Data 1.10e-04 (4.84e-03)	Tok/s 255201 (238287)	Loss/tok 3.4792 (3.5307)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.067 (0.094)	Data 1.12e-04 (3.92e-03)	Tok/s 234595 (238956)	Loss/tok 3.2674 (3.5326)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.067 (0.094)	Data 1.05e-04 (3.29e-03)	Tok/s 231392 (239261)	Loss/tok 3.2826 (3.5363)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.066 (0.094)	Data 1.03e-04 (2.85e-03)	Tok/s 233077 (240111)	Loss/tok 3.2371 (3.5459)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.067 (0.093)	Data 1.06e-04 (2.51e-03)	Tok/s 233736 (240495)	Loss/tok 3.3257 (3.5476)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.067 (0.092)	Data 1.07e-04 (2.24e-03)	Tok/s 235207 (240267)	Loss/tok 3.2185 (3.5360)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.066 (0.091)	Data 1.07e-04 (2.03e-03)	Tok/s 233955 (240189)	Loss/tok 3.3104 (3.5317)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.175 (0.092)	Data 1.05e-04 (1.86e-03)	Tok/s 251717 (240788)	Loss/tok 3.9366 (3.5359)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][120/1291]	Time 0.067 (0.092)	Data 1.13e-04 (1.71e-03)	Tok/s 233162 (241052)	Loss/tok 3.2777 (3.5330)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.100 (0.092)	Data 1.16e-04 (1.59e-03)	Tok/s 253320 (241282)	Loss/tok 3.5223 (3.5298)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.067 (0.091)	Data 1.15e-04 (1.49e-03)	Tok/s 230574 (241006)	Loss/tok 3.1717 (3.5214)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.067 (0.090)	Data 1.12e-04 (1.40e-03)	Tok/s 234250 (240740)	Loss/tok 3.3115 (3.5125)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.067 (0.090)	Data 1.09e-04 (1.32e-03)	Tok/s 229518 (240970)	Loss/tok 3.1269 (3.5100)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][170/1291]	Time 0.100 (0.091)	Data 1.06e-04 (1.25e-03)	Tok/s 249086 (241219)	Loss/tok 3.5482 (3.5167)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.067 (0.091)	Data 1.10e-04 (1.18e-03)	Tok/s 231013 (241280)	Loss/tok 3.2390 (3.5150)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.100 (0.091)	Data 1.29e-04 (1.13e-03)	Tok/s 251445 (241355)	Loss/tok 3.3919 (3.5121)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.067 (0.090)	Data 1.08e-04 (1.08e-03)	Tok/s 232458 (241199)	Loss/tok 3.2976 (3.5108)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][210/1291]	Time 0.134 (0.090)	Data 1.26e-04 (1.03e-03)	Tok/s 258256 (241020)	Loss/tok 3.6838 (3.5075)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.134 (0.090)	Data 1.23e-04 (9.90e-04)	Tok/s 261035 (241042)	Loss/tok 3.6103 (3.5054)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.135 (0.090)	Data 1.08e-04 (9.52e-04)	Tok/s 258609 (241238)	Loss/tok 3.6903 (3.5033)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.067 (0.090)	Data 1.11e-04 (9.17e-04)	Tok/s 232402 (241125)	Loss/tok 3.1063 (3.5018)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.067 (0.090)	Data 1.08e-04 (8.85e-04)	Tok/s 230409 (241319)	Loss/tok 3.2321 (3.5020)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.099 (0.090)	Data 1.08e-04 (8.55e-04)	Tok/s 254567 (241343)	Loss/tok 3.4819 (3.4982)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.174 (0.089)	Data 1.08e-04 (8.28e-04)	Tok/s 256615 (241079)	Loss/tok 3.8111 (3.4944)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.067 (0.089)	Data 1.10e-04 (8.02e-04)	Tok/s 233463 (241214)	Loss/tok 3.2182 (3.4932)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.067 (0.089)	Data 1.12e-04 (7.79e-04)	Tok/s 230792 (241005)	Loss/tok 3.1521 (3.4893)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.066 (0.089)	Data 1.08e-04 (7.57e-04)	Tok/s 233499 (241059)	Loss/tok 3.2059 (3.4889)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.137 (0.089)	Data 1.11e-04 (7.36e-04)	Tok/s 256873 (241026)	Loss/tok 3.6572 (3.4855)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.099 (0.089)	Data 1.07e-04 (7.17e-04)	Tok/s 252563 (241090)	Loss/tok 3.5089 (3.4825)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.136 (0.089)	Data 1.11e-04 (6.98e-04)	Tok/s 258994 (241124)	Loss/tok 3.6614 (3.4821)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][340/1291]	Time 0.099 (0.089)	Data 1.09e-04 (6.81e-04)	Tok/s 254291 (241318)	Loss/tok 3.4125 (3.4828)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.100 (0.089)	Data 1.09e-04 (6.65e-04)	Tok/s 247826 (241286)	Loss/tok 3.5442 (3.4806)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.099 (0.089)	Data 1.12e-04 (6.50e-04)	Tok/s 255554 (241375)	Loss/tok 3.3461 (3.4806)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][370/1291]	Time 0.067 (0.089)	Data 1.23e-04 (6.36e-04)	Tok/s 227551 (241301)	Loss/tok 3.1588 (3.4795)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.066 (0.089)	Data 1.08e-04 (6.22e-04)	Tok/s 235087 (241426)	Loss/tok 3.3037 (3.4791)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.067 (0.089)	Data 1.30e-04 (6.09e-04)	Tok/s 236449 (241486)	Loss/tok 3.2718 (3.4797)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.066 (0.089)	Data 1.17e-04 (5.97e-04)	Tok/s 232970 (241479)	Loss/tok 3.2128 (3.4802)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.099 (0.089)	Data 1.08e-04 (5.85e-04)	Tok/s 254975 (241454)	Loss/tok 3.5202 (3.4792)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.099 (0.089)	Data 1.13e-04 (5.74e-04)	Tok/s 253615 (241544)	Loss/tok 3.5661 (3.4780)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.066 (0.089)	Data 1.34e-04 (5.63e-04)	Tok/s 230281 (241506)	Loss/tok 3.2574 (3.4801)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.099 (0.089)	Data 1.12e-04 (5.53e-04)	Tok/s 255944 (241593)	Loss/tok 3.4249 (3.4789)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.067 (0.089)	Data 1.12e-04 (5.43e-04)	Tok/s 232189 (241669)	Loss/tok 3.2418 (3.4770)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.066 (0.089)	Data 1.12e-04 (5.34e-04)	Tok/s 238051 (241680)	Loss/tok 3.2133 (3.4761)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.067 (0.089)	Data 1.13e-04 (5.25e-04)	Tok/s 231510 (241597)	Loss/tok 3.1144 (3.4735)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.066 (0.089)	Data 1.14e-04 (5.16e-04)	Tok/s 236300 (241639)	Loss/tok 3.2910 (3.4742)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.08e-04)	Tok/s 256765 (241751)	Loss/tok 3.4118 (3.4758)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][500/1291]	Time 0.066 (0.089)	Data 1.08e-04 (5.00e-04)	Tok/s 235647 (241669)	Loss/tok 3.1899 (3.4748)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.100 (0.089)	Data 1.34e-04 (4.93e-04)	Tok/s 257212 (241885)	Loss/tok 3.4093 (3.4749)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.099 (0.090)	Data 1.09e-04 (4.85e-04)	Tok/s 253838 (242003)	Loss/tok 3.5148 (3.4749)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.036 (0.090)	Data 1.08e-04 (4.78e-04)	Tok/s 223611 (242033)	Loss/tok 2.9042 (3.4754)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.135 (0.090)	Data 1.11e-04 (4.72e-04)	Tok/s 258876 (242113)	Loss/tok 3.6636 (3.4759)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.099 (0.090)	Data 1.11e-04 (4.65e-04)	Tok/s 255849 (242142)	Loss/tok 3.5126 (3.4752)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.134 (0.090)	Data 1.10e-04 (4.59e-04)	Tok/s 262104 (242273)	Loss/tok 3.6516 (3.4745)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.53e-04)	Tok/s 251754 (242175)	Loss/tok 3.4461 (3.4719)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.066 (0.090)	Data 1.09e-04 (4.47e-04)	Tok/s 229831 (242221)	Loss/tok 3.2500 (3.4702)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][590/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.41e-04)	Tok/s 253303 (242254)	Loss/tok 3.5096 (3.4711)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.36e-04)	Tok/s 228092 (242267)	Loss/tok 3.1754 (3.4723)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.174 (0.091)	Data 1.13e-04 (4.30e-04)	Tok/s 257373 (242458)	Loss/tok 3.7488 (3.4759)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.100 (0.091)	Data 1.12e-04 (4.25e-04)	Tok/s 251345 (242441)	Loss/tok 3.3934 (3.4755)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.067 (0.091)	Data 1.10e-04 (4.20e-04)	Tok/s 232756 (242414)	Loss/tok 3.1767 (3.4732)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.099 (0.091)	Data 1.12e-04 (4.16e-04)	Tok/s 257680 (242440)	Loss/tok 3.4417 (3.4731)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.135 (0.091)	Data 1.12e-04 (4.11e-04)	Tok/s 260331 (242528)	Loss/tok 3.5868 (3.4735)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.067 (0.091)	Data 1.09e-04 (4.06e-04)	Tok/s 234686 (242507)	Loss/tok 3.2662 (3.4718)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.066 (0.090)	Data 1.30e-04 (4.02e-04)	Tok/s 233723 (242369)	Loss/tok 3.2354 (3.4692)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.98e-04)	Tok/s 232540 (242358)	Loss/tok 3.2520 (3.4675)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.066 (0.090)	Data 1.06e-04 (3.94e-04)	Tok/s 234381 (242343)	Loss/tok 3.2026 (3.4679)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.036 (0.090)	Data 1.09e-04 (3.90e-04)	Tok/s 225009 (242252)	Loss/tok 2.6927 (3.4664)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.066 (0.090)	Data 1.11e-04 (3.86e-04)	Tok/s 235735 (242215)	Loss/tok 3.2950 (3.4647)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][720/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.82e-04)	Tok/s 234821 (242227)	Loss/tok 3.1294 (3.4634)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.036 (0.090)	Data 1.13e-04 (3.79e-04)	Tok/s 219066 (242134)	Loss/tok 2.7293 (3.4625)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.75e-04)	Tok/s 232857 (242098)	Loss/tok 3.1483 (3.4613)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.099 (0.090)	Data 1.10e-04 (3.71e-04)	Tok/s 255169 (242138)	Loss/tok 3.3795 (3.4610)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.067 (0.090)	Data 1.11e-04 (3.68e-04)	Tok/s 234443 (242162)	Loss/tok 3.1673 (3.4611)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.036 (0.090)	Data 1.31e-04 (3.65e-04)	Tok/s 225577 (242219)	Loss/tok 2.7873 (3.4614)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.067 (0.090)	Data 1.21e-04 (3.62e-04)	Tok/s 228976 (242324)	Loss/tok 3.1955 (3.4614)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.099 (0.090)	Data 1.09e-04 (3.58e-04)	Tok/s 252775 (242334)	Loss/tok 3.3958 (3.4608)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.099 (0.090)	Data 1.10e-04 (3.55e-04)	Tok/s 253775 (242324)	Loss/tok 3.4208 (3.4604)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.099 (0.090)	Data 1.23e-04 (3.52e-04)	Tok/s 252787 (242345)	Loss/tok 3.4026 (3.4596)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.135 (0.090)	Data 1.13e-04 (3.50e-04)	Tok/s 259947 (242387)	Loss/tok 3.5430 (3.4586)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.066 (0.090)	Data 1.08e-04 (3.47e-04)	Tok/s 235034 (242368)	Loss/tok 3.2271 (3.4576)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.100 (0.090)	Data 1.16e-04 (3.44e-04)	Tok/s 250153 (242346)	Loss/tok 3.4292 (3.4574)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][850/1291]	Time 0.173 (0.090)	Data 1.17e-04 (3.41e-04)	Tok/s 259861 (242355)	Loss/tok 3.7215 (3.4574)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.099 (0.090)	Data 1.14e-04 (3.39e-04)	Tok/s 252654 (242430)	Loss/tok 3.4162 (3.4568)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.099 (0.090)	Data 1.12e-04 (3.36e-04)	Tok/s 258549 (242453)	Loss/tok 3.4295 (3.4554)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.067 (0.090)	Data 1.09e-04 (3.34e-04)	Tok/s 228388 (242332)	Loss/tok 3.2433 (3.4532)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.100 (0.090)	Data 1.09e-04 (3.31e-04)	Tok/s 250256 (242317)	Loss/tok 3.3484 (3.4524)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.036 (0.089)	Data 1.24e-04 (3.29e-04)	Tok/s 222306 (242234)	Loss/tok 2.6247 (3.4506)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.067 (0.089)	Data 1.30e-04 (3.27e-04)	Tok/s 231939 (242232)	Loss/tok 3.1573 (3.4494)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.24e-04)	Tok/s 254799 (242227)	Loss/tok 3.3945 (3.4486)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.099 (0.089)	Data 1.08e-04 (3.22e-04)	Tok/s 254122 (242318)	Loss/tok 3.3438 (3.4488)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.135 (0.090)	Data 1.09e-04 (3.20e-04)	Tok/s 257888 (242418)	Loss/tok 3.5784 (3.4493)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.066 (0.090)	Data 1.08e-04 (3.18e-04)	Tok/s 231783 (242411)	Loss/tok 3.2921 (3.4485)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.099 (0.090)	Data 1.17e-04 (3.16e-04)	Tok/s 254754 (242470)	Loss/tok 3.4118 (3.4482)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.100 (0.090)	Data 1.05e-04 (3.14e-04)	Tok/s 256663 (242501)	Loss/tok 3.3106 (3.4473)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][980/1291]	Time 0.135 (0.090)	Data 1.10e-04 (3.12e-04)	Tok/s 260779 (242472)	Loss/tok 3.6202 (3.4464)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.099 (0.090)	Data 1.09e-04 (3.10e-04)	Tok/s 258913 (242506)	Loss/tok 3.3998 (3.4457)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.174 (0.089)	Data 1.13e-04 (3.08e-04)	Tok/s 256894 (242433)	Loss/tok 3.6620 (3.4446)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.174 (0.089)	Data 1.18e-04 (3.06e-04)	Tok/s 256123 (242416)	Loss/tok 3.7678 (3.4441)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.099 (0.090)	Data 1.11e-04 (3.04e-04)	Tok/s 252037 (242477)	Loss/tok 3.4091 (3.4438)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.099 (0.089)	Data 1.26e-04 (3.02e-04)	Tok/s 254604 (242428)	Loss/tok 3.3926 (3.4422)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.135 (0.089)	Data 1.12e-04 (3.00e-04)	Tok/s 260267 (242437)	Loss/tok 3.5617 (3.4420)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.99e-04)	Tok/s 232791 (242414)	Loss/tok 3.1470 (3.4417)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.97e-04)	Tok/s 254663 (242429)	Loss/tok 3.4404 (3.4410)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.95e-04)	Tok/s 233290 (242383)	Loss/tok 3.0557 (3.4397)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1080/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.94e-04)	Tok/s 233506 (242415)	Loss/tok 3.3164 (3.4395)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.066 (0.089)	Data 1.22e-04 (2.92e-04)	Tok/s 234866 (242433)	Loss/tok 3.1703 (3.4396)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.173 (0.090)	Data 1.12e-04 (2.90e-04)	Tok/s 256942 (242459)	Loss/tok 3.7822 (3.4407)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.135 (0.090)	Data 1.15e-04 (2.89e-04)	Tok/s 258962 (242430)	Loss/tok 3.5515 (3.4407)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.87e-04)	Tok/s 234358 (242442)	Loss/tok 3.1617 (3.4395)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.86e-04)	Tok/s 255367 (242457)	Loss/tok 3.3538 (3.4392)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.84e-04)	Tok/s 233792 (242389)	Loss/tok 3.1121 (3.4376)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.83e-04)	Tok/s 230312 (242410)	Loss/tok 3.1452 (3.4367)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.81e-04)	Tok/s 252877 (242459)	Loss/tok 3.3026 (3.4367)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.80e-04)	Tok/s 230006 (242478)	Loss/tok 3.2554 (3.4362)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.78e-04)	Tok/s 233092 (242487)	Loss/tok 3.1844 (3.4351)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.77e-04)	Tok/s 251268 (242515)	Loss/tok 3.3407 (3.4349)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.099 (0.089)	Data 1.33e-04 (2.76e-04)	Tok/s 256636 (242487)	Loss/tok 3.3200 (3.4338)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1210/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.74e-04)	Tok/s 221043 (242443)	Loss/tok 2.6244 (3.4327)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.135 (0.089)	Data 1.08e-04 (2.73e-04)	Tok/s 258567 (242431)	Loss/tok 3.5623 (3.4318)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.72e-04)	Tok/s 228200 (242396)	Loss/tok 3.1378 (3.4310)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.70e-04)	Tok/s 237448 (242365)	Loss/tok 3.1225 (3.4298)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.066 (0.089)	Data 1.08e-04 (2.69e-04)	Tok/s 231429 (242318)	Loss/tok 3.1495 (3.4286)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.68e-04)	Tok/s 255539 (242345)	Loss/tok 3.3091 (3.4281)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.67e-04)	Tok/s 253678 (242367)	Loss/tok 3.3487 (3.4274)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.66e-04)	Tok/s 231012 (242390)	Loss/tok 3.2691 (3.4273)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.067 (0.089)	Data 4.48e-05 (2.67e-04)	Tok/s 233629 (242352)	Loss/tok 3.1151 (3.4263)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593144988493, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593144988493, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.510 (0.510)	Decoder iters 149.0 (149.0)	Tok/s 32410 (32410)
0: Running moses detokenizer
0: BLEU(score=21.574029762007466, counts=[36021, 17210, 9376, 5336], totals=[66107, 63104, 60101, 57103], precisions=[54.488934606017516, 27.272439148073023, 15.60040598326151, 9.344517801166313], bp=1.0, sys_len=66107, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593144990530, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2157, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593144990531, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4265	Test BLEU: 21.57
0: Performance: Epoch: 1	Training: 1938606 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593144990531, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593144990531, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144990531, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3916182183
0: TRAIN [2][0/1291]	Time 0.280 (0.280)	Data 1.97e-01 (1.97e-01)	Tok/s 55125 (55125)	Loss/tok 3.1196 (3.1196)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.066 (0.104)	Data 1.09e-04 (1.80e-02)	Tok/s 230752 (214586)	Loss/tok 3.0726 (3.2366)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.067 (0.106)	Data 1.18e-04 (9.48e-03)	Tok/s 225819 (231654)	Loss/tok 3.1168 (3.2934)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.100 (0.098)	Data 1.11e-04 (6.46e-03)	Tok/s 252077 (234218)	Loss/tok 3.2762 (3.2726)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.100 (0.093)	Data 1.35e-04 (4.91e-03)	Tok/s 253690 (235658)	Loss/tok 3.3431 (3.2546)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][50/1291]	Time 0.066 (0.093)	Data 1.09e-04 (3.97e-03)	Tok/s 232725 (238039)	Loss/tok 3.0533 (3.2512)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.177 (0.092)	Data 1.11e-04 (3.34e-03)	Tok/s 254141 (239147)	Loss/tok 3.6336 (3.2549)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.099 (0.090)	Data 1.22e-04 (2.88e-03)	Tok/s 252519 (238993)	Loss/tok 3.2414 (3.2443)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.099 (0.090)	Data 1.09e-04 (2.54e-03)	Tok/s 249743 (239495)	Loss/tok 3.2957 (3.2442)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.099 (0.090)	Data 1.09e-04 (2.28e-03)	Tok/s 257497 (239977)	Loss/tok 3.2342 (3.2407)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.099 (0.088)	Data 1.11e-04 (2.06e-03)	Tok/s 255599 (239851)	Loss/tok 3.2689 (3.2336)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.067 (0.089)	Data 1.13e-04 (1.89e-03)	Tok/s 232821 (240429)	Loss/tok 3.1001 (3.2461)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.067 (0.088)	Data 1.10e-04 (1.74e-03)	Tok/s 233106 (240195)	Loss/tok 3.0190 (3.2413)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][130/1291]	Time 0.066 (0.089)	Data 1.24e-04 (1.62e-03)	Tok/s 234939 (240617)	Loss/tok 3.0750 (3.2548)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.036 (0.089)	Data 1.08e-04 (1.51e-03)	Tok/s 220819 (240764)	Loss/tok 2.7581 (3.2557)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.067 (0.090)	Data 1.20e-04 (1.42e-03)	Tok/s 228985 (241100)	Loss/tok 3.0898 (3.2644)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.099 (0.090)	Data 1.12e-04 (1.34e-03)	Tok/s 253654 (241146)	Loss/tok 3.2398 (3.2658)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.066 (0.089)	Data 1.10e-04 (1.26e-03)	Tok/s 233742 (241145)	Loss/tok 3.0070 (3.2628)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.067 (0.090)	Data 1.09e-04 (1.20e-03)	Tok/s 232480 (241224)	Loss/tok 3.0980 (3.2711)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.100 (0.089)	Data 1.10e-04 (1.14e-03)	Tok/s 255375 (241130)	Loss/tok 3.3664 (3.2674)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.067 (0.090)	Data 1.29e-04 (1.09e-03)	Tok/s 232683 (241374)	Loss/tok 3.0981 (3.2747)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.099 (0.091)	Data 1.08e-04 (1.05e-03)	Tok/s 250747 (241917)	Loss/tok 3.2428 (3.2831)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.067 (0.090)	Data 1.33e-04 (1.00e-03)	Tok/s 231995 (241742)	Loss/tok 3.0598 (3.2804)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.036 (0.091)	Data 1.38e-04 (9.65e-04)	Tok/s 219972 (241981)	Loss/tok 2.6500 (3.2852)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.100 (0.090)	Data 1.10e-04 (9.30e-04)	Tok/s 254025 (241972)	Loss/tok 3.3074 (3.2823)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.036 (0.090)	Data 1.11e-04 (8.98e-04)	Tok/s 221240 (241621)	Loss/tok 2.5774 (3.2795)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][260/1291]	Time 0.100 (0.090)	Data 1.11e-04 (8.67e-04)	Tok/s 254091 (241784)	Loss/tok 3.2751 (3.2821)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.067 (0.090)	Data 1.27e-04 (8.40e-04)	Tok/s 228853 (241919)	Loss/tok 3.1128 (3.2820)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.099 (0.090)	Data 2.59e-04 (8.15e-04)	Tok/s 253774 (242099)	Loss/tok 3.1940 (3.2838)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.067 (0.090)	Data 1.11e-04 (7.91e-04)	Tok/s 232961 (241966)	Loss/tok 3.1696 (3.2820)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.067 (0.091)	Data 1.08e-04 (7.68e-04)	Tok/s 229770 (242224)	Loss/tok 3.1366 (3.2861)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.135 (0.091)	Data 1.09e-04 (7.47e-04)	Tok/s 262796 (242271)	Loss/tok 3.4265 (3.2873)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.100 (0.090)	Data 1.09e-04 (7.27e-04)	Tok/s 252100 (242117)	Loss/tok 3.3388 (3.2849)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.067 (0.090)	Data 1.13e-04 (7.09e-04)	Tok/s 231302 (242190)	Loss/tok 3.2240 (3.2826)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.100 (0.090)	Data 1.09e-04 (6.91e-04)	Tok/s 251509 (242331)	Loss/tok 3.2264 (3.2814)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.100 (0.090)	Data 1.11e-04 (6.75e-04)	Tok/s 252417 (242478)	Loss/tok 3.3223 (3.2805)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.067 (0.090)	Data 1.10e-04 (6.59e-04)	Tok/s 230126 (242525)	Loss/tok 3.0676 (3.2817)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.067 (0.090)	Data 1.12e-04 (6.44e-04)	Tok/s 231827 (242303)	Loss/tok 3.1147 (3.2795)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][380/1291]	Time 0.066 (0.090)	Data 1.11e-04 (6.31e-04)	Tok/s 230866 (242347)	Loss/tok 3.0797 (3.2798)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.067 (0.090)	Data 1.10e-04 (6.17e-04)	Tok/s 232577 (242211)	Loss/tok 3.1052 (3.2782)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.067 (0.090)	Data 1.13e-04 (6.05e-04)	Tok/s 234513 (242268)	Loss/tok 3.0608 (3.2827)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.099 (0.090)	Data 1.13e-04 (5.93e-04)	Tok/s 253813 (242102)	Loss/tok 3.4556 (3.2800)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.82e-04)	Tok/s 230309 (242157)	Loss/tok 3.0660 (3.2806)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.066 (0.089)	Data 1.09e-04 (5.71e-04)	Tok/s 232254 (242147)	Loss/tok 3.0617 (3.2799)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][440/1291]	Time 0.067 (0.090)	Data 1.28e-04 (5.60e-04)	Tok/s 229840 (242159)	Loss/tok 3.0042 (3.2812)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.067 (0.090)	Data 1.09e-04 (5.51e-04)	Tok/s 231835 (242175)	Loss/tok 3.0340 (3.2794)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.066 (0.090)	Data 1.09e-04 (5.41e-04)	Tok/s 231629 (242273)	Loss/tok 3.1537 (3.2804)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.067 (0.090)	Data 1.24e-04 (5.32e-04)	Tok/s 234987 (242279)	Loss/tok 3.0597 (3.2794)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.100 (0.089)	Data 1.12e-04 (5.23e-04)	Tok/s 254224 (242274)	Loss/tok 3.3510 (3.2780)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.100 (0.089)	Data 1.10e-04 (5.15e-04)	Tok/s 251083 (242260)	Loss/tok 3.2677 (3.2772)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.07e-04)	Tok/s 234373 (242272)	Loss/tok 3.0703 (3.2764)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.99e-04)	Tok/s 235526 (242226)	Loss/tok 2.9838 (3.2756)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.066 (0.089)	Data 1.10e-04 (4.92e-04)	Tok/s 236389 (242293)	Loss/tok 3.1026 (3.2773)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.099 (0.089)	Data 1.12e-04 (4.85e-04)	Tok/s 252990 (242230)	Loss/tok 3.3331 (3.2762)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.099 (0.089)	Data 1.11e-04 (4.78e-04)	Tok/s 251792 (242297)	Loss/tok 3.2353 (3.2788)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.036 (0.089)	Data 1.13e-04 (4.71e-04)	Tok/s 222970 (242220)	Loss/tok 2.5877 (3.2791)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.066 (0.089)	Data 1.25e-04 (4.65e-04)	Tok/s 230567 (242123)	Loss/tok 3.0026 (3.2786)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][570/1291]	Time 0.067 (0.089)	Data 1.31e-04 (4.59e-04)	Tok/s 236546 (242066)	Loss/tok 3.0853 (3.2773)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.100 (0.089)	Data 1.14e-04 (4.53e-04)	Tok/s 252596 (242083)	Loss/tok 3.2656 (3.2775)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.47e-04)	Tok/s 231673 (241999)	Loss/tok 3.1035 (3.2762)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.036 (0.088)	Data 1.10e-04 (4.42e-04)	Tok/s 222912 (241875)	Loss/tok 2.5472 (3.2747)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.100 (0.088)	Data 1.33e-04 (4.36e-04)	Tok/s 252283 (241901)	Loss/tok 3.1321 (3.2743)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.134 (0.089)	Data 1.09e-04 (4.31e-04)	Tok/s 263636 (242031)	Loss/tok 3.4968 (3.2761)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.135 (0.088)	Data 1.11e-04 (4.26e-04)	Tok/s 256736 (241989)	Loss/tok 3.5166 (3.2750)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.21e-04)	Tok/s 232418 (242037)	Loss/tok 3.0685 (3.2767)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.137 (0.089)	Data 1.14e-04 (4.16e-04)	Tok/s 254180 (242106)	Loss/tok 3.4998 (3.2777)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.035 (0.089)	Data 1.12e-04 (4.12e-04)	Tok/s 226298 (242072)	Loss/tok 2.6181 (3.2768)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.08e-04)	Tok/s 253124 (242174)	Loss/tok 3.2668 (3.2780)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.036 (0.089)	Data 1.10e-04 (4.03e-04)	Tok/s 224022 (242151)	Loss/tok 2.5661 (3.2772)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.100 (0.088)	Data 1.10e-04 (3.99e-04)	Tok/s 251328 (242152)	Loss/tok 3.2972 (3.2763)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][700/1291]	Time 0.036 (0.088)	Data 1.08e-04 (3.95e-04)	Tok/s 220643 (242075)	Loss/tok 2.7013 (3.2759)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.175 (0.088)	Data 1.10e-04 (3.91e-04)	Tok/s 257657 (242021)	Loss/tok 3.6733 (3.2756)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.87e-04)	Tok/s 232904 (242005)	Loss/tok 3.0617 (3.2750)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.036 (0.088)	Data 1.24e-04 (3.83e-04)	Tok/s 218741 (242001)	Loss/tok 2.6976 (3.2746)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.036 (0.088)	Data 1.14e-04 (3.80e-04)	Tok/s 224462 (241888)	Loss/tok 2.6793 (3.2737)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.174 (0.088)	Data 1.12e-04 (3.76e-04)	Tok/s 255444 (241921)	Loss/tok 3.6683 (3.2749)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.135 (0.088)	Data 1.16e-04 (3.73e-04)	Tok/s 256598 (241920)	Loss/tok 3.4876 (3.2754)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.174 (0.088)	Data 1.12e-04 (3.69e-04)	Tok/s 257522 (241951)	Loss/tok 3.6454 (3.2759)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.66e-04)	Tok/s 252865 (242094)	Loss/tok 3.2456 (3.2775)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.067 (0.088)	Data 1.20e-04 (3.63e-04)	Tok/s 224197 (242015)	Loss/tok 3.0055 (3.2768)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.099 (0.088)	Data 1.10e-04 (3.60e-04)	Tok/s 252871 (242024)	Loss/tok 3.3560 (3.2771)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.067 (0.088)	Data 1.09e-04 (3.57e-04)	Tok/s 232905 (242132)	Loss/tok 3.0570 (3.2786)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.066 (0.088)	Data 1.13e-04 (3.54e-04)	Tok/s 233387 (242066)	Loss/tok 3.0670 (3.2775)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][830/1291]	Time 0.100 (0.088)	Data 1.11e-04 (3.51e-04)	Tok/s 250517 (242095)	Loss/tok 3.3161 (3.2777)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.173 (0.088)	Data 1.34e-04 (3.48e-04)	Tok/s 257532 (242093)	Loss/tok 3.6481 (3.2790)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.101 (0.089)	Data 1.09e-04 (3.45e-04)	Tok/s 250365 (242179)	Loss/tok 3.2906 (3.2798)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][860/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.43e-04)	Tok/s 256070 (242242)	Loss/tok 3.3509 (3.2809)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.136 (0.089)	Data 1.10e-04 (3.40e-04)	Tok/s 262185 (242259)	Loss/tok 3.4205 (3.2816)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][880/1291]	Time 0.173 (0.089)	Data 1.11e-04 (3.37e-04)	Tok/s 258483 (242272)	Loss/tok 3.6910 (3.2831)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.35e-04)	Tok/s 232513 (242256)	Loss/tok 2.9863 (3.2827)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.036 (0.089)	Data 1.10e-04 (3.33e-04)	Tok/s 221851 (242228)	Loss/tok 2.6145 (3.2829)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.30e-04)	Tok/s 254325 (242195)	Loss/tok 3.2156 (3.2830)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.28e-04)	Tok/s 232111 (242170)	Loss/tok 3.0854 (3.2827)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.25e-04)	Tok/s 231381 (242109)	Loss/tok 3.0873 (3.2817)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.23e-04)	Tok/s 253703 (242135)	Loss/tok 3.3530 (3.2812)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.21e-04)	Tok/s 232850 (242071)	Loss/tok 3.0603 (3.2797)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.036 (0.089)	Data 1.30e-04 (3.19e-04)	Tok/s 222237 (242126)	Loss/tok 2.6827 (3.2808)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.135 (0.089)	Data 1.12e-04 (3.17e-04)	Tok/s 258033 (242163)	Loss/tok 3.5188 (3.2814)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.15e-04)	Tok/s 234492 (242165)	Loss/tok 2.9473 (3.2810)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.099 (0.089)	Data 1.14e-04 (3.13e-04)	Tok/s 254932 (242155)	Loss/tok 3.2925 (3.2801)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.11e-04)	Tok/s 235487 (242180)	Loss/tok 3.0713 (3.2804)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1010/1291]	Time 0.136 (0.089)	Data 1.13e-04 (3.09e-04)	Tok/s 259232 (242169)	Loss/tok 3.4162 (3.2796)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.036 (0.089)	Data 1.13e-04 (3.07e-04)	Tok/s 225904 (242147)	Loss/tok 2.7368 (3.2800)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.05e-04)	Tok/s 227854 (242155)	Loss/tok 3.0142 (3.2805)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.04e-04)	Tok/s 231611 (242167)	Loss/tok 3.0280 (3.2824)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.067 (0.089)	Data 1.29e-04 (3.02e-04)	Tok/s 229122 (242170)	Loss/tok 3.0969 (3.2831)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1060/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.00e-04)	Tok/s 253240 (242224)	Loss/tok 3.3696 (3.2844)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.98e-04)	Tok/s 230926 (242237)	Loss/tok 3.1399 (3.2844)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.96e-04)	Tok/s 231471 (242302)	Loss/tok 3.0498 (3.2861)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.135 (0.090)	Data 1.18e-04 (2.95e-04)	Tok/s 257698 (242380)	Loss/tok 3.3915 (3.2878)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.066 (0.090)	Data 1.11e-04 (2.93e-04)	Tok/s 233229 (242411)	Loss/tok 3.0728 (3.2884)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.067 (0.090)	Data 1.10e-04 (2.91e-04)	Tok/s 232520 (242394)	Loss/tok 3.0965 (3.2884)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.036 (0.089)	Data 1.09e-04 (2.90e-04)	Tok/s 222449 (242352)	Loss/tok 2.6837 (3.2874)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.066 (0.090)	Data 1.09e-04 (2.88e-04)	Tok/s 234184 (242387)	Loss/tok 3.0402 (3.2870)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.066 (0.090)	Data 1.21e-04 (2.87e-04)	Tok/s 231908 (242394)	Loss/tok 3.0973 (3.2870)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.85e-04)	Tok/s 236079 (242378)	Loss/tok 3.0362 (3.2861)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.84e-04)	Tok/s 251424 (242378)	Loss/tok 3.3122 (3.2859)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.82e-04)	Tok/s 229721 (242307)	Loss/tok 3.0332 (3.2852)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1180/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.81e-04)	Tok/s 252169 (242311)	Loss/tok 3.2899 (3.2851)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.80e-04)	Tok/s 254031 (242404)	Loss/tok 3.2656 (3.2860)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.78e-04)	Tok/s 230137 (242339)	Loss/tok 3.0655 (3.2861)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.77e-04)	Tok/s 247979 (242365)	Loss/tok 3.2080 (3.2864)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.75e-04)	Tok/s 231047 (242336)	Loss/tok 3.0757 (3.2860)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.174 (0.089)	Data 1.11e-04 (2.74e-04)	Tok/s 256116 (242346)	Loss/tok 3.5932 (3.2859)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.036 (0.089)	Data 1.18e-04 (2.73e-04)	Tok/s 225722 (242338)	Loss/tok 2.6521 (3.2851)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.72e-04)	Tok/s 227081 (242282)	Loss/tok 3.0287 (3.2839)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.036 (0.089)	Data 1.10e-04 (2.70e-04)	Tok/s 221138 (242239)	Loss/tok 2.5765 (3.2832)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.69e-04)	Tok/s 251723 (242230)	Loss/tok 3.2366 (3.2827)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.067 (0.089)	Data 1.31e-04 (2.68e-04)	Tok/s 235050 (242212)	Loss/tok 3.0639 (3.2825)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.036 (0.089)	Data 4.67e-05 (2.70e-04)	Tok/s 221763 (242181)	Loss/tok 2.6129 (3.2820)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593145106133, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593145106134, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.374 (0.374)	Decoder iters 96.0 (96.0)	Tok/s 43315 (43315)
0: Running moses detokenizer
0: BLEU(score=23.025311544253164, counts=[35836, 17641, 9895, 5821], totals=[63364, 60361, 57358, 54359], precisions=[56.55577299412916, 29.225824621858486, 17.25129885979288, 10.708438345076253], bp=0.9795071288979673, sys_len=63364, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593145108007, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2303, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593145108007, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2806	Test BLEU: 23.03
0: Performance: Epoch: 2	Training: 1937275 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593145108007, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593145108007, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593145108007, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3028598808
0: TRAIN [3][0/1291]	Time 0.300 (0.300)	Data 1.82e-01 (1.82e-01)	Tok/s 83486 (83486)	Loss/tok 3.1978 (3.1978)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.100 (0.117)	Data 1.06e-04 (1.66e-02)	Tok/s 248520 (229797)	Loss/tok 3.1987 (3.2192)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][20/1291]	Time 0.099 (0.106)	Data 1.01e-04 (8.75e-03)	Tok/s 252959 (236886)	Loss/tok 3.2384 (3.2240)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.099 (0.098)	Data 1.09e-04 (5.97e-03)	Tok/s 255219 (238615)	Loss/tok 3.1897 (3.1985)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.099 (0.092)	Data 1.04e-04 (4.54e-03)	Tok/s 254998 (238704)	Loss/tok 3.1744 (3.1725)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.099 (0.090)	Data 1.06e-04 (3.67e-03)	Tok/s 258739 (239819)	Loss/tok 3.1193 (3.1603)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.067 (0.091)	Data 1.09e-04 (3.09e-03)	Tok/s 232652 (240553)	Loss/tok 2.8859 (3.1738)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.066 (0.089)	Data 1.08e-04 (2.67e-03)	Tok/s 231929 (240245)	Loss/tok 2.9680 (3.1665)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.135 (0.092)	Data 1.17e-04 (2.35e-03)	Tok/s 257368 (241443)	Loss/tok 3.4126 (3.1878)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.066 (0.091)	Data 1.04e-04 (2.11e-03)	Tok/s 234441 (241320)	Loss/tok 2.9989 (3.1792)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.174 (0.092)	Data 1.03e-04 (1.91e-03)	Tok/s 260649 (241877)	Loss/tok 3.4417 (3.1909)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.067 (0.092)	Data 1.16e-04 (1.75e-03)	Tok/s 237322 (242272)	Loss/tok 2.9727 (3.1907)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.066 (0.092)	Data 1.03e-04 (1.61e-03)	Tok/s 233736 (242565)	Loss/tok 2.9677 (3.1892)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.067 (0.093)	Data 1.05e-04 (1.50e-03)	Tok/s 235786 (242864)	Loss/tok 2.9530 (3.1958)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.035 (0.092)	Data 1.01e-04 (1.40e-03)	Tok/s 222185 (242912)	Loss/tok 2.6547 (3.1946)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][150/1291]	Time 0.099 (0.094)	Data 1.05e-04 (1.31e-03)	Tok/s 253283 (243394)	Loss/tok 3.2354 (3.2021)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.099 (0.093)	Data 1.03e-04 (1.24e-03)	Tok/s 251051 (243129)	Loss/tok 3.2733 (3.2002)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.099 (0.092)	Data 1.07e-04 (1.17e-03)	Tok/s 253669 (242892)	Loss/tok 3.1570 (3.1967)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.066 (0.091)	Data 1.11e-04 (1.11e-03)	Tok/s 231944 (242735)	Loss/tok 3.0410 (3.1920)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.099 (0.091)	Data 1.08e-04 (1.06e-03)	Tok/s 255474 (242669)	Loss/tok 3.2438 (3.1943)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.066 (0.090)	Data 1.08e-04 (1.01e-03)	Tok/s 230187 (242343)	Loss/tok 3.0960 (3.1894)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.099 (0.091)	Data 1.11e-04 (9.72e-04)	Tok/s 256375 (242554)	Loss/tok 3.1946 (3.1949)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.099 (0.091)	Data 1.11e-04 (9.33e-04)	Tok/s 253242 (242667)	Loss/tok 3.2245 (3.1923)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.036 (0.090)	Data 1.23e-04 (8.98e-04)	Tok/s 223669 (242547)	Loss/tok 2.6000 (3.1884)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.099 (0.091)	Data 1.08e-04 (8.65e-04)	Tok/s 254107 (242848)	Loss/tok 3.1739 (3.1901)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.099 (0.091)	Data 1.06e-04 (8.35e-04)	Tok/s 252911 (242850)	Loss/tok 3.1420 (3.1923)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.099 (0.091)	Data 1.15e-04 (8.07e-04)	Tok/s 253700 (242896)	Loss/tok 3.2405 (3.1936)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.066 (0.091)	Data 1.37e-04 (7.82e-04)	Tok/s 230049 (242691)	Loss/tok 2.8976 (3.1893)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][280/1291]	Time 0.035 (0.091)	Data 1.09e-04 (7.58e-04)	Tok/s 222961 (242682)	Loss/tok 2.5439 (3.1895)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.173 (0.091)	Data 1.16e-04 (7.36e-04)	Tok/s 260760 (242759)	Loss/tok 3.4711 (3.1927)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.099 (0.091)	Data 1.16e-04 (7.15e-04)	Tok/s 253216 (242700)	Loss/tok 3.2789 (3.1906)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.134 (0.091)	Data 1.07e-04 (6.96e-04)	Tok/s 260265 (242672)	Loss/tok 3.3859 (3.1889)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.135 (0.091)	Data 1.09e-04 (6.78e-04)	Tok/s 257914 (242848)	Loss/tok 3.4116 (3.1899)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.100 (0.092)	Data 1.07e-04 (6.60e-04)	Tok/s 252393 (242987)	Loss/tok 3.1893 (3.1920)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.066 (0.091)	Data 1.09e-04 (6.44e-04)	Tok/s 232778 (242834)	Loss/tok 3.0123 (3.1886)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.035 (0.091)	Data 1.08e-04 (6.29e-04)	Tok/s 223835 (242750)	Loss/tok 2.5222 (3.1879)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.066 (0.091)	Data 1.14e-04 (6.15e-04)	Tok/s 231398 (242887)	Loss/tok 2.8865 (3.1884)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.099 (0.091)	Data 1.09e-04 (6.02e-04)	Tok/s 251507 (242862)	Loss/tok 3.1975 (3.1859)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.066 (0.091)	Data 1.12e-04 (5.89e-04)	Tok/s 234402 (242775)	Loss/tok 3.0265 (3.1829)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.174 (0.091)	Data 1.11e-04 (5.77e-04)	Tok/s 261042 (242868)	Loss/tok 3.4419 (3.1833)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.134 (0.091)	Data 1.32e-04 (5.65e-04)	Tok/s 260980 (242923)	Loss/tok 3.3414 (3.1838)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][410/1291]	Time 0.067 (0.091)	Data 1.03e-04 (5.54e-04)	Tok/s 229563 (242836)	Loss/tok 2.9956 (3.1828)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.134 (0.091)	Data 1.08e-04 (5.43e-04)	Tok/s 260691 (242814)	Loss/tok 3.3462 (3.1826)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.067 (0.091)	Data 1.03e-04 (5.33e-04)	Tok/s 235583 (242868)	Loss/tok 2.9560 (3.1826)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.173 (0.091)	Data 1.08e-04 (5.24e-04)	Tok/s 256243 (242842)	Loss/tok 3.5484 (3.1834)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.099 (0.091)	Data 1.05e-04 (5.14e-04)	Tok/s 255007 (242955)	Loss/tok 3.1449 (3.1830)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.067 (0.091)	Data 1.06e-04 (5.06e-04)	Tok/s 230569 (243010)	Loss/tok 2.9631 (3.1824)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.091)	Data 1.06e-04 (4.97e-04)	Tok/s 234916 (243093)	Loss/tok 2.9951 (3.1834)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.067 (0.091)	Data 1.06e-04 (4.89e-04)	Tok/s 229836 (243046)	Loss/tok 2.9905 (3.1837)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.067 (0.091)	Data 1.03e-04 (4.81e-04)	Tok/s 231306 (242930)	Loss/tok 2.8890 (3.1821)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.174 (0.091)	Data 1.04e-04 (4.74e-04)	Tok/s 257099 (243059)	Loss/tok 3.4383 (3.1830)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.067 (0.091)	Data 1.04e-04 (4.67e-04)	Tok/s 232219 (243068)	Loss/tok 2.9982 (3.1827)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.099 (0.091)	Data 1.04e-04 (4.60e-04)	Tok/s 251372 (243082)	Loss/tok 3.2305 (3.1816)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.135 (0.091)	Data 1.06e-04 (4.53e-04)	Tok/s 257612 (242940)	Loss/tok 3.3262 (3.1795)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][540/1291]	Time 0.067 (0.091)	Data 1.12e-04 (4.47e-04)	Tok/s 231442 (242914)	Loss/tok 2.9645 (3.1784)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.100 (0.091)	Data 1.10e-04 (4.41e-04)	Tok/s 249165 (242955)	Loss/tok 3.1642 (3.1790)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.135 (0.091)	Data 1.08e-04 (4.35e-04)	Tok/s 258806 (242950)	Loss/tok 3.4174 (3.1782)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.099 (0.091)	Data 1.10e-04 (4.30e-04)	Tok/s 253617 (243026)	Loss/tok 3.1167 (3.1780)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.036 (0.091)	Data 1.08e-04 (4.25e-04)	Tok/s 222784 (242969)	Loss/tok 2.6353 (3.1772)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.099 (0.090)	Data 1.31e-04 (4.19e-04)	Tok/s 252348 (242915)	Loss/tok 3.1254 (3.1755)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.066 (0.091)	Data 1.10e-04 (4.14e-04)	Tok/s 231226 (242935)	Loss/tok 2.9484 (3.1753)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.067 (0.090)	Data 1.08e-04 (4.09e-04)	Tok/s 230206 (242812)	Loss/tok 3.0985 (3.1736)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.067 (0.090)	Data 1.08e-04 (4.05e-04)	Tok/s 234854 (242797)	Loss/tok 2.9101 (3.1737)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.136 (0.091)	Data 1.13e-04 (4.00e-04)	Tok/s 257371 (242890)	Loss/tok 3.3532 (3.1760)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.099 (0.091)	Data 1.12e-04 (3.95e-04)	Tok/s 253468 (242935)	Loss/tok 3.2452 (3.1769)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.066 (0.091)	Data 1.10e-04 (3.91e-04)	Tok/s 233123 (242845)	Loss/tok 3.0443 (3.1756)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][660/1291]	Time 0.100 (0.090)	Data 1.15e-04 (3.87e-04)	Tok/s 252400 (242734)	Loss/tok 3.0514 (3.1740)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][670/1291]	Time 0.067 (0.090)	Data 1.11e-04 (3.83e-04)	Tok/s 230871 (242698)	Loss/tok 2.9415 (3.1735)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.136 (0.090)	Data 2.85e-04 (3.79e-04)	Tok/s 258299 (242728)	Loss/tok 3.2969 (3.1736)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.036 (0.090)	Data 1.07e-04 (3.75e-04)	Tok/s 223216 (242752)	Loss/tok 2.4883 (3.1733)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.067 (0.090)	Data 1.10e-04 (3.72e-04)	Tok/s 232565 (242672)	Loss/tok 2.9554 (3.1723)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.135 (0.090)	Data 1.07e-04 (3.68e-04)	Tok/s 259458 (242725)	Loss/tok 3.3096 (3.1723)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.036 (0.090)	Data 1.11e-04 (3.64e-04)	Tok/s 222516 (242565)	Loss/tok 2.6485 (3.1703)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.099 (0.090)	Data 1.11e-04 (3.61e-04)	Tok/s 252499 (242490)	Loss/tok 3.1611 (3.1684)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.58e-04)	Tok/s 233939 (242427)	Loss/tok 2.9271 (3.1672)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.099 (0.090)	Data 1.08e-04 (3.54e-04)	Tok/s 250873 (242506)	Loss/tok 3.1423 (3.1671)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.100 (0.090)	Data 1.07e-04 (3.51e-04)	Tok/s 253118 (242532)	Loss/tok 3.1534 (3.1687)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.100 (0.090)	Data 1.13e-04 (3.48e-04)	Tok/s 251578 (242502)	Loss/tok 3.1916 (3.1673)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.036 (0.090)	Data 1.08e-04 (3.45e-04)	Tok/s 217779 (242481)	Loss/tok 2.6302 (3.1663)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][790/1291]	Time 0.036 (0.090)	Data 1.08e-04 (3.42e-04)	Tok/s 226872 (242490)	Loss/tok 2.6015 (3.1664)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.099 (0.090)	Data 1.14e-04 (3.39e-04)	Tok/s 253107 (242562)	Loss/tok 3.1647 (3.1669)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.099 (0.090)	Data 1.08e-04 (3.36e-04)	Tok/s 256031 (242473)	Loss/tok 3.1560 (3.1653)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.036 (0.090)	Data 1.19e-04 (3.34e-04)	Tok/s 220262 (242464)	Loss/tok 2.5982 (3.1647)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.134 (0.090)	Data 1.24e-04 (3.31e-04)	Tok/s 260102 (242470)	Loss/tok 3.3145 (3.1644)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.067 (0.090)	Data 1.21e-04 (3.29e-04)	Tok/s 227960 (242484)	Loss/tok 2.9186 (3.1636)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.067 (0.090)	Data 1.30e-04 (3.26e-04)	Tok/s 236632 (242478)	Loss/tok 2.9930 (3.1626)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.099 (0.090)	Data 1.11e-04 (3.24e-04)	Tok/s 254952 (242518)	Loss/tok 3.1331 (3.1622)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.21e-04)	Tok/s 231867 (242498)	Loss/tok 2.9209 (3.1617)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.067 (0.090)	Data 1.09e-04 (3.19e-04)	Tok/s 226993 (242507)	Loss/tok 2.9662 (3.1619)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.175 (0.090)	Data 1.15e-04 (3.17e-04)	Tok/s 258242 (242506)	Loss/tok 3.3896 (3.1622)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.14e-04)	Tok/s 234031 (242437)	Loss/tok 2.8446 (3.1610)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.066 (0.090)	Data 1.10e-04 (3.12e-04)	Tok/s 233290 (242484)	Loss/tok 2.9691 (3.1628)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][920/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.10e-04)	Tok/s 233745 (242416)	Loss/tok 2.9124 (3.1615)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.135 (0.089)	Data 1.05e-04 (3.08e-04)	Tok/s 261651 (242406)	Loss/tok 3.3725 (3.1615)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.135 (0.090)	Data 1.10e-04 (3.06e-04)	Tok/s 259466 (242469)	Loss/tok 3.2847 (3.1612)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.04e-04)	Tok/s 230069 (242410)	Loss/tok 2.9121 (3.1599)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.02e-04)	Tok/s 236135 (242437)	Loss/tok 2.9177 (3.1594)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.066 (0.090)	Data 1.34e-04 (3.00e-04)	Tok/s 234683 (242408)	Loss/tok 2.9037 (3.1603)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.98e-04)	Tok/s 234526 (242389)	Loss/tok 2.9382 (3.1596)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.135 (0.090)	Data 1.12e-04 (2.96e-04)	Tok/s 260037 (242415)	Loss/tok 3.3265 (3.1591)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.94e-04)	Tok/s 232102 (242349)	Loss/tok 2.9138 (3.1581)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.93e-04)	Tok/s 229531 (242341)	Loss/tok 2.9568 (3.1580)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.100 (0.089)	Data 1.33e-04 (2.91e-04)	Tok/s 252872 (242339)	Loss/tok 3.1271 (3.1575)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.89e-04)	Tok/s 231983 (242341)	Loss/tok 2.9929 (3.1567)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.87e-04)	Tok/s 234183 (242386)	Loss/tok 2.9277 (3.1567)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1050/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.86e-04)	Tok/s 256356 (242385)	Loss/tok 3.1204 (3.1565)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.84e-04)	Tok/s 230087 (242336)	Loss/tok 2.9585 (3.1558)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.037 (0.089)	Data 1.11e-04 (2.83e-04)	Tok/s 212219 (242338)	Loss/tok 2.5341 (3.1553)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.81e-04)	Tok/s 234472 (242316)	Loss/tok 2.8921 (3.1548)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.036 (0.089)	Data 1.09e-04 (2.79e-04)	Tok/s 221368 (242223)	Loss/tok 2.5165 (3.1536)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.136 (0.089)	Data 1.31e-04 (2.78e-04)	Tok/s 258330 (242244)	Loss/tok 3.2733 (3.1541)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.036 (0.089)	Data 1.08e-04 (2.76e-04)	Tok/s 221250 (242221)	Loss/tok 2.5482 (3.1534)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.75e-04)	Tok/s 250588 (242241)	Loss/tok 3.1129 (3.1530)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.74e-04)	Tok/s 251160 (242247)	Loss/tok 3.1598 (3.1527)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.72e-04)	Tok/s 232367 (242241)	Loss/tok 2.9321 (3.1522)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.71e-04)	Tok/s 233349 (242206)	Loss/tok 2.9267 (3.1513)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.69e-04)	Tok/s 228925 (242148)	Loss/tok 2.8633 (3.1505)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.036 (0.089)	Data 1.07e-04 (2.68e-04)	Tok/s 221450 (242152)	Loss/tok 2.5842 (3.1509)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1180/1291]	Time 0.067 (0.089)	Data 1.06e-04 (2.67e-04)	Tok/s 232342 (242154)	Loss/tok 2.8535 (3.1503)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.65e-04)	Tok/s 233341 (242162)	Loss/tok 3.0068 (3.1501)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.135 (0.089)	Data 1.06e-04 (2.64e-04)	Tok/s 259718 (242208)	Loss/tok 3.2731 (3.1500)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.63e-04)	Tok/s 252612 (242222)	Loss/tok 2.9844 (3.1490)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.100 (0.089)	Data 1.09e-04 (2.62e-04)	Tok/s 252114 (242204)	Loss/tok 3.1643 (3.1488)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.136 (0.089)	Data 1.08e-04 (2.60e-04)	Tok/s 259912 (242198)	Loss/tok 3.2760 (3.1481)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.59e-04)	Tok/s 232656 (242204)	Loss/tok 3.0212 (3.1476)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.135 (0.089)	Data 1.11e-04 (2.58e-04)	Tok/s 258660 (242194)	Loss/tok 3.3132 (3.1471)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.036 (0.089)	Data 1.08e-04 (2.57e-04)	Tok/s 221324 (242168)	Loss/tok 2.5361 (3.1478)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.036 (0.089)	Data 1.08e-04 (2.56e-04)	Tok/s 221098 (242112)	Loss/tok 2.5534 (3.1468)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.55e-04)	Tok/s 234982 (242111)	Loss/tok 2.9155 (3.1464)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.135 (0.089)	Data 5.03e-05 (2.57e-04)	Tok/s 260446 (242120)	Loss/tok 3.2362 (3.1468)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593145223620, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593145223620, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.383 (0.383)	Decoder iters 101.0 (101.0)	Tok/s 42933 (42933)
0: Running moses detokenizer
0: BLEU(score=24.18643717108594, counts=[37240, 18641, 10644, 6316], totals=[65365, 62362, 59359, 56362], precisions=[56.97238583339708, 29.891600654244574, 17.93156892804798, 11.206131790922962], bp=1.0, sys_len=65365, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593145225539, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2419, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593145225540, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1475	Test BLEU: 24.19
0: Performance: Epoch: 3	Training: 1937194 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593145225540, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593145225540, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 04:20:32 AM
RESULT,RNN_TRANSLATOR,,505,nvidia,2020-06-26 04:12:07 AM
