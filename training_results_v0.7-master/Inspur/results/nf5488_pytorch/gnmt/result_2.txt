Beginning trial 2 of 10
:::MLLOG {"namespace": "", "time_ms": 1593141670023, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593141670069, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593141670069, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593141670069, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593141670069, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593141672463, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 03:21:12 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593141675110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141675111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141675124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141675132, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141675137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141675143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141675140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593141675140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 619424760
:::MLLOG {"namespace": "", "time_ms": 1593141683848, "event_type": "POINT_IN_TIME", "key": "seed", "value": 619424760, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 417771376
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593141697619, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593141697622, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593141697622, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593141697622, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593141697622, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593141699575, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593141699575, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593141699576, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593141699925, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593141699926, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593141699926, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593141699926, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593141699927, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593141699927, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593141699927, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593141699927, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593141699927, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593141699927, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593141699928, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141699928, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 319488211
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.417 (0.417)	Data 2.67e-01 (2.67e-01)	Tok/s 84124 (84124)	Loss/tok 10.6795 (10.6795)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.067 (0.124)	Data 1.11e-04 (2.44e-02)	Tok/s 231830 (227190)	Loss/tok 9.3953 (10.0566)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.102 (0.108)	Data 3.15e-02 (1.62e-02)	Tok/s 152305 (223883)	Loss/tok 9.1002 (9.7498)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.083 (0.100)	Data 1.14e-04 (1.44e-02)	Tok/s 188734 (208771)	Loss/tok 8.7691 (9.5405)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1291]	Time 0.098 (0.096)	Data 1.11e-04 (1.14e-02)	Tok/s 257006 (210152)	Loss/tok 8.6002 (9.3469)	LR 7.222e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][50/1291]	Time 0.100 (0.095)	Data 1.34e-04 (9.16e-03)	Tok/s 253118 (216170)	Loss/tok 8.4435 (9.1860)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.135 (0.092)	Data 1.07e-04 (7.67e-03)	Tok/s 259092 (219520)	Loss/tok 8.3066 (9.0378)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.133 (0.094)	Data 1.10e-04 (6.61e-03)	Tok/s 264583 (223526)	Loss/tok 8.1790 (8.8941)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.036 (0.094)	Data 1.19e-04 (5.81e-03)	Tok/s 225047 (226458)	Loss/tok 7.4249 (8.7732)	LR 1.773e-04
0: TRAIN [0][90/1291]	Time 0.066 (0.094)	Data 1.20e-04 (5.18e-03)	Tok/s 232042 (228455)	Loss/tok 7.7663 (8.6774)	LR 2.232e-04
0: TRAIN [0][100/1291]	Time 0.066 (0.092)	Data 1.12e-04 (4.68e-03)	Tok/s 234344 (229637)	Loss/tok 7.7008 (8.6050)	LR 2.810e-04
0: TRAIN [0][110/1291]	Time 0.133 (0.093)	Data 1.12e-04 (4.27e-03)	Tok/s 262577 (231635)	Loss/tok 8.0603 (8.5344)	LR 3.537e-04
0: TRAIN [0][120/1291]	Time 0.134 (0.093)	Data 2.63e-04 (3.93e-03)	Tok/s 262924 (232899)	Loss/tok 7.9185 (8.4717)	LR 4.453e-04
0: TRAIN [0][130/1291]	Time 0.066 (0.092)	Data 1.09e-04 (3.64e-03)	Tok/s 235987 (233561)	Loss/tok 7.6476 (8.4205)	LR 5.606e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][140/1291]	Time 0.098 (0.092)	Data 1.17e-04 (3.39e-03)	Tok/s 260603 (234395)	Loss/tok 7.7147 (8.3714)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.067 (0.092)	Data 1.36e-04 (3.17e-03)	Tok/s 234381 (235029)	Loss/tok 7.3278 (8.3209)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.066 (0.091)	Data 1.09e-04 (2.98e-03)	Tok/s 232154 (235268)	Loss/tok 7.2356 (8.2740)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.067 (0.091)	Data 1.08e-04 (2.81e-03)	Tok/s 224894 (235797)	Loss/tok 7.0354 (8.2179)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.066 (0.091)	Data 1.09e-04 (2.66e-03)	Tok/s 235330 (236216)	Loss/tok 6.8782 (8.1597)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.066 (0.091)	Data 1.07e-04 (2.53e-03)	Tok/s 231548 (236742)	Loss/tok 6.6310 (8.0949)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.098 (0.090)	Data 1.08e-04 (2.41e-03)	Tok/s 256783 (236817)	Loss/tok 6.8446 (8.0393)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.134 (0.090)	Data 1.34e-04 (2.30e-03)	Tok/s 259213 (237239)	Loss/tok 6.8971 (7.9752)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.172 (0.091)	Data 1.11e-04 (2.20e-03)	Tok/s 257053 (237785)	Loss/tok 6.7573 (7.8996)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.067 (0.091)	Data 1.11e-04 (2.11e-03)	Tok/s 226967 (238216)	Loss/tok 6.0473 (7.8300)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.035 (0.091)	Data 1.11e-04 (2.03e-03)	Tok/s 224936 (238620)	Loss/tok 5.2253 (7.7581)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.067 (0.091)	Data 1.27e-04 (1.95e-03)	Tok/s 228248 (238786)	Loss/tok 5.8409 (7.6916)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.066 (0.091)	Data 1.11e-04 (1.88e-03)	Tok/s 231744 (238934)	Loss/tok 5.6623 (7.6268)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][270/1291]	Time 0.067 (0.092)	Data 1.07e-04 (1.82e-03)	Tok/s 232348 (239244)	Loss/tok 5.4578 (7.5526)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.066 (0.092)	Data 1.10e-04 (1.76e-03)	Tok/s 228574 (239510)	Loss/tok 5.3064 (7.4845)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.067 (0.091)	Data 1.11e-04 (1.70e-03)	Tok/s 230952 (239671)	Loss/tok 5.3543 (7.4208)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.066 (0.091)	Data 1.08e-04 (1.65e-03)	Tok/s 234015 (239640)	Loss/tok 5.2116 (7.3623)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.135 (0.091)	Data 1.08e-04 (1.60e-03)	Tok/s 257341 (239898)	Loss/tok 5.5266 (7.2951)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.066 (0.091)	Data 1.10e-04 (1.55e-03)	Tok/s 236563 (239942)	Loss/tok 5.0025 (7.2331)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.067 (0.091)	Data 1.11e-04 (1.51e-03)	Tok/s 231508 (240281)	Loss/tok 4.8290 (7.1635)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.099 (0.091)	Data 1.10e-04 (1.47e-03)	Tok/s 255492 (240291)	Loss/tok 4.9243 (7.1037)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.067 (0.091)	Data 1.09e-04 (1.43e-03)	Tok/s 231007 (240298)	Loss/tok 4.5014 (7.0454)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.035 (0.091)	Data 1.08e-04 (1.39e-03)	Tok/s 222306 (240528)	Loss/tok 3.8027 (6.9754)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.101 (0.091)	Data 1.10e-04 (1.36e-03)	Tok/s 249127 (240650)	Loss/tok 4.7124 (6.9125)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.098 (0.091)	Data 1.08e-04 (1.32e-03)	Tok/s 253718 (240613)	Loss/tok 4.6331 (6.8605)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.134 (0.091)	Data 1.13e-04 (1.29e-03)	Tok/s 261527 (240801)	Loss/tok 4.8158 (6.7955)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][400/1291]	Time 0.066 (0.091)	Data 1.26e-04 (1.26e-03)	Tok/s 234600 (240839)	Loss/tok 4.1499 (6.7363)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.099 (0.091)	Data 1.29e-04 (1.24e-03)	Tok/s 254028 (240830)	Loss/tok 4.4805 (6.6872)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.066 (0.091)	Data 1.09e-04 (1.21e-03)	Tok/s 233343 (240689)	Loss/tok 4.0839 (6.6441)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.069 (0.090)	Data 1.07e-04 (1.18e-03)	Tok/s 223511 (240540)	Loss/tok 4.1416 (6.5979)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.101 (0.090)	Data 1.12e-04 (1.16e-03)	Tok/s 252285 (240465)	Loss/tok 4.3935 (6.5495)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.101 (0.090)	Data 1.21e-04 (1.14e-03)	Tok/s 248525 (240416)	Loss/tok 4.2959 (6.5020)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.099 (0.090)	Data 1.09e-04 (1.12e-03)	Tok/s 256018 (240490)	Loss/tok 4.3473 (6.4520)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.099 (0.090)	Data 1.15e-04 (1.09e-03)	Tok/s 257035 (240608)	Loss/tok 4.1869 (6.4036)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.098 (0.090)	Data 1.10e-04 (1.07e-03)	Tok/s 257381 (240706)	Loss/tok 4.1770 (6.3550)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.067 (0.090)	Data 1.10e-04 (1.05e-03)	Tok/s 229344 (240741)	Loss/tok 3.9107 (6.3140)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.099 (0.090)	Data 1.09e-04 (1.04e-03)	Tok/s 254142 (240701)	Loss/tok 4.2364 (6.2759)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.067 (0.090)	Data 1.09e-04 (1.02e-03)	Tok/s 230053 (240753)	Loss/tok 3.7393 (6.2329)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.066 (0.090)	Data 1.24e-04 (1.00e-03)	Tok/s 235419 (240834)	Loss/tok 3.8187 (6.1925)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][530/1291]	Time 0.036 (0.090)	Data 1.09e-04 (9.83e-04)	Tok/s 214956 (240829)	Loss/tok 3.2024 (6.1548)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.134 (0.090)	Data 1.34e-04 (9.67e-04)	Tok/s 265033 (240877)	Loss/tok 4.2293 (6.1187)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.036 (0.090)	Data 1.11e-04 (9.52e-04)	Tok/s 223729 (240867)	Loss/tok 3.2609 (6.0833)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.134 (0.090)	Data 1.11e-04 (9.37e-04)	Tok/s 260524 (241017)	Loss/tok 4.1650 (6.0438)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.101 (0.090)	Data 1.13e-04 (9.22e-04)	Tok/s 252980 (241095)	Loss/tok 3.9879 (6.0073)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.173 (0.090)	Data 1.12e-04 (9.08e-04)	Tok/s 259215 (241167)	Loss/tok 4.4680 (5.9709)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.036 (0.090)	Data 1.09e-04 (8.95e-04)	Tok/s 217697 (241084)	Loss/tok 3.2576 (5.9426)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.036 (0.090)	Data 1.21e-04 (8.82e-04)	Tok/s 221787 (241089)	Loss/tok 3.1239 (5.9109)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.067 (0.089)	Data 1.10e-04 (8.69e-04)	Tok/s 230718 (241091)	Loss/tok 3.7582 (5.8809)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.066 (0.089)	Data 1.09e-04 (8.57e-04)	Tok/s 233456 (241053)	Loss/tok 3.6330 (5.8538)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.067 (0.089)	Data 1.08e-04 (8.45e-04)	Tok/s 234037 (241011)	Loss/tok 3.5883 (5.8264)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.066 (0.089)	Data 1.14e-04 (8.34e-04)	Tok/s 235282 (240994)	Loss/tok 3.7046 (5.7991)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.100 (0.089)	Data 1.09e-04 (8.23e-04)	Tok/s 250899 (241075)	Loss/tok 3.9788 (5.7681)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][660/1291]	Time 0.099 (0.089)	Data 1.09e-04 (8.12e-04)	Tok/s 255884 (241197)	Loss/tok 3.8767 (5.7363)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.036 (0.089)	Data 1.10e-04 (8.02e-04)	Tok/s 219942 (241126)	Loss/tok 3.1180 (5.7127)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.066 (0.089)	Data 1.13e-04 (7.92e-04)	Tok/s 233111 (241131)	Loss/tok 3.7024 (5.6861)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][690/1291]	Time 0.067 (0.089)	Data 1.14e-04 (7.82e-04)	Tok/s 234908 (241160)	Loss/tok 3.7106 (5.6595)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.099 (0.089)	Data 1.13e-04 (7.72e-04)	Tok/s 254892 (241202)	Loss/tok 3.8585 (5.6337)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.036 (0.089)	Data 1.10e-04 (7.63e-04)	Tok/s 221531 (241205)	Loss/tok 3.0071 (5.6076)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.135 (0.089)	Data 1.16e-04 (7.54e-04)	Tok/s 259043 (241310)	Loss/tok 4.1160 (5.5813)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.099 (0.089)	Data 1.08e-04 (7.45e-04)	Tok/s 252729 (241301)	Loss/tok 3.9597 (5.5582)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.173 (0.089)	Data 1.08e-04 (7.37e-04)	Tok/s 258455 (241352)	Loss/tok 4.2302 (5.5328)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.066 (0.089)	Data 1.09e-04 (7.28e-04)	Tok/s 238271 (241313)	Loss/tok 3.5449 (5.5126)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.099 (0.089)	Data 1.13e-04 (7.20e-04)	Tok/s 256580 (241332)	Loss/tok 3.7988 (5.4902)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.099 (0.089)	Data 1.18e-04 (7.13e-04)	Tok/s 255912 (241494)	Loss/tok 3.7757 (5.4635)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.172 (0.089)	Data 1.10e-04 (7.05e-04)	Tok/s 261217 (241558)	Loss/tok 4.2048 (5.4401)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.135 (0.089)	Data 1.23e-04 (6.97e-04)	Tok/s 257652 (241543)	Loss/tok 3.9387 (5.4194)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.066 (0.089)	Data 1.09e-04 (6.90e-04)	Tok/s 236354 (241478)	Loss/tok 3.5575 (5.4017)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.067 (0.089)	Data 1.10e-04 (6.83e-04)	Tok/s 231970 (241487)	Loss/tok 3.4690 (5.3816)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][820/1291]	Time 0.036 (0.089)	Data 1.31e-04 (6.76e-04)	Tok/s 222935 (241486)	Loss/tok 2.9758 (5.3631)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.067 (0.089)	Data 1.12e-04 (6.70e-04)	Tok/s 233935 (241474)	Loss/tok 3.6497 (5.3452)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.099 (0.089)	Data 1.13e-04 (6.63e-04)	Tok/s 252275 (241532)	Loss/tok 3.8261 (5.3242)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.067 (0.089)	Data 1.10e-04 (6.56e-04)	Tok/s 231104 (241494)	Loss/tok 3.5593 (5.3078)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.035 (0.089)	Data 1.06e-04 (6.50e-04)	Tok/s 224445 (241466)	Loss/tok 3.0100 (5.2911)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][870/1291]	Time 0.134 (0.089)	Data 1.04e-04 (6.44e-04)	Tok/s 260071 (241549)	Loss/tok 3.9106 (5.2697)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.067 (0.090)	Data 1.03e-04 (6.38e-04)	Tok/s 229781 (241638)	Loss/tok 3.4327 (5.2480)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.100 (0.089)	Data 1.05e-04 (6.32e-04)	Tok/s 254091 (241642)	Loss/tok 3.6788 (5.2315)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.100 (0.090)	Data 1.11e-04 (6.26e-04)	Tok/s 251024 (241715)	Loss/tok 3.7337 (5.2140)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.067 (0.090)	Data 1.07e-04 (6.20e-04)	Tok/s 228929 (241719)	Loss/tok 3.4380 (5.1975)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.100 (0.090)	Data 1.11e-04 (6.15e-04)	Tok/s 250326 (241766)	Loss/tok 3.6530 (5.1801)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.099 (0.090)	Data 1.17e-04 (6.09e-04)	Tok/s 255023 (241788)	Loss/tok 3.7577 (5.1644)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.036 (0.090)	Data 1.04e-04 (6.04e-04)	Tok/s 222483 (241790)	Loss/tok 3.0262 (5.1494)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.036 (0.090)	Data 1.29e-04 (5.99e-04)	Tok/s 222940 (241834)	Loss/tok 2.9552 (5.1331)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.100 (0.090)	Data 1.07e-04 (5.94e-04)	Tok/s 252480 (241907)	Loss/tok 3.6633 (5.1167)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.099 (0.090)	Data 1.10e-04 (5.89e-04)	Tok/s 253165 (241959)	Loss/tok 3.7859 (5.1002)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.135 (0.090)	Data 1.07e-04 (5.84e-04)	Tok/s 259809 (241924)	Loss/tok 3.8838 (5.0874)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][990/1291]	Time 0.135 (0.090)	Data 1.06e-04 (5.79e-04)	Tok/s 255625 (241974)	Loss/tok 3.9430 (5.0719)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.067 (0.090)	Data 1.04e-04 (5.74e-04)	Tok/s 231754 (241973)	Loss/tok 3.4963 (5.0583)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.174 (0.090)	Data 1.13e-04 (5.70e-04)	Tok/s 256274 (241916)	Loss/tok 4.0337 (5.0460)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.135 (0.090)	Data 1.07e-04 (5.65e-04)	Tok/s 260771 (241893)	Loss/tok 3.9166 (5.0336)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.099 (0.089)	Data 1.05e-04 (5.61e-04)	Tok/s 255984 (241898)	Loss/tok 3.6442 (5.0205)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.100 (0.089)	Data 1.09e-04 (5.57e-04)	Tok/s 254805 (241923)	Loss/tok 3.5628 (5.0074)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.067 (0.090)	Data 1.07e-04 (5.52e-04)	Tok/s 233019 (241983)	Loss/tok 3.3243 (4.9928)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.036 (0.089)	Data 1.14e-04 (5.48e-04)	Tok/s 218661 (241977)	Loss/tok 2.7999 (4.9805)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.036 (0.089)	Data 1.06e-04 (5.44e-04)	Tok/s 225994 (241918)	Loss/tok 2.8860 (4.9698)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.100 (0.089)	Data 1.04e-04 (5.40e-04)	Tok/s 253726 (241976)	Loss/tok 3.6090 (4.9564)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.067 (0.089)	Data 1.04e-04 (5.36e-04)	Tok/s 234729 (241929)	Loss/tok 3.3521 (4.9454)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.099 (0.089)	Data 1.06e-04 (5.32e-04)	Tok/s 253593 (241921)	Loss/tok 3.6769 (4.9338)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.099 (0.089)	Data 1.20e-04 (5.28e-04)	Tok/s 255320 (241908)	Loss/tok 3.6284 (4.9224)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1120/1291]	Time 0.100 (0.089)	Data 1.07e-04 (5.25e-04)	Tok/s 251832 (241932)	Loss/tok 3.5874 (4.9105)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1130/1291]	Time 0.067 (0.089)	Data 1.25e-04 (5.21e-04)	Tok/s 231069 (241945)	Loss/tok 3.3928 (4.8985)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.067 (0.089)	Data 1.27e-04 (5.17e-04)	Tok/s 231281 (241939)	Loss/tok 3.4983 (4.8874)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.036 (0.089)	Data 1.04e-04 (5.14e-04)	Tok/s 222051 (241982)	Loss/tok 2.8473 (4.8750)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.067 (0.089)	Data 1.05e-04 (5.10e-04)	Tok/s 232704 (241938)	Loss/tok 3.4442 (4.8650)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.100 (0.089)	Data 1.04e-04 (5.07e-04)	Tok/s 251576 (241994)	Loss/tok 3.6035 (4.8525)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.067 (0.089)	Data 1.05e-04 (5.04e-04)	Tok/s 231895 (241880)	Loss/tok 3.4845 (4.8450)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.036 (0.089)	Data 1.04e-04 (5.00e-04)	Tok/s 225658 (241845)	Loss/tok 2.7811 (4.8356)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.067 (0.089)	Data 1.07e-04 (4.97e-04)	Tok/s 231706 (241859)	Loss/tok 3.3735 (4.8250)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.100 (0.089)	Data 1.06e-04 (4.94e-04)	Tok/s 254190 (241816)	Loss/tok 3.6352 (4.8157)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.067 (0.089)	Data 1.06e-04 (4.91e-04)	Tok/s 233675 (241858)	Loss/tok 3.3728 (4.8044)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.174 (0.089)	Data 1.08e-04 (4.88e-04)	Tok/s 261006 (241933)	Loss/tok 3.8058 (4.7922)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.067 (0.089)	Data 1.07e-04 (4.85e-04)	Tok/s 226966 (241891)	Loss/tok 3.3745 (4.7832)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.134 (0.089)	Data 1.08e-04 (4.82e-04)	Tok/s 262051 (241921)	Loss/tok 3.8167 (4.7729)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1260/1291]	Time 0.099 (0.089)	Data 1.04e-04 (4.79e-04)	Tok/s 256686 (241891)	Loss/tok 3.6294 (4.7645)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.067 (0.089)	Data 1.05e-04 (4.76e-04)	Tok/s 229777 (241865)	Loss/tok 3.3299 (4.7557)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.067 (0.089)	Data 1.05e-04 (4.73e-04)	Tok/s 233490 (241901)	Loss/tok 3.3296 (4.7457)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.135 (0.089)	Data 4.27e-05 (4.73e-04)	Tok/s 260422 (241916)	Loss/tok 3.7940 (4.7360)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593141815708, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141815708, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.511 (0.511)	Decoder iters 149.0 (149.0)	Tok/s 33625 (33625)
0: Running moses detokenizer
0: BLEU(score=19.21179601042199, counts=[35118, 16032, 8516, 4729], totals=[68464, 65461, 62458, 59459], precisions=[51.29411077354522, 24.49091825667191, 13.634762560440617, 7.953379639751762], bp=1.0, sys_len=68464, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593141818057, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.19210000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141818057, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7374	Test BLEU: 19.21
0: Performance: Epoch: 0	Training: 1934648 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593141818057, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141818057, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141818057, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2844533735
0: TRAIN [1][0/1291]	Time 0.228 (0.228)	Data 1.70e-01 (1.70e-01)	Tok/s 35240 (35240)	Loss/tok 2.8276 (2.8276)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.099 (0.125)	Data 1.30e-04 (1.56e-02)	Tok/s 255730 (231442)	Loss/tok 3.4302 (3.6396)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.067 (0.112)	Data 1.07e-04 (8.22e-03)	Tok/s 224388 (236958)	Loss/tok 3.3177 (3.5999)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.066 (0.101)	Data 1.17e-04 (5.61e-03)	Tok/s 231708 (237836)	Loss/tok 3.3093 (3.5504)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.135 (0.101)	Data 1.31e-04 (4.27e-03)	Tok/s 253432 (240582)	Loss/tok 3.8076 (3.5418)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.067 (0.097)	Data 1.10e-04 (3.45e-03)	Tok/s 230791 (240221)	Loss/tok 3.2556 (3.5204)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.067 (0.095)	Data 1.26e-04 (2.91e-03)	Tok/s 230811 (239872)	Loss/tok 3.3657 (3.5087)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.099 (0.095)	Data 1.24e-04 (2.51e-03)	Tok/s 249820 (240086)	Loss/tok 3.4940 (3.5168)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.067 (0.094)	Data 1.19e-04 (2.22e-03)	Tok/s 229322 (240358)	Loss/tok 3.2918 (3.5094)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.068 (0.093)	Data 1.17e-04 (1.99e-03)	Tok/s 228378 (239965)	Loss/tok 3.2991 (3.5114)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][100/1291]	Time 0.036 (0.092)	Data 1.18e-04 (1.80e-03)	Tok/s 220025 (240112)	Loss/tok 2.8988 (3.5043)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.135 (0.092)	Data 1.15e-04 (1.65e-03)	Tok/s 257125 (240624)	Loss/tok 3.7244 (3.5083)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.067 (0.093)	Data 1.13e-04 (1.52e-03)	Tok/s 235941 (240960)	Loss/tok 3.3269 (3.5074)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.067 (0.093)	Data 1.16e-04 (1.42e-03)	Tok/s 231964 (241463)	Loss/tok 3.3579 (3.5067)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.067 (0.093)	Data 1.16e-04 (1.32e-03)	Tok/s 227628 (241590)	Loss/tok 3.2413 (3.5053)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.099 (0.093)	Data 1.14e-04 (1.24e-03)	Tok/s 254389 (241961)	Loss/tok 3.4994 (3.5099)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][160/1291]	Time 0.135 (0.095)	Data 1.15e-04 (1.17e-03)	Tok/s 262822 (242384)	Loss/tok 3.6792 (3.5238)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.136 (0.094)	Data 1.22e-04 (1.11e-03)	Tok/s 260036 (242075)	Loss/tok 3.6340 (3.5231)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.099 (0.095)	Data 1.19e-04 (1.06e-03)	Tok/s 252416 (242226)	Loss/tok 3.5032 (3.5238)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.135 (0.096)	Data 1.13e-04 (1.01e-03)	Tok/s 259079 (242686)	Loss/tok 3.7269 (3.5271)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.174 (0.096)	Data 1.38e-04 (9.64e-04)	Tok/s 258351 (242972)	Loss/tok 3.8741 (3.5298)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.136 (0.095)	Data 1.35e-04 (9.24e-04)	Tok/s 257995 (242606)	Loss/tok 3.7456 (3.5240)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.067 (0.095)	Data 1.18e-04 (8.87e-04)	Tok/s 229730 (242454)	Loss/tok 3.2848 (3.5283)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.136 (0.095)	Data 1.14e-04 (8.54e-04)	Tok/s 254959 (242428)	Loss/tok 3.6919 (3.5265)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.101 (0.095)	Data 1.14e-04 (8.24e-04)	Tok/s 248976 (242191)	Loss/tok 3.4700 (3.5244)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.100 (0.094)	Data 1.37e-04 (7.96e-04)	Tok/s 250630 (241945)	Loss/tok 3.4692 (3.5200)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.067 (0.093)	Data 1.15e-04 (7.70e-04)	Tok/s 231664 (241772)	Loss/tok 3.2990 (3.5165)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.100 (0.093)	Data 1.13e-04 (7.46e-04)	Tok/s 251394 (241737)	Loss/tok 3.5207 (3.5143)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][280/1291]	Time 0.100 (0.092)	Data 1.17e-04 (7.23e-04)	Tok/s 253448 (241492)	Loss/tok 3.5345 (3.5123)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.175 (0.092)	Data 1.18e-04 (7.02e-04)	Tok/s 257039 (241291)	Loss/tok 3.8342 (3.5107)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.068 (0.092)	Data 1.18e-04 (6.83e-04)	Tok/s 230589 (241259)	Loss/tok 3.4094 (3.5122)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.067 (0.092)	Data 1.13e-04 (6.65e-04)	Tok/s 234196 (241120)	Loss/tok 3.2259 (3.5103)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.069 (0.092)	Data 1.12e-04 (6.48e-04)	Tok/s 226159 (241113)	Loss/tok 3.1889 (3.5132)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.066 (0.092)	Data 1.16e-04 (6.32e-04)	Tok/s 232589 (241014)	Loss/tok 3.2040 (3.5122)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.135 (0.091)	Data 1.16e-04 (6.17e-04)	Tok/s 259719 (240983)	Loss/tok 3.6326 (3.5090)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.067 (0.091)	Data 1.18e-04 (6.03e-04)	Tok/s 229769 (240896)	Loss/tok 3.1231 (3.5060)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.135 (0.091)	Data 1.14e-04 (5.89e-04)	Tok/s 261570 (240859)	Loss/tok 3.6155 (3.5029)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.067 (0.091)	Data 1.16e-04 (5.77e-04)	Tok/s 231938 (240883)	Loss/tok 3.1953 (3.5016)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.101 (0.091)	Data 1.14e-04 (5.64e-04)	Tok/s 250655 (240915)	Loss/tok 3.3784 (3.4991)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.067 (0.091)	Data 1.14e-04 (5.53e-04)	Tok/s 232596 (240874)	Loss/tok 3.1517 (3.4951)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.175 (0.091)	Data 1.23e-04 (5.42e-04)	Tok/s 253860 (240828)	Loss/tok 3.7856 (3.4935)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][410/1291]	Time 0.100 (0.090)	Data 1.18e-04 (5.32e-04)	Tok/s 251604 (240765)	Loss/tok 3.5203 (3.4929)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][420/1291]	Time 0.036 (0.090)	Data 1.18e-04 (5.22e-04)	Tok/s 222684 (240729)	Loss/tok 2.7560 (3.4910)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.100 (0.090)	Data 1.43e-04 (5.13e-04)	Tok/s 253980 (240814)	Loss/tok 3.4093 (3.4896)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.100 (0.090)	Data 1.14e-04 (5.04e-04)	Tok/s 251459 (240958)	Loss/tok 3.4489 (3.4881)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.96e-04)	Tok/s 236841 (240794)	Loss/tok 3.2167 (3.4842)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.173 (0.090)	Data 1.22e-04 (4.88e-04)	Tok/s 254689 (240936)	Loss/tok 3.9037 (3.4869)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.100 (0.090)	Data 1.12e-04 (4.80e-04)	Tok/s 252212 (240924)	Loss/tok 3.5226 (3.4857)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.036 (0.090)	Data 1.20e-04 (4.72e-04)	Tok/s 219872 (240944)	Loss/tok 2.7798 (3.4844)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.099 (0.090)	Data 1.18e-04 (4.65e-04)	Tok/s 258589 (240952)	Loss/tok 3.4043 (3.4848)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.068 (0.090)	Data 1.12e-04 (4.58e-04)	Tok/s 232602 (240969)	Loss/tok 3.2328 (3.4837)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.51e-04)	Tok/s 230089 (240992)	Loss/tok 3.2035 (3.4828)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.067 (0.090)	Data 1.17e-04 (4.45e-04)	Tok/s 227643 (240932)	Loss/tok 3.2895 (3.4813)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.39e-04)	Tok/s 230955 (240868)	Loss/tok 3.2251 (3.4795)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.33e-04)	Tok/s 229847 (240940)	Loss/tok 3.1581 (3.4788)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][550/1291]	Time 0.067 (0.090)	Data 1.29e-04 (4.27e-04)	Tok/s 228165 (240910)	Loss/tok 3.1247 (3.4769)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.100 (0.090)	Data 1.18e-04 (4.22e-04)	Tok/s 252659 (240946)	Loss/tok 3.4227 (3.4747)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.067 (0.090)	Data 1.29e-04 (4.16e-04)	Tok/s 233068 (240958)	Loss/tok 3.0996 (3.4725)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][580/1291]	Time 0.100 (0.090)	Data 1.18e-04 (4.11e-04)	Tok/s 250806 (240997)	Loss/tok 3.3719 (3.4707)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.100 (0.090)	Data 1.40e-04 (4.06e-04)	Tok/s 253861 (241090)	Loss/tok 3.3994 (3.4700)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.068 (0.090)	Data 1.15e-04 (4.02e-04)	Tok/s 231089 (241152)	Loss/tok 3.1695 (3.4687)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.136 (0.090)	Data 1.14e-04 (3.97e-04)	Tok/s 257339 (241158)	Loss/tok 3.7077 (3.4677)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.067 (0.090)	Data 1.34e-04 (3.92e-04)	Tok/s 230483 (241278)	Loss/tok 3.1596 (3.4689)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.099 (0.090)	Data 1.17e-04 (3.88e-04)	Tok/s 254439 (241244)	Loss/tok 3.3910 (3.4676)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.067 (0.090)	Data 1.39e-04 (3.84e-04)	Tok/s 231233 (241233)	Loss/tok 3.2689 (3.4656)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.099 (0.090)	Data 1.17e-04 (3.80e-04)	Tok/s 254692 (241132)	Loss/tok 3.4215 (3.4627)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.100 (0.090)	Data 1.16e-04 (3.76e-04)	Tok/s 253112 (241170)	Loss/tok 3.4376 (3.4631)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.135 (0.090)	Data 1.33e-04 (3.72e-04)	Tok/s 256689 (241197)	Loss/tok 3.5856 (3.4645)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.68e-04)	Tok/s 231577 (241237)	Loss/tok 3.1221 (3.4635)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.101 (0.090)	Data 1.26e-04 (3.65e-04)	Tok/s 251568 (241356)	Loss/tok 3.4741 (3.4630)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.62e-04)	Tok/s 235459 (241292)	Loss/tok 3.1325 (3.4608)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][710/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.58e-04)	Tok/s 227553 (241270)	Loss/tok 3.2511 (3.4599)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][720/1291]	Time 0.136 (0.090)	Data 1.39e-04 (3.55e-04)	Tok/s 257988 (241240)	Loss/tok 3.5863 (3.4594)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.101 (0.090)	Data 1.14e-04 (3.52e-04)	Tok/s 248816 (241200)	Loss/tok 3.4428 (3.4588)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.135 (0.090)	Data 1.16e-04 (3.49e-04)	Tok/s 260519 (241167)	Loss/tok 3.6473 (3.4583)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][750/1291]	Time 0.067 (0.090)	Data 1.17e-04 (3.46e-04)	Tok/s 230321 (241219)	Loss/tok 3.1632 (3.4592)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.43e-04)	Tok/s 231492 (241150)	Loss/tok 3.1152 (3.4569)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.066 (0.090)	Data 1.16e-04 (3.40e-04)	Tok/s 235935 (241135)	Loss/tok 3.2244 (3.4556)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.136 (0.090)	Data 1.19e-04 (3.37e-04)	Tok/s 257368 (241158)	Loss/tok 3.5304 (3.4553)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.066 (0.090)	Data 1.37e-04 (3.34e-04)	Tok/s 228096 (241158)	Loss/tok 3.2812 (3.4538)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.101 (0.089)	Data 1.14e-04 (3.31e-04)	Tok/s 248591 (241145)	Loss/tok 3.4381 (3.4524)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.037 (0.089)	Data 1.18e-04 (3.29e-04)	Tok/s 212829 (241157)	Loss/tok 2.6793 (3.4520)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.175 (0.089)	Data 1.33e-04 (3.26e-04)	Tok/s 252460 (241154)	Loss/tok 3.8302 (3.4517)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.067 (0.089)	Data 1.35e-04 (3.24e-04)	Tok/s 237237 (241126)	Loss/tok 3.2095 (3.4500)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.21e-04)	Tok/s 235341 (241142)	Loss/tok 3.1568 (3.4487)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.19e-04)	Tok/s 233076 (241062)	Loss/tok 3.2391 (3.4470)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.17e-04)	Tok/s 228404 (241142)	Loss/tok 3.1247 (3.4471)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.067 (0.089)	Data 1.25e-04 (3.14e-04)	Tok/s 230580 (241115)	Loss/tok 3.1425 (3.4457)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][880/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.12e-04)	Tok/s 253524 (241077)	Loss/tok 3.2738 (3.4438)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.099 (0.089)	Data 1.19e-04 (3.10e-04)	Tok/s 255846 (241176)	Loss/tok 3.4534 (3.4446)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.099 (0.089)	Data 1.20e-04 (3.08e-04)	Tok/s 254691 (241132)	Loss/tok 3.3974 (3.4429)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.100 (0.089)	Data 1.21e-04 (3.06e-04)	Tok/s 250479 (241204)	Loss/tok 3.4090 (3.4433)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.04e-04)	Tok/s 248820 (241242)	Loss/tok 3.3349 (3.4421)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.067 (0.089)	Data 1.42e-04 (3.02e-04)	Tok/s 229753 (241267)	Loss/tok 3.1353 (3.4411)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.067 (0.089)	Data 1.32e-04 (3.00e-04)	Tok/s 230291 (241324)	Loss/tok 3.1148 (3.4403)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.98e-04)	Tok/s 234897 (241375)	Loss/tok 3.1928 (3.4398)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.96e-04)	Tok/s 230463 (241363)	Loss/tok 3.1706 (3.4388)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.099 (0.089)	Data 1.20e-04 (2.94e-04)	Tok/s 258613 (241423)	Loss/tok 3.2768 (3.4376)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.93e-04)	Tok/s 232446 (241455)	Loss/tok 3.1985 (3.4371)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.099 (0.089)	Data 1.17e-04 (2.91e-04)	Tok/s 254308 (241369)	Loss/tok 3.5516 (3.4358)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.89e-04)	Tok/s 231788 (241365)	Loss/tok 3.1625 (3.4352)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1010/1291]	Time 0.100 (0.089)	Data 1.17e-04 (2.87e-04)	Tok/s 254481 (241361)	Loss/tok 3.3527 (3.4342)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1020/1291]	Time 0.137 (0.089)	Data 1.17e-04 (2.86e-04)	Tok/s 256334 (241416)	Loss/tok 3.5516 (3.4342)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.101 (0.089)	Data 1.18e-04 (2.84e-04)	Tok/s 248988 (241428)	Loss/tok 3.4163 (3.4333)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1040/1291]	Time 0.100 (0.089)	Data 1.16e-04 (2.83e-04)	Tok/s 252852 (241437)	Loss/tok 3.3193 (3.4329)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.100 (0.089)	Data 1.20e-04 (2.81e-04)	Tok/s 248809 (241402)	Loss/tok 3.3624 (3.4316)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.134 (0.089)	Data 1.23e-04 (2.79e-04)	Tok/s 259205 (241461)	Loss/tok 3.6185 (3.4312)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.136 (0.089)	Data 1.36e-04 (2.78e-04)	Tok/s 255473 (241479)	Loss/tok 3.6030 (3.4316)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.135 (0.089)	Data 1.13e-04 (2.77e-04)	Tok/s 262998 (241463)	Loss/tok 3.5496 (3.4309)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.036 (0.089)	Data 1.14e-04 (2.75e-04)	Tok/s 224540 (241379)	Loss/tok 2.7298 (3.4293)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.099 (0.089)	Data 1.20e-04 (2.74e-04)	Tok/s 252060 (241335)	Loss/tok 3.3688 (3.4278)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.137 (0.089)	Data 1.15e-04 (2.72e-04)	Tok/s 256819 (241366)	Loss/tok 3.6176 (3.4284)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.036 (0.089)	Data 1.39e-04 (2.71e-04)	Tok/s 220633 (241364)	Loss/tok 2.6847 (3.4287)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.101 (0.089)	Data 1.16e-04 (2.70e-04)	Tok/s 246889 (241357)	Loss/tok 3.3953 (3.4283)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.68e-04)	Tok/s 234945 (241386)	Loss/tok 3.1344 (3.4286)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.136 (0.089)	Data 1.17e-04 (2.67e-04)	Tok/s 257896 (241411)	Loss/tok 3.5446 (3.4284)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.100 (0.089)	Data 1.37e-04 (2.66e-04)	Tok/s 250628 (241431)	Loss/tok 3.4791 (3.4285)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1170/1291]	Time 0.100 (0.089)	Data 1.36e-04 (2.65e-04)	Tok/s 253705 (241425)	Loss/tok 3.3907 (3.4278)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.63e-04)	Tok/s 227679 (241412)	Loss/tok 3.2075 (3.4271)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.175 (0.089)	Data 1.15e-04 (2.62e-04)	Tok/s 254598 (241354)	Loss/tok 3.6835 (3.4262)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.174 (0.089)	Data 1.16e-04 (2.61e-04)	Tok/s 255640 (241347)	Loss/tok 3.7030 (3.4261)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.100 (0.089)	Data 1.18e-04 (2.60e-04)	Tok/s 252888 (241307)	Loss/tok 3.3611 (3.4246)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.134 (0.089)	Data 1.18e-04 (2.59e-04)	Tok/s 262599 (241351)	Loss/tok 3.5141 (3.4246)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.068 (0.089)	Data 1.12e-04 (2.58e-04)	Tok/s 236507 (241368)	Loss/tok 3.1376 (3.4243)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1240/1291]	Time 0.135 (0.089)	Data 1.18e-04 (2.56e-04)	Tok/s 260621 (241404)	Loss/tok 3.4364 (3.4250)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.100 (0.090)	Data 1.16e-04 (2.55e-04)	Tok/s 251291 (241411)	Loss/tok 3.3797 (3.4249)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.101 (0.090)	Data 1.38e-04 (2.54e-04)	Tok/s 251286 (241427)	Loss/tok 3.4127 (3.4242)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.036 (0.089)	Data 1.14e-04 (2.53e-04)	Tok/s 221759 (241384)	Loss/tok 2.6196 (3.4226)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.101 (0.089)	Data 1.12e-04 (2.52e-04)	Tok/s 249469 (241364)	Loss/tok 3.3562 (3.4221)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.067 (0.089)	Data 5.08e-05 (2.54e-04)	Tok/s 229461 (241284)	Loss/tok 3.1991 (3.4210)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593141934000, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593141934001, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.380 (0.380)	Decoder iters 97.0 (97.0)	Tok/s 41798 (41798)
0: Running moses detokenizer
0: BLEU(score=22.11529070505039, counts=[35348, 17030, 9428, 5461], totals=[63395, 60392, 57389, 54392], precisions=[55.758340563135896, 28.199099218439528, 16.428235376117374, 10.040079423444624], bp=0.979996145094517, sys_len=63395, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593141935884, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2212, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593141935884, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4216	Test BLEU: 22.12
0: Performance: Epoch: 1	Training: 1930239 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593141935884, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593141935884, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593141935884, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3937356774
0: TRAIN [2][0/1291]	Time 0.296 (0.296)	Data 1.84e-01 (1.84e-01)	Tok/s 85031 (85031)	Loss/tok 3.2291 (3.2291)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.099 (0.110)	Data 1.24e-04 (1.69e-02)	Tok/s 254960 (228696)	Loss/tok 3.3578 (3.3248)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.066 (0.104)	Data 1.16e-04 (8.89e-03)	Tok/s 231797 (235876)	Loss/tok 3.0147 (3.3421)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.174 (0.099)	Data 1.16e-04 (6.06e-03)	Tok/s 257111 (236639)	Loss/tok 3.6894 (3.3555)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.134 (0.097)	Data 1.19e-04 (4.61e-03)	Tok/s 260387 (238443)	Loss/tok 3.4341 (3.3355)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.174 (0.098)	Data 1.14e-04 (3.73e-03)	Tok/s 257637 (239899)	Loss/tok 3.6763 (3.3480)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.103 (0.096)	Data 1.18e-04 (3.14e-03)	Tok/s 244938 (239871)	Loss/tok 3.2470 (3.3242)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.067 (0.093)	Data 1.18e-04 (2.71e-03)	Tok/s 230066 (239472)	Loss/tok 3.0479 (3.3091)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][80/1291]	Time 0.134 (0.094)	Data 1.12e-04 (2.39e-03)	Tok/s 260004 (240294)	Loss/tok 3.4766 (3.3228)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.135 (0.096)	Data 1.30e-04 (2.14e-03)	Tok/s 257704 (241150)	Loss/tok 3.4980 (3.3269)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.036 (0.093)	Data 1.19e-04 (1.94e-03)	Tok/s 225530 (240470)	Loss/tok 2.7369 (3.3123)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.067 (0.091)	Data 1.07e-04 (1.78e-03)	Tok/s 231153 (240085)	Loss/tok 3.0654 (3.3029)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.134 (0.092)	Data 1.10e-04 (1.64e-03)	Tok/s 257129 (240455)	Loss/tok 3.4871 (3.3084)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.100 (0.092)	Data 1.19e-04 (1.52e-03)	Tok/s 253575 (240447)	Loss/tok 3.3190 (3.3079)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.099 (0.091)	Data 1.08e-04 (1.42e-03)	Tok/s 253425 (240271)	Loss/tok 3.3039 (3.3028)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.067 (0.091)	Data 1.33e-04 (1.34e-03)	Tok/s 236862 (240493)	Loss/tok 3.1676 (3.3053)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.099 (0.091)	Data 1.24e-04 (1.26e-03)	Tok/s 250351 (240786)	Loss/tok 3.2276 (3.3019)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.067 (0.091)	Data 1.05e-04 (1.19e-03)	Tok/s 231872 (240818)	Loss/tok 3.0412 (3.3002)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.067 (0.091)	Data 1.10e-04 (1.13e-03)	Tok/s 224434 (240975)	Loss/tok 3.1747 (3.3004)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.036 (0.091)	Data 1.28e-04 (1.08e-03)	Tok/s 219643 (241330)	Loss/tok 2.6882 (3.3005)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.173 (0.092)	Data 1.08e-04 (1.03e-03)	Tok/s 256745 (241610)	Loss/tok 3.7383 (3.3031)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][210/1291]	Time 0.036 (0.091)	Data 1.12e-04 (9.88e-04)	Tok/s 219850 (241496)	Loss/tok 2.7538 (3.3015)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][220/1291]	Time 0.067 (0.092)	Data 1.10e-04 (9.48e-04)	Tok/s 228876 (241679)	Loss/tok 2.9750 (3.3038)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.067 (0.093)	Data 1.08e-04 (9.12e-04)	Tok/s 233172 (241931)	Loss/tok 3.1515 (3.3144)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.067 (0.093)	Data 1.12e-04 (8.81e-04)	Tok/s 232313 (241839)	Loss/tok 3.0435 (3.3112)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.067 (0.092)	Data 1.10e-04 (8.50e-04)	Tok/s 228959 (241791)	Loss/tok 3.0903 (3.3080)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.175 (0.091)	Data 1.11e-04 (8.22e-04)	Tok/s 257324 (241474)	Loss/tok 3.5575 (3.3045)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.135 (0.091)	Data 1.18e-04 (7.96e-04)	Tok/s 258590 (241349)	Loss/tok 3.4937 (3.3018)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.067 (0.091)	Data 1.12e-04 (7.71e-04)	Tok/s 230835 (241369)	Loss/tok 3.1496 (3.3029)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.067 (0.091)	Data 1.09e-04 (7.48e-04)	Tok/s 233887 (241383)	Loss/tok 3.0927 (3.3001)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.067 (0.091)	Data 1.22e-04 (7.27e-04)	Tok/s 229877 (241506)	Loss/tok 3.0529 (3.3026)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.066 (0.091)	Data 1.13e-04 (7.08e-04)	Tok/s 229967 (241598)	Loss/tok 3.0742 (3.3046)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.067 (0.091)	Data 1.10e-04 (6.89e-04)	Tok/s 234492 (241643)	Loss/tok 3.0953 (3.3062)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.067 (0.092)	Data 1.12e-04 (6.72e-04)	Tok/s 228516 (241820)	Loss/tok 3.0837 (3.3052)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][340/1291]	Time 0.067 (0.091)	Data 1.37e-04 (6.56e-04)	Tok/s 230582 (241715)	Loss/tok 3.1006 (3.3022)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.099 (0.091)	Data 1.11e-04 (6.41e-04)	Tok/s 253461 (241912)	Loss/tok 3.2013 (3.3028)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.067 (0.091)	Data 1.12e-04 (6.26e-04)	Tok/s 234126 (241731)	Loss/tok 3.0275 (3.2991)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.135 (0.091)	Data 1.12e-04 (6.12e-04)	Tok/s 258673 (241628)	Loss/tok 3.4854 (3.2991)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][380/1291]	Time 0.135 (0.091)	Data 1.22e-04 (5.99e-04)	Tok/s 257242 (241775)	Loss/tok 3.4784 (3.3010)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.067 (0.091)	Data 1.08e-04 (5.87e-04)	Tok/s 230183 (241732)	Loss/tok 3.1241 (3.2985)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.100 (0.090)	Data 1.12e-04 (5.75e-04)	Tok/s 249884 (241622)	Loss/tok 3.3292 (3.2978)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.175 (0.090)	Data 1.16e-04 (5.64e-04)	Tok/s 257771 (241597)	Loss/tok 3.6331 (3.2989)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.067 (0.090)	Data 1.15e-04 (5.53e-04)	Tok/s 230984 (241603)	Loss/tok 3.0253 (3.2995)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.067 (0.090)	Data 1.10e-04 (5.43e-04)	Tok/s 233731 (241498)	Loss/tok 3.0995 (3.2993)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][440/1291]	Time 0.099 (0.090)	Data 1.12e-04 (5.33e-04)	Tok/s 255785 (241572)	Loss/tok 3.1683 (3.2993)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.067 (0.090)	Data 1.33e-04 (5.24e-04)	Tok/s 235844 (241620)	Loss/tok 3.0325 (3.2990)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.100 (0.090)	Data 1.17e-04 (5.15e-04)	Tok/s 249196 (241759)	Loss/tok 3.2581 (3.2999)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.173 (0.090)	Data 1.24e-04 (5.07e-04)	Tok/s 256983 (241591)	Loss/tok 3.7298 (3.2989)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.099 (0.090)	Data 1.21e-04 (4.99e-04)	Tok/s 255520 (241584)	Loss/tok 3.1858 (3.2979)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.91e-04)	Tok/s 233081 (241573)	Loss/tok 3.1822 (3.2961)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.83e-04)	Tok/s 235059 (241566)	Loss/tok 3.0715 (3.2944)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.76e-04)	Tok/s 234336 (241483)	Loss/tok 3.0890 (3.2930)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.099 (0.090)	Data 1.10e-04 (4.69e-04)	Tok/s 253645 (241522)	Loss/tok 3.3397 (3.2926)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.135 (0.090)	Data 1.26e-04 (4.62e-04)	Tok/s 261243 (241553)	Loss/tok 3.4915 (3.2926)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.135 (0.089)	Data 1.13e-04 (4.56e-04)	Tok/s 260675 (241538)	Loss/tok 3.4404 (3.2920)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.099 (0.089)	Data 1.10e-04 (4.50e-04)	Tok/s 251539 (241559)	Loss/tok 3.3095 (3.2916)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.099 (0.089)	Data 1.14e-04 (4.44e-04)	Tok/s 253492 (241538)	Loss/tok 3.3136 (3.2906)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][570/1291]	Time 0.067 (0.089)	Data 1.12e-04 (4.38e-04)	Tok/s 234964 (241423)	Loss/tok 3.0549 (3.2895)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.136 (0.089)	Data 1.12e-04 (4.33e-04)	Tok/s 255133 (241308)	Loss/tok 3.4697 (3.2877)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.066 (0.089)	Data 1.12e-04 (4.27e-04)	Tok/s 234145 (241325)	Loss/tok 3.1075 (3.2873)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.067 (0.089)	Data 1.33e-04 (4.22e-04)	Tok/s 235734 (241337)	Loss/tok 3.0608 (3.2870)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.066 (0.089)	Data 1.10e-04 (4.17e-04)	Tok/s 229958 (241326)	Loss/tok 3.1123 (3.2871)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.13e-04)	Tok/s 230338 (241263)	Loss/tok 3.1245 (3.2874)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.099 (0.089)	Data 1.11e-04 (4.08e-04)	Tok/s 253637 (241304)	Loss/tok 3.2089 (3.2871)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.067 (0.089)	Data 1.23e-04 (4.03e-04)	Tok/s 231659 (241309)	Loss/tok 3.0800 (3.2867)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.068 (0.089)	Data 1.13e-04 (3.99e-04)	Tok/s 224016 (241330)	Loss/tok 3.1037 (3.2876)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.95e-04)	Tok/s 233060 (241347)	Loss/tok 3.0989 (3.2879)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.90e-04)	Tok/s 233599 (241294)	Loss/tok 3.1700 (3.2867)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.86e-04)	Tok/s 230090 (241331)	Loss/tok 2.9765 (3.2861)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.83e-04)	Tok/s 234725 (241372)	Loss/tok 3.1344 (3.2858)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][700/1291]	Time 0.134 (0.089)	Data 1.29e-04 (3.79e-04)	Tok/s 258412 (241486)	Loss/tok 3.4512 (3.2878)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.036 (0.089)	Data 1.13e-04 (3.75e-04)	Tok/s 220782 (241405)	Loss/tok 2.6471 (3.2858)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.71e-04)	Tok/s 253235 (241556)	Loss/tok 3.2515 (3.2875)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.68e-04)	Tok/s 252570 (241621)	Loss/tok 3.2731 (3.2877)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.65e-04)	Tok/s 229626 (241511)	Loss/tok 3.0458 (3.2860)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.61e-04)	Tok/s 256453 (241517)	Loss/tok 3.1962 (3.2859)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.58e-04)	Tok/s 249359 (241677)	Loss/tok 3.2988 (3.2886)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.55e-04)	Tok/s 252237 (241666)	Loss/tok 3.2829 (3.2877)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.067 (0.089)	Data 1.29e-04 (3.52e-04)	Tok/s 228380 (241679)	Loss/tok 3.0775 (3.2880)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.49e-04)	Tok/s 228996 (241707)	Loss/tok 3.1115 (3.2881)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.46e-04)	Tok/s 229194 (241676)	Loss/tok 3.0961 (3.2870)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.43e-04)	Tok/s 234132 (241651)	Loss/tok 3.0738 (3.2858)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.40e-04)	Tok/s 255944 (241660)	Loss/tok 3.2701 (3.2862)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][830/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.38e-04)	Tok/s 234097 (241664)	Loss/tok 3.0917 (3.2869)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.135 (0.089)	Data 1.33e-04 (3.35e-04)	Tok/s 256893 (241690)	Loss/tok 3.4902 (3.2876)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.136 (0.089)	Data 1.10e-04 (3.32e-04)	Tok/s 259658 (241720)	Loss/tok 3.4590 (3.2875)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.30e-04)	Tok/s 249934 (241699)	Loss/tok 3.2993 (3.2863)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.27e-04)	Tok/s 254935 (241735)	Loss/tok 3.2201 (3.2855)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.136 (0.089)	Data 1.12e-04 (3.25e-04)	Tok/s 257868 (241732)	Loss/tok 3.4716 (3.2850)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.136 (0.089)	Data 1.14e-04 (3.23e-04)	Tok/s 259617 (241775)	Loss/tok 3.5101 (3.2851)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.20e-04)	Tok/s 228513 (241771)	Loss/tok 3.0616 (3.2846)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.18e-04)	Tok/s 234467 (241780)	Loss/tok 3.0806 (3.2842)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.136 (0.089)	Data 1.14e-04 (3.16e-04)	Tok/s 254889 (241832)	Loss/tok 3.4632 (3.2851)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.14e-04)	Tok/s 254778 (241893)	Loss/tok 3.2861 (3.2854)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.135 (0.089)	Data 1.13e-04 (3.12e-04)	Tok/s 258582 (241941)	Loss/tok 3.4507 (3.2872)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][950/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.10e-04)	Tok/s 235482 (241876)	Loss/tok 3.0391 (3.2872)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.08e-04)	Tok/s 230295 (241866)	Loss/tok 3.1571 (3.2866)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.135 (0.089)	Data 1.09e-04 (3.06e-04)	Tok/s 258000 (241845)	Loss/tok 3.5656 (3.2865)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.067 (0.089)	Data 1.23e-04 (3.04e-04)	Tok/s 226222 (241850)	Loss/tok 3.1111 (3.2856)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.02e-04)	Tok/s 234801 (241862)	Loss/tok 3.0206 (3.2846)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.00e-04)	Tok/s 252995 (241806)	Loss/tok 3.2715 (3.2833)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.98e-04)	Tok/s 260622 (241849)	Loss/tok 3.4012 (3.2838)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.96e-04)	Tok/s 253120 (241909)	Loss/tok 3.2625 (3.2835)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.100 (0.089)	Data 1.18e-04 (2.95e-04)	Tok/s 252205 (241923)	Loss/tok 3.2300 (3.2832)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.100 (0.089)	Data 1.26e-04 (2.93e-04)	Tok/s 253933 (241877)	Loss/tok 3.2398 (3.2829)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.91e-04)	Tok/s 253053 (241933)	Loss/tok 3.2423 (3.2841)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.099 (0.089)	Data 1.17e-04 (2.89e-04)	Tok/s 253036 (241891)	Loss/tok 3.2894 (3.2831)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1070/1291]	Time 0.100 (0.089)	Data 1.09e-04 (2.88e-04)	Tok/s 251891 (241889)	Loss/tok 3.3034 (3.2825)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.86e-04)	Tok/s 254555 (241935)	Loss/tok 3.2512 (3.2826)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1090/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.85e-04)	Tok/s 227491 (241944)	Loss/tok 3.1190 (3.2819)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.83e-04)	Tok/s 248626 (241999)	Loss/tok 3.3551 (3.2823)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.174 (0.089)	Data 1.19e-04 (2.82e-04)	Tok/s 257127 (242022)	Loss/tok 3.6559 (3.2824)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.066 (0.089)	Data 1.29e-04 (2.80e-04)	Tok/s 232079 (242003)	Loss/tok 3.0919 (3.2818)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.79e-04)	Tok/s 230255 (241907)	Loss/tok 2.9826 (3.2807)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.77e-04)	Tok/s 233583 (241879)	Loss/tok 3.0891 (3.2800)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.066 (0.088)	Data 1.09e-04 (2.76e-04)	Tok/s 233060 (241816)	Loss/tok 3.0972 (3.2790)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.74e-04)	Tok/s 236688 (241854)	Loss/tok 3.0133 (3.2792)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.101 (0.089)	Data 1.33e-04 (2.73e-04)	Tok/s 249204 (241851)	Loss/tok 3.2589 (3.2792)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.036 (0.088)	Data 1.13e-04 (2.72e-04)	Tok/s 221615 (241812)	Loss/tok 2.5647 (3.2792)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.70e-04)	Tok/s 229517 (241841)	Loss/tok 3.0851 (3.2796)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.135 (0.089)	Data 1.41e-04 (2.69e-04)	Tok/s 261778 (241843)	Loss/tok 3.4604 (3.2792)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.136 (0.089)	Data 1.10e-04 (2.68e-04)	Tok/s 256929 (241886)	Loss/tok 3.4269 (3.2791)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1220/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.67e-04)	Tok/s 233189 (241860)	Loss/tok 3.0626 (3.2786)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.067 (0.088)	Data 1.10e-04 (2.65e-04)	Tok/s 232492 (241870)	Loss/tok 3.1418 (3.2782)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.135 (0.089)	Data 1.19e-04 (2.64e-04)	Tok/s 256992 (241918)	Loss/tok 3.4070 (3.2788)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1250/1291]	Time 0.173 (0.089)	Data 1.15e-04 (2.63e-04)	Tok/s 258195 (241988)	Loss/tok 3.5398 (3.2812)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.101 (0.089)	Data 1.12e-04 (2.62e-04)	Tok/s 251672 (242005)	Loss/tok 3.4443 (3.2814)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.173 (0.089)	Data 1.12e-04 (2.61e-04)	Tok/s 257992 (242006)	Loss/tok 3.5973 (3.2816)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.59e-04)	Tok/s 232003 (242058)	Loss/tok 3.0469 (3.2819)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.067 (0.089)	Data 4.77e-05 (2.61e-04)	Tok/s 228276 (242032)	Loss/tok 3.1214 (3.2815)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593142051580, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593142051580, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.445 (0.445)	Decoder iters 123.0 (123.0)	Tok/s 36449 (36449)
0: Running moses detokenizer
0: BLEU(score=23.07954027344867, counts=[36322, 17857, 10005, 5824], totals=[65009, 62006, 59003, 56005], precisions=[55.872263840391334, 28.79882592007225, 16.9567649102588, 10.39907151147219], bp=1.0, sys_len=65009, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593142053544, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23079999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593142053544, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2797	Test BLEU: 23.08
0: Performance: Epoch: 2	Training: 1935612 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593142053545, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593142053545, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142053545, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1781169619
0: TRAIN [3][0/1291]	Time 0.263 (0.263)	Data 1.92e-01 (1.92e-01)	Tok/s 59328 (59328)	Loss/tok 2.9625 (2.9625)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.135 (0.113)	Data 1.16e-04 (1.75e-02)	Tok/s 259175 (226104)	Loss/tok 3.3051 (3.2312)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.066 (0.093)	Data 1.13e-04 (9.24e-03)	Tok/s 232975 (229955)	Loss/tok 2.9728 (3.1659)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.067 (0.091)	Data 3.07e-04 (6.31e-03)	Tok/s 235796 (233691)	Loss/tok 2.9992 (3.1672)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.067 (0.090)	Data 1.36e-04 (4.80e-03)	Tok/s 236076 (234305)	Loss/tok 2.9556 (3.1866)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.036 (0.086)	Data 1.20e-04 (3.88e-03)	Tok/s 220914 (234615)	Loss/tok 2.6524 (3.1630)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.070 (0.083)	Data 1.31e-04 (3.26e-03)	Tok/s 220380 (234116)	Loss/tok 2.9577 (3.1423)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.036 (0.082)	Data 1.24e-04 (2.82e-03)	Tok/s 215902 (234628)	Loss/tok 2.6297 (3.1319)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][80/1291]	Time 0.135 (0.084)	Data 1.13e-04 (2.49e-03)	Tok/s 259561 (236059)	Loss/tok 3.3794 (3.1451)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.036 (0.083)	Data 1.12e-04 (2.23e-03)	Tok/s 219598 (236389)	Loss/tok 2.6042 (3.1363)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.100 (0.083)	Data 1.13e-04 (2.02e-03)	Tok/s 249916 (236693)	Loss/tok 3.2206 (3.1305)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.067 (0.082)	Data 1.15e-04 (1.85e-03)	Tok/s 231468 (236814)	Loss/tok 3.0259 (3.1262)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][120/1291]	Time 0.067 (0.083)	Data 1.11e-04 (1.70e-03)	Tok/s 229061 (237417)	Loss/tok 2.9268 (3.1308)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.099 (0.082)	Data 1.15e-04 (1.58e-03)	Tok/s 255145 (237154)	Loss/tok 3.1683 (3.1283)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.135 (0.082)	Data 1.28e-04 (1.48e-03)	Tok/s 259680 (237179)	Loss/tok 3.4115 (3.1267)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.067 (0.082)	Data 1.13e-04 (1.39e-03)	Tok/s 233978 (237380)	Loss/tok 3.0148 (3.1278)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.067 (0.083)	Data 1.17e-04 (1.31e-03)	Tok/s 234929 (238031)	Loss/tok 2.9293 (3.1306)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.100 (0.083)	Data 1.11e-04 (1.24e-03)	Tok/s 256041 (238289)	Loss/tok 3.0695 (3.1280)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.036 (0.084)	Data 1.15e-04 (1.18e-03)	Tok/s 226566 (238816)	Loss/tok 2.6829 (3.1406)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.136 (0.085)	Data 1.17e-04 (1.12e-03)	Tok/s 256888 (239132)	Loss/tok 3.3628 (3.1480)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.067 (0.085)	Data 1.13e-04 (1.07e-03)	Tok/s 233860 (239238)	Loss/tok 3.0548 (3.1532)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.066 (0.085)	Data 1.14e-04 (1.03e-03)	Tok/s 236262 (239342)	Loss/tok 2.9965 (3.1544)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.067 (0.085)	Data 1.38e-04 (9.87e-04)	Tok/s 231947 (239662)	Loss/tok 2.9512 (3.1549)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.136 (0.085)	Data 1.28e-04 (9.49e-04)	Tok/s 254065 (239538)	Loss/tok 3.4325 (3.1548)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][240/1291]	Time 0.067 (0.085)	Data 1.13e-04 (9.15e-04)	Tok/s 228735 (239387)	Loss/tok 2.9373 (3.1525)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.099 (0.085)	Data 1.14e-04 (8.83e-04)	Tok/s 255168 (239471)	Loss/tok 3.1736 (3.1516)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.036 (0.085)	Data 1.15e-04 (8.54e-04)	Tok/s 220854 (239689)	Loss/tok 2.5577 (3.1541)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.036 (0.085)	Data 1.15e-04 (8.27e-04)	Tok/s 222457 (239874)	Loss/tok 2.5453 (3.1537)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.067 (0.085)	Data 1.31e-04 (8.01e-04)	Tok/s 228562 (239842)	Loss/tok 2.9377 (3.1550)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.099 (0.086)	Data 1.12e-04 (7.78e-04)	Tok/s 256879 (240192)	Loss/tok 3.0925 (3.1604)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.100 (0.086)	Data 1.15e-04 (7.56e-04)	Tok/s 252027 (240335)	Loss/tok 3.1228 (3.1579)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.066 (0.086)	Data 1.24e-04 (7.35e-04)	Tok/s 232352 (240399)	Loss/tok 3.0328 (3.1567)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.036 (0.086)	Data 1.11e-04 (7.16e-04)	Tok/s 222209 (240347)	Loss/tok 2.6501 (3.1555)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.100 (0.086)	Data 1.15e-04 (6.98e-04)	Tok/s 255743 (240365)	Loss/tok 3.1898 (3.1535)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.067 (0.086)	Data 1.34e-04 (6.81e-04)	Tok/s 228550 (240445)	Loss/tok 2.9330 (3.1546)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.174 (0.086)	Data 1.24e-04 (6.65e-04)	Tok/s 262175 (240388)	Loss/tok 3.3844 (3.1534)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.067 (0.086)	Data 1.14e-04 (6.50e-04)	Tok/s 229092 (240312)	Loss/tok 3.0060 (3.1506)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][370/1291]	Time 0.100 (0.086)	Data 1.17e-04 (6.35e-04)	Tok/s 248536 (240304)	Loss/tok 3.2076 (3.1495)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.067 (0.086)	Data 1.13e-04 (6.22e-04)	Tok/s 232242 (240447)	Loss/tok 2.9187 (3.1551)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.067 (0.086)	Data 1.14e-04 (6.09e-04)	Tok/s 229640 (240369)	Loss/tok 3.0226 (3.1531)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.173 (0.086)	Data 1.14e-04 (5.97e-04)	Tok/s 258748 (240324)	Loss/tok 3.5908 (3.1529)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.036 (0.086)	Data 1.13e-04 (5.85e-04)	Tok/s 223211 (240309)	Loss/tok 2.5425 (3.1517)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.100 (0.086)	Data 1.37e-04 (5.74e-04)	Tok/s 252784 (240380)	Loss/tok 3.1567 (3.1510)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.067 (0.086)	Data 1.13e-04 (5.63e-04)	Tok/s 226869 (240447)	Loss/tok 3.0143 (3.1518)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.067 (0.086)	Data 1.14e-04 (5.53e-04)	Tok/s 233681 (240422)	Loss/tok 2.9315 (3.1513)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.067 (0.086)	Data 1.13e-04 (5.43e-04)	Tok/s 227766 (240469)	Loss/tok 2.9537 (3.1502)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.067 (0.087)	Data 1.14e-04 (5.34e-04)	Tok/s 227530 (240536)	Loss/tok 2.9058 (3.1511)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.136 (0.086)	Data 1.15e-04 (5.25e-04)	Tok/s 255303 (240523)	Loss/tok 3.3269 (3.1499)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.067 (0.087)	Data 1.14e-04 (5.17e-04)	Tok/s 232730 (240558)	Loss/tok 2.8514 (3.1490)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.067 (0.086)	Data 1.11e-04 (5.09e-04)	Tok/s 225774 (240551)	Loss/tok 3.0223 (3.1473)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][500/1291]	Time 0.100 (0.086)	Data 1.11e-04 (5.01e-04)	Tok/s 252209 (240595)	Loss/tok 3.2262 (3.1462)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.100 (0.086)	Data 1.13e-04 (4.93e-04)	Tok/s 256501 (240720)	Loss/tok 3.1428 (3.1462)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.067 (0.086)	Data 1.13e-04 (4.86e-04)	Tok/s 225546 (240725)	Loss/tok 2.8512 (3.1461)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.067 (0.086)	Data 1.16e-04 (4.79e-04)	Tok/s 234290 (240678)	Loss/tok 3.0157 (3.1457)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.099 (0.086)	Data 1.14e-04 (4.73e-04)	Tok/s 250742 (240722)	Loss/tok 3.1038 (3.1466)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.066 (0.087)	Data 1.10e-04 (4.66e-04)	Tok/s 234513 (240849)	Loss/tok 3.0617 (3.1498)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.135 (0.087)	Data 1.13e-04 (4.60e-04)	Tok/s 257813 (240880)	Loss/tok 3.4270 (3.1500)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.099 (0.087)	Data 1.14e-04 (4.54e-04)	Tok/s 254770 (240838)	Loss/tok 3.0705 (3.1496)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.067 (0.087)	Data 1.17e-04 (4.48e-04)	Tok/s 228518 (240690)	Loss/tok 2.9051 (3.1477)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.175 (0.087)	Data 1.14e-04 (4.43e-04)	Tok/s 257784 (240721)	Loss/tok 3.5587 (3.1489)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.067 (0.086)	Data 1.14e-04 (4.37e-04)	Tok/s 229169 (240600)	Loss/tok 2.9351 (3.1478)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.100 (0.087)	Data 1.14e-04 (4.32e-04)	Tok/s 248879 (240637)	Loss/tok 3.1116 (3.1478)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.099 (0.087)	Data 1.17e-04 (4.27e-04)	Tok/s 252223 (240682)	Loss/tok 3.1553 (3.1485)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][630/1291]	Time 0.175 (0.087)	Data 1.24e-04 (4.22e-04)	Tok/s 255575 (240744)	Loss/tok 3.4310 (3.1500)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.067 (0.087)	Data 1.12e-04 (4.17e-04)	Tok/s 237336 (240684)	Loss/tok 2.9333 (3.1483)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.099 (0.087)	Data 1.17e-04 (4.13e-04)	Tok/s 253443 (240728)	Loss/tok 3.1169 (3.1504)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.067 (0.087)	Data 1.17e-04 (4.08e-04)	Tok/s 231327 (240724)	Loss/tok 2.9249 (3.1500)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.067 (0.087)	Data 1.15e-04 (4.04e-04)	Tok/s 235770 (240708)	Loss/tok 3.0321 (3.1487)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.174 (0.087)	Data 1.36e-04 (4.00e-04)	Tok/s 255148 (240811)	Loss/tok 3.5634 (3.1511)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.100 (0.087)	Data 1.15e-04 (3.96e-04)	Tok/s 252268 (240920)	Loss/tok 3.2138 (3.1527)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.066 (0.087)	Data 1.14e-04 (3.92e-04)	Tok/s 233203 (240909)	Loss/tok 2.9904 (3.1517)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.067 (0.087)	Data 1.12e-04 (3.88e-04)	Tok/s 227842 (240945)	Loss/tok 2.9685 (3.1512)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.135 (0.088)	Data 1.15e-04 (3.84e-04)	Tok/s 258438 (241028)	Loss/tok 3.3103 (3.1529)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.81e-04)	Tok/s 232855 (241030)	Loss/tok 3.0178 (3.1542)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.067 (0.088)	Data 1.14e-04 (3.77e-04)	Tok/s 232686 (241092)	Loss/tok 2.8704 (3.1554)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.136 (0.088)	Data 1.30e-04 (3.74e-04)	Tok/s 256551 (241216)	Loss/tok 3.3560 (3.1563)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][760/1291]	Time 0.100 (0.088)	Data 1.15e-04 (3.70e-04)	Tok/s 252914 (241211)	Loss/tok 3.1325 (3.1553)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.067 (0.088)	Data 1.19e-04 (3.67e-04)	Tok/s 229904 (241194)	Loss/tok 2.8926 (3.1556)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.067 (0.088)	Data 1.18e-04 (3.64e-04)	Tok/s 232866 (241149)	Loss/tok 2.9208 (3.1537)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.135 (0.088)	Data 1.14e-04 (3.61e-04)	Tok/s 258153 (241224)	Loss/tok 3.2728 (3.1535)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.100 (0.088)	Data 1.16e-04 (3.58e-04)	Tok/s 250642 (241300)	Loss/tok 3.0925 (3.1534)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.067 (0.088)	Data 1.37e-04 (3.55e-04)	Tok/s 229413 (241234)	Loss/tok 2.9003 (3.1529)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.036 (0.088)	Data 1.28e-04 (3.52e-04)	Tok/s 217865 (241198)	Loss/tok 2.5221 (3.1523)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.135 (0.088)	Data 1.19e-04 (3.49e-04)	Tok/s 260143 (241222)	Loss/tok 3.3045 (3.1519)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.036 (0.088)	Data 1.16e-04 (3.46e-04)	Tok/s 221034 (241277)	Loss/tok 2.5573 (3.1522)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.067 (0.088)	Data 1.35e-04 (3.44e-04)	Tok/s 231161 (241338)	Loss/tok 2.9933 (3.1519)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.135 (0.088)	Data 1.31e-04 (3.41e-04)	Tok/s 260522 (241317)	Loss/tok 3.3059 (3.1513)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.38e-04)	Tok/s 234814 (241309)	Loss/tok 2.9547 (3.1504)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][880/1291]	Time 0.100 (0.088)	Data 1.12e-04 (3.36e-04)	Tok/s 251817 (241368)	Loss/tok 3.1132 (3.1504)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.067 (0.088)	Data 1.14e-04 (3.34e-04)	Tok/s 238224 (241421)	Loss/tok 2.9814 (3.1511)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.088)	Data 1.35e-04 (3.31e-04)	Tok/s 232459 (241387)	Loss/tok 2.9384 (3.1502)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.101 (0.088)	Data 1.35e-04 (3.29e-04)	Tok/s 251571 (241404)	Loss/tok 3.1092 (3.1492)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.135 (0.088)	Data 1.35e-04 (3.27e-04)	Tok/s 259120 (241485)	Loss/tok 3.3610 (3.1497)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.135 (0.088)	Data 1.16e-04 (3.24e-04)	Tok/s 260413 (241477)	Loss/tok 3.2674 (3.1496)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.101 (0.088)	Data 1.12e-04 (3.22e-04)	Tok/s 248866 (241432)	Loss/tok 3.0744 (3.1484)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.100 (0.088)	Data 1.15e-04 (3.20e-04)	Tok/s 250823 (241482)	Loss/tok 3.1767 (3.1481)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.135 (0.088)	Data 1.35e-04 (3.18e-04)	Tok/s 257796 (241545)	Loss/tok 3.1928 (3.1477)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.174 (0.088)	Data 1.15e-04 (3.16e-04)	Tok/s 258177 (241525)	Loss/tok 3.4553 (3.1486)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.14e-04)	Tok/s 252802 (241615)	Loss/tok 3.1204 (3.1499)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.12e-04)	Tok/s 234330 (241660)	Loss/tok 2.8984 (3.1508)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.100 (0.089)	Data 1.28e-04 (3.10e-04)	Tok/s 252407 (241641)	Loss/tok 3.0997 (3.1497)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1010/1291]	Time 0.174 (0.089)	Data 1.14e-04 (3.08e-04)	Tok/s 256445 (241681)	Loss/tok 3.5415 (3.1514)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.066 (0.089)	Data 1.17e-04 (3.07e-04)	Tok/s 238699 (241696)	Loss/tok 2.8096 (3.1508)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.100 (0.089)	Data 1.16e-04 (3.05e-04)	Tok/s 253605 (241762)	Loss/tok 3.0670 (3.1507)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.067 (0.089)	Data 1.34e-04 (3.03e-04)	Tok/s 231518 (241757)	Loss/tok 3.0510 (3.1508)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.01e-04)	Tok/s 233370 (241743)	Loss/tok 2.8934 (3.1499)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.99e-04)	Tok/s 260754 (241737)	Loss/tok 3.2416 (3.1492)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.136 (0.089)	Data 1.17e-04 (2.98e-04)	Tok/s 261459 (241824)	Loss/tok 3.1985 (3.1495)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.099 (0.089)	Data 1.22e-04 (2.96e-04)	Tok/s 254134 (241794)	Loss/tok 3.1011 (3.1489)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.036 (0.089)	Data 1.16e-04 (2.94e-04)	Tok/s 217493 (241768)	Loss/tok 2.4760 (3.1481)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.93e-04)	Tok/s 232379 (241799)	Loss/tok 2.9167 (3.1483)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.91e-04)	Tok/s 231372 (241753)	Loss/tok 2.9620 (3.1479)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.099 (0.089)	Data 1.18e-04 (2.90e-04)	Tok/s 252409 (241698)	Loss/tok 3.1830 (3.1475)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.88e-04)	Tok/s 255444 (241638)	Loss/tok 3.0832 (3.1462)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1140/1291]	Time 0.136 (0.089)	Data 1.14e-04 (2.87e-04)	Tok/s 258385 (241665)	Loss/tok 3.2437 (3.1463)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.85e-04)	Tok/s 231419 (241686)	Loss/tok 2.8819 (3.1461)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.136 (0.089)	Data 1.12e-04 (2.84e-04)	Tok/s 256355 (241729)	Loss/tok 3.1820 (3.1463)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.82e-04)	Tok/s 237087 (241782)	Loss/tok 2.9048 (3.1466)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.067 (0.089)	Data 1.19e-04 (2.81e-04)	Tok/s 231389 (241720)	Loss/tok 2.9234 (3.1457)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.80e-04)	Tok/s 253632 (241756)	Loss/tok 3.0802 (3.1452)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.067 (0.089)	Data 1.18e-04 (2.78e-04)	Tok/s 231413 (241755)	Loss/tok 2.8252 (3.1453)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.067 (0.089)	Data 1.40e-04 (2.77e-04)	Tok/s 237580 (241815)	Loss/tok 2.8731 (3.1458)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.036 (0.089)	Data 1.17e-04 (2.76e-04)	Tok/s 219345 (241806)	Loss/tok 2.5175 (3.1453)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.067 (0.089)	Data 1.36e-04 (2.74e-04)	Tok/s 230117 (241803)	Loss/tok 2.9336 (3.1460)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.067 (0.089)	Data 1.18e-04 (2.73e-04)	Tok/s 234905 (241810)	Loss/tok 2.9716 (3.1456)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.066 (0.089)	Data 1.18e-04 (2.72e-04)	Tok/s 240788 (241818)	Loss/tok 3.0008 (3.1455)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.71e-04)	Tok/s 225122 (241784)	Loss/tok 2.5443 (3.1452)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1270/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.70e-04)	Tok/s 254357 (241810)	Loss/tok 3.0475 (3.1445)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.68e-04)	Tok/s 249582 (241799)	Loss/tok 3.1051 (3.1441)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.174 (0.089)	Data 4.29e-05 (2.70e-04)	Tok/s 254335 (241829)	Loss/tok 3.5787 (3.1445)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593142169291, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593142169291, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.401 (0.401)	Decoder iters 107.0 (107.0)	Tok/s 41140 (41140)
0: Running moses detokenizer
0: BLEU(score=24.277483486620095, counts=[37218, 18758, 10676, 6356], totals=[65364, 62361, 59358, 56362], precisions=[56.93959977969524, 30.07969724667661, 17.985781192088684, 11.277101593272063], bp=1.0, sys_len=65364, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593142171237, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24280000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593142171237, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1443	Test BLEU: 24.28
0: Performance: Epoch: 3	Training: 1934749 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593142171238, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593142171238, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 03:29:38 AM
RESULT,RNN_TRANSLATOR,,506,nvidia,2020-06-26 03:21:12 AM
