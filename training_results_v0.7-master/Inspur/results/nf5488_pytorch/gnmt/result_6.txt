Beginning trial 6 of 10
:::MLLOG {"namespace": "", "time_ms": 1593143705598, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593143705644, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593143705644, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593143705644, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593143705644, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593143708068, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 03:55:08 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593143710652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143710652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143710652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143710652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143710658, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143710666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143710672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143710693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3051009535
:::MLLOG {"namespace": "", "time_ms": 1593143719388, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3051009535, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3475429522
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593143733158, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593143733160, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593143733161, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593143733161, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593143733161, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593143735121, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593143735121, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593143735121, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593143735476, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593143735477, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593143735477, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593143735478, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593143735478, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593143735479, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593143735479, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593143735479, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593143735479, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593143735479, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593143735479, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143735479, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 92340346
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.381 (0.381)	Data 2.66e-01 (2.66e-01)	Tok/s 65734 (65734)	Loss/tok 10.7240 (10.7240)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.134 (0.130)	Data 1.14e-04 (3.62e-02)	Tok/s 261313 (192807)	Loss/tok 9.6701 (10.0313)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.099 (0.108)	Data 1.12e-04 (1.90e-02)	Tok/s 255561 (216460)	Loss/tok 9.2018 (9.7015)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.102 (0.103)	Data 1.14e-04 (1.47e-02)	Tok/s 243423 (220581)	Loss/tok 8.8487 (9.4618)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.108 (0.096)	Data 3.84e-03 (1.15e-02)	Tok/s 234169 (221053)	Loss/tok 8.5793 (9.2932)	LR 7.390e-05
0: TRAIN [0][50/1291]	Time 0.039 (0.095)	Data 1.26e-04 (9.27e-03)	Tok/s 208070 (225380)	Loss/tok 7.9679 (9.1191)	LR 9.303e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][60/1291]	Time 0.135 (0.097)	Data 1.18e-04 (7.77e-03)	Tok/s 261079 (229384)	Loss/tok 8.2216 (8.9525)	LR 1.145e-04
0: TRAIN [0][70/1291]	Time 0.099 (0.096)	Data 1.25e-04 (6.69e-03)	Tok/s 254862 (231703)	Loss/tok 8.0440 (8.8187)	LR 1.441e-04
0: TRAIN [0][80/1291]	Time 0.099 (0.095)	Data 1.36e-04 (5.88e-03)	Tok/s 255101 (233112)	Loss/tok 7.9677 (8.7175)	LR 1.814e-04
0: TRAIN [0][90/1291]	Time 0.066 (0.093)	Data 1.16e-04 (5.25e-03)	Tok/s 234362 (233964)	Loss/tok 7.7351 (8.6352)	LR 2.284e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][100/1291]	Time 0.037 (0.092)	Data 1.14e-04 (4.74e-03)	Tok/s 211032 (234514)	Loss/tok 7.2138 (8.5669)	LR 2.810e-04
0: TRAIN [0][110/1291]	Time 0.066 (0.091)	Data 1.09e-04 (4.32e-03)	Tok/s 238023 (235099)	Loss/tok 7.6969 (8.5065)	LR 3.537e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][120/1291]	Time 0.035 (0.090)	Data 1.11e-04 (3.97e-03)	Tok/s 221106 (235425)	Loss/tok 7.0407 (8.4589)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.066 (0.090)	Data 1.10e-04 (3.68e-03)	Tok/s 232499 (236119)	Loss/tok 7.5914 (8.4064)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.098 (0.090)	Data 1.25e-04 (3.43e-03)	Tok/s 256787 (236578)	Loss/tok 7.7202 (8.3555)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.066 (0.090)	Data 1.11e-04 (3.21e-03)	Tok/s 234586 (237135)	Loss/tok 7.3489 (8.3019)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.02e-03)	Tok/s 236020 (237400)	Loss/tok 7.3171 (8.2528)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.172 (0.091)	Data 1.15e-04 (2.85e-03)	Tok/s 260198 (238083)	Loss/tok 7.5039 (8.1933)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.068 (0.091)	Data 1.27e-04 (2.70e-03)	Tok/s 227620 (238382)	Loss/tok 6.9646 (8.1334)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.067 (0.091)	Data 1.08e-04 (2.56e-03)	Tok/s 235832 (238583)	Loss/tok 6.8669 (8.0773)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.067 (0.090)	Data 1.12e-04 (2.44e-03)	Tok/s 226461 (238569)	Loss/tok 6.7646 (8.0271)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.066 (0.090)	Data 1.10e-04 (2.33e-03)	Tok/s 233023 (238718)	Loss/tok 6.5028 (7.9729)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.23e-03)	Tok/s 232276 (238632)	Loss/tok 6.3446 (7.9220)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.14e-03)	Tok/s 236339 (238735)	Loss/tok 6.2617 (7.8651)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][240/1291]	Time 0.036 (0.088)	Data 1.11e-04 (2.05e-03)	Tok/s 215325 (238699)	Loss/tok 5.0910 (7.8051)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.036 (0.089)	Data 1.11e-04 (1.98e-03)	Tok/s 222302 (238939)	Loss/tok 5.1241 (7.7375)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.035 (0.087)	Data 1.17e-04 (1.90e-03)	Tok/s 225215 (238594)	Loss/tok 4.8160 (7.6899)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.135 (0.087)	Data 1.11e-04 (1.84e-03)	Tok/s 257995 (238736)	Loss/tok 6.1850 (7.6242)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.067 (0.088)	Data 1.08e-04 (1.78e-03)	Tok/s 233380 (238989)	Loss/tok 5.6014 (7.5520)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.135 (0.088)	Data 1.21e-04 (1.72e-03)	Tok/s 255461 (239107)	Loss/tok 5.9641 (7.4910)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.100 (0.088)	Data 1.11e-04 (1.67e-03)	Tok/s 254545 (239361)	Loss/tok 5.5317 (7.4229)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.069 (0.088)	Data 1.09e-04 (1.62e-03)	Tok/s 227811 (239543)	Loss/tok 5.1646 (7.3525)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.099 (0.089)	Data 1.11e-04 (1.57e-03)	Tok/s 254314 (239722)	Loss/tok 5.3333 (7.2881)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.099 (0.089)	Data 1.09e-04 (1.53e-03)	Tok/s 252711 (239971)	Loss/tok 5.3086 (7.2191)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.135 (0.089)	Data 1.24e-04 (1.48e-03)	Tok/s 261734 (240156)	Loss/tok 5.3739 (7.1532)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.066 (0.089)	Data 1.11e-04 (1.45e-03)	Tok/s 233514 (240012)	Loss/tok 4.7718 (7.1004)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.134 (0.089)	Data 1.21e-04 (1.41e-03)	Tok/s 263333 (240103)	Loss/tok 5.1007 (7.0415)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][370/1291]	Time 0.100 (0.088)	Data 1.11e-04 (1.37e-03)	Tok/s 253503 (240094)	Loss/tok 4.8421 (6.9849)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.067 (0.088)	Data 1.10e-04 (1.34e-03)	Tok/s 230007 (240019)	Loss/tok 4.3643 (6.9307)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.036 (0.088)	Data 1.11e-04 (1.31e-03)	Tok/s 222011 (240080)	Loss/tok 3.7151 (6.8731)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.099 (0.088)	Data 1.18e-04 (1.28e-03)	Tok/s 252143 (240005)	Loss/tok 4.6732 (6.8160)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.099 (0.088)	Data 1.12e-04 (1.25e-03)	Tok/s 254001 (240074)	Loss/tok 4.7057 (6.7577)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.134 (0.088)	Data 1.11e-04 (1.22e-03)	Tok/s 262708 (240129)	Loss/tok 4.7655 (6.7009)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.174 (0.089)	Data 1.11e-04 (1.20e-03)	Tok/s 257189 (240313)	Loss/tok 4.7895 (6.6367)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.176 (0.089)	Data 1.08e-04 (1.17e-03)	Tok/s 254094 (240282)	Loss/tok 4.8884 (6.5854)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.066 (0.089)	Data 1.16e-04 (1.15e-03)	Tok/s 233113 (240316)	Loss/tok 4.0965 (6.5356)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.100 (0.089)	Data 1.12e-04 (1.13e-03)	Tok/s 250478 (240395)	Loss/tok 4.3399 (6.4853)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.067 (0.089)	Data 1.14e-04 (1.11e-03)	Tok/s 231029 (240366)	Loss/tok 4.0232 (6.4404)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.135 (0.089)	Data 1.17e-04 (1.09e-03)	Tok/s 257693 (240402)	Loss/tok 4.5048 (6.3960)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.099 (0.088)	Data 1.11e-04 (1.07e-03)	Tok/s 251752 (240368)	Loss/tok 4.2255 (6.3553)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][500/1291]	Time 0.100 (0.088)	Data 1.09e-04 (1.05e-03)	Tok/s 253780 (240397)	Loss/tok 4.1993 (6.3127)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.067 (0.088)	Data 1.10e-04 (1.03e-03)	Tok/s 239060 (240431)	Loss/tok 3.8777 (6.2690)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.067 (0.088)	Data 1.09e-04 (1.01e-03)	Tok/s 232564 (240318)	Loss/tok 3.8844 (6.2361)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.067 (0.088)	Data 1.31e-04 (9.94e-04)	Tok/s 232524 (240276)	Loss/tok 3.8641 (6.2003)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.068 (0.088)	Data 1.12e-04 (9.78e-04)	Tok/s 224023 (240222)	Loss/tok 3.9078 (6.1648)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.067 (0.088)	Data 1.10e-04 (9.62e-04)	Tok/s 231530 (240192)	Loss/tok 3.8820 (6.1289)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.036 (0.087)	Data 1.10e-04 (9.47e-04)	Tok/s 220689 (240185)	Loss/tok 3.2327 (6.0936)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.067 (0.087)	Data 1.11e-04 (9.33e-04)	Tok/s 230587 (240147)	Loss/tok 3.8616 (6.0591)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.067 (0.088)	Data 1.11e-04 (9.18e-04)	Tok/s 232321 (240182)	Loss/tok 3.7566 (6.0228)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.067 (0.088)	Data 1.08e-04 (9.05e-04)	Tok/s 229516 (240256)	Loss/tok 3.7209 (5.9870)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.067 (0.088)	Data 1.33e-04 (8.92e-04)	Tok/s 230684 (240321)	Loss/tok 3.7249 (5.9510)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.100 (0.088)	Data 1.10e-04 (8.79e-04)	Tok/s 251617 (240343)	Loss/tok 4.0738 (5.9215)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.100 (0.088)	Data 1.10e-04 (8.67e-04)	Tok/s 253224 (240434)	Loss/tok 3.9604 (5.8848)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][630/1291]	Time 0.135 (0.088)	Data 1.10e-04 (8.55e-04)	Tok/s 261103 (240446)	Loss/tok 4.1294 (5.8534)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.067 (0.088)	Data 1.10e-04 (8.43e-04)	Tok/s 228300 (240392)	Loss/tok 3.6932 (5.8254)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][650/1291]	Time 0.174 (0.088)	Data 1.13e-04 (8.32e-04)	Tok/s 255155 (240409)	Loss/tok 4.3756 (5.7955)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.101 (0.088)	Data 1.16e-04 (8.21e-04)	Tok/s 248099 (240411)	Loss/tok 4.0490 (5.7685)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.066 (0.088)	Data 1.12e-04 (8.10e-04)	Tok/s 235958 (240414)	Loss/tok 3.7117 (5.7418)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.067 (0.088)	Data 1.09e-04 (8.00e-04)	Tok/s 230622 (240360)	Loss/tok 3.6037 (5.7163)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.067 (0.088)	Data 1.07e-04 (7.90e-04)	Tok/s 233657 (240312)	Loss/tok 3.5231 (5.6916)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.067 (0.088)	Data 1.14e-04 (7.81e-04)	Tok/s 226262 (240358)	Loss/tok 3.5678 (5.6641)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.099 (0.088)	Data 1.09e-04 (7.71e-04)	Tok/s 257831 (240381)	Loss/tok 3.9075 (5.6389)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.099 (0.088)	Data 1.10e-04 (7.62e-04)	Tok/s 251111 (240436)	Loss/tok 3.8438 (5.6123)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.099 (0.088)	Data 1.11e-04 (7.53e-04)	Tok/s 252392 (240450)	Loss/tok 3.8674 (5.5871)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.067 (0.088)	Data 1.09e-04 (7.45e-04)	Tok/s 232757 (240484)	Loss/tok 3.5733 (5.5629)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.036 (0.088)	Data 1.34e-04 (7.36e-04)	Tok/s 220439 (240450)	Loss/tok 3.0475 (5.5405)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.067 (0.088)	Data 1.11e-04 (7.28e-04)	Tok/s 230273 (240501)	Loss/tok 3.5657 (5.5158)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.067 (0.088)	Data 1.11e-04 (7.20e-04)	Tok/s 232849 (240570)	Loss/tok 3.4573 (5.4912)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][780/1291]	Time 0.067 (0.088)	Data 1.19e-04 (7.12e-04)	Tok/s 227290 (240570)	Loss/tok 3.6069 (5.4693)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.100 (0.088)	Data 1.11e-04 (7.05e-04)	Tok/s 252477 (240538)	Loss/tok 3.8398 (5.4496)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.067 (0.088)	Data 1.10e-04 (6.97e-04)	Tok/s 231060 (240540)	Loss/tok 3.5842 (5.4282)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.100 (0.088)	Data 1.12e-04 (6.90e-04)	Tok/s 254283 (240497)	Loss/tok 3.7290 (5.4086)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.067 (0.088)	Data 1.07e-04 (6.83e-04)	Tok/s 225882 (240475)	Loss/tok 3.4831 (5.3888)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.100 (0.088)	Data 1.10e-04 (6.76e-04)	Tok/s 253491 (240494)	Loss/tok 3.7932 (5.3688)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.067 (0.088)	Data 1.07e-04 (6.69e-04)	Tok/s 231147 (240429)	Loss/tok 3.5453 (5.3519)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.099 (0.088)	Data 1.12e-04 (6.63e-04)	Tok/s 258833 (240473)	Loss/tok 3.7383 (5.3319)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.134 (0.088)	Data 1.10e-04 (6.57e-04)	Tok/s 260263 (240498)	Loss/tok 3.9870 (5.3125)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.067 (0.088)	Data 1.25e-04 (6.50e-04)	Tok/s 227106 (240560)	Loss/tok 3.5731 (5.2918)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.134 (0.088)	Data 1.09e-04 (6.44e-04)	Tok/s 259466 (240554)	Loss/tok 3.9746 (5.2744)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.066 (0.088)	Data 1.09e-04 (6.38e-04)	Tok/s 234963 (240478)	Loss/tok 3.5119 (5.2597)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.100 (0.088)	Data 1.11e-04 (6.32e-04)	Tok/s 251718 (240493)	Loss/tok 3.6902 (5.2423)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][910/1291]	Time 0.036 (0.088)	Data 1.11e-04 (6.27e-04)	Tok/s 223361 (240501)	Loss/tok 2.9211 (5.2249)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.067 (0.088)	Data 1.12e-04 (6.21e-04)	Tok/s 229849 (240494)	Loss/tok 3.5371 (5.2088)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.135 (0.088)	Data 1.10e-04 (6.16e-04)	Tok/s 258529 (240534)	Loss/tok 3.9570 (5.1907)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][940/1291]	Time 0.068 (0.088)	Data 1.13e-04 (6.10e-04)	Tok/s 227475 (240588)	Loss/tok 3.4278 (5.1725)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.099 (0.088)	Data 1.11e-04 (6.05e-04)	Tok/s 252620 (240643)	Loss/tok 3.7464 (5.1558)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.175 (0.089)	Data 1.33e-04 (6.00e-04)	Tok/s 256618 (240775)	Loss/tok 3.9811 (5.1361)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.067 (0.089)	Data 1.12e-04 (5.95e-04)	Tok/s 233926 (240757)	Loss/tok 3.4562 (5.1213)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.100 (0.088)	Data 1.09e-04 (5.90e-04)	Tok/s 254912 (240692)	Loss/tok 3.6872 (5.1089)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.100 (0.088)	Data 1.12e-04 (5.85e-04)	Tok/s 251219 (240735)	Loss/tok 3.5690 (5.0937)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.069 (0.088)	Data 1.24e-04 (5.80e-04)	Tok/s 223910 (240724)	Loss/tok 3.4044 (5.0793)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.175 (0.088)	Data 1.10e-04 (5.76e-04)	Tok/s 252469 (240717)	Loss/tok 4.1415 (5.0647)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.068 (0.088)	Data 1.13e-04 (5.71e-04)	Tok/s 229727 (240728)	Loss/tok 3.3647 (5.0509)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.099 (0.088)	Data 1.11e-04 (5.67e-04)	Tok/s 252277 (240756)	Loss/tok 3.5910 (5.0371)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.067 (0.088)	Data 1.10e-04 (5.62e-04)	Tok/s 230573 (240782)	Loss/tok 3.3992 (5.0232)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.58e-04)	Tok/s 254464 (240830)	Loss/tok 3.6365 (5.0081)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1060/1291]	Time 0.136 (0.089)	Data 1.05e-04 (5.54e-04)	Tok/s 256667 (240860)	Loss/tok 3.8451 (4.9943)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.136 (0.089)	Data 1.12e-04 (5.50e-04)	Tok/s 255148 (240856)	Loss/tok 3.9099 (4.9814)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1080/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.46e-04)	Tok/s 252434 (240889)	Loss/tok 3.7336 (4.9678)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.134 (0.089)	Data 1.08e-04 (5.42e-04)	Tok/s 259180 (240931)	Loss/tok 3.8317 (4.9556)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.135 (0.089)	Data 1.33e-04 (5.38e-04)	Tok/s 256902 (240960)	Loss/tok 3.8324 (4.9429)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.175 (0.089)	Data 1.09e-04 (5.34e-04)	Tok/s 257064 (240994)	Loss/tok 4.0397 (4.9303)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.175 (0.089)	Data 1.12e-04 (5.30e-04)	Tok/s 255044 (240994)	Loss/tok 4.0146 (4.9190)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.036 (0.089)	Data 1.11e-04 (5.27e-04)	Tok/s 223736 (241011)	Loss/tok 2.8650 (4.9077)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.067 (0.089)	Data 1.08e-04 (5.23e-04)	Tok/s 231280 (240972)	Loss/tok 3.5394 (4.8977)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.067 (0.089)	Data 1.08e-04 (5.20e-04)	Tok/s 235253 (240974)	Loss/tok 3.4116 (4.8865)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.037 (0.089)	Data 1.10e-04 (5.16e-04)	Tok/s 214513 (240986)	Loss/tok 2.7623 (4.8754)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.067 (0.089)	Data 1.09e-04 (5.13e-04)	Tok/s 227544 (240996)	Loss/tok 3.3638 (4.8646)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.101 (0.089)	Data 1.10e-04 (5.09e-04)	Tok/s 250914 (241057)	Loss/tok 3.6179 (4.8521)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.067 (0.089)	Data 1.12e-04 (5.06e-04)	Tok/s 231078 (241029)	Loss/tok 3.4084 (4.8421)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1200/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.03e-04)	Tok/s 230943 (241054)	Loss/tok 3.3816 (4.8313)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.99e-04)	Tok/s 230131 (241082)	Loss/tok 3.3764 (4.8205)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.134 (0.089)	Data 1.32e-04 (4.96e-04)	Tok/s 261988 (241149)	Loss/tok 3.9530 (4.8090)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.93e-04)	Tok/s 230117 (241119)	Loss/tok 3.3806 (4.8000)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.067 (0.089)	Data 1.12e-04 (4.90e-04)	Tok/s 228853 (241152)	Loss/tok 3.3989 (4.7893)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.135 (0.089)	Data 1.12e-04 (4.87e-04)	Tok/s 259922 (241108)	Loss/tok 3.7838 (4.7807)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.134 (0.089)	Data 1.10e-04 (4.84e-04)	Tok/s 261511 (241184)	Loss/tok 3.7150 (4.7692)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.81e-04)	Tok/s 232671 (241176)	Loss/tok 3.2918 (4.7597)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.78e-04)	Tok/s 229348 (241175)	Loss/tok 3.3786 (4.7505)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.099 (0.089)	Data 4.46e-05 (4.78e-04)	Tok/s 255038 (241221)	Loss/tok 3.5868 (4.7403)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593143851548, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143851548, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.497 (0.497)	Decoder iters 149.0 (149.0)	Tok/s 33482 (33482)
0: Running moses detokenizer
0: BLEU(score=19.541032877603826, counts=[34360, 15665, 8345, 4628], totals=[66042, 63039, 60036, 57039], precisions=[52.027497653008695, 24.849696219800443, 13.899993337330935, 8.1137467346903], bp=1.0, sys_len=66042, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593143853651, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1954, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143853651, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7386	Test BLEU: 19.54
0: Performance: Epoch: 0	Training: 1929789 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593143853652, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143853652, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143853652, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 84234360
0: TRAIN [1][0/1291]	Time 0.292 (0.292)	Data 1.86e-01 (1.86e-01)	Tok/s 86370 (86370)	Loss/tok 3.4799 (3.4799)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.099 (0.108)	Data 1.14e-04 (1.70e-02)	Tok/s 255651 (231811)	Loss/tok 3.4600 (3.4564)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.067 (0.090)	Data 1.17e-04 (8.98e-03)	Tok/s 231776 (233684)	Loss/tok 3.2515 (3.3934)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][30/1291]	Time 0.100 (0.095)	Data 1.24e-04 (6.12e-03)	Tok/s 251668 (237634)	Loss/tok 3.4718 (3.4737)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.175 (0.098)	Data 1.38e-04 (4.66e-03)	Tok/s 256903 (240285)	Loss/tok 3.8996 (3.5211)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.100 (0.097)	Data 1.19e-04 (3.77e-03)	Tok/s 252308 (241547)	Loss/tok 3.5534 (3.5173)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.135 (0.097)	Data 1.37e-04 (3.17e-03)	Tok/s 257099 (242081)	Loss/tok 3.6414 (3.5141)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.099 (0.094)	Data 1.15e-04 (2.74e-03)	Tok/s 254392 (241615)	Loss/tok 3.4662 (3.4981)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.066 (0.097)	Data 1.17e-04 (2.42e-03)	Tok/s 231023 (242525)	Loss/tok 3.3056 (3.5172)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][90/1291]	Time 0.066 (0.098)	Data 1.20e-04 (2.17e-03)	Tok/s 238368 (243095)	Loss/tok 3.2551 (3.5261)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.067 (0.098)	Data 1.15e-04 (1.96e-03)	Tok/s 231377 (243312)	Loss/tok 3.2804 (3.5192)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.136 (0.097)	Data 1.24e-04 (1.80e-03)	Tok/s 255157 (243156)	Loss/tok 3.7283 (3.5171)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.067 (0.094)	Data 1.45e-04 (1.66e-03)	Tok/s 233925 (242231)	Loss/tok 3.2952 (3.5040)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.067 (0.093)	Data 1.20e-04 (1.54e-03)	Tok/s 229984 (241867)	Loss/tok 3.3383 (3.4974)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.067 (0.093)	Data 1.16e-04 (1.44e-03)	Tok/s 229267 (241841)	Loss/tok 3.2292 (3.4983)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.100 (0.092)	Data 1.16e-04 (1.35e-03)	Tok/s 253862 (241879)	Loss/tok 3.4935 (3.4924)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.101 (0.092)	Data 1.15e-04 (1.28e-03)	Tok/s 251669 (242028)	Loss/tok 3.4699 (3.4864)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.136 (0.093)	Data 1.16e-04 (1.21e-03)	Tok/s 254521 (242418)	Loss/tok 3.6029 (3.4928)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.099 (0.093)	Data 1.13e-04 (1.15e-03)	Tok/s 253922 (242542)	Loss/tok 3.4720 (3.4966)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.066 (0.093)	Data 1.19e-04 (1.09e-03)	Tok/s 233522 (242635)	Loss/tok 3.2801 (3.4938)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.136 (0.093)	Data 1.18e-04 (1.05e-03)	Tok/s 258184 (242663)	Loss/tok 3.6067 (3.4934)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.100 (0.093)	Data 1.12e-04 (1.00e-03)	Tok/s 253273 (242743)	Loss/tok 3.4897 (3.4933)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][220/1291]	Time 0.100 (0.092)	Data 1.34e-04 (9.62e-04)	Tok/s 251934 (242570)	Loss/tok 3.4606 (3.4906)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.067 (0.092)	Data 1.15e-04 (9.25e-04)	Tok/s 231726 (242406)	Loss/tok 3.1911 (3.4896)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.067 (0.092)	Data 1.17e-04 (8.92e-04)	Tok/s 230809 (242124)	Loss/tok 3.1536 (3.4866)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.067 (0.091)	Data 1.15e-04 (8.61e-04)	Tok/s 234541 (242048)	Loss/tok 3.2468 (3.4861)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.136 (0.092)	Data 1.17e-04 (8.34e-04)	Tok/s 256776 (242191)	Loss/tok 3.6747 (3.4906)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.036 (0.092)	Data 1.15e-04 (8.08e-04)	Tok/s 219026 (242089)	Loss/tok 2.7572 (3.4910)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.067 (0.091)	Data 1.16e-04 (7.84e-04)	Tok/s 231971 (242061)	Loss/tok 3.2197 (3.4868)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.099 (0.092)	Data 1.17e-04 (7.61e-04)	Tok/s 254552 (242099)	Loss/tok 3.5160 (3.4882)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.136 (0.093)	Data 1.13e-04 (7.39e-04)	Tok/s 256709 (242240)	Loss/tok 3.5888 (3.4986)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.067 (0.092)	Data 1.25e-04 (7.19e-04)	Tok/s 235960 (242073)	Loss/tok 3.2588 (3.4946)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.100 (0.092)	Data 1.16e-04 (7.01e-04)	Tok/s 253460 (242115)	Loss/tok 3.5275 (3.4940)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.068 (0.092)	Data 1.13e-04 (6.83e-04)	Tok/s 230612 (242003)	Loss/tok 3.2210 (3.4904)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.067 (0.092)	Data 1.14e-04 (6.66e-04)	Tok/s 231009 (242083)	Loss/tok 3.3169 (3.4888)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][350/1291]	Time 0.036 (0.092)	Data 1.13e-04 (6.51e-04)	Tok/s 219378 (242121)	Loss/tok 2.7804 (3.4887)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.067 (0.091)	Data 1.14e-04 (6.36e-04)	Tok/s 231028 (242003)	Loss/tok 3.2881 (3.4862)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.175 (0.092)	Data 1.14e-04 (6.22e-04)	Tok/s 253295 (241988)	Loss/tok 3.8112 (3.4881)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.100 (0.092)	Data 1.13e-04 (6.09e-04)	Tok/s 253146 (241902)	Loss/tok 3.4305 (3.4868)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.067 (0.091)	Data 1.15e-04 (5.96e-04)	Tok/s 234949 (241885)	Loss/tok 3.3420 (3.4863)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.067 (0.092)	Data 1.16e-04 (5.84e-04)	Tok/s 228357 (241931)	Loss/tok 3.2118 (3.4871)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.036 (0.091)	Data 1.15e-04 (5.73e-04)	Tok/s 214226 (241860)	Loss/tok 2.7638 (3.4845)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.091)	Data 1.16e-04 (5.62e-04)	Tok/s 231890 (241655)	Loss/tok 3.3262 (3.4818)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.100 (0.090)	Data 1.40e-04 (5.52e-04)	Tok/s 251441 (241466)	Loss/tok 3.4063 (3.4782)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.137 (0.090)	Data 1.21e-04 (5.42e-04)	Tok/s 254074 (241394)	Loss/tok 3.6799 (3.4759)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.101 (0.090)	Data 1.16e-04 (5.32e-04)	Tok/s 252191 (241389)	Loss/tok 3.4294 (3.4744)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.100 (0.090)	Data 1.12e-04 (5.23e-04)	Tok/s 252221 (241471)	Loss/tok 3.4195 (3.4741)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.176 (0.090)	Data 1.19e-04 (5.15e-04)	Tok/s 255822 (241527)	Loss/tok 3.8698 (3.4756)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][480/1291]	Time 0.036 (0.090)	Data 1.13e-04 (5.06e-04)	Tok/s 222320 (241508)	Loss/tok 2.6979 (3.4757)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.99e-04)	Tok/s 253655 (241528)	Loss/tok 3.5397 (3.4754)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.067 (0.090)	Data 1.33e-04 (4.91e-04)	Tok/s 232142 (241488)	Loss/tok 3.2286 (3.4733)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.84e-04)	Tok/s 228117 (241441)	Loss/tok 3.2341 (3.4709)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.068 (0.090)	Data 1.35e-04 (4.77e-04)	Tok/s 232083 (241423)	Loss/tok 3.3232 (3.4691)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.067 (0.090)	Data 1.15e-04 (4.70e-04)	Tok/s 228586 (241388)	Loss/tok 3.1928 (3.4668)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.067 (0.090)	Data 1.15e-04 (4.63e-04)	Tok/s 226986 (241420)	Loss/tok 3.2581 (3.4656)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.57e-04)	Tok/s 236809 (241343)	Loss/tok 3.2349 (3.4633)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.099 (0.089)	Data 1.15e-04 (4.51e-04)	Tok/s 251681 (241417)	Loss/tok 3.3156 (3.4621)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.036 (0.089)	Data 1.14e-04 (4.45e-04)	Tok/s 222159 (241440)	Loss/tok 2.7083 (3.4609)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.099 (0.089)	Data 1.28e-04 (4.39e-04)	Tok/s 254161 (241416)	Loss/tok 3.4072 (3.4594)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.099 (0.089)	Data 1.14e-04 (4.34e-04)	Tok/s 251720 (241474)	Loss/tok 3.4525 (3.4587)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.099 (0.089)	Data 1.13e-04 (4.29e-04)	Tok/s 252281 (241509)	Loss/tok 3.4070 (3.4594)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][610/1291]	Time 0.099 (0.089)	Data 1.34e-04 (4.24e-04)	Tok/s 254879 (241504)	Loss/tok 3.4314 (3.4586)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.067 (0.089)	Data 1.18e-04 (4.19e-04)	Tok/s 233951 (241504)	Loss/tok 3.1090 (3.4573)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.066 (0.089)	Data 1.13e-04 (4.14e-04)	Tok/s 238151 (241538)	Loss/tok 3.2399 (3.4569)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.099 (0.090)	Data 1.33e-04 (4.09e-04)	Tok/s 252729 (241554)	Loss/tok 3.3638 (3.4572)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.067 (0.090)	Data 1.21e-04 (4.05e-04)	Tok/s 231804 (241620)	Loss/tok 3.2718 (3.4592)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.066 (0.090)	Data 1.15e-04 (4.01e-04)	Tok/s 235159 (241624)	Loss/tok 3.1555 (3.4580)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.067 (0.090)	Data 1.17e-04 (3.96e-04)	Tok/s 231197 (241637)	Loss/tok 3.2084 (3.4575)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.066 (0.090)	Data 1.16e-04 (3.92e-04)	Tok/s 234322 (241555)	Loss/tok 3.2694 (3.4563)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.066 (0.090)	Data 1.14e-04 (3.88e-04)	Tok/s 233840 (241563)	Loss/tok 3.2258 (3.4556)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.100 (0.090)	Data 1.15e-04 (3.85e-04)	Tok/s 252994 (241603)	Loss/tok 3.4609 (3.4546)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.099 (0.090)	Data 1.15e-04 (3.81e-04)	Tok/s 254434 (241638)	Loss/tok 3.3452 (3.4534)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.175 (0.090)	Data 1.13e-04 (3.77e-04)	Tok/s 254684 (241620)	Loss/tok 3.7121 (3.4538)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][730/1291]	Time 0.099 (0.090)	Data 1.16e-04 (3.74e-04)	Tok/s 255521 (241670)	Loss/tok 3.3740 (3.4529)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.066 (0.090)	Data 1.15e-04 (3.70e-04)	Tok/s 235216 (241608)	Loss/tok 3.2157 (3.4515)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][750/1291]	Time 0.099 (0.090)	Data 1.32e-04 (3.67e-04)	Tok/s 253353 (241693)	Loss/tok 3.3407 (3.4517)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.64e-04)	Tok/s 231991 (241618)	Loss/tok 3.1400 (3.4504)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.100 (0.090)	Data 1.24e-04 (3.60e-04)	Tok/s 255632 (241709)	Loss/tok 3.3509 (3.4507)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.57e-04)	Tok/s 231697 (241738)	Loss/tok 3.1612 (3.4498)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.099 (0.090)	Data 1.23e-04 (3.54e-04)	Tok/s 256298 (241730)	Loss/tok 3.3694 (3.4491)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.176 (0.090)	Data 1.13e-04 (3.51e-04)	Tok/s 253972 (241704)	Loss/tok 3.7615 (3.4498)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.067 (0.090)	Data 1.23e-04 (3.49e-04)	Tok/s 234838 (241668)	Loss/tok 3.1329 (3.4492)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.136 (0.090)	Data 1.14e-04 (3.46e-04)	Tok/s 262750 (241697)	Loss/tok 3.4996 (3.4490)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.066 (0.090)	Data 1.18e-04 (3.43e-04)	Tok/s 239233 (241741)	Loss/tok 3.2527 (3.4485)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.40e-04)	Tok/s 230624 (241588)	Loss/tok 3.1739 (3.4460)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.38e-04)	Tok/s 233293 (241667)	Loss/tok 3.1216 (3.4471)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.136 (0.090)	Data 1.16e-04 (3.35e-04)	Tok/s 256157 (241694)	Loss/tok 3.6488 (3.4466)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.135 (0.090)	Data 1.14e-04 (3.33e-04)	Tok/s 261710 (241729)	Loss/tok 3.5194 (3.4463)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][880/1291]	Time 0.066 (0.090)	Data 1.12e-04 (3.30e-04)	Tok/s 230836 (241722)	Loss/tok 3.1868 (3.4458)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.28e-04)	Tok/s 230816 (241735)	Loss/tok 3.2083 (3.4449)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.067 (0.090)	Data 1.17e-04 (3.25e-04)	Tok/s 227737 (241708)	Loss/tok 3.2769 (3.4453)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.067 (0.090)	Data 1.38e-04 (3.23e-04)	Tok/s 234917 (241677)	Loss/tok 3.1826 (3.4441)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.21e-04)	Tok/s 231193 (241703)	Loss/tok 3.0981 (3.4442)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.099 (0.090)	Data 1.12e-04 (3.19e-04)	Tok/s 252322 (241732)	Loss/tok 3.4442 (3.4439)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.037 (0.090)	Data 1.14e-04 (3.17e-04)	Tok/s 216843 (241692)	Loss/tok 2.7786 (3.4424)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.15e-04)	Tok/s 235163 (241657)	Loss/tok 3.1560 (3.4408)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.13e-04)	Tok/s 220247 (241625)	Loss/tok 2.6945 (3.4410)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.099 (0.090)	Data 1.15e-04 (3.11e-04)	Tok/s 258002 (241693)	Loss/tok 3.4625 (3.4414)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.099 (0.090)	Data 1.14e-04 (3.09e-04)	Tok/s 251875 (241697)	Loss/tok 3.3891 (3.4405)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.07e-04)	Tok/s 231865 (241684)	Loss/tok 3.1047 (3.4395)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.100 (0.090)	Data 1.15e-04 (3.05e-04)	Tok/s 249931 (241734)	Loss/tok 3.4245 (3.4387)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1010/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.03e-04)	Tok/s 233207 (241700)	Loss/tok 3.1612 (3.4374)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.01e-04)	Tok/s 249947 (241705)	Loss/tok 3.3955 (3.4370)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.067 (0.090)	Data 1.14e-04 (2.99e-04)	Tok/s 227022 (241772)	Loss/tok 3.1775 (3.4364)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.98e-04)	Tok/s 254225 (241782)	Loss/tok 3.3237 (3.4354)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.066 (0.090)	Data 1.15e-04 (2.96e-04)	Tok/s 230356 (241830)	Loss/tok 3.1686 (3.4354)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.100 (0.090)	Data 1.13e-04 (2.94e-04)	Tok/s 252620 (241866)	Loss/tok 3.4082 (3.4349)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1070/1291]	Time 0.100 (0.090)	Data 1.15e-04 (2.92e-04)	Tok/s 251663 (241922)	Loss/tok 3.4537 (3.4350)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.136 (0.090)	Data 1.25e-04 (2.91e-04)	Tok/s 253485 (241895)	Loss/tok 3.7288 (3.4345)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.136 (0.090)	Data 1.16e-04 (2.89e-04)	Tok/s 258231 (241878)	Loss/tok 3.5451 (3.4332)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.068 (0.090)	Data 1.50e-04 (2.88e-04)	Tok/s 227950 (241869)	Loss/tok 3.2793 (3.4324)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.135 (0.090)	Data 1.40e-04 (2.86e-04)	Tok/s 259653 (241864)	Loss/tok 3.5027 (3.4315)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.85e-04)	Tok/s 230375 (241831)	Loss/tok 3.1365 (3.4298)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.135 (0.090)	Data 1.16e-04 (2.83e-04)	Tok/s 261718 (241895)	Loss/tok 3.5830 (3.4302)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.099 (0.090)	Data 1.16e-04 (2.82e-04)	Tok/s 251694 (241889)	Loss/tok 3.3237 (3.4291)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.80e-04)	Tok/s 247913 (241848)	Loss/tok 3.4275 (3.4280)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.036 (0.089)	Data 1.16e-04 (2.79e-04)	Tok/s 217075 (241819)	Loss/tok 2.7652 (3.4269)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.78e-04)	Tok/s 252793 (241845)	Loss/tok 3.4647 (3.4261)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.068 (0.089)	Data 1.16e-04 (2.76e-04)	Tok/s 232914 (241888)	Loss/tok 3.1437 (3.4254)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1190/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.75e-04)	Tok/s 230927 (241851)	Loss/tok 3.1820 (3.4242)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1200/1291]	Time 0.067 (0.089)	Data 1.40e-04 (2.74e-04)	Tok/s 230539 (241834)	Loss/tok 3.2457 (3.4236)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.134 (0.089)	Data 1.15e-04 (2.72e-04)	Tok/s 263260 (241815)	Loss/tok 3.6338 (3.4234)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.100 (0.089)	Data 1.16e-04 (2.71e-04)	Tok/s 253267 (241825)	Loss/tok 3.3903 (3.4228)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.70e-04)	Tok/s 230175 (241856)	Loss/tok 3.1383 (3.4227)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.101 (0.089)	Data 1.15e-04 (2.69e-04)	Tok/s 254604 (241860)	Loss/tok 3.3511 (3.4216)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.036 (0.089)	Data 1.20e-04 (2.68e-04)	Tok/s 219583 (241894)	Loss/tok 2.7890 (3.4214)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.136 (0.089)	Data 1.32e-04 (2.66e-04)	Tok/s 259284 (241927)	Loss/tok 3.4916 (3.4214)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.136 (0.089)	Data 1.39e-04 (2.65e-04)	Tok/s 261047 (241914)	Loss/tok 3.4640 (3.4204)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.036 (0.089)	Data 1.37e-04 (2.64e-04)	Tok/s 220734 (241860)	Loss/tok 2.8553 (3.4195)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.036 (0.089)	Data 5.01e-05 (2.66e-04)	Tok/s 224491 (241830)	Loss/tok 2.7251 (3.4189)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593143969440, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593143969440, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.410 (0.410)	Decoder iters 113.0 (113.0)	Tok/s 38945 (38945)
0: Running moses detokenizer
0: BLEU(score=21.89243136166755, counts=[35239, 16957, 9330, 5349], totals=[63891, 60888, 57885, 54887], precisions=[55.15487314332222, 27.849494153199316, 16.11816532780513, 9.745477071073296], bp=0.9877886209495805, sys_len=63891, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593143971553, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2189, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593143971553, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4220	Test BLEU: 21.89
0: Performance: Epoch: 1	Training: 1934382 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593143971554, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593143971554, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143971554, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 850155311
0: TRAIN [2][0/1291]	Time 0.259 (0.259)	Data 1.91e-01 (1.91e-01)	Tok/s 59776 (59776)	Loss/tok 3.0326 (3.0326)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.099 (0.093)	Data 1.14e-04 (1.74e-02)	Tok/s 251975 (223549)	Loss/tok 3.3371 (3.1511)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.099 (0.093)	Data 1.22e-04 (9.19e-03)	Tok/s 253574 (233912)	Loss/tok 3.3305 (3.2320)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.036 (0.083)	Data 1.17e-04 (6.27e-03)	Tok/s 213551 (232293)	Loss/tok 2.6166 (3.1828)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][40/1291]	Time 0.100 (0.082)	Data 1.15e-04 (4.77e-03)	Tok/s 253306 (233883)	Loss/tok 3.2580 (3.1814)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.067 (0.082)	Data 1.17e-04 (3.86e-03)	Tok/s 228110 (235001)	Loss/tok 3.1137 (3.1852)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.070 (0.082)	Data 1.18e-04 (3.24e-03)	Tok/s 224841 (236043)	Loss/tok 3.0544 (3.1826)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][70/1291]	Time 0.099 (0.087)	Data 1.14e-04 (2.80e-03)	Tok/s 254796 (237961)	Loss/tok 3.2366 (3.2317)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.067 (0.087)	Data 1.13e-04 (2.47e-03)	Tok/s 233933 (238480)	Loss/tok 2.9888 (3.2360)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.067 (0.086)	Data 1.35e-04 (2.21e-03)	Tok/s 230260 (238430)	Loss/tok 3.0871 (3.2394)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.067 (0.086)	Data 1.11e-04 (2.01e-03)	Tok/s 234628 (238707)	Loss/tok 3.0061 (3.2382)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.100 (0.086)	Data 1.16e-04 (1.84e-03)	Tok/s 251833 (238988)	Loss/tok 3.2767 (3.2388)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.100 (0.087)	Data 1.18e-04 (1.69e-03)	Tok/s 253088 (239437)	Loss/tok 3.2441 (3.2523)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][130/1291]	Time 0.036 (0.087)	Data 1.40e-04 (1.57e-03)	Tok/s 223517 (239401)	Loss/tok 2.5985 (3.2619)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.100 (0.087)	Data 1.24e-04 (1.47e-03)	Tok/s 254793 (239638)	Loss/tok 3.1904 (3.2601)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.174 (0.088)	Data 1.14e-04 (1.38e-03)	Tok/s 257873 (240093)	Loss/tok 3.6190 (3.2705)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.036 (0.087)	Data 1.19e-04 (1.30e-03)	Tok/s 219869 (239746)	Loss/tok 2.5870 (3.2639)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.137 (0.087)	Data 1.15e-04 (1.23e-03)	Tok/s 254061 (239817)	Loss/tok 3.5162 (3.2630)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.067 (0.087)	Data 1.14e-04 (1.17e-03)	Tok/s 225760 (239962)	Loss/tok 3.0404 (3.2639)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.100 (0.088)	Data 1.13e-04 (1.12e-03)	Tok/s 251080 (240221)	Loss/tok 3.2397 (3.2686)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.100 (0.088)	Data 1.11e-04 (1.07e-03)	Tok/s 250597 (240183)	Loss/tok 3.2657 (3.2693)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.067 (0.087)	Data 1.13e-04 (1.02e-03)	Tok/s 231394 (240325)	Loss/tok 3.1596 (3.2678)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.099 (0.088)	Data 1.11e-04 (9.81e-04)	Tok/s 255105 (240426)	Loss/tok 3.2414 (3.2711)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.175 (0.088)	Data 1.11e-04 (9.43e-04)	Tok/s 256798 (240593)	Loss/tok 3.5827 (3.2749)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.100 (0.089)	Data 1.33e-04 (9.09e-04)	Tok/s 252598 (240749)	Loss/tok 3.2739 (3.2792)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][250/1291]	Time 0.100 (0.088)	Data 1.13e-04 (8.77e-04)	Tok/s 252349 (240826)	Loss/tok 3.2816 (3.2775)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.067 (0.089)	Data 1.13e-04 (8.48e-04)	Tok/s 224926 (240924)	Loss/tok 3.0532 (3.2784)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.135 (0.089)	Data 1.12e-04 (8.22e-04)	Tok/s 257877 (241029)	Loss/tok 3.5080 (3.2796)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.099 (0.088)	Data 1.16e-04 (7.97e-04)	Tok/s 254441 (240971)	Loss/tok 3.3120 (3.2801)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.036 (0.088)	Data 1.17e-04 (7.73e-04)	Tok/s 223105 (240918)	Loss/tok 2.6769 (3.2804)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.100 (0.088)	Data 1.17e-04 (7.52e-04)	Tok/s 253012 (240892)	Loss/tok 3.3130 (3.2784)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][310/1291]	Time 0.099 (0.089)	Data 1.12e-04 (7.31e-04)	Tok/s 257970 (240992)	Loss/tok 3.1982 (3.2843)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.100 (0.088)	Data 1.14e-04 (7.12e-04)	Tok/s 254388 (240923)	Loss/tok 3.1814 (3.2819)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.099 (0.089)	Data 1.08e-04 (6.94e-04)	Tok/s 256718 (241104)	Loss/tok 3.3217 (3.2832)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.067 (0.089)	Data 1.10e-04 (6.77e-04)	Tok/s 228361 (241176)	Loss/tok 3.0515 (3.2812)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.067 (0.088)	Data 1.33e-04 (6.61e-04)	Tok/s 231804 (241153)	Loss/tok 3.1540 (3.2798)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.174 (0.088)	Data 1.10e-04 (6.46e-04)	Tok/s 255348 (241177)	Loss/tok 3.7180 (3.2808)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.100 (0.089)	Data 1.09e-04 (6.31e-04)	Tok/s 249911 (241262)	Loss/tok 3.3528 (3.2821)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.099 (0.088)	Data 1.15e-04 (6.18e-04)	Tok/s 254991 (241219)	Loss/tok 3.3122 (3.2824)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.067 (0.088)	Data 1.07e-04 (6.05e-04)	Tok/s 229649 (241072)	Loss/tok 3.1304 (3.2803)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.099 (0.088)	Data 1.10e-04 (5.93e-04)	Tok/s 251079 (241155)	Loss/tok 3.2173 (3.2810)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.100 (0.089)	Data 1.14e-04 (5.81e-04)	Tok/s 250792 (241390)	Loss/tok 3.3086 (3.2831)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.100 (0.089)	Data 1.11e-04 (5.70e-04)	Tok/s 252885 (241466)	Loss/tok 3.2541 (3.2837)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.099 (0.089)	Data 1.23e-04 (5.59e-04)	Tok/s 251419 (241517)	Loss/tok 3.3338 (3.2845)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][440/1291]	Time 0.099 (0.089)	Data 1.08e-04 (5.49e-04)	Tok/s 252306 (241646)	Loss/tok 3.2073 (3.2838)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.067 (0.089)	Data 1.08e-04 (5.39e-04)	Tok/s 231505 (241658)	Loss/tok 3.0515 (3.2821)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.100 (0.089)	Data 1.08e-04 (5.30e-04)	Tok/s 255825 (241810)	Loss/tok 3.2535 (3.2838)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.099 (0.089)	Data 1.08e-04 (5.21e-04)	Tok/s 254507 (241807)	Loss/tok 3.2554 (3.2846)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.067 (0.089)	Data 1.09e-04 (5.13e-04)	Tok/s 230933 (241687)	Loss/tok 3.2390 (3.2831)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.100 (0.089)	Data 1.11e-04 (5.05e-04)	Tok/s 253347 (241737)	Loss/tok 3.2536 (3.2839)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.067 (0.089)	Data 1.32e-04 (4.97e-04)	Tok/s 230667 (241659)	Loss/tok 3.0901 (3.2828)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.067 (0.089)	Data 1.08e-04 (4.89e-04)	Tok/s 230057 (241626)	Loss/tok 3.0477 (3.2843)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.82e-04)	Tok/s 235114 (241649)	Loss/tok 3.0645 (3.2850)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.136 (0.089)	Data 1.31e-04 (4.75e-04)	Tok/s 259259 (241811)	Loss/tok 3.4007 (3.2874)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.066 (0.089)	Data 1.08e-04 (4.68e-04)	Tok/s 229366 (241703)	Loss/tok 3.0562 (3.2864)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.036 (0.089)	Data 1.33e-04 (4.62e-04)	Tok/s 222635 (241717)	Loss/tok 2.6798 (3.2868)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.56e-04)	Tok/s 233076 (241670)	Loss/tok 3.0250 (3.2849)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][570/1291]	Time 0.100 (0.089)	Data 1.09e-04 (4.50e-04)	Tok/s 250540 (241784)	Loss/tok 3.2715 (3.2853)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.44e-04)	Tok/s 232946 (241768)	Loss/tok 3.1715 (3.2840)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.100 (0.089)	Data 1.13e-04 (4.38e-04)	Tok/s 254638 (241807)	Loss/tok 3.1809 (3.2839)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][600/1291]	Time 0.100 (0.089)	Data 1.35e-04 (4.33e-04)	Tok/s 255016 (241795)	Loss/tok 3.2771 (3.2834)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.036 (0.089)	Data 1.10e-04 (4.28e-04)	Tok/s 227030 (241694)	Loss/tok 2.6220 (3.2822)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.067 (0.089)	Data 1.35e-04 (4.23e-04)	Tok/s 231012 (241678)	Loss/tok 3.0889 (3.2819)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.099 (0.089)	Data 1.11e-04 (4.18e-04)	Tok/s 252606 (241775)	Loss/tok 3.2823 (3.2821)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.068 (0.089)	Data 1.09e-04 (4.13e-04)	Tok/s 229378 (241872)	Loss/tok 3.0598 (3.2830)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.102 (0.089)	Data 1.11e-04 (4.08e-04)	Tok/s 246559 (241943)	Loss/tok 3.3791 (3.2844)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.04e-04)	Tok/s 231335 (241961)	Loss/tok 3.1068 (3.2832)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.099 (0.089)	Data 1.25e-04 (4.00e-04)	Tok/s 253260 (241818)	Loss/tok 3.2956 (3.2809)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.099 (0.089)	Data 1.08e-04 (3.95e-04)	Tok/s 253984 (241906)	Loss/tok 3.2621 (3.2823)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.91e-04)	Tok/s 231721 (241732)	Loss/tok 3.1108 (3.2802)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.067 (0.089)	Data 1.07e-04 (3.87e-04)	Tok/s 229269 (241764)	Loss/tok 3.1288 (3.2809)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.099 (0.089)	Data 1.22e-04 (3.83e-04)	Tok/s 253969 (241851)	Loss/tok 3.2266 (3.2813)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.80e-04)	Tok/s 216981 (241796)	Loss/tok 2.6237 (3.2811)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][730/1291]	Time 0.101 (0.089)	Data 1.11e-04 (3.76e-04)	Tok/s 252171 (241890)	Loss/tok 3.2251 (3.2840)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.73e-04)	Tok/s 229776 (241875)	Loss/tok 3.0299 (3.2841)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.69e-04)	Tok/s 231785 (241885)	Loss/tok 3.1040 (3.2837)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.100 (0.089)	Data 1.08e-04 (3.66e-04)	Tok/s 253334 (241969)	Loss/tok 3.2260 (3.2841)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.135 (0.089)	Data 1.13e-04 (3.62e-04)	Tok/s 261954 (242027)	Loss/tok 3.3964 (3.2847)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.59e-04)	Tok/s 231322 (242053)	Loss/tok 3.1047 (3.2844)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.56e-04)	Tok/s 252884 (242062)	Loss/tok 3.2629 (3.2841)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.135 (0.089)	Data 1.06e-04 (3.53e-04)	Tok/s 257577 (242062)	Loss/tok 3.4275 (3.2846)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.50e-04)	Tok/s 228728 (241984)	Loss/tok 3.0798 (3.2835)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.47e-04)	Tok/s 229087 (242023)	Loss/tok 3.1503 (3.2834)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.035 (0.089)	Data 1.08e-04 (3.44e-04)	Tok/s 221667 (241972)	Loss/tok 2.6814 (3.2828)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.036 (0.089)	Data 1.08e-04 (3.42e-04)	Tok/s 223229 (241909)	Loss/tok 2.6594 (3.2818)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.100 (0.089)	Data 1.30e-04 (3.39e-04)	Tok/s 250770 (241923)	Loss/tok 3.4225 (3.2812)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][860/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.36e-04)	Tok/s 234185 (241994)	Loss/tok 3.0152 (3.2811)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.34e-04)	Tok/s 229948 (241983)	Loss/tok 3.1274 (3.2808)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][880/1291]	Time 0.174 (0.089)	Data 1.10e-04 (3.31e-04)	Tok/s 257719 (242008)	Loss/tok 3.6345 (3.2830)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.29e-04)	Tok/s 252951 (242083)	Loss/tok 3.2614 (3.2836)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.066 (0.089)	Data 1.18e-04 (3.26e-04)	Tok/s 234634 (242081)	Loss/tok 3.1404 (3.2837)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.036 (0.089)	Data 1.32e-04 (3.24e-04)	Tok/s 218535 (242043)	Loss/tok 2.7104 (3.2822)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.22e-04)	Tok/s 224124 (242080)	Loss/tok 2.6056 (3.2819)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.20e-04)	Tok/s 230893 (242052)	Loss/tok 3.1490 (3.2816)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.17e-04)	Tok/s 231814 (242042)	Loss/tok 3.0403 (3.2806)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.15e-04)	Tok/s 253285 (242023)	Loss/tok 3.2269 (3.2795)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.173 (0.089)	Data 1.11e-04 (3.13e-04)	Tok/s 258127 (242070)	Loss/tok 3.5970 (3.2808)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.036 (0.089)	Data 1.08e-04 (3.11e-04)	Tok/s 222533 (242053)	Loss/tok 2.6625 (3.2804)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.173 (0.089)	Data 1.11e-04 (3.09e-04)	Tok/s 256162 (242109)	Loss/tok 3.5589 (3.2829)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.174 (0.089)	Data 1.08e-04 (3.07e-04)	Tok/s 257680 (242172)	Loss/tok 3.6214 (3.2846)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.134 (0.090)	Data 1.08e-04 (3.05e-04)	Tok/s 262688 (242202)	Loss/tok 3.4650 (3.2848)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1010/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.03e-04)	Tok/s 236341 (242157)	Loss/tok 3.0384 (3.2846)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1020/1291]	Time 0.036 (0.089)	Data 1.10e-04 (3.01e-04)	Tok/s 222234 (242146)	Loss/tok 2.6778 (3.2849)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.00e-04)	Tok/s 234466 (242154)	Loss/tok 3.0419 (3.2850)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.174 (0.089)	Data 1.08e-04 (2.98e-04)	Tok/s 257238 (242145)	Loss/tok 3.6061 (3.2850)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.137 (0.090)	Data 1.13e-04 (2.96e-04)	Tok/s 257603 (242257)	Loss/tok 3.4689 (3.2858)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.066 (0.090)	Data 1.12e-04 (2.94e-04)	Tok/s 231994 (242320)	Loss/tok 3.1122 (3.2875)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.101 (0.090)	Data 1.20e-04 (2.93e-04)	Tok/s 250002 (242327)	Loss/tok 3.3297 (3.2871)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.067 (0.090)	Data 1.12e-04 (2.91e-04)	Tok/s 231857 (242361)	Loss/tok 3.0172 (3.2871)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.067 (0.090)	Data 1.11e-04 (2.90e-04)	Tok/s 232058 (242370)	Loss/tok 3.0097 (3.2870)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.067 (0.090)	Data 1.11e-04 (2.88e-04)	Tok/s 230813 (242343)	Loss/tok 3.0105 (3.2863)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.100 (0.090)	Data 1.33e-04 (2.86e-04)	Tok/s 250319 (242340)	Loss/tok 3.1250 (3.2854)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.067 (0.090)	Data 1.10e-04 (2.85e-04)	Tok/s 228240 (242302)	Loss/tok 3.0425 (3.2853)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.067 (0.090)	Data 1.07e-04 (2.83e-04)	Tok/s 232200 (242275)	Loss/tok 3.0688 (3.2846)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.067 (0.090)	Data 1.19e-04 (2.82e-04)	Tok/s 230762 (242281)	Loss/tok 3.1024 (3.2839)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1150/1291]	Time 0.067 (0.090)	Data 1.11e-04 (2.80e-04)	Tok/s 233871 (242283)	Loss/tok 3.0767 (3.2828)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1160/1291]	Time 0.067 (0.090)	Data 1.09e-04 (2.79e-04)	Tok/s 229131 (242292)	Loss/tok 3.0290 (3.2829)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.099 (0.090)	Data 1.10e-04 (2.78e-04)	Tok/s 253127 (242344)	Loss/tok 3.2348 (3.2826)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.036 (0.089)	Data 1.28e-04 (2.76e-04)	Tok/s 218833 (242298)	Loss/tok 2.6256 (3.2818)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.75e-04)	Tok/s 233133 (242247)	Loss/tok 3.0501 (3.2811)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.73e-04)	Tok/s 233276 (242262)	Loss/tok 3.0965 (3.2817)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.72e-04)	Tok/s 235445 (242246)	Loss/tok 2.9845 (3.2812)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.71e-04)	Tok/s 230852 (242273)	Loss/tok 3.0404 (3.2813)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.135 (0.090)	Data 1.21e-04 (2.70e-04)	Tok/s 261813 (242307)	Loss/tok 3.3834 (3.2817)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.68e-04)	Tok/s 232526 (242263)	Loss/tok 3.0278 (3.2809)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.67e-04)	Tok/s 253742 (242227)	Loss/tok 3.2635 (3.2802)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.66e-04)	Tok/s 232702 (242193)	Loss/tok 3.0971 (3.2795)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.65e-04)	Tok/s 231135 (242155)	Loss/tok 3.1118 (3.2787)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.63e-04)	Tok/s 253339 (242161)	Loss/tok 3.2257 (3.2788)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1290/1291]	Time 0.098 (0.089)	Data 4.01e-05 (2.65e-04)	Tok/s 255466 (242077)	Loss/tok 3.2162 (3.2778)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593144087203, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593144087203, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.455 (0.455)	Decoder iters 126.0 (126.0)	Tok/s 36021 (36021)
0: Running moses detokenizer
0: BLEU(score=23.01460577706494, counts=[36183, 17722, 9928, 5812], totals=[64859, 61856, 58853, 55856], precisions=[55.787169089871874, 28.650413864459388, 16.86914855657316, 10.405327986250358], bp=1.0, sys_len=64859, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593144089173, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23010000000000003, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593144089174, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2788	Test BLEU: 23.01
0: Performance: Epoch: 2	Training: 1936266 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593144089174, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593144089174, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144089174, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3464821755
0: TRAIN [3][0/1291]	Time 0.277 (0.277)	Data 1.98e-01 (1.98e-01)	Tok/s 55930 (55930)	Loss/tok 2.9045 (2.9045)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.066 (0.098)	Data 1.12e-04 (1.81e-02)	Tok/s 233389 (222446)	Loss/tok 2.9490 (3.1057)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.099 (0.095)	Data 1.12e-04 (9.52e-03)	Tok/s 255070 (231962)	Loss/tok 3.2265 (3.1815)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.036 (0.096)	Data 1.18e-04 (6.49e-03)	Tok/s 223724 (236661)	Loss/tok 2.5831 (3.2069)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.134 (0.092)	Data 1.11e-04 (4.93e-03)	Tok/s 262155 (237247)	Loss/tok 3.4309 (3.1933)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.068 (0.092)	Data 1.14e-04 (3.99e-03)	Tok/s 227907 (238226)	Loss/tok 2.9689 (3.1893)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.103 (0.090)	Data 1.11e-04 (3.35e-03)	Tok/s 243895 (238157)	Loss/tok 3.1703 (3.1843)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.036 (0.091)	Data 1.13e-04 (2.90e-03)	Tok/s 222631 (238852)	Loss/tok 2.5837 (3.1938)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.066 (0.088)	Data 1.14e-04 (2.56e-03)	Tok/s 235352 (238072)	Loss/tok 2.9647 (3.1853)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.100 (0.086)	Data 1.11e-04 (2.29e-03)	Tok/s 254009 (238143)	Loss/tok 3.3680 (3.1792)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.066 (0.085)	Data 1.13e-04 (2.07e-03)	Tok/s 234053 (237947)	Loss/tok 3.0332 (3.1710)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.067 (0.085)	Data 1.13e-04 (1.90e-03)	Tok/s 230124 (238283)	Loss/tok 2.9757 (3.1709)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.066 (0.085)	Data 1.12e-04 (1.75e-03)	Tok/s 235761 (238614)	Loss/tok 3.0437 (3.1765)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][130/1291]	Time 0.068 (0.085)	Data 1.10e-04 (1.62e-03)	Tok/s 225485 (238651)	Loss/tok 3.1039 (3.1758)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.066 (0.084)	Data 1.13e-04 (1.52e-03)	Tok/s 229465 (238447)	Loss/tok 2.9459 (3.1699)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.100 (0.085)	Data 1.11e-04 (1.42e-03)	Tok/s 249709 (238858)	Loss/tok 3.2468 (3.1729)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.036 (0.084)	Data 1.13e-04 (1.34e-03)	Tok/s 225519 (238761)	Loss/tok 2.6354 (3.1681)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][170/1291]	Time 0.100 (0.085)	Data 1.19e-04 (1.27e-03)	Tok/s 251529 (238971)	Loss/tok 3.2265 (3.1746)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.100 (0.085)	Data 1.12e-04 (1.21e-03)	Tok/s 253204 (239190)	Loss/tok 3.1057 (3.1752)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.100 (0.085)	Data 1.10e-04 (1.15e-03)	Tok/s 252331 (238981)	Loss/tok 3.1708 (3.1748)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.066 (0.086)	Data 1.11e-04 (1.10e-03)	Tok/s 234282 (239222)	Loss/tok 3.0181 (3.1821)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.134 (0.086)	Data 1.13e-04 (1.05e-03)	Tok/s 260893 (239393)	Loss/tok 3.4328 (3.1818)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.099 (0.086)	Data 1.10e-04 (1.01e-03)	Tok/s 251205 (239755)	Loss/tok 3.2121 (3.1862)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.067 (0.086)	Data 1.09e-04 (9.71e-04)	Tok/s 236810 (239675)	Loss/tok 3.0107 (3.1851)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.134 (0.086)	Data 1.11e-04 (9.35e-04)	Tok/s 262159 (239881)	Loss/tok 3.3645 (3.1856)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.067 (0.086)	Data 1.10e-04 (9.03e-04)	Tok/s 232829 (239762)	Loss/tok 2.9890 (3.1848)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.135 (0.086)	Data 1.09e-04 (8.72e-04)	Tok/s 255093 (239782)	Loss/tok 3.3500 (3.1816)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.100 (0.087)	Data 1.14e-04 (8.44e-04)	Tok/s 253737 (239955)	Loss/tok 3.2347 (3.1844)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.066 (0.087)	Data 1.11e-04 (8.18e-04)	Tok/s 233884 (240070)	Loss/tok 3.0021 (3.1826)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.099 (0.086)	Data 1.11e-04 (7.94e-04)	Tok/s 255207 (240120)	Loss/tok 3.2193 (3.1795)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][300/1291]	Time 0.100 (0.087)	Data 1.33e-04 (7.72e-04)	Tok/s 252674 (240270)	Loss/tok 3.1535 (3.1781)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.173 (0.087)	Data 1.20e-04 (7.51e-04)	Tok/s 256540 (240332)	Loss/tok 3.4496 (3.1803)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.100 (0.087)	Data 1.43e-04 (7.31e-04)	Tok/s 253514 (240275)	Loss/tok 3.2120 (3.1773)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.099 (0.087)	Data 1.11e-04 (7.13e-04)	Tok/s 254970 (240400)	Loss/tok 3.2801 (3.1761)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.035 (0.086)	Data 1.14e-04 (6.95e-04)	Tok/s 224921 (240307)	Loss/tok 2.5711 (3.1738)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.067 (0.086)	Data 1.14e-04 (6.78e-04)	Tok/s 232813 (240309)	Loss/tok 2.9399 (3.1720)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.100 (0.086)	Data 1.13e-04 (6.63e-04)	Tok/s 253949 (240411)	Loss/tok 3.0957 (3.1714)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.099 (0.086)	Data 1.11e-04 (6.48e-04)	Tok/s 258509 (240463)	Loss/tok 3.0256 (3.1706)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.100 (0.087)	Data 1.11e-04 (6.34e-04)	Tok/s 251712 (240640)	Loss/tok 3.3097 (3.1704)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.067 (0.086)	Data 1.11e-04 (6.21e-04)	Tok/s 229467 (240590)	Loss/tok 2.9384 (3.1674)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.099 (0.087)	Data 1.18e-04 (6.08e-04)	Tok/s 251764 (240820)	Loss/tok 3.1844 (3.1708)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.134 (0.087)	Data 1.12e-04 (5.96e-04)	Tok/s 259977 (240922)	Loss/tok 3.2965 (3.1718)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.099 (0.087)	Data 1.12e-04 (5.85e-04)	Tok/s 254247 (240946)	Loss/tok 3.1769 (3.1704)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][430/1291]	Time 0.099 (0.087)	Data 1.10e-04 (5.74e-04)	Tok/s 253491 (241000)	Loss/tok 3.1555 (3.1689)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.100 (0.087)	Data 1.12e-04 (5.64e-04)	Tok/s 250339 (241099)	Loss/tok 3.1107 (3.1692)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.134 (0.087)	Data 1.17e-04 (5.54e-04)	Tok/s 261158 (241147)	Loss/tok 3.2969 (3.1698)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.067 (0.087)	Data 1.12e-04 (5.44e-04)	Tok/s 233411 (241183)	Loss/tok 2.9746 (3.1681)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.135 (0.088)	Data 1.15e-04 (5.35e-04)	Tok/s 257515 (241199)	Loss/tok 3.4097 (3.1689)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.066 (0.087)	Data 1.17e-04 (5.26e-04)	Tok/s 231909 (241130)	Loss/tok 2.9804 (3.1664)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.135 (0.087)	Data 1.10e-04 (5.18e-04)	Tok/s 258487 (241116)	Loss/tok 3.3756 (3.1651)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.135 (0.087)	Data 1.11e-04 (5.10e-04)	Tok/s 257560 (241170)	Loss/tok 3.3152 (3.1649)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.174 (0.088)	Data 1.13e-04 (5.02e-04)	Tok/s 258461 (241321)	Loss/tok 3.4031 (3.1679)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.066 (0.088)	Data 1.12e-04 (4.95e-04)	Tok/s 231229 (241353)	Loss/tok 2.8491 (3.1675)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.099 (0.088)	Data 1.13e-04 (4.88e-04)	Tok/s 250774 (241407)	Loss/tok 3.1806 (3.1659)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.066 (0.087)	Data 1.10e-04 (4.81e-04)	Tok/s 235329 (241378)	Loss/tok 3.0045 (3.1643)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][550/1291]	Time 0.067 (0.087)	Data 1.08e-04 (4.74e-04)	Tok/s 226288 (241248)	Loss/tok 3.0124 (3.1620)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.100 (0.087)	Data 1.09e-04 (4.68e-04)	Tok/s 255971 (241216)	Loss/tok 3.0818 (3.1608)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.036 (0.087)	Data 1.13e-04 (4.62e-04)	Tok/s 225509 (241193)	Loss/tok 2.5883 (3.1591)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.036 (0.087)	Data 1.20e-04 (4.56e-04)	Tok/s 219310 (241279)	Loss/tok 2.5353 (3.1622)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.174 (0.087)	Data 1.28e-04 (4.50e-04)	Tok/s 256952 (241206)	Loss/tok 3.5651 (3.1613)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.100 (0.087)	Data 1.11e-04 (4.45e-04)	Tok/s 250963 (241271)	Loss/tok 3.1909 (3.1611)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.067 (0.087)	Data 1.11e-04 (4.39e-04)	Tok/s 233276 (241307)	Loss/tok 3.0288 (3.1603)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.067 (0.087)	Data 1.11e-04 (4.34e-04)	Tok/s 235216 (241290)	Loss/tok 3.0650 (3.1592)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.067 (0.087)	Data 1.16e-04 (4.29e-04)	Tok/s 236361 (241204)	Loss/tok 2.9751 (3.1579)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.067 (0.087)	Data 1.11e-04 (4.24e-04)	Tok/s 233592 (241232)	Loss/tok 2.9582 (3.1572)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.067 (0.087)	Data 1.11e-04 (4.19e-04)	Tok/s 228306 (241259)	Loss/tok 2.9645 (3.1559)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.099 (0.087)	Data 1.32e-04 (4.15e-04)	Tok/s 256692 (241260)	Loss/tok 3.2175 (3.1561)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.067 (0.087)	Data 1.12e-04 (4.10e-04)	Tok/s 233519 (241314)	Loss/tok 2.9512 (3.1567)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][680/1291]	Time 0.135 (0.087)	Data 1.34e-04 (4.06e-04)	Tok/s 259581 (241369)	Loss/tok 3.2958 (3.1566)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.135 (0.087)	Data 1.32e-04 (4.02e-04)	Tok/s 258381 (241484)	Loss/tok 3.3427 (3.1575)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.100 (0.087)	Data 1.21e-04 (3.98e-04)	Tok/s 253111 (241568)	Loss/tok 3.0598 (3.1569)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.099 (0.088)	Data 1.17e-04 (3.94e-04)	Tok/s 255894 (241711)	Loss/tok 3.1698 (3.1575)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.90e-04)	Tok/s 232764 (241826)	Loss/tok 3.0303 (3.1601)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.100 (0.088)	Data 1.17e-04 (3.86e-04)	Tok/s 247644 (241895)	Loss/tok 3.1148 (3.1602)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.82e-04)	Tok/s 228838 (241918)	Loss/tok 2.9881 (3.1596)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.067 (0.088)	Data 1.15e-04 (3.79e-04)	Tok/s 236042 (241967)	Loss/tok 2.9312 (3.1592)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.067 (0.088)	Data 1.36e-04 (3.75e-04)	Tok/s 232583 (242048)	Loss/tok 2.9703 (3.1591)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.036 (0.088)	Data 1.13e-04 (3.72e-04)	Tok/s 224814 (241986)	Loss/tok 2.4812 (3.1578)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.69e-04)	Tok/s 233868 (242018)	Loss/tok 2.9220 (3.1597)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.099 (0.089)	Data 1.23e-04 (3.65e-04)	Tok/s 255230 (242088)	Loss/tok 3.0552 (3.1603)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.62e-04)	Tok/s 252541 (242043)	Loss/tok 3.1803 (3.1588)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][810/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.59e-04)	Tok/s 230627 (241967)	Loss/tok 2.9748 (3.1576)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.067 (0.088)	Data 1.31e-04 (3.56e-04)	Tok/s 227404 (241965)	Loss/tok 2.9317 (3.1566)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.53e-04)	Tok/s 234274 (241989)	Loss/tok 2.9255 (3.1556)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.50e-04)	Tok/s 233095 (241936)	Loss/tok 2.9557 (3.1548)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.100 (0.088)	Data 1.12e-04 (3.48e-04)	Tok/s 252245 (241942)	Loss/tok 3.1450 (3.1544)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.100 (0.088)	Data 1.11e-04 (3.45e-04)	Tok/s 256446 (242016)	Loss/tok 3.0878 (3.1542)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.100 (0.088)	Data 1.16e-04 (3.42e-04)	Tok/s 253523 (241958)	Loss/tok 3.0023 (3.1528)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.40e-04)	Tok/s 234265 (241926)	Loss/tok 2.9404 (3.1517)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.37e-04)	Tok/s 255182 (242012)	Loss/tok 3.1471 (3.1515)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.100 (0.088)	Data 1.08e-04 (3.35e-04)	Tok/s 254316 (242051)	Loss/tok 3.1005 (3.1519)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.175 (0.088)	Data 1.10e-04 (3.32e-04)	Tok/s 258750 (242114)	Loss/tok 3.3948 (3.1526)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.135 (0.089)	Data 1.20e-04 (3.30e-04)	Tok/s 256507 (242164)	Loss/tok 3.3516 (3.1526)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.28e-04)	Tok/s 254261 (242170)	Loss/tok 3.1259 (3.1536)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][940/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.25e-04)	Tok/s 233768 (242159)	Loss/tok 2.9211 (3.1530)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.23e-04)	Tok/s 249033 (242249)	Loss/tok 3.1114 (3.1539)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.21e-04)	Tok/s 234667 (242210)	Loss/tok 2.9080 (3.1522)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.19e-04)	Tok/s 251966 (242188)	Loss/tok 3.1283 (3.1519)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.135 (0.089)	Data 1.14e-04 (3.17e-04)	Tok/s 262153 (242224)	Loss/tok 3.3457 (3.1526)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.15e-04)	Tok/s 228781 (242219)	Loss/tok 2.9632 (3.1534)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.135 (0.089)	Data 1.24e-04 (3.13e-04)	Tok/s 260112 (242218)	Loss/tok 3.2107 (3.1526)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.11e-04)	Tok/s 234717 (242281)	Loss/tok 2.9284 (3.1533)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.09e-04)	Tok/s 231268 (242295)	Loss/tok 2.8975 (3.1529)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.07e-04)	Tok/s 252406 (242346)	Loss/tok 3.1181 (3.1531)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.05e-04)	Tok/s 253650 (242316)	Loss/tok 3.0633 (3.1520)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.135 (0.089)	Data 1.07e-04 (3.03e-04)	Tok/s 258852 (242369)	Loss/tok 3.1998 (3.1518)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.01e-04)	Tok/s 231657 (242369)	Loss/tok 2.9402 (3.1512)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1070/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.00e-04)	Tok/s 222706 (242313)	Loss/tok 2.4975 (3.1501)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.067 (0.089)	Data 1.23e-04 (2.98e-04)	Tok/s 234786 (242310)	Loss/tok 2.9575 (3.1494)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.96e-04)	Tok/s 232677 (242325)	Loss/tok 2.9736 (3.1498)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.174 (0.089)	Data 1.12e-04 (2.95e-04)	Tok/s 253813 (242298)	Loss/tok 3.4077 (3.1494)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.93e-04)	Tok/s 231529 (242325)	Loss/tok 2.9026 (3.1499)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.036 (0.089)	Data 1.09e-04 (2.92e-04)	Tok/s 219065 (242227)	Loss/tok 2.5841 (3.1486)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.90e-04)	Tok/s 233580 (242199)	Loss/tok 2.9671 (3.1487)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.137 (0.089)	Data 1.10e-04 (2.88e-04)	Tok/s 256668 (242212)	Loss/tok 3.2287 (3.1484)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.174 (0.089)	Data 1.11e-04 (2.87e-04)	Tok/s 259578 (242128)	Loss/tok 3.5129 (3.1478)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.85e-04)	Tok/s 253959 (242141)	Loss/tok 3.1073 (3.1474)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.84e-04)	Tok/s 233885 (242112)	Loss/tok 2.9095 (3.1470)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.83e-04)	Tok/s 234506 (242123)	Loss/tok 2.8540 (3.1475)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1190/1291]	Time 0.134 (0.089)	Data 1.09e-04 (2.81e-04)	Tok/s 259145 (242083)	Loss/tok 3.2632 (3.1468)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.174 (0.089)	Data 1.13e-04 (2.80e-04)	Tok/s 259053 (242139)	Loss/tok 3.3838 (3.1481)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.78e-04)	Tok/s 234819 (242155)	Loss/tok 2.9641 (3.1479)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.77e-04)	Tok/s 231348 (242108)	Loss/tok 2.9629 (3.1470)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.76e-04)	Tok/s 254815 (242102)	Loss/tok 3.2131 (3.1466)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.74e-04)	Tok/s 251807 (242073)	Loss/tok 3.1107 (3.1457)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.035 (0.089)	Data 1.10e-04 (2.73e-04)	Tok/s 222867 (242074)	Loss/tok 2.4503 (3.1452)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.72e-04)	Tok/s 237652 (242086)	Loss/tok 2.9307 (3.1451)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.71e-04)	Tok/s 236014 (242122)	Loss/tok 2.9445 (3.1453)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.70e-04)	Tok/s 233257 (242153)	Loss/tok 3.0149 (3.1450)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.100 (0.089)	Data 4.29e-05 (2.71e-04)	Tok/s 252742 (242149)	Loss/tok 3.1472 (3.1452)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593144204887, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593144204887, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.487 (0.487)	Decoder iters 149.0 (149.0)	Tok/s 33514 (33514)
0: Running moses detokenizer
0: BLEU(score=24.06243264497004, counts=[37181, 18637, 10606, 6280], totals=[65508, 62505, 59503, 56507], precisions=[56.757953227086766, 29.816814654827613, 17.824311379258187, 11.113667333250747], bp=1.0, sys_len=65508, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593144206865, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24059999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593144206866, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1471	Test BLEU: 24.06
0: Performance: Epoch: 3	Training: 1936059 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593144206866, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593144206866, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 04:03:34 AM
RESULT,RNN_TRANSLATOR,,506,nvidia,2020-06-26 03:55:08 AM
