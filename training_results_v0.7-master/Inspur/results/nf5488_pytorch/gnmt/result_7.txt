Beginning trial 7 of 10
:::MLLOG {"namespace": "", "time_ms": 1593144215001, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593144215047, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593144215047, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593144215048, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593144215048, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593144217435, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 04:03:37 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593144220090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144220096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144220104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144220104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144220104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144220104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144220111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593144220116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3991782959
:::MLLOG {"namespace": "", "time_ms": 1593144229487, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3991782959, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 811865851
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593144243194, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593144243196, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593144243196, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593144243196, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593144243196, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593144245167, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593144245168, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593144245168, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593144245515, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593144245516, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593144245516, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593144245517, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593144245517, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593144245517, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593144245518, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593144245518, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593144245518, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593144245518, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593144245518, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144245518, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3243865541
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.379 (0.379)	Data 2.77e-01 (2.77e-01)	Tok/s 66438 (66438)	Loss/tok 10.6284 (10.6284)	LR 2.942e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/1291]	Time 0.100 (0.131)	Data 1.20e-04 (3.65e-02)	Tok/s 248538 (194770)	Loss/tok 9.6397 (9.9777)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.099 (0.113)	Data 1.10e-04 (1.92e-02)	Tok/s 254162 (215653)	Loss/tok 9.1925 (9.6618)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.110 (0.106)	Data 1.12e-02 (1.34e-02)	Tok/s 227953 (223801)	Loss/tok 8.9271 (9.4559)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1291]	Time 0.099 (0.102)	Data 1.21e-04 (1.07e-02)	Tok/s 254682 (227792)	Loss/tok 9.2017 (9.3011)	LR 7.222e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][50/1291]	Time 0.066 (0.096)	Data 1.07e-04 (8.62e-03)	Tok/s 240823 (229960)	Loss/tok 8.3692 (9.2134)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.035 (0.093)	Data 1.25e-04 (7.23e-03)	Tok/s 222608 (230452)	Loss/tok 7.6402 (9.0916)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.170 (0.092)	Data 1.06e-04 (6.23e-03)	Tok/s 262950 (231825)	Loss/tok 8.2772 (8.9659)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.133 (0.091)	Data 1.07e-04 (5.47e-03)	Tok/s 262358 (232977)	Loss/tok 8.2113 (8.8579)	LR 1.773e-04
0: TRAIN [0][90/1291]	Time 0.066 (0.089)	Data 1.06e-04 (4.88e-03)	Tok/s 232087 (233622)	Loss/tok 7.7561 (8.7629)	LR 2.232e-04
0: TRAIN [0][100/1291]	Time 0.098 (0.092)	Data 1.11e-04 (4.41e-03)	Tok/s 256788 (235698)	Loss/tok 7.8954 (8.6606)	LR 2.810e-04
0: TRAIN [0][110/1291]	Time 0.066 (0.094)	Data 1.06e-04 (4.02e-03)	Tok/s 231397 (236734)	Loss/tok 7.7029 (8.5843)	LR 3.537e-04
0: TRAIN [0][120/1291]	Time 0.099 (0.092)	Data 1.12e-04 (3.70e-03)	Tok/s 257315 (237019)	Loss/tok 7.9355 (8.5265)	LR 4.453e-04
0: TRAIN [0][130/1291]	Time 0.066 (0.092)	Data 1.28e-04 (3.42e-03)	Tok/s 228327 (237715)	Loss/tok 7.6208 (8.4716)	LR 5.606e-04
0: TRAIN [0][140/1291]	Time 0.066 (0.092)	Data 1.10e-04 (3.19e-03)	Tok/s 231158 (238148)	Loss/tok 7.5873 (8.4209)	LR 7.057e-04
0: TRAIN [0][150/1291]	Time 0.098 (0.092)	Data 1.05e-04 (2.99e-03)	Tok/s 254338 (238974)	Loss/tok 7.5676 (8.3652)	LR 8.885e-04
0: TRAIN [0][160/1291]	Time 0.133 (0.092)	Data 1.09e-04 (2.81e-03)	Tok/s 263163 (239407)	Loss/tok 7.5539 (8.3067)	LR 1.119e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][170/1291]	Time 0.099 (0.093)	Data 1.06e-04 (2.65e-03)	Tok/s 255562 (240114)	Loss/tok 7.2577 (8.2391)	LR 1.408e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][180/1291]	Time 0.099 (0.093)	Data 1.07e-04 (2.51e-03)	Tok/s 256254 (240236)	Loss/tok 7.1296 (8.1758)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.067 (0.093)	Data 1.08e-04 (2.38e-03)	Tok/s 231505 (240716)	Loss/tok 6.6919 (8.1046)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.134 (0.094)	Data 1.09e-04 (2.27e-03)	Tok/s 261139 (241339)	Loss/tok 6.7572 (8.0243)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.135 (0.095)	Data 1.10e-04 (2.17e-03)	Tok/s 259931 (241538)	Loss/tok 7.1476 (7.9573)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.067 (0.094)	Data 1.33e-04 (2.08e-03)	Tok/s 232758 (241563)	Loss/tok 6.1837 (7.8992)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.066 (0.094)	Data 1.06e-04 (1.99e-03)	Tok/s 232897 (241612)	Loss/tok 6.0836 (7.8345)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.100 (0.093)	Data 1.22e-04 (1.91e-03)	Tok/s 257690 (241287)	Loss/tok 6.1376 (7.7804)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.099 (0.092)	Data 1.08e-04 (1.84e-03)	Tok/s 256301 (241253)	Loss/tok 6.0487 (7.7176)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.133 (0.092)	Data 1.11e-04 (1.77e-03)	Tok/s 261271 (241242)	Loss/tok 6.0328 (7.6549)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.035 (0.092)	Data 1.12e-04 (1.71e-03)	Tok/s 223323 (241393)	Loss/tok 4.6655 (7.5852)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.099 (0.092)	Data 1.12e-04 (1.66e-03)	Tok/s 256673 (241699)	Loss/tok 5.6615 (7.5099)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.100 (0.093)	Data 1.11e-04 (1.60e-03)	Tok/s 252981 (241925)	Loss/tok 5.5170 (7.4350)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.036 (0.093)	Data 1.08e-04 (1.55e-03)	Tok/s 222341 (241793)	Loss/tok 4.4595 (7.3759)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][310/1291]	Time 0.099 (0.093)	Data 1.08e-04 (1.51e-03)	Tok/s 254409 (241895)	Loss/tok 5.3893 (7.3103)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.067 (0.092)	Data 1.08e-04 (1.46e-03)	Tok/s 237401 (241820)	Loss/tok 4.8237 (7.2531)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.099 (0.092)	Data 1.35e-04 (1.42e-03)	Tok/s 253470 (241769)	Loss/tok 5.0653 (7.1945)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.100 (0.092)	Data 1.41e-04 (1.38e-03)	Tok/s 250770 (241984)	Loss/tok 4.9228 (7.1257)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.099 (0.092)	Data 1.11e-04 (1.35e-03)	Tok/s 257388 (242195)	Loss/tok 4.9017 (7.0592)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.036 (0.092)	Data 1.10e-04 (1.31e-03)	Tok/s 218389 (242148)	Loss/tok 3.8562 (7.0028)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.133 (0.092)	Data 1.33e-04 (1.28e-03)	Tok/s 261430 (242203)	Loss/tok 5.0434 (6.9442)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.067 (0.092)	Data 1.34e-04 (1.25e-03)	Tok/s 225444 (242176)	Loss/tok 4.4542 (6.8883)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.134 (0.092)	Data 1.29e-04 (1.22e-03)	Tok/s 260261 (242300)	Loss/tok 4.9660 (6.8234)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.174 (0.092)	Data 1.20e-04 (1.20e-03)	Tok/s 256712 (242200)	Loss/tok 4.9641 (6.7673)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.067 (0.092)	Data 1.12e-04 (1.17e-03)	Tok/s 231103 (242071)	Loss/tok 4.2066 (6.7180)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.067 (0.092)	Data 1.11e-04 (1.14e-03)	Tok/s 229361 (242093)	Loss/tok 4.0838 (6.6647)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.066 (0.092)	Data 1.10e-04 (1.12e-03)	Tok/s 234195 (242169)	Loss/tok 4.1920 (6.6119)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][440/1291]	Time 0.134 (0.092)	Data 1.15e-04 (1.10e-03)	Tok/s 262590 (242187)	Loss/tok 4.6094 (6.5645)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.099 (0.091)	Data 1.18e-04 (1.08e-03)	Tok/s 254926 (242043)	Loss/tok 4.3552 (6.5223)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.100 (0.092)	Data 1.31e-04 (1.06e-03)	Tok/s 254922 (242134)	Loss/tok 4.4444 (6.4709)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.036 (0.091)	Data 1.09e-04 (1.04e-03)	Tok/s 223512 (241933)	Loss/tok 3.2970 (6.4352)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.135 (0.091)	Data 1.30e-04 (1.02e-03)	Tok/s 262663 (241910)	Loss/tok 4.4781 (6.3919)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.100 (0.091)	Data 1.12e-04 (9.98e-04)	Tok/s 251001 (242043)	Loss/tok 4.2032 (6.3424)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.099 (0.091)	Data 1.12e-04 (9.80e-04)	Tok/s 253813 (242196)	Loss/tok 4.2038 (6.2957)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.099 (0.091)	Data 1.11e-04 (9.63e-04)	Tok/s 256427 (242128)	Loss/tok 4.1981 (6.2590)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.066 (0.091)	Data 1.13e-04 (9.47e-04)	Tok/s 234422 (242045)	Loss/tok 3.8264 (6.2221)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.100 (0.091)	Data 1.13e-04 (9.31e-04)	Tok/s 250599 (242143)	Loss/tok 4.1289 (6.1779)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.067 (0.091)	Data 1.09e-04 (9.16e-04)	Tok/s 231543 (241973)	Loss/tok 3.7959 (6.1475)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.066 (0.091)	Data 1.10e-04 (9.02e-04)	Tok/s 230683 (241910)	Loss/tok 3.7606 (6.1089)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.100 (0.090)	Data 1.08e-04 (8.88e-04)	Tok/s 251556 (241823)	Loss/tok 4.0977 (6.0770)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][570/1291]	Time 0.035 (0.090)	Data 1.11e-04 (8.74e-04)	Tok/s 224225 (241817)	Loss/tok 3.2415 (6.0431)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.100 (0.090)	Data 1.09e-04 (8.61e-04)	Tok/s 251828 (241808)	Loss/tok 3.9006 (6.0088)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][590/1291]	Time 0.099 (0.090)	Data 1.07e-04 (8.48e-04)	Tok/s 257725 (241845)	Loss/tok 3.9183 (5.9736)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.134 (0.090)	Data 1.22e-04 (8.36e-04)	Tok/s 260849 (241912)	Loss/tok 4.2035 (5.9392)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.134 (0.090)	Data 1.18e-04 (8.25e-04)	Tok/s 260163 (241925)	Loss/tok 4.1749 (5.9067)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][620/1291]	Time 0.174 (0.091)	Data 1.12e-04 (8.13e-04)	Tok/s 256053 (242032)	Loss/tok 4.4398 (5.8694)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.036 (0.091)	Data 1.11e-04 (8.02e-04)	Tok/s 217012 (242135)	Loss/tok 3.0717 (5.8353)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.067 (0.091)	Data 1.10e-04 (7.91e-04)	Tok/s 231680 (242091)	Loss/tok 3.7470 (5.8093)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.035 (0.091)	Data 1.10e-04 (7.81e-04)	Tok/s 221108 (241994)	Loss/tok 3.1052 (5.7841)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.099 (0.090)	Data 1.11e-04 (7.71e-04)	Tok/s 254955 (241940)	Loss/tok 3.9305 (5.7591)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.066 (0.090)	Data 1.08e-04 (7.61e-04)	Tok/s 233694 (241978)	Loss/tok 3.6295 (5.7303)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.036 (0.090)	Data 1.11e-04 (7.52e-04)	Tok/s 225156 (242032)	Loss/tok 3.0110 (5.7020)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.175 (0.091)	Data 1.09e-04 (7.43e-04)	Tok/s 253880 (242101)	Loss/tok 4.2698 (5.6735)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.035 (0.090)	Data 1.36e-04 (7.34e-04)	Tok/s 225248 (242050)	Loss/tok 3.1149 (5.6511)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.100 (0.090)	Data 1.11e-04 (7.25e-04)	Tok/s 251928 (242099)	Loss/tok 3.7767 (5.6255)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.068 (0.090)	Data 1.09e-04 (7.17e-04)	Tok/s 230616 (242119)	Loss/tok 3.6850 (5.6019)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.067 (0.090)	Data 1.12e-04 (7.08e-04)	Tok/s 231746 (242105)	Loss/tok 3.6226 (5.5795)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][740/1291]	Time 0.067 (0.090)	Data 1.11e-04 (7.00e-04)	Tok/s 232489 (242115)	Loss/tok 3.6201 (5.5560)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.100 (0.090)	Data 1.11e-04 (6.93e-04)	Tok/s 253374 (242153)	Loss/tok 3.8714 (5.5312)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.067 (0.091)	Data 1.18e-04 (6.85e-04)	Tok/s 232530 (242295)	Loss/tok 3.5587 (5.5036)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.175 (0.091)	Data 1.27e-04 (6.78e-04)	Tok/s 252603 (242306)	Loss/tok 4.2733 (5.4812)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.067 (0.091)	Data 1.29e-04 (6.70e-04)	Tok/s 226757 (242257)	Loss/tok 3.5955 (5.4618)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.100 (0.090)	Data 1.12e-04 (6.63e-04)	Tok/s 254134 (242233)	Loss/tok 3.7713 (5.4418)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.067 (0.090)	Data 1.10e-04 (6.56e-04)	Tok/s 233049 (242192)	Loss/tok 3.5980 (5.4228)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.068 (0.090)	Data 1.10e-04 (6.50e-04)	Tok/s 229351 (242147)	Loss/tok 3.5083 (5.4039)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.067 (0.090)	Data 1.23e-04 (6.43e-04)	Tok/s 228316 (242094)	Loss/tok 3.4213 (5.3855)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.175 (0.090)	Data 1.23e-04 (6.37e-04)	Tok/s 254789 (242190)	Loss/tok 4.2214 (5.3624)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.067 (0.090)	Data 1.12e-04 (6.31e-04)	Tok/s 230094 (242150)	Loss/tok 3.5286 (5.3453)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.067 (0.090)	Data 1.13e-04 (6.25e-04)	Tok/s 230347 (242251)	Loss/tok 3.4937 (5.3238)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.099 (0.090)	Data 1.11e-04 (6.19e-04)	Tok/s 251979 (242231)	Loss/tok 3.8171 (5.3065)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][870/1291]	Time 0.067 (0.090)	Data 1.17e-04 (6.13e-04)	Tok/s 229938 (242182)	Loss/tok 3.5803 (5.2903)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.099 (0.090)	Data 1.14e-04 (6.07e-04)	Tok/s 257625 (242224)	Loss/tok 3.7235 (5.2715)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.100 (0.090)	Data 1.07e-04 (6.02e-04)	Tok/s 249336 (242222)	Loss/tok 3.7519 (5.2532)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.067 (0.090)	Data 1.34e-04 (5.96e-04)	Tok/s 230636 (242225)	Loss/tok 3.4612 (5.2366)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.099 (0.090)	Data 1.33e-04 (5.91e-04)	Tok/s 254671 (242195)	Loss/tok 3.7044 (5.2220)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.100 (0.090)	Data 1.13e-04 (5.86e-04)	Tok/s 250129 (242160)	Loss/tok 3.7501 (5.2060)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.135 (0.090)	Data 1.24e-04 (5.81e-04)	Tok/s 259567 (242157)	Loss/tok 3.9088 (5.1907)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.099 (0.090)	Data 1.10e-04 (5.76e-04)	Tok/s 254580 (242178)	Loss/tok 3.7343 (5.1743)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.100 (0.090)	Data 1.11e-04 (5.71e-04)	Tok/s 248637 (242130)	Loss/tok 3.7866 (5.1604)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.134 (0.090)	Data 1.09e-04 (5.66e-04)	Tok/s 261398 (242092)	Loss/tok 3.9576 (5.1459)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.101 (0.090)	Data 1.12e-04 (5.62e-04)	Tok/s 250921 (242062)	Loss/tok 3.6877 (5.1317)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.57e-04)	Tok/s 232561 (242120)	Loss/tok 3.3194 (5.1154)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.067 (0.090)	Data 1.13e-04 (5.52e-04)	Tok/s 232852 (242128)	Loss/tok 3.4379 (5.1006)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1000/1291]	Time 0.136 (0.090)	Data 1.15e-04 (5.48e-04)	Tok/s 258119 (242097)	Loss/tok 3.9069 (5.0875)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.067 (0.089)	Data 1.13e-04 (5.44e-04)	Tok/s 229229 (242073)	Loss/tok 3.5088 (5.0744)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1020/1291]	Time 0.067 (0.089)	Data 1.34e-04 (5.40e-04)	Tok/s 232908 (242043)	Loss/tok 3.4334 (5.0614)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.036 (0.089)	Data 1.13e-04 (5.35e-04)	Tok/s 217960 (241993)	Loss/tok 2.8043 (5.0495)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.134 (0.089)	Data 1.12e-04 (5.31e-04)	Tok/s 264245 (242041)	Loss/tok 3.7665 (5.0345)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.175 (0.089)	Data 1.34e-04 (5.27e-04)	Tok/s 257106 (242016)	Loss/tok 4.1291 (5.0219)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.067 (0.089)	Data 1.14e-04 (5.24e-04)	Tok/s 234434 (242014)	Loss/tok 3.4152 (5.0091)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.099 (0.089)	Data 1.13e-04 (5.20e-04)	Tok/s 255340 (242003)	Loss/tok 3.6362 (4.9965)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.066 (0.089)	Data 1.10e-04 (5.16e-04)	Tok/s 239125 (241908)	Loss/tok 3.3831 (4.9860)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.067 (0.089)	Data 1.12e-04 (5.12e-04)	Tok/s 233251 (241837)	Loss/tok 3.4453 (4.9752)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.135 (0.089)	Data 1.39e-04 (5.09e-04)	Tok/s 257155 (241902)	Loss/tok 3.9433 (4.9617)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.135 (0.089)	Data 1.14e-04 (5.05e-04)	Tok/s 257793 (241974)	Loss/tok 3.8150 (4.9472)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.135 (0.089)	Data 1.12e-04 (5.02e-04)	Tok/s 259448 (241957)	Loss/tok 3.9214 (4.9362)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.068 (0.089)	Data 1.12e-04 (4.98e-04)	Tok/s 234408 (241988)	Loss/tok 3.4296 (4.9241)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1140/1291]	Time 0.066 (0.090)	Data 1.10e-04 (4.95e-04)	Tok/s 230860 (242066)	Loss/tok 3.4806 (4.9104)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1150/1291]	Time 0.175 (0.090)	Data 1.15e-04 (4.92e-04)	Tok/s 256996 (242038)	Loss/tok 3.9178 (4.8991)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.036 (0.090)	Data 1.09e-04 (4.88e-04)	Tok/s 216973 (242007)	Loss/tok 2.8154 (4.8889)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.068 (0.090)	Data 1.13e-04 (4.85e-04)	Tok/s 229221 (241975)	Loss/tok 3.3839 (4.8784)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.82e-04)	Tok/s 228646 (242024)	Loss/tok 3.3950 (4.8665)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.067 (0.090)	Data 1.15e-04 (4.79e-04)	Tok/s 227982 (242021)	Loss/tok 3.3657 (4.8565)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.036 (0.089)	Data 1.31e-04 (4.76e-04)	Tok/s 225934 (242022)	Loss/tok 2.8919 (4.8462)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.73e-04)	Tok/s 235273 (241977)	Loss/tok 3.3759 (4.8369)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.70e-04)	Tok/s 251166 (241983)	Loss/tok 3.5181 (4.8265)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.67e-04)	Tok/s 229630 (241933)	Loss/tok 3.3033 (4.8175)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.068 (0.089)	Data 1.11e-04 (4.64e-04)	Tok/s 227835 (241885)	Loss/tok 3.3809 (4.8088)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.61e-04)	Tok/s 227517 (241861)	Loss/tok 3.3212 (4.7995)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.59e-04)	Tok/s 229791 (241810)	Loss/tok 3.4274 (4.7909)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.036 (0.089)	Data 1.15e-04 (4.56e-04)	Tok/s 220677 (241793)	Loss/tok 2.8405 (4.7820)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1280/1291]	Time 0.134 (0.089)	Data 1.34e-04 (4.53e-04)	Tok/s 258766 (241814)	Loss/tok 3.7994 (4.7720)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1290/1291]	Time 0.134 (0.089)	Data 4.24e-05 (4.54e-04)	Tok/s 263205 (241854)	Loss/tok 3.5701 (4.7615)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593144361264, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144361264, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.497 (0.497)	Decoder iters 149.0 (149.0)	Tok/s 32628 (32628)
0: Running moses detokenizer
0: BLEU(score=19.337767115457392, counts=[34239, 15551, 8269, 4595], totals=[66268, 63265, 60262, 57264], precisions=[51.667471479447094, 24.58073184225085, 13.721748365470777, 8.024238614138028], bp=1.0, sys_len=66268, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593144363530, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1934, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144363531, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7611	Test BLEU: 19.34
0: Performance: Epoch: 0	Training: 1935180 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593144363531, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144363531, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144363531, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 409185704
0: TRAIN [1][0/1291]	Time 0.334 (0.334)	Data 1.83e-01 (1.83e-01)	Tok/s 104789 (104789)	Loss/tok 3.6637 (3.6637)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.069 (0.099)	Data 1.18e-04 (1.67e-02)	Tok/s 229012 (218431)	Loss/tok 3.2444 (3.4247)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.099 (0.102)	Data 1.16e-04 (8.81e-03)	Tok/s 255510 (232049)	Loss/tok 3.5688 (3.5129)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.036 (0.094)	Data 1.18e-04 (6.00e-03)	Tok/s 225355 (234008)	Loss/tok 2.8336 (3.4848)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.067 (0.090)	Data 1.22e-04 (4.57e-03)	Tok/s 232230 (234614)	Loss/tok 3.2229 (3.4627)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.135 (0.089)	Data 1.12e-04 (3.70e-03)	Tok/s 257007 (236007)	Loss/tok 3.6733 (3.4581)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.100 (0.088)	Data 1.47e-04 (3.11e-03)	Tok/s 255430 (237043)	Loss/tok 3.5137 (3.4479)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.067 (0.088)	Data 1.11e-04 (2.69e-03)	Tok/s 232509 (237147)	Loss/tok 3.1736 (3.4515)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.100 (0.087)	Data 1.11e-04 (2.37e-03)	Tok/s 250640 (237926)	Loss/tok 3.4907 (3.4475)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.100 (0.090)	Data 1.13e-04 (2.12e-03)	Tok/s 252251 (239570)	Loss/tok 3.4626 (3.4699)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.067 (0.089)	Data 1.11e-04 (1.92e-03)	Tok/s 229094 (239428)	Loss/tok 3.3025 (3.4643)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.099 (0.090)	Data 1.08e-04 (1.76e-03)	Tok/s 254169 (239940)	Loss/tok 3.4546 (3.4753)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.036 (0.088)	Data 1.15e-04 (1.62e-03)	Tok/s 220345 (239423)	Loss/tok 2.8610 (3.4649)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][130/1291]	Time 0.066 (0.089)	Data 1.13e-04 (1.51e-03)	Tok/s 234580 (239769)	Loss/tok 3.2304 (3.4760)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][140/1291]	Time 0.067 (0.088)	Data 1.16e-04 (1.41e-03)	Tok/s 232498 (239428)	Loss/tok 3.2636 (3.4748)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.066 (0.087)	Data 1.36e-04 (1.32e-03)	Tok/s 233148 (239112)	Loss/tok 3.1774 (3.4772)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.100 (0.087)	Data 1.11e-04 (1.25e-03)	Tok/s 248481 (239368)	Loss/tok 3.4367 (3.4747)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.067 (0.087)	Data 1.12e-04 (1.18e-03)	Tok/s 234718 (239432)	Loss/tok 3.2474 (3.4732)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.067 (0.087)	Data 1.35e-04 (1.12e-03)	Tok/s 232902 (239566)	Loss/tok 3.2812 (3.4722)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.175 (0.087)	Data 1.16e-04 (1.07e-03)	Tok/s 255228 (239527)	Loss/tok 3.8334 (3.4763)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.100 (0.088)	Data 1.12e-04 (1.02e-03)	Tok/s 250692 (239857)	Loss/tok 3.5426 (3.4755)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.134 (0.088)	Data 1.10e-04 (9.80e-04)	Tok/s 260549 (240063)	Loss/tok 3.7187 (3.4742)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.067 (0.087)	Data 1.19e-04 (9.41e-04)	Tok/s 233628 (239945)	Loss/tok 3.2283 (3.4708)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.099 (0.088)	Data 1.42e-04 (9.05e-04)	Tok/s 252416 (240221)	Loss/tok 3.5217 (3.4746)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.036 (0.088)	Data 1.12e-04 (8.72e-04)	Tok/s 219841 (240389)	Loss/tok 2.8106 (3.4772)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.175 (0.088)	Data 1.33e-04 (8.42e-04)	Tok/s 254975 (240446)	Loss/tok 3.8875 (3.4774)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.067 (0.088)	Data 1.31e-04 (8.15e-04)	Tok/s 230291 (240501)	Loss/tok 3.2499 (3.4743)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][270/1291]	Time 0.100 (0.087)	Data 1.17e-04 (7.89e-04)	Tok/s 255863 (240578)	Loss/tok 3.3597 (3.4716)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][280/1291]	Time 0.174 (0.088)	Data 1.15e-04 (7.65e-04)	Tok/s 257880 (240561)	Loss/tok 3.8257 (3.4731)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.067 (0.087)	Data 1.38e-04 (7.42e-04)	Tok/s 233708 (240567)	Loss/tok 3.2447 (3.4710)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.134 (0.088)	Data 1.10e-04 (7.21e-04)	Tok/s 260171 (240745)	Loss/tok 3.7032 (3.4734)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.099 (0.088)	Data 1.08e-04 (7.02e-04)	Tok/s 253136 (240866)	Loss/tok 3.4652 (3.4726)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.066 (0.088)	Data 1.12e-04 (6.83e-04)	Tok/s 236332 (240757)	Loss/tok 3.2023 (3.4689)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.067 (0.088)	Data 1.12e-04 (6.66e-04)	Tok/s 230848 (240861)	Loss/tok 3.2492 (3.4678)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.100 (0.088)	Data 1.12e-04 (6.50e-04)	Tok/s 252128 (241010)	Loss/tok 3.3471 (3.4690)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.099 (0.088)	Data 1.29e-04 (6.35e-04)	Tok/s 254236 (241112)	Loss/tok 3.3423 (3.4692)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.099 (0.088)	Data 1.34e-04 (6.20e-04)	Tok/s 253698 (241338)	Loss/tok 3.4714 (3.4692)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.067 (0.088)	Data 1.13e-04 (6.07e-04)	Tok/s 231614 (241230)	Loss/tok 3.1910 (3.4672)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.099 (0.088)	Data 1.23e-04 (5.94e-04)	Tok/s 253126 (241347)	Loss/tok 3.5234 (3.4689)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.099 (0.088)	Data 1.13e-04 (5.81e-04)	Tok/s 255822 (241263)	Loss/tok 3.4529 (3.4663)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.066 (0.088)	Data 1.14e-04 (5.70e-04)	Tok/s 230219 (241354)	Loss/tok 3.1870 (3.4648)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][410/1291]	Time 0.036 (0.088)	Data 1.11e-04 (5.59e-04)	Tok/s 223063 (241385)	Loss/tok 2.8035 (3.4647)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.088)	Data 1.12e-04 (5.48e-04)	Tok/s 231259 (241404)	Loss/tok 3.2601 (3.4650)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.099 (0.088)	Data 1.35e-04 (5.38e-04)	Tok/s 254181 (241459)	Loss/tok 3.4966 (3.4640)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.100 (0.088)	Data 1.31e-04 (5.28e-04)	Tok/s 254177 (241441)	Loss/tok 3.5428 (3.4638)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.066 (0.088)	Data 1.13e-04 (5.19e-04)	Tok/s 231756 (241420)	Loss/tok 3.1661 (3.4633)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.066 (0.088)	Data 1.16e-04 (5.11e-04)	Tok/s 234024 (241445)	Loss/tok 3.2842 (3.4615)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.036 (0.088)	Data 1.15e-04 (5.02e-04)	Tok/s 218404 (241391)	Loss/tok 2.7485 (3.4594)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.036 (0.088)	Data 1.33e-04 (4.94e-04)	Tok/s 224922 (241503)	Loss/tok 2.7227 (3.4612)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.099 (0.088)	Data 1.14e-04 (4.86e-04)	Tok/s 254383 (241576)	Loss/tok 3.4007 (3.4617)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.067 (0.088)	Data 1.20e-04 (4.79e-04)	Tok/s 236063 (241627)	Loss/tok 3.2344 (3.4616)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][510/1291]	Time 0.099 (0.089)	Data 1.08e-04 (4.72e-04)	Tok/s 254016 (241647)	Loss/tok 3.3570 (3.4613)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.100 (0.089)	Data 1.11e-04 (4.65e-04)	Tok/s 249750 (241760)	Loss/tok 3.4259 (3.4611)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.134 (0.089)	Data 1.10e-04 (4.58e-04)	Tok/s 259863 (241757)	Loss/tok 3.6991 (3.4601)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.099 (0.089)	Data 1.06e-04 (4.52e-04)	Tok/s 255234 (241848)	Loss/tok 3.5310 (3.4606)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.134 (0.089)	Data 1.34e-04 (4.46e-04)	Tok/s 260668 (241895)	Loss/tok 3.7217 (3.4620)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.100 (0.089)	Data 1.11e-04 (4.40e-04)	Tok/s 251539 (241932)	Loss/tok 3.4220 (3.4618)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.174 (0.090)	Data 1.09e-04 (4.34e-04)	Tok/s 256408 (242057)	Loss/tok 3.8661 (3.4653)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.135 (0.090)	Data 1.08e-04 (4.29e-04)	Tok/s 260376 (242117)	Loss/tok 3.6422 (3.4650)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.067 (0.090)	Data 1.34e-04 (4.23e-04)	Tok/s 229682 (242182)	Loss/tok 3.2327 (3.4640)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.18e-04)	Tok/s 254996 (242289)	Loss/tok 3.5509 (3.4657)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.036 (0.090)	Data 1.31e-04 (4.13e-04)	Tok/s 217301 (242200)	Loss/tok 2.8821 (3.4638)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.173 (0.090)	Data 1.11e-04 (4.08e-04)	Tok/s 257789 (242200)	Loss/tok 3.7715 (3.4632)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.135 (0.089)	Data 1.32e-04 (4.04e-04)	Tok/s 258460 (242164)	Loss/tok 3.6434 (3.4613)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][640/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.99e-04)	Tok/s 231162 (242191)	Loss/tok 3.1587 (3.4605)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.95e-04)	Tok/s 235628 (242160)	Loss/tok 3.1338 (3.4590)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.90e-04)	Tok/s 232042 (242098)	Loss/tok 3.2642 (3.4572)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.86e-04)	Tok/s 222216 (242010)	Loss/tok 2.7892 (3.4553)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.035 (0.089)	Data 1.20e-04 (3.82e-04)	Tok/s 222837 (242053)	Loss/tok 2.7354 (3.4565)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.78e-04)	Tok/s 232234 (242113)	Loss/tok 3.1475 (3.4560)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.74e-04)	Tok/s 230902 (242024)	Loss/tok 3.1395 (3.4543)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.036 (0.089)	Data 1.15e-04 (3.71e-04)	Tok/s 223847 (242001)	Loss/tok 2.8061 (3.4547)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.067 (0.089)	Data 1.26e-04 (3.67e-04)	Tok/s 233129 (241909)	Loss/tok 3.2169 (3.4527)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.100 (0.088)	Data 1.10e-04 (3.64e-04)	Tok/s 254297 (241868)	Loss/tok 3.3885 (3.4509)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.035 (0.088)	Data 1.10e-04 (3.61e-04)	Tok/s 225903 (241778)	Loss/tok 2.6997 (3.4487)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.135 (0.088)	Data 1.08e-04 (3.57e-04)	Tok/s 259391 (241809)	Loss/tok 3.6854 (3.4493)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.099 (0.088)	Data 1.25e-04 (3.54e-04)	Tok/s 254224 (241828)	Loss/tok 3.4365 (3.4483)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][770/1291]	Time 0.066 (0.088)	Data 1.07e-04 (3.51e-04)	Tok/s 233999 (241802)	Loss/tok 3.1665 (3.4477)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.099 (0.088)	Data 1.10e-04 (3.48e-04)	Tok/s 254973 (241792)	Loss/tok 3.4289 (3.4463)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][790/1291]	Time 0.135 (0.088)	Data 1.25e-04 (3.45e-04)	Tok/s 258467 (241892)	Loss/tok 3.6577 (3.4479)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.134 (0.088)	Data 1.26e-04 (3.42e-04)	Tok/s 259141 (241880)	Loss/tok 3.6362 (3.4468)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.068 (0.089)	Data 1.08e-04 (3.39e-04)	Tok/s 229667 (241970)	Loss/tok 3.1823 (3.4472)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.36e-04)	Tok/s 230658 (242000)	Loss/tok 3.1262 (3.4461)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.34e-04)	Tok/s 230207 (242078)	Loss/tok 3.1817 (3.4474)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.134 (0.089)	Data 1.11e-04 (3.31e-04)	Tok/s 257185 (242126)	Loss/tok 3.5980 (3.4469)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.29e-04)	Tok/s 229561 (242068)	Loss/tok 3.1688 (3.4461)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.26e-04)	Tok/s 251194 (241997)	Loss/tok 3.5252 (3.4457)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.036 (0.089)	Data 1.15e-04 (3.24e-04)	Tok/s 217069 (242051)	Loss/tok 2.7029 (3.4456)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.21e-04)	Tok/s 235424 (242064)	Loss/tok 3.2030 (3.4455)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.036 (0.089)	Data 1.09e-04 (3.19e-04)	Tok/s 222076 (242021)	Loss/tok 2.7358 (3.4441)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.17e-04)	Tok/s 253728 (242018)	Loss/tok 3.3782 (3.4442)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.135 (0.089)	Data 1.34e-04 (3.15e-04)	Tok/s 258601 (242010)	Loss/tok 3.5858 (3.4439)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][920/1291]	Time 0.100 (0.089)	Data 1.24e-04 (3.12e-04)	Tok/s 252125 (242083)	Loss/tok 3.4786 (3.4439)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.10e-04)	Tok/s 231341 (242093)	Loss/tok 3.1347 (3.4435)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][940/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.08e-04)	Tok/s 232309 (242095)	Loss/tok 3.1577 (3.4435)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.067 (0.089)	Data 1.20e-04 (3.06e-04)	Tok/s 231193 (242104)	Loss/tok 3.2407 (3.4432)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.175 (0.089)	Data 1.15e-04 (3.04e-04)	Tok/s 255266 (242143)	Loss/tok 3.7812 (3.4440)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.02e-04)	Tok/s 230459 (242157)	Loss/tok 3.1707 (3.4430)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.00e-04)	Tok/s 254312 (242134)	Loss/tok 3.3321 (3.4417)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.98e-04)	Tok/s 229592 (242172)	Loss/tok 3.1104 (3.4411)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.97e-04)	Tok/s 231206 (242169)	Loss/tok 3.0988 (3.4403)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.135 (0.089)	Data 1.12e-04 (2.95e-04)	Tok/s 259547 (242158)	Loss/tok 3.5939 (3.4396)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.036 (0.089)	Data 1.10e-04 (2.93e-04)	Tok/s 219947 (242131)	Loss/tok 2.7075 (3.4387)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.176 (0.089)	Data 1.12e-04 (2.91e-04)	Tok/s 254001 (242229)	Loss/tok 3.8471 (3.4390)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.90e-04)	Tok/s 249670 (242228)	Loss/tok 3.3488 (3.4390)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.88e-04)	Tok/s 231770 (242224)	Loss/tok 3.1937 (3.4383)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.86e-04)	Tok/s 231070 (242262)	Loss/tok 3.0271 (3.4379)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1070/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.85e-04)	Tok/s 222449 (242245)	Loss/tok 2.6009 (3.4370)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.83e-04)	Tok/s 234236 (242162)	Loss/tok 3.1326 (3.4361)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1090/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.82e-04)	Tok/s 231513 (242172)	Loss/tok 3.1685 (3.4366)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.135 (0.089)	Data 1.32e-04 (2.80e-04)	Tok/s 258757 (242147)	Loss/tok 3.4949 (3.4357)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.099 (0.089)	Data 1.07e-04 (2.79e-04)	Tok/s 253854 (242093)	Loss/tok 3.3443 (3.4342)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.135 (0.089)	Data 1.11e-04 (2.77e-04)	Tok/s 260387 (242151)	Loss/tok 3.5679 (3.4338)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.76e-04)	Tok/s 249473 (242143)	Loss/tok 3.3199 (3.4330)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.74e-04)	Tok/s 233359 (242086)	Loss/tok 3.1144 (3.4312)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.036 (0.089)	Data 1.10e-04 (2.73e-04)	Tok/s 219544 (242089)	Loss/tok 2.7375 (3.4305)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.71e-04)	Tok/s 253463 (242090)	Loss/tok 3.2881 (3.4307)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.135 (0.089)	Data 1.13e-04 (2.70e-04)	Tok/s 259661 (242113)	Loss/tok 3.6177 (3.4309)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.69e-04)	Tok/s 231873 (242092)	Loss/tok 3.0936 (3.4296)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.68e-04)	Tok/s 252374 (242120)	Loss/tok 3.4080 (3.4295)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.067 (0.089)	Data 1.24e-04 (2.66e-04)	Tok/s 233246 (242124)	Loss/tok 3.1363 (3.4288)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.65e-04)	Tok/s 250562 (242146)	Loss/tok 3.3183 (3.4281)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1220/1291]	Time 0.136 (0.089)	Data 1.11e-04 (2.64e-04)	Tok/s 257343 (242125)	Loss/tok 3.5262 (3.4274)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.174 (0.089)	Data 1.31e-04 (2.63e-04)	Tok/s 258746 (242111)	Loss/tok 3.7171 (3.4277)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.036 (0.089)	Data 1.15e-04 (2.61e-04)	Tok/s 216960 (242087)	Loss/tok 2.6142 (3.4269)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.036 (0.089)	Data 1.10e-04 (2.60e-04)	Tok/s 218228 (242036)	Loss/tok 2.6659 (3.4261)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.59e-04)	Tok/s 253959 (242075)	Loss/tok 3.3264 (3.4260)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.58e-04)	Tok/s 233772 (242069)	Loss/tok 3.1262 (3.4250)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.100 (0.089)	Data 1.07e-04 (2.57e-04)	Tok/s 250898 (242120)	Loss/tok 3.4371 (3.4261)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.066 (0.089)	Data 4.79e-05 (2.58e-04)	Tok/s 237786 (242103)	Loss/tok 3.1105 (3.4252)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593144479176, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593144479176, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.492 (0.492)	Decoder iters 149.0 (149.0)	Tok/s 33099 (33099)
0: Running moses detokenizer
0: BLEU(score=21.803080758602864, counts=[35789, 17110, 9404, 5397], totals=[65492, 62489, 59487, 56491], precisions=[54.646369022170646, 27.38081902414825, 15.808495973910267, 9.55373422315059], bp=1.0, sys_len=65492, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593144481205, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.218, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593144481205, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4259	Test BLEU: 21.80
0: Performance: Epoch: 1	Training: 1936579 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593144481206, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593144481206, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144481206, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 207072892
0: TRAIN [2][0/1291]	Time 0.266 (0.266)	Data 1.93e-01 (1.93e-01)	Tok/s 57227 (57227)	Loss/tok 3.0750 (3.0750)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.067 (0.120)	Data 1.18e-04 (1.77e-02)	Tok/s 233751 (229307)	Loss/tok 3.0875 (3.3639)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.099 (0.115)	Data 1.12e-04 (9.32e-03)	Tok/s 256839 (237307)	Loss/tok 3.1969 (3.3724)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.067 (0.108)	Data 1.26e-04 (6.35e-03)	Tok/s 235964 (239194)	Loss/tok 3.1132 (3.3633)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.067 (0.103)	Data 1.13e-04 (4.83e-03)	Tok/s 233448 (240410)	Loss/tok 3.0727 (3.3455)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][50/1291]	Time 0.067 (0.101)	Data 1.12e-04 (3.91e-03)	Tok/s 236586 (241483)	Loss/tok 2.9891 (3.3310)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][60/1291]	Time 0.067 (0.104)	Data 1.14e-04 (3.29e-03)	Tok/s 229985 (242348)	Loss/tok 3.1648 (3.3544)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.036 (0.099)	Data 1.12e-04 (2.84e-03)	Tok/s 227673 (241082)	Loss/tok 2.6665 (3.3333)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.100 (0.096)	Data 1.11e-04 (2.50e-03)	Tok/s 254657 (241135)	Loss/tok 3.2567 (3.3188)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.100 (0.094)	Data 1.10e-04 (2.24e-03)	Tok/s 247255 (240883)	Loss/tok 3.3768 (3.3068)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.135 (0.093)	Data 1.11e-04 (2.03e-03)	Tok/s 257543 (240450)	Loss/tok 3.5384 (3.3048)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.036 (0.092)	Data 1.24e-04 (1.86e-03)	Tok/s 222649 (240373)	Loss/tok 2.6813 (3.3030)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.036 (0.091)	Data 1.12e-04 (1.72e-03)	Tok/s 222220 (240494)	Loss/tok 2.6488 (3.2949)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.099 (0.091)	Data 1.12e-04 (1.59e-03)	Tok/s 254470 (240877)	Loss/tok 3.3131 (3.2948)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.067 (0.091)	Data 1.13e-04 (1.49e-03)	Tok/s 228812 (240977)	Loss/tok 3.0820 (3.2900)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.100 (0.091)	Data 1.15e-04 (1.40e-03)	Tok/s 252514 (241314)	Loss/tok 3.1510 (3.2907)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.067 (0.091)	Data 1.33e-04 (1.32e-03)	Tok/s 234836 (241029)	Loss/tok 3.1089 (3.2897)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.100 (0.091)	Data 1.39e-04 (1.25e-03)	Tok/s 255265 (241120)	Loss/tok 3.2007 (3.2874)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.100 (0.091)	Data 1.16e-04 (1.19e-03)	Tok/s 250853 (241026)	Loss/tok 3.3015 (3.2866)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][190/1291]	Time 0.100 (0.091)	Data 1.15e-04 (1.13e-03)	Tok/s 251224 (241154)	Loss/tok 3.3730 (3.2913)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.136 (0.092)	Data 1.09e-04 (1.08e-03)	Tok/s 254609 (241612)	Loss/tok 3.4423 (3.2951)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.100 (0.091)	Data 1.19e-04 (1.04e-03)	Tok/s 252393 (241528)	Loss/tok 3.3472 (3.2920)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.136 (0.091)	Data 1.34e-04 (9.94e-04)	Tok/s 253739 (241634)	Loss/tok 3.4695 (3.2921)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.101 (0.092)	Data 1.11e-04 (9.56e-04)	Tok/s 250832 (241981)	Loss/tok 3.3197 (3.2967)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.067 (0.092)	Data 1.12e-04 (9.21e-04)	Tok/s 233178 (241823)	Loss/tok 3.1126 (3.2942)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.067 (0.091)	Data 1.12e-04 (8.89e-04)	Tok/s 235257 (241497)	Loss/tok 3.0869 (3.2893)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][260/1291]	Time 0.067 (0.092)	Data 1.18e-04 (8.59e-04)	Tok/s 225867 (241720)	Loss/tok 3.0519 (3.2939)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.067 (0.092)	Data 1.13e-04 (8.32e-04)	Tok/s 234805 (241839)	Loss/tok 3.1656 (3.2971)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.067 (0.091)	Data 1.09e-04 (8.06e-04)	Tok/s 228094 (241804)	Loss/tok 3.0993 (3.2944)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.067 (0.092)	Data 1.13e-04 (7.82e-04)	Tok/s 232220 (241913)	Loss/tok 3.1472 (3.2953)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.100 (0.092)	Data 1.26e-04 (7.60e-04)	Tok/s 250575 (241968)	Loss/tok 3.3341 (3.2960)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.067 (0.092)	Data 1.33e-04 (7.40e-04)	Tok/s 232089 (242047)	Loss/tok 3.0064 (3.2961)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.067 (0.091)	Data 1.31e-04 (7.20e-04)	Tok/s 231177 (241760)	Loss/tok 3.1816 (3.2925)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.067 (0.091)	Data 1.14e-04 (7.02e-04)	Tok/s 231865 (241717)	Loss/tok 3.0819 (3.2917)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.067 (0.091)	Data 1.13e-04 (6.85e-04)	Tok/s 231707 (241718)	Loss/tok 3.1059 (3.2942)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.175 (0.092)	Data 1.10e-04 (6.69e-04)	Tok/s 254823 (241921)	Loss/tok 3.5731 (3.2983)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.036 (0.092)	Data 1.27e-04 (6.53e-04)	Tok/s 223721 (242032)	Loss/tok 2.6189 (3.2999)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.135 (0.092)	Data 1.17e-04 (6.39e-04)	Tok/s 261444 (242070)	Loss/tok 3.3757 (3.2997)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.036 (0.092)	Data 1.16e-04 (6.25e-04)	Tok/s 221075 (242007)	Loss/tok 2.6199 (3.2996)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][390/1291]	Time 0.067 (0.092)	Data 1.18e-04 (6.12e-04)	Tok/s 231664 (242069)	Loss/tok 3.1028 (3.3007)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.067 (0.092)	Data 1.10e-04 (6.00e-04)	Tok/s 230882 (242014)	Loss/tok 3.0384 (3.2994)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.101 (0.092)	Data 1.19e-04 (5.88e-04)	Tok/s 248093 (241990)	Loss/tok 3.3319 (3.2991)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.067 (0.092)	Data 1.15e-04 (5.77e-04)	Tok/s 232240 (242129)	Loss/tok 3.1638 (3.3028)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.136 (0.092)	Data 1.20e-04 (5.66e-04)	Tok/s 257841 (242156)	Loss/tok 3.4756 (3.3027)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.036 (0.092)	Data 1.11e-04 (5.56e-04)	Tok/s 223138 (242227)	Loss/tok 2.6896 (3.3016)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.100 (0.092)	Data 1.14e-04 (5.46e-04)	Tok/s 255717 (242179)	Loss/tok 3.3109 (3.3009)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.100 (0.092)	Data 1.14e-04 (5.37e-04)	Tok/s 248072 (242193)	Loss/tok 3.3017 (3.3023)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.100 (0.092)	Data 1.15e-04 (5.28e-04)	Tok/s 253093 (242167)	Loss/tok 3.1673 (3.3009)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.067 (0.092)	Data 1.14e-04 (5.19e-04)	Tok/s 236996 (242317)	Loss/tok 3.1166 (3.3036)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.036 (0.092)	Data 1.13e-04 (5.11e-04)	Tok/s 221874 (242305)	Loss/tok 2.6974 (3.3034)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.067 (0.092)	Data 1.13e-04 (5.03e-04)	Tok/s 234010 (242313)	Loss/tok 2.9925 (3.3027)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.101 (0.092)	Data 1.18e-04 (4.96e-04)	Tok/s 250782 (242278)	Loss/tok 3.2176 (3.3008)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][520/1291]	Time 0.067 (0.092)	Data 1.18e-04 (4.88e-04)	Tok/s 235215 (242175)	Loss/tok 3.1177 (3.2988)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.067 (0.092)	Data 1.11e-04 (4.81e-04)	Tok/s 230006 (242296)	Loss/tok 3.0746 (3.2994)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.136 (0.092)	Data 1.15e-04 (4.75e-04)	Tok/s 257077 (242209)	Loss/tok 3.5964 (3.2989)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.100 (0.092)	Data 1.14e-04 (4.68e-04)	Tok/s 252814 (242208)	Loss/tok 3.2570 (3.2978)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.067 (0.091)	Data 1.20e-04 (4.62e-04)	Tok/s 228281 (242164)	Loss/tok 3.0770 (3.2972)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.100 (0.091)	Data 1.13e-04 (4.56e-04)	Tok/s 252795 (242125)	Loss/tok 3.2219 (3.2952)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][580/1291]	Time 0.036 (0.091)	Data 1.13e-04 (4.50e-04)	Tok/s 219948 (242079)	Loss/tok 2.6379 (3.2949)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.099 (0.091)	Data 1.14e-04 (4.44e-04)	Tok/s 254419 (242112)	Loss/tok 3.3246 (3.2950)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.067 (0.091)	Data 1.09e-04 (4.39e-04)	Tok/s 229485 (242040)	Loss/tok 3.0794 (3.2934)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.135 (0.091)	Data 1.14e-04 (4.34e-04)	Tok/s 259306 (242093)	Loss/tok 3.4442 (3.2937)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.036 (0.091)	Data 1.32e-04 (4.29e-04)	Tok/s 221891 (242078)	Loss/tok 2.5963 (3.2931)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.099 (0.091)	Data 1.12e-04 (4.24e-04)	Tok/s 254495 (242116)	Loss/tok 3.1749 (3.2934)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.100 (0.091)	Data 1.13e-04 (4.19e-04)	Tok/s 249900 (242105)	Loss/tok 3.2701 (3.2923)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.069 (0.090)	Data 1.12e-04 (4.14e-04)	Tok/s 223234 (242032)	Loss/tok 3.0473 (3.2909)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.10e-04)	Tok/s 231358 (241974)	Loss/tok 3.1278 (3.2905)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.067 (0.090)	Data 1.16e-04 (4.05e-04)	Tok/s 228836 (241967)	Loss/tok 3.0249 (3.2898)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.101 (0.091)	Data 1.25e-04 (4.01e-04)	Tok/s 251090 (242018)	Loss/tok 3.2545 (3.2916)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.135 (0.090)	Data 1.13e-04 (3.97e-04)	Tok/s 260117 (241980)	Loss/tok 3.5173 (3.2908)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.100 (0.090)	Data 1.11e-04 (3.93e-04)	Tok/s 253541 (242007)	Loss/tok 3.2072 (3.2904)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][710/1291]	Time 0.100 (0.090)	Data 1.27e-04 (3.89e-04)	Tok/s 251583 (242045)	Loss/tok 3.2214 (3.2908)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.067 (0.091)	Data 1.14e-04 (3.85e-04)	Tok/s 239683 (242065)	Loss/tok 3.1530 (3.2918)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.135 (0.091)	Data 1.13e-04 (3.82e-04)	Tok/s 257875 (242116)	Loss/tok 3.3846 (3.2917)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.100 (0.091)	Data 1.11e-04 (3.78e-04)	Tok/s 253090 (242091)	Loss/tok 3.2470 (3.2906)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.067 (0.091)	Data 1.16e-04 (3.74e-04)	Tok/s 233063 (242132)	Loss/tok 3.0449 (3.2901)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][760/1291]	Time 0.135 (0.091)	Data 1.13e-04 (3.71e-04)	Tok/s 258715 (242204)	Loss/tok 3.5766 (3.2927)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.067 (0.091)	Data 1.19e-04 (3.68e-04)	Tok/s 231854 (242125)	Loss/tok 3.0727 (3.2911)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.067 (0.091)	Data 1.09e-04 (3.65e-04)	Tok/s 230064 (242157)	Loss/tok 3.0641 (3.2905)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.067 (0.091)	Data 1.16e-04 (3.61e-04)	Tok/s 230862 (242233)	Loss/tok 3.0388 (3.2905)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.099 (0.091)	Data 1.15e-04 (3.58e-04)	Tok/s 251606 (242271)	Loss/tok 3.2301 (3.2905)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.100 (0.091)	Data 1.20e-04 (3.55e-04)	Tok/s 254034 (242288)	Loss/tok 3.2259 (3.2898)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.067 (0.090)	Data 1.08e-04 (3.53e-04)	Tok/s 230911 (242180)	Loss/tok 2.9954 (3.2882)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.036 (0.090)	Data 1.13e-04 (3.50e-04)	Tok/s 219267 (242113)	Loss/tok 2.6816 (3.2874)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.135 (0.090)	Data 1.11e-04 (3.47e-04)	Tok/s 255941 (242136)	Loss/tok 3.4972 (3.2868)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.099 (0.090)	Data 1.16e-04 (3.44e-04)	Tok/s 255541 (242084)	Loss/tok 3.3194 (3.2861)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.036 (0.090)	Data 1.35e-04 (3.41e-04)	Tok/s 223940 (242077)	Loss/tok 2.6588 (3.2858)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.036 (0.090)	Data 1.09e-04 (3.39e-04)	Tok/s 227093 (242065)	Loss/tok 2.6105 (3.2857)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.36e-04)	Tok/s 236924 (242063)	Loss/tok 3.1584 (3.2850)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][890/1291]	Time 0.136 (0.090)	Data 1.16e-04 (3.34e-04)	Tok/s 257379 (242100)	Loss/tok 3.3887 (3.2843)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.31e-04)	Tok/s 230386 (242082)	Loss/tok 2.9578 (3.2836)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.29e-04)	Tok/s 230553 (242025)	Loss/tok 3.1347 (3.2828)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][920/1291]	Time 0.099 (0.090)	Data 1.21e-04 (3.27e-04)	Tok/s 250625 (242063)	Loss/tok 3.3512 (3.2836)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.099 (0.090)	Data 1.16e-04 (3.24e-04)	Tok/s 255524 (242098)	Loss/tok 3.2304 (3.2837)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.067 (0.090)	Data 1.22e-04 (3.22e-04)	Tok/s 227758 (242045)	Loss/tok 3.1111 (3.2830)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.173 (0.090)	Data 1.14e-04 (3.20e-04)	Tok/s 255486 (242042)	Loss/tok 3.6935 (3.2830)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.036 (0.090)	Data 1.17e-04 (3.18e-04)	Tok/s 225242 (242004)	Loss/tok 2.7091 (3.2827)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.036 (0.090)	Data 1.13e-04 (3.16e-04)	Tok/s 223191 (241989)	Loss/tok 2.7272 (3.2832)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.099 (0.090)	Data 1.13e-04 (3.14e-04)	Tok/s 254089 (241999)	Loss/tok 3.3396 (3.2824)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.12e-04)	Tok/s 227605 (241987)	Loss/tok 3.0470 (3.2822)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.099 (0.090)	Data 1.10e-04 (3.10e-04)	Tok/s 253117 (242005)	Loss/tok 3.2359 (3.2820)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.099 (0.090)	Data 1.18e-04 (3.08e-04)	Tok/s 257997 (242048)	Loss/tok 3.1845 (3.2817)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.06e-04)	Tok/s 235012 (242075)	Loss/tok 3.0932 (3.2829)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.04e-04)	Tok/s 232618 (242005)	Loss/tok 3.0733 (3.2818)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.135 (0.090)	Data 1.13e-04 (3.02e-04)	Tok/s 261138 (242056)	Loss/tok 3.4240 (3.2823)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1050/1291]	Time 0.036 (0.090)	Data 1.11e-04 (3.01e-04)	Tok/s 220186 (242043)	Loss/tok 2.6433 (3.2827)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.99e-04)	Tok/s 227460 (242010)	Loss/tok 3.0290 (3.2818)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.067 (0.090)	Data 1.21e-04 (2.97e-04)	Tok/s 233863 (242058)	Loss/tok 3.0889 (3.2824)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.95e-04)	Tok/s 233687 (241980)	Loss/tok 3.1075 (3.2813)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.94e-04)	Tok/s 230214 (241952)	Loss/tok 3.0543 (3.2813)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.92e-04)	Tok/s 234950 (241917)	Loss/tok 3.0903 (3.2807)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.91e-04)	Tok/s 251874 (241884)	Loss/tok 3.2153 (3.2798)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.89e-04)	Tok/s 250518 (241941)	Loss/tok 3.2477 (3.2805)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.88e-04)	Tok/s 259277 (241986)	Loss/tok 3.4018 (3.2817)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1140/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.86e-04)	Tok/s 228085 (241965)	Loss/tok 2.9874 (3.2818)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.85e-04)	Tok/s 254225 (241903)	Loss/tok 3.2827 (3.2814)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.83e-04)	Tok/s 251479 (241841)	Loss/tok 3.3773 (3.2803)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.067 (0.089)	Data 1.28e-04 (2.82e-04)	Tok/s 229414 (241820)	Loss/tok 3.1880 (3.2813)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.80e-04)	Tok/s 234312 (241834)	Loss/tok 3.0918 (3.2811)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.79e-04)	Tok/s 231132 (241800)	Loss/tok 3.0359 (3.2804)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.78e-04)	Tok/s 226266 (241768)	Loss/tok 3.1115 (3.2801)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.76e-04)	Tok/s 233865 (241838)	Loss/tok 3.0212 (3.2808)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.75e-04)	Tok/s 253624 (241813)	Loss/tok 3.3220 (3.2798)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.100 (0.089)	Data 1.20e-04 (2.74e-04)	Tok/s 252236 (241842)	Loss/tok 3.2125 (3.2794)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.72e-04)	Tok/s 232371 (241856)	Loss/tok 3.0214 (3.2790)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.100 (0.089)	Data 1.25e-04 (2.71e-04)	Tok/s 252403 (241857)	Loss/tok 3.3154 (3.2787)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1260/1291]	Time 0.101 (0.089)	Data 1.28e-04 (2.70e-04)	Tok/s 250687 (241855)	Loss/tok 3.2269 (3.2784)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.174 (0.089)	Data 1.13e-04 (2.69e-04)	Tok/s 256660 (241871)	Loss/tok 3.7025 (3.2787)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.67e-04)	Tok/s 225325 (241853)	Loss/tok 2.9610 (3.2784)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1290/1291]	Time 0.099 (0.089)	Data 4.82e-05 (2.69e-04)	Tok/s 255771 (241893)	Loss/tok 3.1671 (3.2793)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593144596952, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593144596952, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.372 (0.372)	Decoder iters 96.0 (96.0)	Tok/s 43091 (43091)
0: Running moses detokenizer
0: BLEU(score=22.79809548402345, counts=[35728, 17476, 9804, 5721], totals=[63548, 60545, 57542, 54542], precisions=[56.2220683577768, 28.864480964571808, 17.037989642348197, 10.48916431373987], bp=0.9824062477480041, sys_len=63548, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593144598836, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.228, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593144598837, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2787	Test BLEU: 22.80
0: Performance: Epoch: 2	Training: 1934805 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593144598837, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593144598837, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593144598837, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 206833844
0: TRAIN [3][0/1291]	Time 0.291 (0.291)	Data 1.94e-01 (1.94e-01)	Tok/s 52791 (52791)	Loss/tok 3.0451 (3.0451)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.100 (0.103)	Data 1.13e-04 (1.77e-02)	Tok/s 251837 (223362)	Loss/tok 3.1935 (3.1543)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.135 (0.097)	Data 1.22e-04 (9.34e-03)	Tok/s 256191 (232499)	Loss/tok 3.2815 (3.1470)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.100 (0.102)	Data 1.16e-04 (6.37e-03)	Tok/s 252302 (238554)	Loss/tok 3.2049 (3.1944)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.135 (0.096)	Data 1.17e-04 (4.84e-03)	Tok/s 262151 (238461)	Loss/tok 3.2473 (3.1704)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.134 (0.092)	Data 1.08e-04 (3.91e-03)	Tok/s 258949 (237717)	Loss/tok 3.4345 (3.1688)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.103 (0.087)	Data 1.07e-04 (3.29e-03)	Tok/s 245135 (236738)	Loss/tok 3.1166 (3.1456)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.099 (0.087)	Data 1.09e-04 (2.84e-03)	Tok/s 250970 (237545)	Loss/tok 3.1591 (3.1461)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.067 (0.087)	Data 1.08e-04 (2.51e-03)	Tok/s 230687 (237903)	Loss/tok 2.9315 (3.1530)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.136 (0.088)	Data 1.11e-04 (2.24e-03)	Tok/s 257547 (238314)	Loss/tok 3.3331 (3.1586)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][100/1291]	Time 0.068 (0.088)	Data 1.08e-04 (2.04e-03)	Tok/s 225473 (238759)	Loss/tok 3.0696 (3.1631)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.136 (0.088)	Data 1.07e-04 (1.86e-03)	Tok/s 257590 (238807)	Loss/tok 3.3575 (3.1590)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.067 (0.086)	Data 1.07e-04 (1.72e-03)	Tok/s 235093 (238640)	Loss/tok 3.0615 (3.1517)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.067 (0.086)	Data 1.19e-04 (1.60e-03)	Tok/s 228651 (238432)	Loss/tok 3.0327 (3.1518)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.067 (0.086)	Data 1.09e-04 (1.49e-03)	Tok/s 230568 (238672)	Loss/tok 3.0097 (3.1547)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.067 (0.085)	Data 1.07e-04 (1.40e-03)	Tok/s 231424 (238411)	Loss/tok 3.0408 (3.1503)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.067 (0.086)	Data 1.04e-04 (1.32e-03)	Tok/s 234997 (238844)	Loss/tok 3.0599 (3.1580)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.067 (0.086)	Data 1.34e-04 (1.25e-03)	Tok/s 226933 (238837)	Loss/tok 2.9886 (3.1570)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.100 (0.086)	Data 1.09e-04 (1.19e-03)	Tok/s 251021 (239181)	Loss/tok 3.2218 (3.1582)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.067 (0.087)	Data 1.08e-04 (1.13e-03)	Tok/s 232227 (239631)	Loss/tok 3.0314 (3.1653)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.067 (0.087)	Data 1.08e-04 (1.08e-03)	Tok/s 228602 (239818)	Loss/tok 3.0410 (3.1657)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.174 (0.087)	Data 1.11e-04 (1.03e-03)	Tok/s 258290 (239819)	Loss/tok 3.4845 (3.1667)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][220/1291]	Time 0.100 (0.086)	Data 1.06e-04 (9.92e-04)	Tok/s 250922 (239502)	Loss/tok 3.1673 (3.1607)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.067 (0.087)	Data 1.10e-04 (9.54e-04)	Tok/s 224694 (239682)	Loss/tok 3.0141 (3.1675)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.036 (0.087)	Data 1.31e-04 (9.19e-04)	Tok/s 224089 (239751)	Loss/tok 2.5854 (3.1686)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.101 (0.087)	Data 1.19e-04 (8.87e-04)	Tok/s 251929 (239889)	Loss/tok 3.2731 (3.1702)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.067 (0.087)	Data 1.26e-04 (8.57e-04)	Tok/s 229987 (239665)	Loss/tok 2.9221 (3.1666)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.067 (0.087)	Data 1.12e-04 (8.30e-04)	Tok/s 232383 (239743)	Loss/tok 3.0398 (3.1707)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.068 (0.087)	Data 1.21e-04 (8.04e-04)	Tok/s 227063 (239698)	Loss/tok 2.9589 (3.1673)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.100 (0.087)	Data 1.07e-04 (7.81e-04)	Tok/s 254319 (239874)	Loss/tok 3.2036 (3.1659)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.099 (0.087)	Data 1.20e-04 (7.58e-04)	Tok/s 252983 (239876)	Loss/tok 3.2381 (3.1666)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.100 (0.088)	Data 1.10e-04 (7.38e-04)	Tok/s 249806 (240115)	Loss/tok 3.1644 (3.1715)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.099 (0.087)	Data 1.08e-04 (7.18e-04)	Tok/s 254869 (240000)	Loss/tok 3.0879 (3.1678)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.067 (0.087)	Data 1.09e-04 (7.00e-04)	Tok/s 227809 (240049)	Loss/tok 2.9725 (3.1665)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.100 (0.087)	Data 1.11e-04 (6.83e-04)	Tok/s 253341 (240096)	Loss/tok 3.1316 (3.1649)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][350/1291]	Time 0.067 (0.087)	Data 1.21e-04 (6.66e-04)	Tok/s 229227 (240095)	Loss/tok 2.9851 (3.1650)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.099 (0.088)	Data 1.08e-04 (6.51e-04)	Tok/s 253046 (240228)	Loss/tok 3.1365 (3.1702)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.100 (0.087)	Data 1.07e-04 (6.36e-04)	Tok/s 251659 (240166)	Loss/tok 3.1434 (3.1681)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.067 (0.087)	Data 1.30e-04 (6.23e-04)	Tok/s 230681 (240237)	Loss/tok 2.9341 (3.1690)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.137 (0.087)	Data 1.11e-04 (6.10e-04)	Tok/s 252707 (240176)	Loss/tok 3.2793 (3.1678)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.100 (0.087)	Data 1.08e-04 (5.97e-04)	Tok/s 253807 (240105)	Loss/tok 3.1087 (3.1650)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.068 (0.087)	Data 1.14e-04 (5.85e-04)	Tok/s 226757 (240038)	Loss/tok 2.9615 (3.1627)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.067 (0.087)	Data 1.08e-04 (5.74e-04)	Tok/s 226680 (240062)	Loss/tok 2.9108 (3.1627)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.100 (0.087)	Data 1.27e-04 (5.64e-04)	Tok/s 254605 (240127)	Loss/tok 3.1031 (3.1628)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.036 (0.087)	Data 1.07e-04 (5.53e-04)	Tok/s 222245 (240130)	Loss/tok 2.5842 (3.1614)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.068 (0.087)	Data 1.06e-04 (5.44e-04)	Tok/s 228580 (240150)	Loss/tok 2.9747 (3.1609)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.068 (0.087)	Data 1.06e-04 (5.34e-04)	Tok/s 228120 (240144)	Loss/tok 2.9343 (3.1649)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.087)	Data 1.08e-04 (5.25e-04)	Tok/s 230139 (240170)	Loss/tok 2.9899 (3.1652)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][480/1291]	Time 0.100 (0.087)	Data 1.06e-04 (5.17e-04)	Tok/s 253325 (240222)	Loss/tok 3.1543 (3.1635)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.100 (0.087)	Data 1.10e-04 (5.08e-04)	Tok/s 251937 (240234)	Loss/tok 3.1255 (3.1623)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.067 (0.087)	Data 1.06e-04 (5.00e-04)	Tok/s 229632 (240294)	Loss/tok 2.9261 (3.1628)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.100 (0.088)	Data 1.32e-04 (4.93e-04)	Tok/s 248340 (240441)	Loss/tok 3.1423 (3.1632)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.067 (0.088)	Data 1.08e-04 (4.85e-04)	Tok/s 233243 (240531)	Loss/tok 3.0363 (3.1643)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.068 (0.088)	Data 1.09e-04 (4.78e-04)	Tok/s 225511 (240495)	Loss/tok 3.0528 (3.1654)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.136 (0.088)	Data 1.13e-04 (4.72e-04)	Tok/s 256972 (240518)	Loss/tok 3.2784 (3.1649)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.067 (0.088)	Data 1.08e-04 (4.65e-04)	Tok/s 230122 (240515)	Loss/tok 2.9119 (3.1643)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.135 (0.088)	Data 1.13e-04 (4.59e-04)	Tok/s 260808 (240504)	Loss/tok 3.2769 (3.1628)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.067 (0.087)	Data 1.10e-04 (4.53e-04)	Tok/s 231166 (240399)	Loss/tok 2.9220 (3.1606)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.067 (0.087)	Data 1.09e-04 (4.47e-04)	Tok/s 229194 (240452)	Loss/tok 3.0548 (3.1616)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.100 (0.088)	Data 1.11e-04 (4.41e-04)	Tok/s 254884 (240474)	Loss/tok 3.1147 (3.1619)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.100 (0.088)	Data 1.11e-04 (4.36e-04)	Tok/s 255479 (240532)	Loss/tok 3.1252 (3.1618)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][610/1291]	Time 0.067 (0.088)	Data 1.11e-04 (4.30e-04)	Tok/s 234483 (240514)	Loss/tok 2.9283 (3.1616)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.173 (0.088)	Data 1.11e-04 (4.25e-04)	Tok/s 259107 (240505)	Loss/tok 3.4384 (3.1619)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.101 (0.088)	Data 1.11e-04 (4.20e-04)	Tok/s 245240 (240497)	Loss/tok 3.2601 (3.1609)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.066 (0.088)	Data 1.13e-04 (4.16e-04)	Tok/s 237037 (240551)	Loss/tok 2.9748 (3.1618)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.136 (0.088)	Data 1.10e-04 (4.11e-04)	Tok/s 256653 (240669)	Loss/tok 3.3065 (3.1632)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.100 (0.088)	Data 1.08e-04 (4.06e-04)	Tok/s 250554 (240667)	Loss/tok 3.1128 (3.1615)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.067 (0.088)	Data 1.22e-04 (4.02e-04)	Tok/s 229593 (240719)	Loss/tok 2.9593 (3.1617)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.174 (0.088)	Data 1.10e-04 (3.98e-04)	Tok/s 254496 (240728)	Loss/tok 3.5077 (3.1618)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.067 (0.088)	Data 1.09e-04 (3.94e-04)	Tok/s 226152 (240657)	Loss/tok 2.9251 (3.1606)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.100 (0.088)	Data 1.12e-04 (3.90e-04)	Tok/s 251795 (240586)	Loss/tok 3.1738 (3.1588)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.036 (0.088)	Data 1.14e-04 (3.86e-04)	Tok/s 220715 (240591)	Loss/tok 2.4673 (3.1584)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.067 (0.088)	Data 1.09e-04 (3.82e-04)	Tok/s 230829 (240552)	Loss/tok 2.9669 (3.1581)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.036 (0.087)	Data 1.10e-04 (3.78e-04)	Tok/s 221529 (240554)	Loss/tok 2.5594 (3.1573)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][740/1291]	Time 0.135 (0.088)	Data 1.12e-04 (3.75e-04)	Tok/s 256279 (240644)	Loss/tok 3.4515 (3.1588)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.067 (0.088)	Data 1.08e-04 (3.71e-04)	Tok/s 229645 (240665)	Loss/tok 2.8811 (3.1602)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.036 (0.088)	Data 1.12e-04 (3.68e-04)	Tok/s 219611 (240781)	Loss/tok 2.5075 (3.1619)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.136 (0.088)	Data 1.08e-04 (3.64e-04)	Tok/s 254688 (240833)	Loss/tok 3.2031 (3.1615)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.61e-04)	Tok/s 231041 (240786)	Loss/tok 2.8998 (3.1599)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.036 (0.088)	Data 1.10e-04 (3.58e-04)	Tok/s 215547 (240727)	Loss/tok 2.5608 (3.1584)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.135 (0.088)	Data 1.30e-04 (3.55e-04)	Tok/s 256542 (240752)	Loss/tok 3.3876 (3.1577)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.52e-04)	Tok/s 228953 (240778)	Loss/tok 2.9624 (3.1577)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.067 (0.088)	Data 1.08e-04 (3.49e-04)	Tok/s 226094 (240819)	Loss/tok 2.8362 (3.1588)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.100 (0.088)	Data 1.14e-04 (3.46e-04)	Tok/s 252167 (240858)	Loss/tok 3.1241 (3.1580)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.44e-04)	Tok/s 233060 (240903)	Loss/tok 2.8424 (3.1578)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.101 (0.089)	Data 1.08e-04 (3.41e-04)	Tok/s 252304 (240940)	Loss/tok 3.1761 (3.1592)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][860/1291]	Time 0.101 (0.089)	Data 1.09e-04 (3.38e-04)	Tok/s 249164 (241077)	Loss/tok 3.1083 (3.1594)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.036 (0.089)	Data 1.08e-04 (3.36e-04)	Tok/s 225464 (241110)	Loss/tok 2.5444 (3.1586)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.101 (0.089)	Data 1.07e-04 (3.33e-04)	Tok/s 250841 (241031)	Loss/tok 3.1105 (3.1570)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.31e-04)	Tok/s 254333 (241046)	Loss/tok 3.1092 (3.1564)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.28e-04)	Tok/s 231050 (241016)	Loss/tok 2.9901 (3.1563)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.26e-04)	Tok/s 230201 (241005)	Loss/tok 3.0165 (3.1556)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.23e-04)	Tok/s 237103 (240893)	Loss/tok 2.9837 (3.1540)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.036 (0.088)	Data 1.10e-04 (3.21e-04)	Tok/s 217651 (240906)	Loss/tok 2.5570 (3.1539)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.19e-04)	Tok/s 231139 (241004)	Loss/tok 2.8746 (3.1547)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.17e-04)	Tok/s 231622 (241023)	Loss/tok 2.9671 (3.1544)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.15e-04)	Tok/s 231669 (240911)	Loss/tok 2.9596 (3.1536)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.101 (0.088)	Data 1.08e-04 (3.13e-04)	Tok/s 251366 (240893)	Loss/tok 3.0986 (3.1532)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.11e-04)	Tok/s 234920 (240957)	Loss/tok 2.8965 (3.1530)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][990/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.08e-04)	Tok/s 230378 (241011)	Loss/tok 2.9292 (3.1541)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.07e-04)	Tok/s 229462 (240952)	Loss/tok 2.9632 (3.1531)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.101 (0.089)	Data 1.10e-04 (3.05e-04)	Tok/s 248875 (241015)	Loss/tok 3.1110 (3.1547)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.03e-04)	Tok/s 252462 (241017)	Loss/tok 3.0832 (3.1536)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.101 (0.089)	Data 1.13e-04 (3.01e-04)	Tok/s 250609 (241070)	Loss/tok 3.1506 (3.1541)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.174 (0.089)	Data 1.07e-04 (2.99e-04)	Tok/s 256129 (241079)	Loss/tok 3.4407 (3.1548)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.97e-04)	Tok/s 234265 (241029)	Loss/tok 2.9776 (3.1546)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.96e-04)	Tok/s 251664 (241034)	Loss/tok 3.0716 (3.1540)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.94e-04)	Tok/s 221253 (240993)	Loss/tok 2.4892 (3.1534)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.92e-04)	Tok/s 234114 (241012)	Loss/tok 2.9695 (3.1532)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.136 (0.089)	Data 1.12e-04 (2.91e-04)	Tok/s 256377 (241076)	Loss/tok 3.2951 (3.1539)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.100 (0.089)	Data 1.20e-04 (2.89e-04)	Tok/s 250559 (241066)	Loss/tok 3.1766 (3.1532)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.101 (0.089)	Data 1.07e-04 (2.87e-04)	Tok/s 249606 (241114)	Loss/tok 3.2269 (3.1527)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1120/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.86e-04)	Tok/s 253991 (241166)	Loss/tok 3.0903 (3.1522)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.100 (0.089)	Data 1.09e-04 (2.84e-04)	Tok/s 251952 (241187)	Loss/tok 3.1315 (3.1512)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.83e-04)	Tok/s 235477 (241203)	Loss/tok 2.8845 (3.1505)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.81e-04)	Tok/s 248140 (241293)	Loss/tok 3.1771 (3.1514)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.80e-04)	Tok/s 252314 (241324)	Loss/tok 3.1257 (3.1506)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.068 (0.089)	Data 1.11e-04 (2.78e-04)	Tok/s 232144 (241325)	Loss/tok 2.9274 (3.1507)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.77e-04)	Tok/s 235495 (241350)	Loss/tok 2.9342 (3.1512)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.100 (0.089)	Data 1.33e-04 (2.76e-04)	Tok/s 252749 (241370)	Loss/tok 2.9746 (3.1500)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.100 (0.090)	Data 1.14e-04 (2.74e-04)	Tok/s 251428 (241454)	Loss/tok 3.0837 (3.1507)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.067 (0.090)	Data 1.11e-04 (2.73e-04)	Tok/s 231193 (241427)	Loss/tok 2.8600 (3.1508)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.067 (0.090)	Data 1.09e-04 (2.72e-04)	Tok/s 230410 (241421)	Loss/tok 2.9660 (3.1508)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.067 (0.090)	Data 1.09e-04 (2.70e-04)	Tok/s 230381 (241378)	Loss/tok 3.0395 (3.1510)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.69e-04)	Tok/s 250379 (241365)	Loss/tok 3.1678 (3.1504)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1250/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.68e-04)	Tok/s 234609 (241361)	Loss/tok 2.8634 (3.1500)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.174 (0.089)	Data 1.10e-04 (2.67e-04)	Tok/s 258089 (241325)	Loss/tok 3.3806 (3.1500)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.65e-04)	Tok/s 231441 (241334)	Loss/tok 2.8865 (3.1497)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.067 (0.089)	Data 1.27e-04 (2.64e-04)	Tok/s 232019 (241310)	Loss/tok 2.8942 (3.1489)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.066 (0.089)	Data 4.29e-05 (2.66e-04)	Tok/s 232880 (241256)	Loss/tok 2.9137 (3.1479)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593144714870, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593144714870, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.480 (0.480)	Decoder iters 149.0 (149.0)	Tok/s 34106 (34106)
0: Running moses detokenizer
0: BLEU(score=24.038142403105052, counts=[36935, 18469, 10530, 6274], totals=[65209, 62206, 59203, 56204], precisions=[56.64095446947507, 29.6900620518921, 17.78626083137679, 11.162906554693617], bp=1.0, sys_len=65209, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593144716872, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2404, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593144716872, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1469	Test BLEU: 24.04
0: Performance: Epoch: 3	Training: 1929798 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593144716872, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593144716873, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 04:12:04 AM
RESULT,RNN_TRANSLATOR,,507,nvidia,2020-06-26 04:03:37 AM
