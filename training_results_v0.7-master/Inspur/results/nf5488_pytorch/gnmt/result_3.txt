Beginning trial 3 of 10
:::MLLOG {"namespace": "", "time_ms": 1593142179429, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593142179476, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593142179476, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593142179476, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593142179476, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593142181845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 03:29:42 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593142184434, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142184434, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142184434, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142184443, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142184446, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142184472, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142184474, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142184471, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 866251768
:::MLLOG {"namespace": "", "time_ms": 1593142193091, "event_type": "POINT_IN_TIME", "key": "seed", "value": 866251768, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3327182082
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
[0]
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
0: Initializing dwu fp16 optimizer
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593142206839, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593142206841, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593142206841, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593142206841, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593142206841, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593142208795, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593142208796, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593142208796, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593142209140, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593142209141, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593142209141, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593142209142, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593142209142, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593142209142, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593142209143, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593142209143, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593142209143, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593142209143, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593142209143, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142209143, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1630777500
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.348 (0.348)	Data 2.77e-01 (2.77e-01)	Tok/s 44645 (44645)	Loss/tok 10.6909 (10.6909)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.122 (0.126)	Data 1.11e-04 (3.88e-02)	Tok/s 208424 (184708)	Loss/tok 9.6594 (10.0117)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.086 (0.107)	Data 3.20e-03 (2.12e-02)	Tok/s 181409 (200545)	Loss/tok 9.1084 (9.7114)	LR 4.663e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][30/1291]	Time 0.084 (0.100)	Data 1.79e-02 (1.68e-02)	Tok/s 187993 (200830)	Loss/tok 8.8414 (9.5476)	LR 5.736e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][40/1291]	Time 0.173 (0.097)	Data 2.83e-02 (1.73e-02)	Tok/s 203188 (194690)	Loss/tok 9.0923 (9.3865)	LR 7.057e-05
0: TRAIN [0][50/1291]	Time 0.098 (0.094)	Data 1.18e-04 (1.39e-02)	Tok/s 256846 (204503)	Loss/tok 8.4840 (9.2168)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.067 (0.097)	Data 1.13e-04 (1.16e-02)	Tok/s 235650 (212228)	Loss/tok 8.2158 (9.0314)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.099 (0.094)	Data 1.16e-04 (1.00e-02)	Tok/s 259948 (216188)	Loss/tok 8.1349 (8.9119)	LR 1.408e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][80/1291]	Time 0.066 (0.094)	Data 1.16e-04 (8.79e-03)	Tok/s 232684 (219621)	Loss/tok 7.8895 (8.7971)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.132 (0.093)	Data 1.14e-04 (7.84e-03)	Tok/s 263438 (222127)	Loss/tok 8.3272 (8.7136)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.067 (0.093)	Data 1.34e-04 (7.07e-03)	Tok/s 231671 (224707)	Loss/tok 7.8195 (8.6318)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.099 (0.093)	Data 1.14e-04 (6.45e-03)	Tok/s 250663 (226220)	Loss/tok 7.9213 (8.5646)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.066 (0.093)	Data 1.13e-04 (5.92e-03)	Tok/s 237971 (228113)	Loss/tok 7.6453 (8.5021)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.066 (0.093)	Data 1.14e-04 (5.48e-03)	Tok/s 235059 (229444)	Loss/tok 7.7360 (8.4490)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.099 (0.093)	Data 1.34e-04 (5.10e-03)	Tok/s 256193 (230670)	Loss/tok 7.7232 (8.3973)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.133 (0.094)	Data 1.09e-04 (4.77e-03)	Tok/s 262618 (231686)	Loss/tok 7.8241 (8.3470)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.066 (0.094)	Data 1.37e-04 (4.48e-03)	Tok/s 233879 (232629)	Loss/tok 7.3059 (8.2915)	LR 1.093e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][170/1291]	Time 0.134 (0.094)	Data 1.14e-04 (4.23e-03)	Tok/s 262711 (233446)	Loss/tok 7.8740 (8.2466)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.066 (0.094)	Data 1.12e-04 (4.00e-03)	Tok/s 236684 (234355)	Loss/tok 6.9275 (8.1884)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.066 (0.093)	Data 1.10e-04 (3.80e-03)	Tok/s 227500 (234445)	Loss/tok 6.7998 (8.1343)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.133 (0.093)	Data 1.12e-04 (3.61e-03)	Tok/s 258758 (234756)	Loss/tok 7.0291 (8.0732)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.066 (0.093)	Data 1.12e-04 (3.45e-03)	Tok/s 231215 (235220)	Loss/tok 6.3989 (8.0094)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.066 (0.093)	Data 1.12e-04 (3.30e-03)	Tok/s 231110 (235596)	Loss/tok 6.3413 (7.9447)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.068 (0.093)	Data 1.13e-04 (3.16e-03)	Tok/s 226489 (235785)	Loss/tok 5.9920 (7.8801)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.132 (0.093)	Data 1.12e-04 (3.03e-03)	Tok/s 265789 (236272)	Loss/tok 6.3338 (7.8074)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.036 (0.093)	Data 1.10e-04 (2.92e-03)	Tok/s 224277 (236613)	Loss/tok 5.1650 (7.7430)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.066 (0.092)	Data 1.14e-04 (2.81e-03)	Tok/s 232927 (236814)	Loss/tok 5.6117 (7.6816)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.067 (0.092)	Data 1.18e-04 (2.71e-03)	Tok/s 231792 (236951)	Loss/tok 5.5871 (7.6171)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.067 (0.092)	Data 1.36e-04 (2.62e-03)	Tok/s 235036 (237368)	Loss/tok 5.3790 (7.5451)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.175 (0.092)	Data 1.11e-04 (2.53e-03)	Tok/s 256755 (237742)	Loss/tok 5.8713 (7.4726)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][300/1291]	Time 0.133 (0.092)	Data 1.17e-04 (2.45e-03)	Tok/s 259634 (237843)	Loss/tok 5.6215 (7.4082)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.136 (0.093)	Data 1.12e-04 (2.38e-03)	Tok/s 258282 (238245)	Loss/tok 5.5986 (7.3320)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.066 (0.093)	Data 1.11e-04 (2.30e-03)	Tok/s 231884 (238538)	Loss/tok 4.7785 (7.2641)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.067 (0.092)	Data 1.12e-04 (2.24e-03)	Tok/s 226707 (238539)	Loss/tok 4.8278 (7.2043)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.133 (0.093)	Data 1.10e-04 (2.18e-03)	Tok/s 265400 (238716)	Loss/tok 5.2175 (7.1411)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.136 (0.092)	Data 1.12e-04 (2.12e-03)	Tok/s 258461 (238899)	Loss/tok 5.1530 (7.0784)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.066 (0.092)	Data 1.12e-04 (2.06e-03)	Tok/s 235482 (238865)	Loss/tok 4.5240 (7.0223)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.136 (0.092)	Data 1.11e-04 (2.01e-03)	Tok/s 258454 (238956)	Loss/tok 5.1315 (6.9642)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.066 (0.092)	Data 1.09e-04 (1.96e-03)	Tok/s 234084 (238940)	Loss/tok 4.5236 (6.9128)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.099 (0.091)	Data 1.10e-04 (1.91e-03)	Tok/s 254127 (238920)	Loss/tok 4.5508 (6.8605)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.066 (0.091)	Data 1.21e-04 (1.87e-03)	Tok/s 233244 (238876)	Loss/tok 4.2825 (6.8065)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.067 (0.091)	Data 1.13e-04 (1.82e-03)	Tok/s 230115 (238964)	Loss/tok 4.1667 (6.7492)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.172 (0.091)	Data 1.12e-04 (1.78e-03)	Tok/s 257875 (239046)	Loss/tok 4.9851 (6.6948)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][430/1291]	Time 0.174 (0.091)	Data 1.12e-04 (1.75e-03)	Tok/s 253703 (239227)	Loss/tok 4.9337 (6.6369)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.067 (0.091)	Data 1.12e-04 (1.71e-03)	Tok/s 233825 (239382)	Loss/tok 4.0214 (6.5833)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.067 (0.091)	Data 1.08e-04 (1.67e-03)	Tok/s 235381 (239411)	Loss/tok 4.0682 (6.5356)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.066 (0.091)	Data 1.29e-04 (1.64e-03)	Tok/s 232072 (239310)	Loss/tok 4.0039 (6.4943)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.037 (0.091)	Data 1.13e-04 (1.61e-03)	Tok/s 219790 (239394)	Loss/tok 3.3416 (6.4484)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.037 (0.091)	Data 1.11e-04 (1.58e-03)	Tok/s 211995 (239315)	Loss/tok 3.3117 (6.4094)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.100 (0.091)	Data 1.12e-04 (1.55e-03)	Tok/s 250112 (239535)	Loss/tok 4.2990 (6.3566)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.133 (0.091)	Data 1.12e-04 (1.52e-03)	Tok/s 262880 (239630)	Loss/tok 4.3669 (6.3142)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.067 (0.091)	Data 1.13e-04 (1.49e-03)	Tok/s 228762 (239675)	Loss/tok 3.9354 (6.2713)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.066 (0.091)	Data 1.10e-04 (1.46e-03)	Tok/s 236821 (239763)	Loss/tok 3.9034 (6.2309)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.067 (0.091)	Data 1.09e-04 (1.44e-03)	Tok/s 232030 (239892)	Loss/tok 3.8460 (6.1890)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.099 (0.091)	Data 1.12e-04 (1.41e-03)	Tok/s 252772 (240024)	Loss/tok 4.1654 (6.1473)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.101 (0.091)	Data 1.10e-04 (1.39e-03)	Tok/s 248435 (240078)	Loss/tok 4.0731 (6.1107)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][560/1291]	Time 0.066 (0.091)	Data 1.11e-04 (1.37e-03)	Tok/s 233158 (240062)	Loss/tok 3.7739 (6.0770)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.100 (0.091)	Data 1.14e-04 (1.35e-03)	Tok/s 253594 (240187)	Loss/tok 4.0138 (6.0398)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.066 (0.091)	Data 1.13e-04 (1.32e-03)	Tok/s 236079 (240139)	Loss/tok 3.6342 (6.0099)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.136 (0.091)	Data 1.11e-04 (1.30e-03)	Tok/s 258004 (240178)	Loss/tok 4.3115 (5.9755)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.067 (0.091)	Data 1.44e-04 (1.28e-03)	Tok/s 232008 (240282)	Loss/tok 3.8039 (5.9419)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.100 (0.091)	Data 1.12e-04 (1.27e-03)	Tok/s 253286 (240318)	Loss/tok 4.0204 (5.9105)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.067 (0.091)	Data 1.11e-04 (1.25e-03)	Tok/s 229779 (240312)	Loss/tok 3.6928 (5.8806)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.067 (0.091)	Data 1.12e-04 (1.23e-03)	Tok/s 229654 (240323)	Loss/tok 3.7183 (5.8513)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.099 (0.091)	Data 1.11e-04 (1.21e-03)	Tok/s 254189 (240339)	Loss/tok 3.8856 (5.8233)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.067 (0.091)	Data 1.12e-04 (1.19e-03)	Tok/s 227373 (240359)	Loss/tok 3.6191 (5.7945)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.036 (0.091)	Data 1.12e-04 (1.18e-03)	Tok/s 222781 (240392)	Loss/tok 3.0637 (5.7652)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.135 (0.091)	Data 1.11e-04 (1.16e-03)	Tok/s 260452 (240394)	Loss/tok 4.1924 (5.7393)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][680/1291]	Time 0.174 (0.091)	Data 1.12e-04 (1.15e-03)	Tok/s 256161 (240425)	Loss/tok 4.3186 (5.7122)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.036 (0.090)	Data 1.10e-04 (1.13e-03)	Tok/s 221540 (240387)	Loss/tok 3.1777 (5.6895)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.066 (0.090)	Data 1.54e-04 (1.12e-03)	Tok/s 231684 (240345)	Loss/tok 3.6426 (5.6669)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.135 (0.090)	Data 1.13e-04 (1.10e-03)	Tok/s 258241 (240403)	Loss/tok 4.0282 (5.6410)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.036 (0.090)	Data 1.15e-04 (1.09e-03)	Tok/s 220236 (240349)	Loss/tok 3.0716 (5.6202)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.136 (0.090)	Data 1.16e-04 (1.08e-03)	Tok/s 257852 (240349)	Loss/tok 4.0805 (5.5973)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.099 (0.090)	Data 1.12e-04 (1.06e-03)	Tok/s 254233 (240344)	Loss/tok 3.8746 (5.5741)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.101 (0.090)	Data 1.11e-04 (1.05e-03)	Tok/s 251565 (240358)	Loss/tok 3.8007 (5.5508)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.067 (0.090)	Data 1.10e-04 (1.04e-03)	Tok/s 231210 (240432)	Loss/tok 3.5990 (5.5270)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.067 (0.090)	Data 1.11e-04 (1.03e-03)	Tok/s 231759 (240529)	Loss/tok 3.4977 (5.5019)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.172 (0.090)	Data 1.14e-04 (1.02e-03)	Tok/s 259609 (240560)	Loss/tok 4.2004 (5.4795)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.135 (0.090)	Data 1.11e-04 (1.00e-03)	Tok/s 256913 (240525)	Loss/tok 3.9971 (5.4602)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.174 (0.090)	Data 1.23e-04 (9.93e-04)	Tok/s 255589 (240512)	Loss/tok 4.2146 (5.4410)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][810/1291]	Time 0.067 (0.090)	Data 1.11e-04 (9.82e-04)	Tok/s 233459 (240605)	Loss/tok 3.5015 (5.4174)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.036 (0.090)	Data 1.35e-04 (9.71e-04)	Tok/s 222194 (240648)	Loss/tok 3.1000 (5.3975)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.036 (0.090)	Data 1.32e-04 (9.61e-04)	Tok/s 215917 (240686)	Loss/tok 3.0056 (5.3770)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.099 (0.090)	Data 1.14e-04 (9.51e-04)	Tok/s 254128 (240738)	Loss/tok 3.8017 (5.3571)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.067 (0.090)	Data 1.34e-04 (9.41e-04)	Tok/s 226426 (240795)	Loss/tok 3.5392 (5.3368)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.066 (0.090)	Data 1.13e-04 (9.31e-04)	Tok/s 231208 (240789)	Loss/tok 3.4437 (5.3193)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.067 (0.090)	Data 1.08e-04 (9.22e-04)	Tok/s 234555 (240743)	Loss/tok 3.4150 (5.3035)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.100 (0.090)	Data 1.14e-04 (9.13e-04)	Tok/s 252827 (240756)	Loss/tok 3.7463 (5.2861)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.100 (0.090)	Data 1.13e-04 (9.04e-04)	Tok/s 253522 (240790)	Loss/tok 3.6997 (5.2688)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.099 (0.090)	Data 1.12e-04 (8.95e-04)	Tok/s 255623 (240765)	Loss/tok 3.7237 (5.2532)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.100 (0.090)	Data 1.13e-04 (8.87e-04)	Tok/s 248362 (240750)	Loss/tok 3.7179 (5.2372)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.036 (0.089)	Data 1.32e-04 (8.78e-04)	Tok/s 219967 (240724)	Loss/tok 2.9110 (5.2224)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.135 (0.089)	Data 1.10e-04 (8.70e-04)	Tok/s 259810 (240751)	Loss/tok 3.8663 (5.2055)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][940/1291]	Time 0.134 (0.090)	Data 1.15e-04 (8.62e-04)	Tok/s 261397 (240822)	Loss/tok 3.8767 (5.1883)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.067 (0.089)	Data 1.10e-04 (8.54e-04)	Tok/s 231013 (240788)	Loss/tok 3.3617 (5.1734)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][960/1291]	Time 0.099 (0.090)	Data 1.10e-04 (8.47e-04)	Tok/s 253879 (240844)	Loss/tok 3.6874 (5.1570)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.099 (0.090)	Data 1.10e-04 (8.39e-04)	Tok/s 254992 (240844)	Loss/tok 3.7340 (5.1421)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.066 (0.090)	Data 1.15e-04 (8.32e-04)	Tok/s 235665 (240874)	Loss/tok 3.4683 (5.1271)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.067 (0.089)	Data 1.16e-04 (8.24e-04)	Tok/s 232342 (240875)	Loss/tok 3.4583 (5.1130)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.067 (0.089)	Data 1.11e-04 (8.17e-04)	Tok/s 235509 (240838)	Loss/tok 3.4418 (5.0998)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.067 (0.089)	Data 1.09e-04 (8.10e-04)	Tok/s 231080 (240830)	Loss/tok 3.4521 (5.0869)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.134 (0.089)	Data 1.11e-04 (8.04e-04)	Tok/s 256432 (240875)	Loss/tok 3.9441 (5.0720)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.099 (0.090)	Data 1.12e-04 (7.97e-04)	Tok/s 252991 (240960)	Loss/tok 3.7292 (5.0566)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.099 (0.089)	Data 1.14e-04 (7.90e-04)	Tok/s 250577 (240960)	Loss/tok 3.6640 (5.0444)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.174 (0.090)	Data 1.11e-04 (7.84e-04)	Tok/s 258993 (241011)	Loss/tok 3.9205 (5.0296)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.099 (0.090)	Data 1.11e-04 (7.78e-04)	Tok/s 255038 (241058)	Loss/tok 3.6861 (5.0153)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.067 (0.090)	Data 1.13e-04 (7.71e-04)	Tok/s 229916 (241041)	Loss/tok 3.5105 (5.0032)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.067 (0.090)	Data 1.14e-04 (7.65e-04)	Tok/s 229987 (241066)	Loss/tok 3.3836 (4.9902)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1090/1291]	Time 0.067 (0.090)	Data 1.13e-04 (7.59e-04)	Tok/s 236348 (241143)	Loss/tok 3.4729 (4.9764)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.134 (0.090)	Data 1.15e-04 (7.53e-04)	Tok/s 262987 (241166)	Loss/tok 3.7988 (4.9641)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1110/1291]	Time 0.067 (0.090)	Data 1.15e-04 (7.48e-04)	Tok/s 230722 (241152)	Loss/tok 3.3661 (4.9529)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.174 (0.090)	Data 1.10e-04 (7.42e-04)	Tok/s 256856 (241218)	Loss/tok 3.9724 (4.9396)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.067 (0.090)	Data 1.34e-04 (7.37e-04)	Tok/s 232168 (241243)	Loss/tok 3.3788 (4.9279)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.135 (0.090)	Data 1.26e-04 (7.31e-04)	Tok/s 260742 (241323)	Loss/tok 3.8456 (4.9147)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.067 (0.090)	Data 1.14e-04 (7.26e-04)	Tok/s 232933 (241332)	Loss/tok 3.4190 (4.9035)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.173 (0.090)	Data 1.30e-04 (7.21e-04)	Tok/s 260068 (241289)	Loss/tok 3.9024 (4.8931)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.135 (0.090)	Data 1.13e-04 (7.15e-04)	Tok/s 258322 (241300)	Loss/tok 3.8755 (4.8824)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.067 (0.090)	Data 1.11e-04 (7.10e-04)	Tok/s 228550 (241254)	Loss/tok 3.3767 (4.8729)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.067 (0.090)	Data 1.13e-04 (7.05e-04)	Tok/s 234229 (241285)	Loss/tok 3.3899 (4.8617)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.100 (0.090)	Data 1.12e-04 (7.00e-04)	Tok/s 251785 (241319)	Loss/tok 3.5553 (4.8504)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.067 (0.089)	Data 1.10e-04 (6.95e-04)	Tok/s 234374 (241300)	Loss/tok 3.3842 (4.8405)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.133 (0.089)	Data 1.11e-04 (6.91e-04)	Tok/s 261506 (241322)	Loss/tok 3.9086 (4.8300)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.067 (0.089)	Data 1.12e-04 (6.86e-04)	Tok/s 228557 (241300)	Loss/tok 3.3469 (4.8201)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1240/1291]	Time 0.067 (0.089)	Data 1.12e-04 (6.81e-04)	Tok/s 228447 (241322)	Loss/tok 3.3023 (4.8097)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.067 (0.089)	Data 1.13e-04 (6.77e-04)	Tok/s 231551 (241353)	Loss/tok 3.2480 (4.7993)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.099 (0.089)	Data 1.13e-04 (6.73e-04)	Tok/s 254623 (241337)	Loss/tok 3.5769 (4.7900)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.100 (0.089)	Data 1.12e-04 (6.68e-04)	Tok/s 251295 (241350)	Loss/tok 3.5442 (4.7801)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1280/1291]	Time 0.134 (0.089)	Data 1.11e-04 (6.64e-04)	Tok/s 260635 (241357)	Loss/tok 3.9112 (4.7707)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.067 (0.089)	Data 4.32e-05 (6.62e-04)	Tok/s 232336 (241331)	Loss/tok 3.3242 (4.7620)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593142325157, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142325158, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.487 (0.487)	Decoder iters 149.0 (149.0)	Tok/s 33028 (33028)
0: Running moses detokenizer
0: BLEU(score=20.047495685479518, counts=[34166, 15729, 8410, 4651], totals=[64436, 61433, 58430, 55432], precisions=[53.0231547582097, 25.603503003271857, 14.393291117576588, 8.39046038389378], bp=0.996282301830049, sys_len=64436, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593142327160, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2005, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142327161, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7641	Test BLEU: 20.05
0: Performance: Epoch: 0	Training: 1930617 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593142327161, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142327161, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142327161, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 394447372
0: TRAIN [1][0/1291]	Time 0.308 (0.308)	Data 1.89e-01 (1.89e-01)	Tok/s 81062 (81062)	Loss/tok 3.5654 (3.5654)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.068 (0.098)	Data 1.13e-04 (1.73e-02)	Tok/s 220993 (224453)	Loss/tok 3.3254 (3.4043)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.173 (0.100)	Data 1.15e-04 (9.12e-03)	Tok/s 256290 (233700)	Loss/tok 3.9834 (3.5280)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.067 (0.097)	Data 1.37e-04 (6.22e-03)	Tok/s 226486 (236641)	Loss/tok 3.2996 (3.5158)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.068 (0.096)	Data 1.29e-04 (4.73e-03)	Tok/s 231149 (238294)	Loss/tok 3.3222 (3.5214)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.100 (0.097)	Data 1.25e-04 (3.83e-03)	Tok/s 253420 (240265)	Loss/tok 3.4744 (3.5283)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.067 (0.095)	Data 1.17e-04 (3.22e-03)	Tok/s 226704 (240111)	Loss/tok 3.3390 (3.5135)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.067 (0.094)	Data 1.19e-04 (2.78e-03)	Tok/s 232589 (240282)	Loss/tok 3.2912 (3.5104)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.099 (0.094)	Data 1.13e-04 (2.45e-03)	Tok/s 252224 (240749)	Loss/tok 3.4273 (3.5025)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.067 (0.094)	Data 1.13e-04 (2.20e-03)	Tok/s 235029 (241395)	Loss/tok 3.2969 (3.5023)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.174 (0.097)	Data 1.12e-04 (1.99e-03)	Tok/s 258000 (241994)	Loss/tok 3.7610 (3.5265)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][110/1291]	Time 0.099 (0.094)	Data 1.28e-04 (1.82e-03)	Tok/s 258256 (241431)	Loss/tok 3.6732 (3.5198)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.067 (0.094)	Data 1.16e-04 (1.68e-03)	Tok/s 234985 (241706)	Loss/tok 3.2103 (3.5184)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.099 (0.094)	Data 1.11e-04 (1.56e-03)	Tok/s 256199 (241756)	Loss/tok 3.5074 (3.5216)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.067 (0.094)	Data 1.14e-04 (1.46e-03)	Tok/s 232366 (241946)	Loss/tok 3.1766 (3.5130)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.134 (0.093)	Data 1.11e-04 (1.37e-03)	Tok/s 260163 (241907)	Loss/tok 3.7968 (3.5101)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.099 (0.092)	Data 1.13e-04 (1.29e-03)	Tok/s 253962 (241656)	Loss/tok 3.4938 (3.5063)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.100 (0.093)	Data 1.37e-04 (1.22e-03)	Tok/s 255608 (241919)	Loss/tok 3.3769 (3.5081)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.101 (0.093)	Data 1.14e-04 (1.17e-03)	Tok/s 248546 (241780)	Loss/tok 3.5611 (3.5112)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.098 (0.092)	Data 1.15e-04 (1.11e-03)	Tok/s 255071 (241562)	Loss/tok 3.4679 (3.5052)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.100 (0.092)	Data 1.21e-04 (1.06e-03)	Tok/s 250142 (241660)	Loss/tok 3.4859 (3.5049)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.099 (0.092)	Data 1.15e-04 (1.02e-03)	Tok/s 254149 (241628)	Loss/tok 3.4368 (3.5052)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.036 (0.091)	Data 1.54e-04 (9.77e-04)	Tok/s 220156 (241419)	Loss/tok 2.7341 (3.4999)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][230/1291]	Time 0.067 (0.091)	Data 1.41e-04 (9.40e-04)	Tok/s 227157 (241417)	Loss/tok 3.1754 (3.4987)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.099 (0.091)	Data 1.14e-04 (9.06e-04)	Tok/s 252238 (241377)	Loss/tok 3.5925 (3.4954)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.174 (0.090)	Data 1.29e-04 (8.74e-04)	Tok/s 255985 (241248)	Loss/tok 3.8791 (3.4948)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.135 (0.091)	Data 1.14e-04 (8.45e-04)	Tok/s 260052 (241463)	Loss/tok 3.5843 (3.4952)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.036 (0.091)	Data 1.39e-04 (8.19e-04)	Tok/s 218850 (241499)	Loss/tok 2.7338 (3.4971)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.099 (0.091)	Data 1.18e-04 (7.94e-04)	Tok/s 256147 (241762)	Loss/tok 3.4643 (3.4973)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.099 (0.091)	Data 1.22e-04 (7.71e-04)	Tok/s 252358 (241734)	Loss/tok 3.4672 (3.4979)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.036 (0.091)	Data 1.12e-04 (7.49e-04)	Tok/s 221584 (241605)	Loss/tok 2.8324 (3.4981)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.099 (0.090)	Data 1.17e-04 (7.29e-04)	Tok/s 253871 (241525)	Loss/tok 3.4096 (3.4941)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.100 (0.090)	Data 1.22e-04 (7.10e-04)	Tok/s 252649 (241425)	Loss/tok 3.4658 (3.4912)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.067 (0.090)	Data 1.15e-04 (6.92e-04)	Tok/s 231580 (241400)	Loss/tok 3.2053 (3.4882)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.067 (0.090)	Data 1.16e-04 (6.75e-04)	Tok/s 233281 (241530)	Loss/tok 3.2489 (3.4864)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.100 (0.090)	Data 1.14e-04 (6.59e-04)	Tok/s 247495 (241608)	Loss/tok 3.3834 (3.4876)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][360/1291]	Time 0.135 (0.090)	Data 1.15e-04 (6.44e-04)	Tok/s 261296 (241734)	Loss/tok 3.6324 (3.4873)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.134 (0.090)	Data 1.40e-04 (6.30e-04)	Tok/s 260828 (241922)	Loss/tok 3.6558 (3.4874)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.036 (0.090)	Data 1.15e-04 (6.16e-04)	Tok/s 219859 (241845)	Loss/tok 2.7456 (3.4841)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.099 (0.090)	Data 1.16e-04 (6.04e-04)	Tok/s 253942 (241779)	Loss/tok 3.5665 (3.4831)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.135 (0.090)	Data 1.15e-04 (5.91e-04)	Tok/s 256992 (241723)	Loss/tok 3.7542 (3.4832)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.099 (0.090)	Data 1.18e-04 (5.80e-04)	Tok/s 251333 (241823)	Loss/tok 3.4840 (3.4818)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.134 (0.090)	Data 1.13e-04 (5.69e-04)	Tok/s 259251 (241888)	Loss/tok 3.6269 (3.4822)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.136 (0.090)	Data 1.35e-04 (5.59e-04)	Tok/s 256126 (241926)	Loss/tok 3.7236 (3.4809)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.134 (0.090)	Data 1.13e-04 (5.49e-04)	Tok/s 262747 (241876)	Loss/tok 3.5414 (3.4787)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.099 (0.090)	Data 1.17e-04 (5.39e-04)	Tok/s 253928 (242009)	Loss/tok 3.5169 (3.4796)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.036 (0.090)	Data 1.14e-04 (5.30e-04)	Tok/s 215279 (242022)	Loss/tok 2.7598 (3.4808)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.102 (0.090)	Data 1.16e-04 (5.21e-04)	Tok/s 246693 (242082)	Loss/tok 3.4533 (3.4825)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.036 (0.090)	Data 1.27e-04 (5.13e-04)	Tok/s 220941 (242057)	Loss/tok 2.8217 (3.4807)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][490/1291]	Time 0.067 (0.090)	Data 1.16e-04 (5.05e-04)	Tok/s 230430 (242031)	Loss/tok 3.2183 (3.4820)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.100 (0.091)	Data 1.15e-04 (4.97e-04)	Tok/s 251846 (242089)	Loss/tok 3.3738 (3.4836)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.100 (0.091)	Data 1.18e-04 (4.90e-04)	Tok/s 252507 (242159)	Loss/tok 3.5077 (3.4831)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.066 (0.091)	Data 1.20e-04 (4.83e-04)	Tok/s 235690 (242280)	Loss/tok 3.2245 (3.4851)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.036 (0.091)	Data 1.15e-04 (4.76e-04)	Tok/s 220866 (242126)	Loss/tok 2.6813 (3.4818)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.067 (0.091)	Data 1.36e-04 (4.69e-04)	Tok/s 230802 (242162)	Loss/tok 3.1754 (3.4812)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.067 (0.091)	Data 1.15e-04 (4.63e-04)	Tok/s 227140 (242125)	Loss/tok 3.2764 (3.4802)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.099 (0.090)	Data 1.18e-04 (4.56e-04)	Tok/s 257064 (242123)	Loss/tok 3.4240 (3.4778)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.135 (0.091)	Data 1.13e-04 (4.50e-04)	Tok/s 260964 (242182)	Loss/tok 3.5659 (3.4772)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.100 (0.091)	Data 1.24e-04 (4.45e-04)	Tok/s 254858 (242176)	Loss/tok 3.4651 (3.4768)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.067 (0.091)	Data 1.13e-04 (4.39e-04)	Tok/s 230223 (242161)	Loss/tok 3.2026 (3.4757)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.099 (0.090)	Data 1.31e-04 (4.34e-04)	Tok/s 253815 (242052)	Loss/tok 3.4183 (3.4729)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][610/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.29e-04)	Tok/s 231210 (242100)	Loss/tok 3.1391 (3.4721)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.067 (0.090)	Data 1.34e-04 (4.24e-04)	Tok/s 235181 (242058)	Loss/tok 3.1688 (3.4706)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.099 (0.090)	Data 1.19e-04 (4.19e-04)	Tok/s 253978 (242147)	Loss/tok 3.3500 (3.4693)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][640/1291]	Time 0.100 (0.090)	Data 1.13e-04 (4.14e-04)	Tok/s 250234 (242131)	Loss/tok 3.4720 (3.4718)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.037 (0.090)	Data 1.13e-04 (4.10e-04)	Tok/s 210058 (242103)	Loss/tok 2.7017 (3.4706)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.068 (0.090)	Data 1.15e-04 (4.05e-04)	Tok/s 227166 (242053)	Loss/tok 3.2978 (3.4692)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.036 (0.090)	Data 1.15e-04 (4.01e-04)	Tok/s 213919 (241986)	Loss/tok 2.6654 (3.4683)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.100 (0.090)	Data 1.31e-04 (3.97e-04)	Tok/s 252299 (241985)	Loss/tok 3.4438 (3.4660)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.135 (0.090)	Data 1.35e-04 (3.93e-04)	Tok/s 259514 (242054)	Loss/tok 3.5634 (3.4656)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.101 (0.090)	Data 1.12e-04 (3.89e-04)	Tok/s 251322 (242010)	Loss/tok 3.3511 (3.4637)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.067 (0.090)	Data 1.17e-04 (3.85e-04)	Tok/s 228285 (242026)	Loss/tok 3.1718 (3.4646)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.101 (0.090)	Data 1.33e-04 (3.81e-04)	Tok/s 246647 (242023)	Loss/tok 3.3469 (3.4628)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.78e-04)	Tok/s 236417 (242107)	Loss/tok 3.1627 (3.4649)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.136 (0.090)	Data 1.16e-04 (3.74e-04)	Tok/s 255330 (242154)	Loss/tok 3.6028 (3.4643)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.136 (0.091)	Data 1.21e-04 (3.71e-04)	Tok/s 259493 (242187)	Loss/tok 3.6124 (3.4636)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.069 (0.091)	Data 1.14e-04 (3.67e-04)	Tok/s 227099 (242282)	Loss/tok 3.1848 (3.4628)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][770/1291]	Time 0.068 (0.091)	Data 1.14e-04 (3.64e-04)	Tok/s 228640 (242244)	Loss/tok 3.1807 (3.4617)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.037 (0.091)	Data 1.13e-04 (3.61e-04)	Tok/s 215654 (242156)	Loss/tok 2.7125 (3.4611)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.100 (0.090)	Data 1.20e-04 (3.58e-04)	Tok/s 248838 (242117)	Loss/tok 3.4075 (3.4604)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.101 (0.090)	Data 1.16e-04 (3.55e-04)	Tok/s 251903 (242073)	Loss/tok 3.4926 (3.4595)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.52e-04)	Tok/s 227671 (241963)	Loss/tok 3.1413 (3.4584)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.067 (0.090)	Data 1.32e-04 (3.49e-04)	Tok/s 234832 (241970)	Loss/tok 3.1948 (3.4582)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.101 (0.090)	Data 1.19e-04 (3.46e-04)	Tok/s 248393 (242019)	Loss/tok 3.3526 (3.4584)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.44e-04)	Tok/s 232088 (241888)	Loss/tok 3.2653 (3.4564)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.41e-04)	Tok/s 229877 (241872)	Loss/tok 3.2098 (3.4553)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.068 (0.090)	Data 1.14e-04 (3.39e-04)	Tok/s 231650 (241831)	Loss/tok 3.1798 (3.4543)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.067 (0.090)	Data 1.20e-04 (3.36e-04)	Tok/s 232719 (241807)	Loss/tok 3.1851 (3.4541)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.100 (0.090)	Data 1.16e-04 (3.34e-04)	Tok/s 256205 (241796)	Loss/tok 3.5257 (3.4535)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][890/1291]	Time 0.100 (0.090)	Data 1.30e-04 (3.31e-04)	Tok/s 254103 (241856)	Loss/tok 3.3404 (3.4532)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.036 (0.090)	Data 1.41e-04 (3.29e-04)	Tok/s 217750 (241868)	Loss/tok 2.8220 (3.4540)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.101 (0.090)	Data 1.14e-04 (3.27e-04)	Tok/s 248060 (241849)	Loss/tok 3.3600 (3.4539)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.101 (0.090)	Data 1.13e-04 (3.24e-04)	Tok/s 247782 (241892)	Loss/tok 3.3594 (3.4537)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][930/1291]	Time 0.136 (0.090)	Data 1.13e-04 (3.22e-04)	Tok/s 259026 (241960)	Loss/tok 3.4649 (3.4538)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.036 (0.091)	Data 1.15e-04 (3.20e-04)	Tok/s 218146 (241999)	Loss/tok 2.8682 (3.4540)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.135 (0.091)	Data 1.36e-04 (3.18e-04)	Tok/s 258764 (242003)	Loss/tok 3.6321 (3.4533)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.16e-04)	Tok/s 231355 (241936)	Loss/tok 3.2021 (3.4513)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.137 (0.091)	Data 1.36e-04 (3.14e-04)	Tok/s 255867 (242037)	Loss/tok 3.4664 (3.4511)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.104 (0.090)	Data 1.16e-04 (3.12e-04)	Tok/s 241739 (241962)	Loss/tok 3.3979 (3.4494)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.099 (0.090)	Data 1.15e-04 (3.10e-04)	Tok/s 254819 (241941)	Loss/tok 3.3714 (3.4487)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.068 (0.090)	Data 1.14e-04 (3.08e-04)	Tok/s 229068 (241872)	Loss/tok 3.1881 (3.4472)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.06e-04)	Tok/s 229916 (241876)	Loss/tok 3.1616 (3.4460)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.068 (0.090)	Data 1.15e-04 (3.04e-04)	Tok/s 226536 (241923)	Loss/tok 3.2280 (3.4462)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.02e-04)	Tok/s 232892 (241895)	Loss/tok 3.1216 (3.4457)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.136 (0.090)	Data 1.13e-04 (3.01e-04)	Tok/s 257494 (241865)	Loss/tok 3.6457 (3.4454)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1050/1291]	Time 0.066 (0.090)	Data 1.15e-04 (2.99e-04)	Tok/s 232249 (241844)	Loss/tok 3.2258 (3.4448)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.037 (0.090)	Data 1.15e-04 (2.97e-04)	Tok/s 219677 (241755)	Loss/tok 2.7768 (3.4434)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.036 (0.090)	Data 1.13e-04 (2.95e-04)	Tok/s 219404 (241706)	Loss/tok 2.6863 (3.4421)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.100 (0.090)	Data 1.15e-04 (2.94e-04)	Tok/s 254302 (241692)	Loss/tok 3.3574 (3.4410)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.099 (0.090)	Data 1.15e-04 (2.92e-04)	Tok/s 256409 (241693)	Loss/tok 3.3430 (3.4410)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.135 (0.090)	Data 1.16e-04 (2.91e-04)	Tok/s 258994 (241717)	Loss/tok 3.5571 (3.4404)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.067 (0.090)	Data 1.15e-04 (2.89e-04)	Tok/s 225797 (241718)	Loss/tok 3.1572 (3.4401)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.101 (0.090)	Data 1.17e-04 (2.87e-04)	Tok/s 250518 (241701)	Loss/tok 3.3466 (3.4389)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.173 (0.090)	Data 1.14e-04 (2.86e-04)	Tok/s 259126 (241693)	Loss/tok 3.6457 (3.4381)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.101 (0.090)	Data 1.13e-04 (2.84e-04)	Tok/s 251286 (241682)	Loss/tok 3.3550 (3.4377)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.068 (0.090)	Data 1.14e-04 (2.83e-04)	Tok/s 227900 (241661)	Loss/tok 3.1715 (3.4366)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.100 (0.090)	Data 1.12e-04 (2.82e-04)	Tok/s 250106 (241669)	Loss/tok 3.4137 (3.4364)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.067 (0.090)	Data 1.12e-04 (2.80e-04)	Tok/s 227721 (241669)	Loss/tok 3.2106 (3.4360)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1180/1291]	Time 0.036 (0.090)	Data 1.16e-04 (2.79e-04)	Tok/s 216700 (241638)	Loss/tok 2.6708 (3.4353)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.067 (0.090)	Data 1.14e-04 (2.77e-04)	Tok/s 233928 (241633)	Loss/tok 3.1225 (3.4351)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.101 (0.089)	Data 1.15e-04 (2.76e-04)	Tok/s 255292 (241601)	Loss/tok 3.2859 (3.4338)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.066 (0.089)	Data 1.15e-04 (2.75e-04)	Tok/s 232438 (241601)	Loss/tok 3.1518 (3.4327)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.068 (0.089)	Data 1.14e-04 (2.73e-04)	Tok/s 226649 (241553)	Loss/tok 3.0611 (3.4316)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.068 (0.089)	Data 1.16e-04 (2.72e-04)	Tok/s 229177 (241567)	Loss/tok 3.1307 (3.4308)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.101 (0.089)	Data 1.14e-04 (2.71e-04)	Tok/s 247150 (241606)	Loss/tok 3.3634 (3.4304)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.067 (0.089)	Data 1.20e-04 (2.70e-04)	Tok/s 234772 (241589)	Loss/tok 3.1330 (3.4294)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.69e-04)	Tok/s 234007 (241585)	Loss/tok 3.1299 (3.4285)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.68e-04)	Tok/s 225415 (241572)	Loss/tok 3.0596 (3.4277)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.067 (0.089)	Data 1.18e-04 (2.66e-04)	Tok/s 226026 (241583)	Loss/tok 3.1672 (3.4278)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.067 (0.089)	Data 4.17e-05 (2.68e-04)	Tok/s 232405 (241521)	Loss/tok 3.1199 (3.4264)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593142443080, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593142443080, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.483 (0.483)	Decoder iters 149.0 (149.0)	Tok/s 32187 (32187)
0: Running moses detokenizer
0: BLEU(score=21.394778033019605, counts=[34964, 16581, 9064, 5172], totals=[63678, 60675, 57672, 54675], precisions=[54.90750337636232, 27.327564894932014, 15.716465529199612, 9.459533607681756], bp=0.9844495733508278, sys_len=63678, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593142445183, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2139, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593142445183, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4253	Test BLEU: 21.39
0: Performance: Epoch: 1	Training: 1931492 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593142445184, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593142445184, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142445184, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 4241940708
0: TRAIN [2][0/1291]	Time 0.305 (0.305)	Data 1.89e-01 (1.89e-01)	Tok/s 81228 (81228)	Loss/tok 3.3203 (3.3203)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][10/1291]	Time 0.068 (0.110)	Data 1.16e-04 (1.73e-02)	Tok/s 229287 (227979)	Loss/tok 3.1154 (3.3198)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.099 (0.100)	Data 1.14e-04 (9.10e-03)	Tok/s 253376 (234692)	Loss/tok 3.3045 (3.3054)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.067 (0.092)	Data 1.35e-04 (6.21e-03)	Tok/s 232256 (236042)	Loss/tok 3.1070 (3.2681)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][40/1291]	Time 0.067 (0.088)	Data 1.17e-04 (4.72e-03)	Tok/s 230705 (235549)	Loss/tok 3.0253 (3.2593)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][50/1291]	Time 0.067 (0.089)	Data 1.25e-04 (3.82e-03)	Tok/s 233904 (236839)	Loss/tok 3.0584 (3.2808)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.100 (0.089)	Data 1.38e-04 (3.21e-03)	Tok/s 251406 (238144)	Loss/tok 3.2186 (3.2755)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.067 (0.090)	Data 1.14e-04 (2.78e-03)	Tok/s 230669 (238292)	Loss/tok 3.0810 (3.2860)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.100 (0.088)	Data 1.15e-04 (2.45e-03)	Tok/s 253323 (238234)	Loss/tok 3.2137 (3.2714)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.067 (0.088)	Data 1.21e-04 (2.19e-03)	Tok/s 229725 (238351)	Loss/tok 3.0954 (3.2745)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.067 (0.087)	Data 1.15e-04 (1.99e-03)	Tok/s 232371 (238427)	Loss/tok 3.0373 (3.2697)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.099 (0.089)	Data 1.29e-04 (1.82e-03)	Tok/s 252930 (239614)	Loss/tok 3.2656 (3.2827)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.067 (0.089)	Data 1.26e-04 (1.68e-03)	Tok/s 235018 (239708)	Loss/tok 3.1162 (3.2834)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.099 (0.089)	Data 1.15e-04 (1.56e-03)	Tok/s 254242 (240006)	Loss/tok 3.3234 (3.2867)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.135 (0.090)	Data 1.17e-04 (1.46e-03)	Tok/s 260375 (240688)	Loss/tok 3.4052 (3.2918)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.100 (0.090)	Data 1.24e-04 (1.37e-03)	Tok/s 250867 (240709)	Loss/tok 3.2786 (3.2900)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.135 (0.091)	Data 1.21e-04 (1.29e-03)	Tok/s 258105 (240850)	Loss/tok 3.4403 (3.2984)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][170/1291]	Time 0.100 (0.090)	Data 1.14e-04 (1.22e-03)	Tok/s 249980 (240755)	Loss/tok 3.3212 (3.2991)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.100 (0.091)	Data 1.14e-04 (1.16e-03)	Tok/s 250366 (240890)	Loss/tok 3.4159 (3.3058)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.100 (0.090)	Data 1.17e-04 (1.11e-03)	Tok/s 251893 (240903)	Loss/tok 3.2883 (3.3023)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.067 (0.091)	Data 1.13e-04 (1.06e-03)	Tok/s 231186 (241148)	Loss/tok 3.0614 (3.3048)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.067 (0.091)	Data 1.15e-04 (1.01e-03)	Tok/s 231921 (241316)	Loss/tok 3.0323 (3.3014)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.174 (0.091)	Data 1.34e-04 (9.73e-04)	Tok/s 254542 (241547)	Loss/tok 3.6780 (3.3044)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.067 (0.091)	Data 1.11e-04 (9.36e-04)	Tok/s 230588 (241470)	Loss/tok 3.0283 (3.3029)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.067 (0.091)	Data 1.13e-04 (9.01e-04)	Tok/s 235270 (241553)	Loss/tok 3.0959 (3.3025)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.100 (0.091)	Data 1.13e-04 (8.70e-04)	Tok/s 252726 (241559)	Loss/tok 3.2981 (3.3009)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.067 (0.090)	Data 1.22e-04 (8.41e-04)	Tok/s 231892 (241460)	Loss/tok 3.1290 (3.2973)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.067 (0.091)	Data 1.17e-04 (8.15e-04)	Tok/s 232539 (241518)	Loss/tok 3.0829 (3.3005)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.036 (0.090)	Data 1.13e-04 (7.90e-04)	Tok/s 218549 (241286)	Loss/tok 2.6808 (3.2968)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.100 (0.090)	Data 1.15e-04 (7.66e-04)	Tok/s 253168 (241367)	Loss/tok 3.2196 (3.2951)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][300/1291]	Time 0.134 (0.090)	Data 1.15e-04 (7.45e-04)	Tok/s 262591 (241420)	Loss/tok 3.5404 (3.2958)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.067 (0.090)	Data 1.12e-04 (7.25e-04)	Tok/s 228330 (241442)	Loss/tok 3.0518 (3.2967)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.067 (0.090)	Data 1.14e-04 (7.06e-04)	Tok/s 233354 (241365)	Loss/tok 3.1316 (3.2963)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.036 (0.090)	Data 1.12e-04 (6.88e-04)	Tok/s 221516 (241455)	Loss/tok 2.7070 (3.2992)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.067 (0.090)	Data 1.14e-04 (6.71e-04)	Tok/s 232382 (241548)	Loss/tok 3.1299 (3.3003)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.099 (0.090)	Data 1.16e-04 (6.55e-04)	Tok/s 252703 (241781)	Loss/tok 3.3289 (3.3009)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][360/1291]	Time 0.099 (0.091)	Data 1.15e-04 (6.41e-04)	Tok/s 255417 (241826)	Loss/tok 3.3609 (3.3014)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.036 (0.091)	Data 1.31e-04 (6.26e-04)	Tok/s 221494 (241872)	Loss/tok 2.6290 (3.3011)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.100 (0.091)	Data 1.33e-04 (6.13e-04)	Tok/s 249790 (242019)	Loss/tok 3.3303 (3.3016)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.067 (0.091)	Data 1.12e-04 (6.01e-04)	Tok/s 234058 (241952)	Loss/tok 3.0387 (3.3002)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.100 (0.090)	Data 1.34e-04 (5.89e-04)	Tok/s 254827 (241885)	Loss/tok 3.2520 (3.3004)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.101 (0.090)	Data 1.13e-04 (5.77e-04)	Tok/s 248615 (241862)	Loss/tok 3.2503 (3.2977)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.100 (0.090)	Data 1.12e-04 (5.66e-04)	Tok/s 253707 (241824)	Loss/tok 3.3794 (3.2957)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.067 (0.090)	Data 1.12e-04 (5.56e-04)	Tok/s 227481 (241659)	Loss/tok 3.1005 (3.2932)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.135 (0.090)	Data 1.18e-04 (5.46e-04)	Tok/s 259340 (241767)	Loss/tok 3.4609 (3.2940)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.36e-04)	Tok/s 235952 (241747)	Loss/tok 3.1452 (3.2947)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.27e-04)	Tok/s 227478 (241760)	Loss/tok 3.1183 (3.2935)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.067 (0.090)	Data 1.13e-04 (5.19e-04)	Tok/s 233994 (241827)	Loss/tok 2.9818 (3.2934)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.067 (0.090)	Data 1.14e-04 (5.10e-04)	Tok/s 232105 (241768)	Loss/tok 3.0737 (3.2939)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][490/1291]	Time 0.036 (0.089)	Data 1.13e-04 (5.02e-04)	Tok/s 219663 (241579)	Loss/tok 2.7218 (3.2926)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][500/1291]	Time 0.067 (0.089)	Data 1.17e-04 (4.95e-04)	Tok/s 233173 (241551)	Loss/tok 3.1050 (3.2912)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.87e-04)	Tok/s 233903 (241582)	Loss/tok 3.0100 (3.2903)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.100 (0.089)	Data 1.21e-04 (4.80e-04)	Tok/s 251941 (241506)	Loss/tok 3.3097 (3.2893)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.135 (0.089)	Data 1.18e-04 (4.73e-04)	Tok/s 258921 (241569)	Loss/tok 3.3923 (3.2905)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.67e-04)	Tok/s 227195 (241514)	Loss/tok 3.0598 (3.2901)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.100 (0.089)	Data 1.36e-04 (4.60e-04)	Tok/s 252531 (241487)	Loss/tok 3.1941 (3.2886)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.067 (0.089)	Data 1.24e-04 (4.54e-04)	Tok/s 230426 (241407)	Loss/tok 3.0390 (3.2876)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][570/1291]	Time 0.066 (0.089)	Data 1.15e-04 (4.48e-04)	Tok/s 238404 (241356)	Loss/tok 3.0263 (3.2877)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.036 (0.088)	Data 1.13e-04 (4.43e-04)	Tok/s 218674 (241297)	Loss/tok 2.6388 (3.2860)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.100 (0.088)	Data 1.13e-04 (4.37e-04)	Tok/s 254191 (241314)	Loss/tok 3.3093 (3.2855)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.134 (0.089)	Data 1.18e-04 (4.32e-04)	Tok/s 259831 (241415)	Loss/tok 3.4247 (3.2863)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.067 (0.089)	Data 1.15e-04 (4.27e-04)	Tok/s 236447 (241436)	Loss/tok 3.1096 (3.2864)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.174 (0.089)	Data 1.32e-04 (4.22e-04)	Tok/s 256344 (241444)	Loss/tok 3.6660 (3.2871)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.17e-04)	Tok/s 230379 (241399)	Loss/tok 3.0876 (3.2858)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.067 (0.088)	Data 1.39e-04 (4.12e-04)	Tok/s 230701 (241377)	Loss/tok 3.0382 (3.2849)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.134 (0.089)	Data 1.13e-04 (4.08e-04)	Tok/s 261428 (241515)	Loss/tok 3.3874 (3.2867)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.036 (0.089)	Data 1.12e-04 (4.04e-04)	Tok/s 226642 (241412)	Loss/tok 2.6857 (3.2849)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.99e-04)	Tok/s 227819 (241458)	Loss/tok 3.1550 (3.2854)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.100 (0.088)	Data 1.14e-04 (3.95e-04)	Tok/s 253858 (241434)	Loss/tok 3.2460 (3.2840)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.035 (0.089)	Data 1.15e-04 (3.91e-04)	Tok/s 221633 (241412)	Loss/tok 2.6632 (3.2855)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][700/1291]	Time 0.135 (0.089)	Data 1.15e-04 (3.87e-04)	Tok/s 257051 (241444)	Loss/tok 3.4844 (3.2853)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.135 (0.089)	Data 1.14e-04 (3.83e-04)	Tok/s 262180 (241497)	Loss/tok 3.4590 (3.2861)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.80e-04)	Tok/s 233809 (241507)	Loss/tok 3.0497 (3.2850)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.76e-04)	Tok/s 230546 (241520)	Loss/tok 3.0723 (3.2847)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.036 (0.088)	Data 1.15e-04 (3.73e-04)	Tok/s 223542 (241514)	Loss/tok 2.6007 (3.2840)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.036 (0.089)	Data 1.17e-04 (3.69e-04)	Tok/s 223031 (241525)	Loss/tok 2.6840 (3.2841)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.66e-04)	Tok/s 231286 (241545)	Loss/tok 3.1252 (3.2839)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.067 (0.088)	Data 1.36e-04 (3.63e-04)	Tok/s 234539 (241430)	Loss/tok 3.0435 (3.2834)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.60e-04)	Tok/s 256816 (241411)	Loss/tok 3.2450 (3.2826)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.100 (0.088)	Data 1.14e-04 (3.57e-04)	Tok/s 252352 (241459)	Loss/tok 3.3115 (3.2828)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.067 (0.088)	Data 1.14e-04 (3.54e-04)	Tok/s 233401 (241394)	Loss/tok 3.1450 (3.2814)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.174 (0.088)	Data 1.14e-04 (3.51e-04)	Tok/s 255923 (241436)	Loss/tok 3.5540 (3.2826)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.135 (0.088)	Data 1.15e-04 (3.48e-04)	Tok/s 263742 (241479)	Loss/tok 3.4012 (3.2823)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][830/1291]	Time 0.036 (0.088)	Data 1.14e-04 (3.45e-04)	Tok/s 228262 (241524)	Loss/tok 2.7076 (3.2830)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][840/1291]	Time 0.066 (0.089)	Data 1.22e-04 (3.42e-04)	Tok/s 225207 (241544)	Loss/tok 3.1312 (3.2843)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.036 (0.089)	Data 1.19e-04 (3.40e-04)	Tok/s 222589 (241538)	Loss/tok 2.5654 (3.2846)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.37e-04)	Tok/s 253773 (241579)	Loss/tok 3.2271 (3.2856)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.35e-04)	Tok/s 235532 (241597)	Loss/tok 2.9943 (3.2853)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.32e-04)	Tok/s 232298 (241624)	Loss/tok 3.1232 (3.2849)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.099 (0.089)	Data 1.49e-04 (3.30e-04)	Tok/s 252417 (241677)	Loss/tok 3.2393 (3.2861)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.27e-04)	Tok/s 234645 (241642)	Loss/tok 2.9561 (3.2852)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.25e-04)	Tok/s 253969 (241711)	Loss/tok 3.2728 (3.2863)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.23e-04)	Tok/s 251347 (241683)	Loss/tok 3.2689 (3.2853)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.036 (0.089)	Data 1.14e-04 (3.21e-04)	Tok/s 214108 (241636)	Loss/tok 2.6009 (3.2842)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.18e-04)	Tok/s 236055 (241722)	Loss/tok 3.1309 (3.2841)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.16e-04)	Tok/s 232481 (241730)	Loss/tok 3.0402 (3.2835)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.14e-04)	Tok/s 233060 (241653)	Loss/tok 3.0361 (3.2826)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][970/1291]	Time 0.100 (0.089)	Data 1.17e-04 (3.12e-04)	Tok/s 253155 (241706)	Loss/tok 3.2377 (3.2832)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.136 (0.089)	Data 1.13e-04 (3.10e-04)	Tok/s 261377 (241782)	Loss/tok 3.4037 (3.2848)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.08e-04)	Tok/s 250829 (241756)	Loss/tok 3.2401 (3.2839)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.135 (0.089)	Data 1.14e-04 (3.06e-04)	Tok/s 258081 (241802)	Loss/tok 3.4331 (3.2852)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.05e-04)	Tok/s 232963 (241784)	Loss/tok 3.1063 (3.2849)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.03e-04)	Tok/s 231510 (241774)	Loss/tok 3.0404 (3.2850)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.01e-04)	Tok/s 251949 (241775)	Loss/tok 3.1778 (3.2843)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.067 (0.089)	Data 1.37e-04 (2.99e-04)	Tok/s 236553 (241820)	Loss/tok 3.0128 (3.2846)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.067 (0.089)	Data 1.20e-04 (2.97e-04)	Tok/s 232715 (241807)	Loss/tok 3.0559 (3.2842)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.96e-04)	Tok/s 231108 (241735)	Loss/tok 3.0589 (3.2829)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.94e-04)	Tok/s 230565 (241686)	Loss/tok 3.0581 (3.2817)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.93e-04)	Tok/s 232318 (241741)	Loss/tok 3.1093 (3.2819)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.100 (0.088)	Data 1.17e-04 (2.91e-04)	Tok/s 251820 (241697)	Loss/tok 3.2927 (3.2807)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1100/1291]	Time 0.067 (0.089)	Data 1.37e-04 (2.89e-04)	Tok/s 235018 (241719)	Loss/tok 3.0037 (3.2820)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1110/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.88e-04)	Tok/s 230148 (241724)	Loss/tok 3.1626 (3.2816)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.101 (0.089)	Data 1.40e-04 (2.86e-04)	Tok/s 247483 (241743)	Loss/tok 3.2174 (3.2809)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.100 (0.089)	Data 1.17e-04 (2.85e-04)	Tok/s 249901 (241734)	Loss/tok 3.2650 (3.2806)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.83e-04)	Tok/s 252879 (241714)	Loss/tok 3.2972 (3.2803)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.101 (0.089)	Data 1.15e-04 (2.82e-04)	Tok/s 249070 (241759)	Loss/tok 3.3250 (3.2801)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.81e-04)	Tok/s 255100 (241869)	Loss/tok 3.2045 (3.2808)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.79e-04)	Tok/s 229586 (241846)	Loss/tok 2.9764 (3.2810)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.78e-04)	Tok/s 230107 (241925)	Loss/tok 3.0167 (3.2816)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.76e-04)	Tok/s 233269 (241884)	Loss/tok 3.0389 (3.2813)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.100 (0.089)	Data 1.18e-04 (2.75e-04)	Tok/s 250876 (241905)	Loss/tok 3.2542 (3.2810)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.74e-04)	Tok/s 229969 (241911)	Loss/tok 3.1002 (3.2810)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.100 (0.089)	Data 1.37e-04 (2.72e-04)	Tok/s 252135 (241981)	Loss/tok 3.2161 (3.2820)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.71e-04)	Tok/s 235879 (241945)	Loss/tok 3.0789 (3.2813)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1240/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.70e-04)	Tok/s 251778 (241961)	Loss/tok 3.2732 (3.2807)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.067 (0.089)	Data 1.24e-04 (2.69e-04)	Tok/s 236358 (241959)	Loss/tok 2.9844 (3.2804)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.68e-04)	Tok/s 230494 (241994)	Loss/tok 3.0210 (3.2810)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.66e-04)	Tok/s 248328 (241957)	Loss/tok 3.3124 (3.2801)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.135 (0.089)	Data 1.18e-04 (2.65e-04)	Tok/s 259288 (241996)	Loss/tok 3.4671 (3.2809)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.067 (0.089)	Data 4.94e-05 (2.67e-04)	Tok/s 227808 (241989)	Loss/tok 3.1515 (3.2816)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593142560892, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593142560892, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.500 (0.500)	Decoder iters 149.0 (149.0)	Tok/s 34372 (34372)
0: Running moses detokenizer
0: BLEU(score=21.895097358895693, counts=[36817, 17883, 9886, 5704], totals=[67989, 64986, 61985, 58987], precisions=[54.15140684522496, 27.518234696703907, 15.949019924175204, 9.669927272110804], bp=1.0, sys_len=67989, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593142563102, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21899999999999997, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593142563102, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2811	Test BLEU: 21.90
0: Performance: Epoch: 2	Training: 1935381 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593142563103, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593142563103, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142563103, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 4006172877
0: TRAIN [3][0/1291]	Time 0.234 (0.234)	Data 1.98e-01 (1.98e-01)	Tok/s 33843 (33843)	Loss/tok 2.5653 (2.5653)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.099 (0.097)	Data 1.19e-04 (1.81e-02)	Tok/s 255323 (222756)	Loss/tok 3.1349 (3.0662)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.099 (0.100)	Data 1.19e-04 (9.52e-03)	Tok/s 252598 (235048)	Loss/tok 3.1672 (3.1533)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.100 (0.098)	Data 1.14e-04 (6.48e-03)	Tok/s 248699 (238500)	Loss/tok 3.2456 (3.1680)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.036 (0.092)	Data 1.05e-04 (4.93e-03)	Tok/s 222372 (237999)	Loss/tok 2.5704 (3.1409)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.99e-03)	Tok/s 233946 (238380)	Loss/tok 2.9995 (3.1373)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.070 (0.086)	Data 1.11e-04 (3.35e-03)	Tok/s 222897 (237268)	Loss/tok 2.9370 (3.1185)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][70/1291]	Time 0.099 (0.086)	Data 1.20e-04 (2.91e-03)	Tok/s 256553 (238233)	Loss/tok 3.1158 (3.1232)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.100 (0.090)	Data 1.06e-04 (2.57e-03)	Tok/s 253669 (240044)	Loss/tok 3.2336 (3.1581)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.134 (0.089)	Data 1.12e-04 (2.30e-03)	Tok/s 258725 (239705)	Loss/tok 3.4468 (3.1632)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.08e-03)	Tok/s 233306 (239844)	Loss/tok 2.9637 (3.1592)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.135 (0.089)	Data 1.18e-04 (1.90e-03)	Tok/s 258356 (240067)	Loss/tok 3.4347 (3.1662)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.100 (0.088)	Data 1.15e-04 (1.76e-03)	Tok/s 255843 (240025)	Loss/tok 3.1998 (3.1663)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.099 (0.089)	Data 1.33e-04 (1.63e-03)	Tok/s 256125 (240641)	Loss/tok 3.1878 (3.1683)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.067 (0.090)	Data 1.06e-04 (1.52e-03)	Tok/s 234020 (240915)	Loss/tok 3.0533 (3.1784)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.036 (0.089)	Data 1.12e-04 (1.43e-03)	Tok/s 224064 (240561)	Loss/tok 2.6078 (3.1757)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.036 (0.088)	Data 1.08e-04 (1.35e-03)	Tok/s 217872 (240143)	Loss/tok 2.6844 (3.1692)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.099 (0.088)	Data 1.07e-04 (1.28e-03)	Tok/s 253105 (240520)	Loss/tok 3.1759 (3.1753)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.134 (0.088)	Data 1.11e-04 (1.21e-03)	Tok/s 261883 (240574)	Loss/tok 3.3394 (3.1754)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.174 (0.089)	Data 1.11e-04 (1.15e-03)	Tok/s 252685 (240837)	Loss/tok 3.5648 (3.1817)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][200/1291]	Time 0.067 (0.089)	Data 1.36e-04 (1.10e-03)	Tok/s 233009 (240723)	Loss/tok 3.0054 (3.1802)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.067 (0.089)	Data 1.07e-04 (1.06e-03)	Tok/s 230560 (240737)	Loss/tok 2.9850 (3.1801)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.066 (0.088)	Data 1.08e-04 (1.01e-03)	Tok/s 236987 (240754)	Loss/tok 3.0633 (3.1789)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.067 (0.089)	Data 1.09e-04 (9.73e-04)	Tok/s 228946 (240959)	Loss/tok 2.9967 (3.1825)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.067 (0.089)	Data 1.07e-04 (9.38e-04)	Tok/s 228414 (241311)	Loss/tok 3.0938 (3.1833)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.174 (0.089)	Data 1.07e-04 (9.05e-04)	Tok/s 258620 (241365)	Loss/tok 3.4179 (3.1833)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.136 (0.090)	Data 1.10e-04 (8.74e-04)	Tok/s 254937 (241492)	Loss/tok 3.3786 (3.1843)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.067 (0.090)	Data 1.10e-04 (8.46e-04)	Tok/s 232410 (241605)	Loss/tok 3.0354 (3.1862)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.135 (0.091)	Data 1.09e-04 (8.20e-04)	Tok/s 260401 (241844)	Loss/tok 3.2894 (3.1899)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.067 (0.091)	Data 1.08e-04 (7.96e-04)	Tok/s 229618 (242012)	Loss/tok 3.0872 (3.1896)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.100 (0.091)	Data 1.10e-04 (7.73e-04)	Tok/s 251420 (242050)	Loss/tok 3.1311 (3.1893)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.135 (0.091)	Data 1.10e-04 (7.52e-04)	Tok/s 258172 (241977)	Loss/tok 3.3271 (3.1872)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.067 (0.091)	Data 1.10e-04 (7.32e-04)	Tok/s 229801 (242115)	Loss/tok 2.9273 (3.1913)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][330/1291]	Time 0.067 (0.091)	Data 1.09e-04 (7.13e-04)	Tok/s 232323 (242202)	Loss/tok 3.0164 (3.1907)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.067 (0.091)	Data 1.09e-04 (6.96e-04)	Tok/s 232454 (242135)	Loss/tok 2.9995 (3.1886)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.099 (0.091)	Data 1.63e-04 (6.79e-04)	Tok/s 252404 (242144)	Loss/tok 3.1942 (3.1897)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.067 (0.090)	Data 1.37e-04 (6.64e-04)	Tok/s 232429 (242070)	Loss/tok 2.9780 (3.1874)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.067 (0.090)	Data 1.17e-04 (6.49e-04)	Tok/s 234115 (242062)	Loss/tok 2.9476 (3.1852)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.100 (0.091)	Data 1.18e-04 (6.35e-04)	Tok/s 253760 (242292)	Loss/tok 3.1262 (3.1881)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.099 (0.091)	Data 1.15e-04 (6.22e-04)	Tok/s 252577 (242334)	Loss/tok 3.2016 (3.1883)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.099 (0.091)	Data 1.13e-04 (6.10e-04)	Tok/s 252345 (242402)	Loss/tok 3.1106 (3.1880)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.099 (0.092)	Data 1.41e-04 (5.98e-04)	Tok/s 252330 (242586)	Loss/tok 3.1270 (3.1916)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.066 (0.092)	Data 1.14e-04 (5.86e-04)	Tok/s 234726 (242655)	Loss/tok 2.9825 (3.1914)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.067 (0.092)	Data 1.11e-04 (5.76e-04)	Tok/s 234904 (242637)	Loss/tok 2.8984 (3.1898)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.036 (0.091)	Data 1.37e-04 (5.65e-04)	Tok/s 217145 (242522)	Loss/tok 2.5186 (3.1867)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.067 (0.091)	Data 1.29e-04 (5.55e-04)	Tok/s 230129 (242404)	Loss/tok 2.9846 (3.1854)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][460/1291]	Time 0.036 (0.091)	Data 1.18e-04 (5.46e-04)	Tok/s 219452 (242413)	Loss/tok 2.5573 (3.1879)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.091)	Data 1.17e-04 (5.37e-04)	Tok/s 229791 (242424)	Loss/tok 2.9222 (3.1880)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.100 (0.091)	Data 1.22e-04 (5.28e-04)	Tok/s 249358 (242310)	Loss/tok 3.1681 (3.1856)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.066 (0.091)	Data 1.14e-04 (5.20e-04)	Tok/s 234144 (242327)	Loss/tok 2.9727 (3.1873)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.136 (0.091)	Data 1.17e-04 (5.12e-04)	Tok/s 258867 (242350)	Loss/tok 3.3108 (3.1861)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.066 (0.091)	Data 1.14e-04 (5.04e-04)	Tok/s 232101 (242294)	Loss/tok 2.9722 (3.1858)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.067 (0.091)	Data 1.15e-04 (4.97e-04)	Tok/s 231246 (242213)	Loss/tok 2.9475 (3.1834)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.100 (0.091)	Data 1.16e-04 (4.90e-04)	Tok/s 254104 (242231)	Loss/tok 3.0593 (3.1832)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.83e-04)	Tok/s 236844 (242104)	Loss/tok 2.9718 (3.1820)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][550/1291]	Time 0.036 (0.091)	Data 1.16e-04 (4.76e-04)	Tok/s 223941 (242189)	Loss/tok 2.5872 (3.1828)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.067 (0.090)	Data 1.17e-04 (4.70e-04)	Tok/s 229130 (242097)	Loss/tok 3.0157 (3.1805)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.067 (0.090)	Data 1.18e-04 (4.64e-04)	Tok/s 232329 (242082)	Loss/tok 2.9789 (3.1809)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.067 (0.090)	Data 1.16e-04 (4.58e-04)	Tok/s 229590 (242027)	Loss/tok 2.9908 (3.1799)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.135 (0.090)	Data 1.17e-04 (4.52e-04)	Tok/s 257409 (242037)	Loss/tok 3.4036 (3.1798)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.067 (0.090)	Data 1.16e-04 (4.46e-04)	Tok/s 234081 (242111)	Loss/tok 2.9144 (3.1797)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.067 (0.090)	Data 1.15e-04 (4.41e-04)	Tok/s 235097 (242032)	Loss/tok 2.9462 (3.1785)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.066 (0.090)	Data 1.14e-04 (4.36e-04)	Tok/s 229052 (242127)	Loss/tok 2.9629 (3.1788)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.036 (0.090)	Data 1.14e-04 (4.31e-04)	Tok/s 220612 (242062)	Loss/tok 2.5394 (3.1788)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.099 (0.090)	Data 1.18e-04 (4.26e-04)	Tok/s 254019 (242131)	Loss/tok 3.0603 (3.1783)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.135 (0.090)	Data 1.13e-04 (4.21e-04)	Tok/s 257263 (242079)	Loss/tok 3.2714 (3.1777)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.100 (0.090)	Data 1.69e-04 (4.17e-04)	Tok/s 249245 (242074)	Loss/tok 3.1564 (3.1769)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.036 (0.090)	Data 1.14e-04 (4.12e-04)	Tok/s 219913 (242032)	Loss/tok 2.5056 (3.1756)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][680/1291]	Time 0.100 (0.090)	Data 1.17e-04 (4.08e-04)	Tok/s 250755 (241948)	Loss/tok 3.2415 (3.1741)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.036 (0.090)	Data 1.17e-04 (4.04e-04)	Tok/s 221874 (242034)	Loss/tok 2.5939 (3.1738)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.00e-04)	Tok/s 252169 (242073)	Loss/tok 3.1398 (3.1740)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.96e-04)	Tok/s 230794 (242108)	Loss/tok 2.9160 (3.1747)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.92e-04)	Tok/s 232449 (242067)	Loss/tok 2.9174 (3.1738)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.099 (0.090)	Data 1.16e-04 (3.88e-04)	Tok/s 251393 (242111)	Loss/tok 3.0960 (3.1731)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.067 (0.090)	Data 1.18e-04 (3.84e-04)	Tok/s 232399 (241982)	Loss/tok 2.9444 (3.1711)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.134 (0.090)	Data 1.13e-04 (3.81e-04)	Tok/s 259464 (242047)	Loss/tok 3.2590 (3.1704)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.036 (0.090)	Data 1.15e-04 (3.77e-04)	Tok/s 221315 (241973)	Loss/tok 2.5932 (3.1699)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.100 (0.089)	Data 3.08e-04 (3.74e-04)	Tok/s 250736 (242006)	Loss/tok 3.0577 (3.1684)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.71e-04)	Tok/s 233416 (242012)	Loss/tok 2.9886 (3.1693)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.68e-04)	Tok/s 217071 (241876)	Loss/tok 2.4889 (3.1674)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.65e-04)	Tok/s 229820 (241862)	Loss/tok 2.8690 (3.1675)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][810/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.62e-04)	Tok/s 233957 (241832)	Loss/tok 2.8969 (3.1664)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.100 (0.089)	Data 1.35e-04 (3.59e-04)	Tok/s 252959 (241872)	Loss/tok 3.1499 (3.1667)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.100 (0.090)	Data 1.14e-04 (3.56e-04)	Tok/s 253486 (241921)	Loss/tok 3.0545 (3.1672)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.135 (0.090)	Data 1.13e-04 (3.53e-04)	Tok/s 259238 (241962)	Loss/tok 3.2648 (3.1666)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.100 (0.090)	Data 1.29e-04 (3.50e-04)	Tok/s 250859 (242071)	Loss/tok 3.0772 (3.1672)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.067 (0.090)	Data 1.19e-04 (3.48e-04)	Tok/s 230776 (242024)	Loss/tok 2.9588 (3.1662)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.45e-04)	Tok/s 232005 (242021)	Loss/tok 2.9858 (3.1658)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.135 (0.090)	Data 1.13e-04 (3.42e-04)	Tok/s 256100 (242062)	Loss/tok 3.4018 (3.1654)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.135 (0.090)	Data 1.13e-04 (3.40e-04)	Tok/s 258606 (242174)	Loss/tok 3.2541 (3.1664)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.135 (0.090)	Data 1.15e-04 (3.37e-04)	Tok/s 257534 (242155)	Loss/tok 3.3175 (3.1654)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.099 (0.090)	Data 1.14e-04 (3.35e-04)	Tok/s 255189 (242212)	Loss/tok 3.0849 (3.1646)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.33e-04)	Tok/s 232559 (242158)	Loss/tok 2.9037 (3.1634)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][930/1291]	Time 0.067 (0.090)	Data 1.21e-04 (3.30e-04)	Tok/s 230254 (242135)	Loss/tok 2.9828 (3.1624)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.067 (0.089)	Data 1.38e-04 (3.28e-04)	Tok/s 227213 (242082)	Loss/tok 3.0306 (3.1616)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.26e-04)	Tok/s 253350 (242083)	Loss/tok 3.1820 (3.1605)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.24e-04)	Tok/s 229139 (242094)	Loss/tok 2.8763 (3.1597)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.135 (0.089)	Data 1.14e-04 (3.22e-04)	Tok/s 256205 (242083)	Loss/tok 3.2907 (3.1589)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.19e-04)	Tok/s 224456 (242060)	Loss/tok 2.9810 (3.1585)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.17e-04)	Tok/s 249040 (242048)	Loss/tok 3.0985 (3.1575)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.100 (0.089)	Data 1.31e-04 (3.15e-04)	Tok/s 250515 (242121)	Loss/tok 3.2023 (3.1579)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.13e-04)	Tok/s 233291 (242135)	Loss/tok 2.9466 (3.1571)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.100 (0.089)	Data 1.18e-04 (3.12e-04)	Tok/s 251866 (242141)	Loss/tok 3.1120 (3.1566)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.036 (0.089)	Data 1.15e-04 (3.10e-04)	Tok/s 218339 (242164)	Loss/tok 2.5415 (3.1569)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.036 (0.089)	Data 1.12e-04 (3.08e-04)	Tok/s 221168 (242061)	Loss/tok 2.5423 (3.1556)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.06e-04)	Tok/s 254357 (242035)	Loss/tok 3.1873 (3.1562)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1060/1291]	Time 0.099 (0.089)	Data 1.24e-04 (3.04e-04)	Tok/s 253044 (242064)	Loss/tok 3.1089 (3.1560)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.037 (0.089)	Data 1.18e-04 (3.02e-04)	Tok/s 214275 (242001)	Loss/tok 2.5617 (3.1556)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.01e-04)	Tok/s 253446 (241994)	Loss/tok 3.0778 (3.1554)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.100 (0.089)	Data 1.36e-04 (2.99e-04)	Tok/s 254439 (242009)	Loss/tok 3.1499 (3.1549)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.98e-04)	Tok/s 254103 (241968)	Loss/tok 3.1318 (3.1537)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.96e-04)	Tok/s 229869 (241934)	Loss/tok 3.0288 (3.1533)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.135 (0.089)	Data 1.13e-04 (2.94e-04)	Tok/s 259551 (241959)	Loss/tok 3.2971 (3.1532)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.036 (0.089)	Data 1.13e-04 (2.93e-04)	Tok/s 219126 (241953)	Loss/tok 2.5434 (3.1531)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.135 (0.089)	Data 1.21e-04 (2.91e-04)	Tok/s 260800 (241979)	Loss/tok 3.2418 (3.1527)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.90e-04)	Tok/s 255474 (242003)	Loss/tok 3.0891 (3.1525)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.88e-04)	Tok/s 253251 (241972)	Loss/tok 3.1713 (3.1515)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.87e-04)	Tok/s 232002 (241915)	Loss/tok 2.9673 (3.1504)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.85e-04)	Tok/s 255131 (241925)	Loss/tok 3.1583 (3.1501)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1190/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.84e-04)	Tok/s 230580 (241948)	Loss/tok 2.8481 (3.1503)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.036 (0.089)	Data 1.15e-04 (2.83e-04)	Tok/s 222718 (241941)	Loss/tok 2.5949 (3.1502)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.81e-04)	Tok/s 252282 (241932)	Loss/tok 3.1808 (3.1497)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.068 (0.089)	Data 1.23e-04 (2.80e-04)	Tok/s 230546 (241911)	Loss/tok 2.8579 (3.1488)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.79e-04)	Tok/s 233881 (241961)	Loss/tok 2.8437 (3.1490)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.067 (0.089)	Data 1.19e-04 (2.77e-04)	Tok/s 229508 (241953)	Loss/tok 2.9376 (3.1487)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.76e-04)	Tok/s 233529 (241921)	Loss/tok 2.8968 (3.1476)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.75e-04)	Tok/s 227793 (241891)	Loss/tok 2.9650 (3.1467)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.74e-04)	Tok/s 252595 (241931)	Loss/tok 3.2448 (3.1473)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.135 (0.089)	Data 1.38e-04 (2.73e-04)	Tok/s 258775 (242006)	Loss/tok 3.3545 (3.1486)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.067 (0.089)	Data 5.25e-05 (2.74e-04)	Tok/s 231677 (242063)	Loss/tok 2.9653 (3.1490)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593142678742, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593142678743, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.429 (0.429)	Decoder iters 119.0 (119.0)	Tok/s 38422 (38422)
0: Running moses detokenizer
0: BLEU(score=24.097706192421196, counts=[37017, 18583, 10655, 6355], totals=[65559, 62556, 59554, 56555], precisions=[56.463643435683885, 29.70618325979922, 17.89132551969641, 11.236849084961541], bp=1.0, sys_len=65559, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593142680713, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24100000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593142680714, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1482	Test BLEU: 24.10
0: Performance: Epoch: 3	Training: 1936054 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593142680714, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593142680714, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 03:38:08 AM
RESULT,RNN_TRANSLATOR,,506,nvidia,2020-06-26 03:29:42 AM
