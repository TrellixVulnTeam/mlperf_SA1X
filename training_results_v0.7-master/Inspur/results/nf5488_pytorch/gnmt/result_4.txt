Beginning trial 4 of 10
:::MLLOG {"namespace": "", "time_ms": 1593142688836, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593142688882, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593142688882, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593142688882, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593142688882, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593142691259, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 03:38:11 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593142693835, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142693846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142693851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142693850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142693868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142693880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142693885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593142693928, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3072890827
:::MLLOG {"namespace": "", "time_ms": 1593142702407, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3072890827, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3637014058
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
0: Initializing dwu fp16 optimizer
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593142716171, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593142716173, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593142716174, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593142716174, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593142716174, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593142718184, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593142718184, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593142718184, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593142718523, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593142718524, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593142718524, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593142718524, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593142718525, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593142718525, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593142718525, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593142718525, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593142718525, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593142718525, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593142718525, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142718526, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1874645652
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.347 (0.347)	Data 2.77e-01 (2.77e-01)	Tok/s 44426 (44426)	Loss/tok 10.5576 (10.5576)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.102 (0.131)	Data 1.13e-04 (3.81e-02)	Tok/s 249553 (165428)	Loss/tok 9.5482 (9.8622)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.045 (0.109)	Data 1.08e-04 (2.00e-02)	Tok/s 173868 (192751)	Loss/tok 8.7562 (9.5866)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.066 (0.103)	Data 1.09e-04 (1.45e-02)	Tok/s 236751 (203224)	Loss/tok 8.7471 (9.3732)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][40/1291]	Time 0.099 (0.105)	Data 1.12e-04 (1.16e-02)	Tok/s 255546 (211814)	Loss/tok 8.6792 (9.2421)	LR 7.057e-05
0: TRAIN [0][50/1291]	Time 0.067 (0.104)	Data 1.08e-04 (9.37e-03)	Tok/s 231332 (219021)	Loss/tok 8.3135 (9.0840)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.067 (0.102)	Data 1.06e-04 (7.85e-03)	Tok/s 233149 (222899)	Loss/tok 8.1461 (8.9561)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.067 (0.100)	Data 1.09e-04 (6.76e-03)	Tok/s 226922 (225300)	Loss/tok 7.9192 (8.8417)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.067 (0.098)	Data 1.09e-04 (5.94e-03)	Tok/s 232302 (227565)	Loss/tok 7.8141 (8.7414)	LR 1.773e-04
0: TRAIN [0][90/1291]	Time 0.098 (0.098)	Data 1.09e-04 (5.30e-03)	Tok/s 258877 (229412)	Loss/tok 7.9812 (8.6557)	LR 2.232e-04
0: TRAIN [0][100/1291]	Time 0.067 (0.096)	Data 1.08e-04 (4.79e-03)	Tok/s 234844 (230600)	Loss/tok 7.7093 (8.5860)	LR 2.810e-04
0: TRAIN [0][110/1291]	Time 0.098 (0.095)	Data 1.08e-04 (4.37e-03)	Tok/s 255083 (232163)	Loss/tok 7.9247 (8.5250)	LR 3.537e-04
0: TRAIN [0][120/1291]	Time 0.066 (0.095)	Data 1.11e-04 (4.01e-03)	Tok/s 236565 (233136)	Loss/tok 7.6714 (8.4672)	LR 4.453e-04
0: TRAIN [0][130/1291]	Time 0.036 (0.094)	Data 1.08e-04 (3.72e-03)	Tok/s 218185 (233488)	Loss/tok 6.9554 (8.4149)	LR 5.606e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][140/1291]	Time 0.067 (0.093)	Data 1.17e-04 (3.46e-03)	Tok/s 232670 (234189)	Loss/tok 7.5586 (8.3785)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.066 (0.092)	Data 1.05e-04 (3.24e-03)	Tok/s 231641 (234470)	Loss/tok 7.4471 (8.3396)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.067 (0.092)	Data 1.06e-04 (3.04e-03)	Tok/s 233158 (235192)	Loss/tok 7.3267 (8.2932)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.036 (0.091)	Data 1.09e-04 (2.87e-03)	Tok/s 223362 (235591)	Loss/tok 6.5220 (8.2449)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.036 (0.090)	Data 1.07e-04 (2.72e-03)	Tok/s 227715 (235612)	Loss/tok 6.1209 (8.1978)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.58e-03)	Tok/s 235198 (236057)	Loss/tok 6.8707 (8.1429)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.134 (0.090)	Data 1.07e-04 (2.46e-03)	Tok/s 259034 (236806)	Loss/tok 7.0369 (8.0727)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.098 (0.090)	Data 1.08e-04 (2.35e-03)	Tok/s 259683 (237200)	Loss/tok 6.7883 (8.0129)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.134 (0.090)	Data 1.10e-04 (2.25e-03)	Tok/s 258355 (237521)	Loss/tok 6.7570 (7.9453)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.066 (0.090)	Data 1.10e-04 (2.16e-03)	Tok/s 233117 (237940)	Loss/tok 6.2545 (7.8776)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.036 (0.090)	Data 1.13e-04 (2.07e-03)	Tok/s 217310 (238250)	Loss/tok 5.2654 (7.8093)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.066 (0.090)	Data 1.13e-04 (1.99e-03)	Tok/s 235866 (238494)	Loss/tok 5.8264 (7.7434)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.099 (0.090)	Data 1.12e-04 (1.92e-03)	Tok/s 252478 (238664)	Loss/tok 5.9829 (7.6796)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][270/1291]	Time 0.066 (0.090)	Data 1.28e-04 (1.86e-03)	Tok/s 233699 (238739)	Loss/tok 5.5239 (7.6162)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.134 (0.090)	Data 1.35e-04 (1.79e-03)	Tok/s 264549 (239064)	Loss/tok 5.8873 (7.5418)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.099 (0.090)	Data 1.12e-04 (1.74e-03)	Tok/s 252391 (239198)	Loss/tok 5.6369 (7.4783)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.066 (0.090)	Data 1.12e-04 (1.68e-03)	Tok/s 236809 (239421)	Loss/tok 5.1619 (7.4103)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.036 (0.089)	Data 1.10e-04 (1.63e-03)	Tok/s 217433 (239332)	Loss/tok 4.2819 (7.3529)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.134 (0.089)	Data 1.11e-04 (1.58e-03)	Tok/s 259342 (239429)	Loss/tok 5.4890 (7.2924)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.099 (0.089)	Data 1.08e-04 (1.54e-03)	Tok/s 252889 (239576)	Loss/tok 5.1953 (7.2306)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.134 (0.089)	Data 1.12e-04 (1.50e-03)	Tok/s 259149 (239797)	Loss/tok 5.3008 (7.1605)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.066 (0.089)	Data 1.34e-04 (1.46e-03)	Tok/s 229018 (239655)	Loss/tok 4.7172 (7.1119)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.067 (0.088)	Data 1.12e-04 (1.42e-03)	Tok/s 231674 (239609)	Loss/tok 4.6168 (7.0569)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.134 (0.088)	Data 1.13e-04 (1.39e-03)	Tok/s 260795 (239793)	Loss/tok 5.1567 (6.9937)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.036 (0.088)	Data 1.11e-04 (1.35e-03)	Tok/s 219317 (239912)	Loss/tok 3.8741 (6.9364)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][390/1291]	Time 0.100 (0.088)	Data 1.10e-04 (1.32e-03)	Tok/s 252242 (239859)	Loss/tok 4.7023 (6.8861)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.066 (0.088)	Data 1.18e-04 (1.29e-03)	Tok/s 235740 (239772)	Loss/tok 4.2920 (6.8316)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.066 (0.088)	Data 1.34e-04 (1.26e-03)	Tok/s 230936 (239844)	Loss/tok 4.2281 (6.7739)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.100 (0.088)	Data 1.33e-04 (1.24e-03)	Tok/s 252583 (240084)	Loss/tok 4.6531 (6.7131)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.067 (0.088)	Data 1.10e-04 (1.21e-03)	Tok/s 227551 (240278)	Loss/tok 4.2364 (6.6552)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.068 (0.089)	Data 1.14e-04 (1.18e-03)	Tok/s 225348 (240583)	Loss/tok 4.1771 (6.5929)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.066 (0.089)	Data 1.16e-04 (1.16e-03)	Tok/s 230941 (240706)	Loss/tok 4.1561 (6.5364)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.099 (0.089)	Data 1.12e-04 (1.14e-03)	Tok/s 254045 (240829)	Loss/tok 4.3834 (6.4870)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.134 (0.089)	Data 1.13e-04 (1.12e-03)	Tok/s 259892 (241045)	Loss/tok 4.3741 (6.4314)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.099 (0.089)	Data 1.12e-04 (1.10e-03)	Tok/s 255246 (240968)	Loss/tok 4.3914 (6.3914)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.067 (0.089)	Data 1.17e-04 (1.08e-03)	Tok/s 231275 (241133)	Loss/tok 3.8707 (6.3428)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.067 (0.089)	Data 1.12e-04 (1.06e-03)	Tok/s 224849 (241225)	Loss/tok 3.9125 (6.2992)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.099 (0.089)	Data 1.09e-04 (1.04e-03)	Tok/s 253827 (241352)	Loss/tok 4.2686 (6.2567)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][520/1291]	Time 0.099 (0.089)	Data 1.13e-04 (1.02e-03)	Tok/s 254586 (241427)	Loss/tok 4.1548 (6.2140)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.066 (0.089)	Data 1.10e-04 (1.00e-03)	Tok/s 233800 (241426)	Loss/tok 3.8111 (6.1773)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.037 (0.089)	Data 1.12e-04 (9.87e-04)	Tok/s 214641 (241352)	Loss/tok 3.2865 (6.1423)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.174 (0.089)	Data 1.16e-04 (9.71e-04)	Tok/s 259184 (241390)	Loss/tok 4.4039 (6.1024)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.135 (0.089)	Data 1.11e-04 (9.55e-04)	Tok/s 260337 (241419)	Loss/tok 4.3283 (6.0659)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.174 (0.089)	Data 1.09e-04 (9.41e-04)	Tok/s 255467 (241513)	Loss/tok 4.5780 (6.0251)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.100 (0.090)	Data 1.12e-04 (9.27e-04)	Tok/s 254023 (241636)	Loss/tok 4.0040 (5.9870)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.036 (0.089)	Data 1.24e-04 (9.13e-04)	Tok/s 222221 (241572)	Loss/tok 3.2498 (5.9553)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.135 (0.089)	Data 1.08e-04 (9.00e-04)	Tok/s 260066 (241623)	Loss/tok 4.1305 (5.9206)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.067 (0.089)	Data 1.08e-04 (8.87e-04)	Tok/s 229510 (241599)	Loss/tok 3.6619 (5.8903)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.036 (0.089)	Data 1.09e-04 (8.74e-04)	Tok/s 221012 (241586)	Loss/tok 3.1042 (5.8588)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.135 (0.090)	Data 1.21e-04 (8.62e-04)	Tok/s 261258 (241647)	Loss/tok 4.1715 (5.8261)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.100 (0.090)	Data 1.09e-04 (8.51e-04)	Tok/s 251949 (241754)	Loss/tok 3.8864 (5.7922)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][650/1291]	Time 0.099 (0.090)	Data 1.10e-04 (8.39e-04)	Tok/s 252110 (241837)	Loss/tok 4.0086 (5.7598)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.174 (0.090)	Data 1.07e-04 (8.28e-04)	Tok/s 258139 (241881)	Loss/tok 4.2579 (5.7314)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.174 (0.090)	Data 1.12e-04 (8.18e-04)	Tok/s 255417 (241898)	Loss/tok 4.3324 (5.7042)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.067 (0.090)	Data 1.08e-04 (8.07e-04)	Tok/s 229362 (241874)	Loss/tok 3.6456 (5.6790)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.100 (0.090)	Data 1.10e-04 (7.97e-04)	Tok/s 249801 (241936)	Loss/tok 3.8996 (5.6514)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.067 (0.090)	Data 1.11e-04 (7.87e-04)	Tok/s 229963 (241886)	Loss/tok 3.6386 (5.6275)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.100 (0.090)	Data 1.29e-04 (7.78e-04)	Tok/s 253794 (241954)	Loss/tok 3.8933 (5.6008)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.099 (0.090)	Data 1.09e-04 (7.69e-04)	Tok/s 256094 (241920)	Loss/tok 3.7475 (5.5780)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.036 (0.090)	Data 1.11e-04 (7.60e-04)	Tok/s 214804 (241888)	Loss/tok 3.0394 (5.5554)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.067 (0.090)	Data 1.12e-04 (7.51e-04)	Tok/s 233601 (241904)	Loss/tok 3.5532 (5.5331)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.036 (0.090)	Data 1.16e-04 (7.43e-04)	Tok/s 223741 (241919)	Loss/tok 3.0457 (5.5102)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.036 (0.090)	Data 1.12e-04 (7.34e-04)	Tok/s 219172 (241953)	Loss/tok 3.0321 (5.4870)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.100 (0.090)	Data 1.09e-04 (7.26e-04)	Tok/s 249273 (242003)	Loss/tok 3.8415 (5.4640)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][780/1291]	Time 0.135 (0.090)	Data 1.08e-04 (7.18e-04)	Tok/s 261810 (241936)	Loss/tok 3.8723 (5.4452)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.067 (0.090)	Data 1.11e-04 (7.11e-04)	Tok/s 231189 (241909)	Loss/tok 3.4095 (5.4253)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.066 (0.090)	Data 1.13e-04 (7.03e-04)	Tok/s 235904 (241908)	Loss/tok 3.5464 (5.4058)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.099 (0.090)	Data 1.09e-04 (6.96e-04)	Tok/s 254513 (241922)	Loss/tok 3.8685 (5.3863)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.134 (0.090)	Data 1.08e-04 (6.89e-04)	Tok/s 259841 (241932)	Loss/tok 3.9579 (5.3667)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.100 (0.090)	Data 1.09e-04 (6.82e-04)	Tok/s 252892 (242002)	Loss/tok 3.6644 (5.3459)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][840/1291]	Time 0.134 (0.090)	Data 1.11e-04 (6.75e-04)	Tok/s 261697 (242007)	Loss/tok 3.8650 (5.3262)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.100 (0.090)	Data 1.08e-04 (6.69e-04)	Tok/s 254301 (242047)	Loss/tok 3.8257 (5.3073)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.099 (0.090)	Data 1.14e-04 (6.62e-04)	Tok/s 251657 (242004)	Loss/tok 3.8117 (5.2904)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.036 (0.090)	Data 1.28e-04 (6.56e-04)	Tok/s 219844 (241971)	Loss/tok 2.9476 (5.2742)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.174 (0.090)	Data 1.15e-04 (6.50e-04)	Tok/s 257955 (241953)	Loss/tok 4.1471 (5.2569)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.100 (0.090)	Data 1.12e-04 (6.44e-04)	Tok/s 253318 (242018)	Loss/tok 3.6536 (5.2381)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.067 (0.090)	Data 1.09e-04 (6.38e-04)	Tok/s 233337 (241963)	Loss/tok 3.5320 (5.2230)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.173 (0.090)	Data 1.09e-04 (6.32e-04)	Tok/s 260291 (241954)	Loss/tok 4.0510 (5.2070)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.100 (0.090)	Data 1.10e-04 (6.26e-04)	Tok/s 251016 (242023)	Loss/tok 3.7674 (5.1900)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.067 (0.090)	Data 1.11e-04 (6.21e-04)	Tok/s 232129 (241998)	Loss/tok 3.4546 (5.1752)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.099 (0.090)	Data 1.17e-04 (6.15e-04)	Tok/s 252936 (241961)	Loss/tok 3.6354 (5.1607)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.134 (0.090)	Data 1.09e-04 (6.10e-04)	Tok/s 259492 (242013)	Loss/tok 3.8814 (5.1431)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.174 (0.090)	Data 1.11e-04 (6.05e-04)	Tok/s 254559 (242011)	Loss/tok 4.1503 (5.1281)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][970/1291]	Time 0.099 (0.090)	Data 1.13e-04 (6.00e-04)	Tok/s 254307 (242015)	Loss/tok 3.7402 (5.1136)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.100 (0.090)	Data 1.09e-04 (5.95e-04)	Tok/s 254405 (242044)	Loss/tok 3.6517 (5.0988)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.067 (0.090)	Data 1.15e-04 (5.90e-04)	Tok/s 232183 (242003)	Loss/tok 3.4467 (5.0862)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.036 (0.090)	Data 1.12e-04 (5.85e-04)	Tok/s 218968 (242023)	Loss/tok 2.9848 (5.0718)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.81e-04)	Tok/s 230541 (242062)	Loss/tok 3.4471 (5.0570)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.174 (0.090)	Data 1.09e-04 (5.76e-04)	Tok/s 256713 (242065)	Loss/tok 4.0340 (5.0432)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.135 (0.090)	Data 1.09e-04 (5.72e-04)	Tok/s 258300 (242108)	Loss/tok 3.7681 (5.0286)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.067 (0.090)	Data 1.10e-04 (5.67e-04)	Tok/s 227749 (242063)	Loss/tok 3.3613 (5.0167)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.066 (0.090)	Data 1.12e-04 (5.63e-04)	Tok/s 229089 (242014)	Loss/tok 3.4318 (5.0048)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.59e-04)	Tok/s 232169 (241996)	Loss/tok 3.4350 (4.9924)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.099 (0.090)	Data 1.11e-04 (5.55e-04)	Tok/s 250560 (242021)	Loss/tok 3.7554 (4.9792)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.136 (0.089)	Data 1.09e-04 (5.51e-04)	Tok/s 253809 (241976)	Loss/tok 3.8687 (4.9679)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.066 (0.089)	Data 1.08e-04 (5.47e-04)	Tok/s 230458 (241917)	Loss/tok 3.4152 (4.9567)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1100/1291]	Time 0.067 (0.089)	Data 1.07e-04 (5.43e-04)	Tok/s 232102 (241893)	Loss/tok 3.4455 (4.9453)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.135 (0.089)	Data 1.10e-04 (5.39e-04)	Tok/s 260661 (241897)	Loss/tok 3.8657 (4.9337)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.134 (0.089)	Data 1.12e-04 (5.35e-04)	Tok/s 263371 (241872)	Loss/tok 3.8399 (4.9230)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.31e-04)	Tok/s 251543 (241885)	Loss/tok 3.7635 (4.9114)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.28e-04)	Tok/s 235637 (241895)	Loss/tok 3.3182 (4.8996)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.067 (0.089)	Data 1.35e-04 (5.24e-04)	Tok/s 232008 (241893)	Loss/tok 3.3926 (4.8888)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.21e-04)	Tok/s 227481 (241912)	Loss/tok 3.3710 (4.8774)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.174 (0.089)	Data 1.10e-04 (5.17e-04)	Tok/s 254535 (241895)	Loss/tok 4.0546 (4.8670)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.067 (0.089)	Data 1.13e-04 (5.14e-04)	Tok/s 233935 (241890)	Loss/tok 3.4088 (4.8565)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.067 (0.089)	Data 1.11e-04 (5.10e-04)	Tok/s 234809 (241857)	Loss/tok 3.3257 (4.8472)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.07e-04)	Tok/s 232635 (241876)	Loss/tok 3.2999 (4.8368)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.067 (0.089)	Data 1.07e-04 (5.04e-04)	Tok/s 232469 (241889)	Loss/tok 3.4322 (4.8263)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.067 (0.089)	Data 1.09e-04 (5.01e-04)	Tok/s 230025 (241873)	Loss/tok 3.3804 (4.8168)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [0][1230/1291]	Time 0.099 (0.089)	Data 1.32e-04 (4.97e-04)	Tok/s 254831 (241908)	Loss/tok 3.6042 (4.8063)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [0][1240/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.94e-04)	Tok/s 230232 (241927)	Loss/tok 3.2562 (4.7957)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.067 (0.089)	Data 1.26e-04 (4.91e-04)	Tok/s 232041 (241891)	Loss/tok 3.3945 (4.7876)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.100 (0.089)	Data 1.10e-04 (4.88e-04)	Tok/s 250332 (241916)	Loss/tok 3.5741 (4.7777)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.85e-04)	Tok/s 232816 (241917)	Loss/tok 3.2864 (4.7681)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.82e-04)	Tok/s 232830 (241897)	Loss/tok 3.4149 (4.7596)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.134 (0.089)	Data 4.41e-05 (4.83e-04)	Tok/s 262195 (241888)	Loss/tok 3.6992 (4.7500)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593142834354, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142834354, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.495 (0.495)	Decoder iters 149.0 (149.0)	Tok/s 33482 (33482)
0: Running moses detokenizer
0: BLEU(score=19.906945590850047, counts=[34052, 15572, 8334, 4627], totals=[64535, 61532, 58530, 55533], precisions=[52.76516618888975, 25.30715725151141, 14.238851870835468, 8.331982785010714], bp=0.9978175241431266, sys_len=64535, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593142836527, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1991, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142836527, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7487	Test BLEU: 19.91
0: Performance: Epoch: 0	Training: 1934760 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593142836527, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142836527, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142836528, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1734175970
0: TRAIN [1][0/1291]	Time 0.298 (0.298)	Data 1.99e-01 (1.99e-01)	Tok/s 84100 (84100)	Loss/tok 3.4808 (3.4808)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.066 (0.091)	Data 1.14e-04 (1.82e-02)	Tok/s 232290 (220721)	Loss/tok 3.3277 (3.3516)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.135 (0.091)	Data 1.23e-04 (9.60e-03)	Tok/s 257729 (231127)	Loss/tok 3.7346 (3.4555)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.035 (0.087)	Data 1.15e-04 (6.54e-03)	Tok/s 223053 (233378)	Loss/tok 2.7483 (3.4416)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.136 (0.088)	Data 1.13e-04 (4.98e-03)	Tok/s 256318 (235816)	Loss/tok 3.7620 (3.4587)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.067 (0.084)	Data 1.21e-04 (4.02e-03)	Tok/s 230183 (235179)	Loss/tok 3.2015 (3.4243)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][60/1291]	Time 0.067 (0.087)	Data 1.39e-04 (3.38e-03)	Tok/s 233213 (236813)	Loss/tok 3.3527 (3.4734)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][70/1291]	Time 0.066 (0.088)	Data 1.19e-04 (2.92e-03)	Tok/s 236580 (237879)	Loss/tok 3.3135 (3.4803)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.067 (0.086)	Data 1.13e-04 (2.58e-03)	Tok/s 232941 (237628)	Loss/tok 3.1934 (3.4605)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.068 (0.090)	Data 1.10e-04 (2.31e-03)	Tok/s 230283 (239068)	Loss/tok 3.2096 (3.4892)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.09e-03)	Tok/s 228761 (239087)	Loss/tok 3.3941 (3.4823)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.067 (0.087)	Data 1.15e-04 (1.91e-03)	Tok/s 228957 (238664)	Loss/tok 3.2684 (3.4712)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.067 (0.087)	Data 1.25e-04 (1.76e-03)	Tok/s 228464 (238745)	Loss/tok 3.2415 (3.4725)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.100 (0.087)	Data 1.11e-04 (1.64e-03)	Tok/s 251336 (238995)	Loss/tok 3.5185 (3.4711)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.100 (0.089)	Data 1.12e-04 (1.53e-03)	Tok/s 253588 (240006)	Loss/tok 3.4867 (3.4827)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.067 (0.089)	Data 1.30e-04 (1.44e-03)	Tok/s 234095 (240444)	Loss/tok 3.1966 (3.4833)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.174 (0.089)	Data 1.12e-04 (1.35e-03)	Tok/s 257762 (240675)	Loss/tok 3.9004 (3.4867)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.101 (0.090)	Data 1.10e-04 (1.28e-03)	Tok/s 247733 (240718)	Loss/tok 3.4765 (3.4859)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.036 (0.090)	Data 1.16e-04 (1.22e-03)	Tok/s 222134 (240857)	Loss/tok 2.8952 (3.4935)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.100 (0.090)	Data 1.13e-04 (1.16e-03)	Tok/s 252758 (240938)	Loss/tok 3.4989 (3.4898)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][200/1291]	Time 0.067 (0.089)	Data 1.13e-04 (1.11e-03)	Tok/s 234341 (240976)	Loss/tok 3.2040 (3.4870)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.067 (0.089)	Data 1.15e-04 (1.06e-03)	Tok/s 238984 (241013)	Loss/tok 3.2404 (3.4876)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.067 (0.089)	Data 1.25e-04 (1.02e-03)	Tok/s 230723 (240920)	Loss/tok 3.2279 (3.4831)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.067 (0.089)	Data 1.13e-04 (9.79e-04)	Tok/s 231570 (241026)	Loss/tok 3.2200 (3.4834)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.174 (0.089)	Data 1.13e-04 (9.43e-04)	Tok/s 255480 (241077)	Loss/tok 3.8065 (3.4842)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.066 (0.089)	Data 1.11e-04 (9.10e-04)	Tok/s 232617 (241192)	Loss/tok 3.2230 (3.4862)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.100 (0.090)	Data 1.26e-04 (8.80e-04)	Tok/s 252693 (241500)	Loss/tok 3.4332 (3.4877)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.100 (0.090)	Data 1.10e-04 (8.52e-04)	Tok/s 250741 (241511)	Loss/tok 3.4900 (3.4846)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.067 (0.089)	Data 1.13e-04 (8.27e-04)	Tok/s 233189 (241352)	Loss/tok 3.3450 (3.4834)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.175 (0.089)	Data 1.36e-04 (8.03e-04)	Tok/s 256426 (241295)	Loss/tok 3.8941 (3.4868)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.100 (0.089)	Data 1.13e-04 (7.80e-04)	Tok/s 249036 (241059)	Loss/tok 3.4572 (3.4831)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.175 (0.089)	Data 1.13e-04 (7.59e-04)	Tok/s 253801 (240920)	Loss/tok 3.9163 (3.4817)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.101 (0.089)	Data 1.15e-04 (7.39e-04)	Tok/s 246031 (241135)	Loss/tok 3.4483 (3.4838)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][330/1291]	Time 0.066 (0.089)	Data 1.11e-04 (7.20e-04)	Tok/s 232135 (241149)	Loss/tok 3.1850 (3.4818)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.137 (0.088)	Data 1.10e-04 (7.02e-04)	Tok/s 256685 (241067)	Loss/tok 3.6636 (3.4785)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.067 (0.088)	Data 1.11e-04 (6.86e-04)	Tok/s 230526 (241091)	Loss/tok 3.2209 (3.4768)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.067 (0.088)	Data 1.10e-04 (6.70e-04)	Tok/s 232311 (240988)	Loss/tok 3.3660 (3.4763)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][370/1291]	Time 0.135 (0.089)	Data 1.09e-04 (6.55e-04)	Tok/s 257066 (241072)	Loss/tok 3.7759 (3.4789)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.100 (0.089)	Data 1.11e-04 (6.41e-04)	Tok/s 250634 (241160)	Loss/tok 3.4278 (3.4758)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.100 (0.089)	Data 1.11e-04 (6.27e-04)	Tok/s 249244 (241233)	Loss/tok 3.4615 (3.4735)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.174 (0.089)	Data 1.13e-04 (6.14e-04)	Tok/s 256377 (241418)	Loss/tok 3.7847 (3.4764)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.099 (0.089)	Data 1.11e-04 (6.02e-04)	Tok/s 254621 (241448)	Loss/tok 3.4359 (3.4757)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.089)	Data 1.25e-04 (5.91e-04)	Tok/s 231811 (241374)	Loss/tok 3.1925 (3.4724)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.067 (0.089)	Data 1.29e-04 (5.80e-04)	Tok/s 232309 (241465)	Loss/tok 3.2613 (3.4739)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.067 (0.089)	Data 1.13e-04 (5.69e-04)	Tok/s 230893 (241557)	Loss/tok 3.2416 (3.4730)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.067 (0.089)	Data 1.11e-04 (5.59e-04)	Tok/s 228539 (241624)	Loss/tok 3.2231 (3.4747)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.067 (0.090)	Data 1.12e-04 (5.49e-04)	Tok/s 229538 (241698)	Loss/tok 3.2415 (3.4743)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.067 (0.090)	Data 1.18e-04 (5.40e-04)	Tok/s 229906 (241734)	Loss/tok 3.1953 (3.4739)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.175 (0.089)	Data 1.33e-04 (5.31e-04)	Tok/s 255304 (241666)	Loss/tok 3.8428 (3.4745)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.23e-04)	Tok/s 230906 (241707)	Loss/tok 3.1413 (3.4741)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][500/1291]	Time 0.099 (0.090)	Data 1.11e-04 (5.15e-04)	Tok/s 255782 (241824)	Loss/tok 3.4399 (3.4746)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.100 (0.090)	Data 1.12e-04 (5.07e-04)	Tok/s 250241 (241750)	Loss/tok 3.4117 (3.4722)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.100 (0.090)	Data 1.10e-04 (5.00e-04)	Tok/s 252857 (241829)	Loss/tok 3.4632 (3.4714)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.067 (0.090)	Data 1.15e-04 (4.92e-04)	Tok/s 230138 (241777)	Loss/tok 3.1691 (3.4710)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.85e-04)	Tok/s 228163 (241860)	Loss/tok 3.2272 (3.4715)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.100 (0.090)	Data 1.11e-04 (4.79e-04)	Tok/s 252587 (241809)	Loss/tok 3.4590 (3.4691)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.067 (0.089)	Data 1.07e-04 (4.72e-04)	Tok/s 230370 (241785)	Loss/tok 3.3094 (3.4679)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][570/1291]	Time 0.067 (0.090)	Data 1.41e-04 (4.66e-04)	Tok/s 232049 (241841)	Loss/tok 3.2548 (3.4710)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.067 (0.090)	Data 1.17e-04 (4.60e-04)	Tok/s 232940 (241869)	Loss/tok 3.1427 (3.4704)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.036 (0.090)	Data 1.09e-04 (4.54e-04)	Tok/s 221981 (241744)	Loss/tok 2.7873 (3.4674)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.067 (0.090)	Data 1.26e-04 (4.48e-04)	Tok/s 231404 (241826)	Loss/tok 3.1823 (3.4676)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.067 (0.090)	Data 1.10e-04 (4.43e-04)	Tok/s 228753 (241845)	Loss/tok 3.1959 (3.4662)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.134 (0.090)	Data 1.08e-04 (4.37e-04)	Tok/s 259823 (241986)	Loss/tok 3.6487 (3.4669)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.066 (0.090)	Data 1.16e-04 (4.32e-04)	Tok/s 231583 (242060)	Loss/tok 3.1855 (3.4657)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.28e-04)	Tok/s 254427 (242093)	Loss/tok 3.3756 (3.4641)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.037 (0.090)	Data 1.11e-04 (4.23e-04)	Tok/s 215426 (242077)	Loss/tok 2.7422 (3.4628)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.18e-04)	Tok/s 227328 (242063)	Loss/tok 3.2171 (3.4629)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.14e-04)	Tok/s 235755 (242001)	Loss/tok 3.0882 (3.4630)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.067 (0.090)	Data 1.34e-04 (4.10e-04)	Tok/s 232184 (242027)	Loss/tok 3.1666 (3.4628)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.036 (0.090)	Data 1.12e-04 (4.05e-04)	Tok/s 221200 (242107)	Loss/tok 2.8231 (3.4637)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][700/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.01e-04)	Tok/s 236708 (242062)	Loss/tok 3.1500 (3.4619)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.97e-04)	Tok/s 229570 (241963)	Loss/tok 3.1900 (3.4607)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.036 (0.089)	Data 1.18e-04 (3.93e-04)	Tok/s 215864 (241865)	Loss/tok 2.7550 (3.4588)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.89e-04)	Tok/s 254137 (241975)	Loss/tok 3.3705 (3.4579)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.86e-04)	Tok/s 232800 (242010)	Loss/tok 3.1876 (3.4576)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.135 (0.090)	Data 1.19e-04 (3.82e-04)	Tok/s 262034 (242081)	Loss/tok 3.5376 (3.4590)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.79e-04)	Tok/s 229137 (241971)	Loss/tok 3.1684 (3.4569)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.100 (0.089)	Data 1.26e-04 (3.75e-04)	Tok/s 249871 (241996)	Loss/tok 3.4113 (3.4557)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.72e-04)	Tok/s 255639 (242032)	Loss/tok 3.3224 (3.4565)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.69e-04)	Tok/s 252585 (242033)	Loss/tok 3.4625 (3.4561)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.067 (0.090)	Data 1.11e-04 (3.66e-04)	Tok/s 234802 (242095)	Loss/tok 3.1967 (3.4576)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.100 (0.090)	Data 1.11e-04 (3.62e-04)	Tok/s 254400 (242038)	Loss/tok 3.4505 (3.4561)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.067 (0.090)	Data 1.10e-04 (3.59e-04)	Tok/s 231829 (242062)	Loss/tok 3.2331 (3.4555)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][830/1291]	Time 0.067 (0.090)	Data 1.11e-04 (3.56e-04)	Tok/s 234465 (242120)	Loss/tok 3.1740 (3.4549)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.067 (0.090)	Data 1.10e-04 (3.53e-04)	Tok/s 230289 (242077)	Loss/tok 3.2243 (3.4545)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.51e-04)	Tok/s 230529 (242048)	Loss/tok 3.2676 (3.4533)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.100 (0.090)	Data 1.12e-04 (3.48e-04)	Tok/s 254163 (242088)	Loss/tok 3.3707 (3.4537)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.100 (0.090)	Data 1.11e-04 (3.45e-04)	Tok/s 248326 (242112)	Loss/tok 3.5054 (3.4532)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.036 (0.090)	Data 1.17e-04 (3.43e-04)	Tok/s 221162 (242067)	Loss/tok 2.7397 (3.4525)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.135 (0.090)	Data 1.11e-04 (3.40e-04)	Tok/s 257273 (242089)	Loss/tok 3.6504 (3.4518)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.38e-04)	Tok/s 231468 (242056)	Loss/tok 3.2885 (3.4503)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.136 (0.090)	Data 1.10e-04 (3.35e-04)	Tok/s 255799 (242117)	Loss/tok 3.6550 (3.4502)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.067 (0.090)	Data 1.10e-04 (3.33e-04)	Tok/s 232313 (242114)	Loss/tok 3.1522 (3.4495)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.100 (0.089)	Data 1.25e-04 (3.31e-04)	Tok/s 251832 (242086)	Loss/tok 3.3568 (3.4483)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.174 (0.090)	Data 1.14e-04 (3.28e-04)	Tok/s 256463 (242138)	Loss/tok 3.6633 (3.4487)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.174 (0.090)	Data 1.10e-04 (3.26e-04)	Tok/s 254225 (242165)	Loss/tok 3.8233 (3.4486)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][960/1291]	Time 0.135 (0.090)	Data 1.11e-04 (3.24e-04)	Tok/s 262326 (242151)	Loss/tok 3.6101 (3.4476)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][970/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.22e-04)	Tok/s 232158 (242126)	Loss/tok 3.0804 (3.4469)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.100 (0.090)	Data 1.13e-04 (3.20e-04)	Tok/s 254119 (242132)	Loss/tok 3.4023 (3.4462)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.101 (0.090)	Data 1.11e-04 (3.18e-04)	Tok/s 250595 (242166)	Loss/tok 3.4760 (3.4466)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.16e-04)	Tok/s 230784 (242171)	Loss/tok 3.1092 (3.4457)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.136 (0.090)	Data 1.12e-04 (3.14e-04)	Tok/s 260670 (242123)	Loss/tok 3.5525 (3.4446)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.135 (0.090)	Data 1.11e-04 (3.12e-04)	Tok/s 259376 (242158)	Loss/tok 3.5266 (3.4444)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.101 (0.090)	Data 1.12e-04 (3.10e-04)	Tok/s 250592 (242178)	Loss/tok 3.3698 (3.4439)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.08e-04)	Tok/s 232243 (242181)	Loss/tok 3.1892 (3.4434)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.068 (0.090)	Data 1.14e-04 (3.06e-04)	Tok/s 229735 (242203)	Loss/tok 3.1780 (3.4428)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.04e-04)	Tok/s 231555 (242179)	Loss/tok 3.1814 (3.4420)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.02e-04)	Tok/s 225798 (242117)	Loss/tok 3.2599 (3.4408)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.173 (0.090)	Data 1.14e-04 (3.01e-04)	Tok/s 258400 (242082)	Loss/tok 3.7240 (3.4416)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.036 (0.090)	Data 1.11e-04 (2.99e-04)	Tok/s 221121 (242045)	Loss/tok 2.7379 (3.4402)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1100/1291]	Time 0.135 (0.090)	Data 1.11e-04 (2.97e-04)	Tok/s 260362 (241995)	Loss/tok 3.5348 (3.4388)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.036 (0.089)	Data 1.14e-04 (2.96e-04)	Tok/s 223760 (241905)	Loss/tok 2.7286 (3.4372)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.137 (0.089)	Data 1.31e-04 (2.94e-04)	Tok/s 255598 (241906)	Loss/tok 3.4885 (3.4360)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.92e-04)	Tok/s 231559 (241874)	Loss/tok 3.2179 (3.4350)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.136 (0.089)	Data 1.35e-04 (2.91e-04)	Tok/s 260518 (241894)	Loss/tok 3.4645 (3.4341)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1150/1291]	Time 0.174 (0.089)	Data 1.13e-04 (2.89e-04)	Tok/s 257550 (241911)	Loss/tok 3.7472 (3.4344)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.068 (0.089)	Data 1.11e-04 (2.88e-04)	Tok/s 227823 (241809)	Loss/tok 3.1212 (3.4329)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.068 (0.089)	Data 1.12e-04 (2.86e-04)	Tok/s 228692 (241755)	Loss/tok 3.1968 (3.4318)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.85e-04)	Tok/s 253480 (241724)	Loss/tok 3.4861 (3.4315)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.84e-04)	Tok/s 249853 (241705)	Loss/tok 3.4070 (3.4302)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.101 (0.089)	Data 1.14e-04 (2.82e-04)	Tok/s 252686 (241683)	Loss/tok 3.2696 (3.4287)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.174 (0.089)	Data 1.13e-04 (2.81e-04)	Tok/s 258094 (241707)	Loss/tok 3.6451 (3.4289)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.067 (0.089)	Data 1.24e-04 (2.79e-04)	Tok/s 230303 (241732)	Loss/tok 3.1789 (3.4287)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.78e-04)	Tok/s 227807 (241725)	Loss/tok 3.1607 (3.4285)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.100 (0.089)	Data 1.33e-04 (2.77e-04)	Tok/s 249917 (241788)	Loss/tok 3.3851 (3.4293)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.099 (0.089)	Data 1.14e-04 (2.75e-04)	Tok/s 251415 (241816)	Loss/tok 3.3743 (3.4295)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.74e-04)	Tok/s 230894 (241772)	Loss/tok 3.1657 (3.4282)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.036 (0.089)	Data 1.09e-04 (2.73e-04)	Tok/s 222302 (241704)	Loss/tok 2.7284 (3.4270)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1280/1291]	Time 0.036 (0.089)	Data 1.13e-04 (2.72e-04)	Tok/s 220942 (241722)	Loss/tok 2.6475 (3.4265)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.067 (0.089)	Data 4.24e-05 (2.73e-04)	Tok/s 233737 (241770)	Loss/tok 3.1091 (3.4264)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593142952352, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593142952352, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.393 (0.393)	Decoder iters 104.0 (104.0)	Tok/s 41222 (41222)
0: Running moses detokenizer
0: BLEU(score=21.802634694292824, counts=[35814, 17149, 9422, 5337], totals=[65399, 62396, 59393, 56394], precisions=[54.762305234024986, 27.484133598307583, 15.863822335965517, 9.46377274178104], bp=1.0, sys_len=65399, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593142954304, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.218, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593142954304, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4275	Test BLEU: 21.80
0: Performance: Epoch: 1	Training: 1933973 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593142954305, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593142954305, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593142954305, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2109092782
0: TRAIN [2][0/1291]	Time 0.303 (0.303)	Data 1.85e-01 (1.85e-01)	Tok/s 82751 (82751)	Loss/tok 3.3105 (3.3105)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.173 (0.114)	Data 1.30e-04 (1.69e-02)	Tok/s 257050 (227620)	Loss/tok 3.7167 (3.3490)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.099 (0.113)	Data 1.10e-04 (8.91e-03)	Tok/s 254014 (237996)	Loss/tok 3.2620 (3.3762)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.175 (0.107)	Data 1.17e-04 (6.07e-03)	Tok/s 255259 (240098)	Loss/tok 3.6272 (3.3531)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][40/1291]	Time 0.173 (0.103)	Data 1.06e-04 (4.62e-03)	Tok/s 255957 (240263)	Loss/tok 3.6760 (3.3534)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.036 (0.100)	Data 1.13e-04 (3.74e-03)	Tok/s 220593 (239754)	Loss/tok 2.6574 (3.3531)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.066 (0.101)	Data 1.07e-04 (3.14e-03)	Tok/s 232940 (241292)	Loss/tok 3.0752 (3.3599)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.099 (0.101)	Data 1.10e-04 (2.72e-03)	Tok/s 255131 (241712)	Loss/tok 3.3218 (3.3550)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.100 (0.100)	Data 1.04e-04 (2.39e-03)	Tok/s 251903 (241688)	Loss/tok 3.3916 (3.3493)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.099 (0.100)	Data 1.10e-04 (2.14e-03)	Tok/s 251759 (242326)	Loss/tok 3.2152 (3.3448)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.134 (0.099)	Data 1.06e-04 (1.94e-03)	Tok/s 259823 (242703)	Loss/tok 3.4226 (3.3379)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.099 (0.098)	Data 1.10e-04 (1.78e-03)	Tok/s 254422 (242834)	Loss/tok 3.3095 (3.3315)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.067 (0.098)	Data 1.08e-04 (1.64e-03)	Tok/s 233744 (243018)	Loss/tok 3.1524 (3.3332)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.067 (0.097)	Data 1.21e-04 (1.52e-03)	Tok/s 231511 (243129)	Loss/tok 3.0654 (3.3291)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.134 (0.096)	Data 1.23e-04 (1.42e-03)	Tok/s 258431 (242771)	Loss/tok 3.5059 (3.3220)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.135 (0.096)	Data 1.07e-04 (1.34e-03)	Tok/s 260818 (243172)	Loss/tok 3.3417 (3.3184)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.067 (0.095)	Data 1.24e-04 (1.26e-03)	Tok/s 231293 (243050)	Loss/tok 3.1078 (3.3127)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][170/1291]	Time 0.066 (0.094)	Data 1.06e-04 (1.19e-03)	Tok/s 231168 (242849)	Loss/tok 3.1531 (3.3078)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.067 (0.094)	Data 1.08e-04 (1.13e-03)	Tok/s 232746 (242922)	Loss/tok 3.1209 (3.3081)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.099 (0.093)	Data 1.30e-04 (1.08e-03)	Tok/s 249097 (242493)	Loss/tok 3.3949 (3.3027)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.099 (0.092)	Data 1.07e-04 (1.03e-03)	Tok/s 254329 (242514)	Loss/tok 3.1991 (3.2994)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.067 (0.091)	Data 1.10e-04 (9.88e-04)	Tok/s 228377 (242286)	Loss/tok 3.0383 (3.2940)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.067 (0.091)	Data 1.13e-04 (9.48e-04)	Tok/s 233790 (242355)	Loss/tok 3.0744 (3.2929)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.099 (0.091)	Data 1.06e-04 (9.12e-04)	Tok/s 250150 (242217)	Loss/tok 3.3027 (3.2910)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.100 (0.091)	Data 1.10e-04 (8.79e-04)	Tok/s 252660 (242480)	Loss/tok 3.2729 (3.2924)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.066 (0.091)	Data 1.08e-04 (8.48e-04)	Tok/s 236083 (242234)	Loss/tok 3.0004 (3.2911)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.036 (0.090)	Data 1.11e-04 (8.20e-04)	Tok/s 221466 (242007)	Loss/tok 2.7150 (3.2880)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.036 (0.090)	Data 1.08e-04 (7.94e-04)	Tok/s 224000 (241936)	Loss/tok 2.6344 (3.2864)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.135 (0.090)	Data 1.13e-04 (7.69e-04)	Tok/s 259577 (242067)	Loss/tok 3.4840 (3.2870)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][290/1291]	Time 0.100 (0.090)	Data 1.08e-04 (7.47e-04)	Tok/s 252275 (242028)	Loss/tok 3.3484 (3.2855)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.066 (0.089)	Data 1.05e-04 (7.26e-04)	Tok/s 231607 (241846)	Loss/tok 3.1219 (3.2826)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.100 (0.089)	Data 1.06e-04 (7.06e-04)	Tok/s 251310 (241764)	Loss/tok 3.3166 (3.2798)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.174 (0.090)	Data 1.08e-04 (6.87e-04)	Tok/s 254764 (241959)	Loss/tok 3.6381 (3.2841)	LR 2.875e-03
0: Gradient norm: nan
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][330/1291]	Time 0.067 (0.090)	Data 1.03e-04 (6.70e-04)	Tok/s 233900 (242022)	Loss/tok 3.0108 (3.2863)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.135 (0.090)	Data 1.22e-04 (6.54e-04)	Tok/s 261115 (241919)	Loss/tok 3.4504 (3.2876)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.135 (0.090)	Data 1.11e-04 (6.38e-04)	Tok/s 260711 (242138)	Loss/tok 3.4428 (3.2900)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.067 (0.091)	Data 1.11e-04 (6.24e-04)	Tok/s 231179 (242246)	Loss/tok 2.9933 (3.2903)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.099 (0.091)	Data 1.25e-04 (6.10e-04)	Tok/s 252200 (242337)	Loss/tok 3.4668 (3.2910)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.099 (0.091)	Data 1.06e-04 (5.97e-04)	Tok/s 255824 (242369)	Loss/tok 3.2462 (3.2891)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.067 (0.090)	Data 1.08e-04 (5.84e-04)	Tok/s 231560 (242300)	Loss/tok 3.0609 (3.2874)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.100 (0.090)	Data 1.06e-04 (5.72e-04)	Tok/s 251498 (242423)	Loss/tok 3.3415 (3.2880)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.100 (0.091)	Data 1.18e-04 (5.61e-04)	Tok/s 253303 (242531)	Loss/tok 3.3153 (3.2903)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.100 (0.091)	Data 1.23e-04 (5.51e-04)	Tok/s 251272 (242442)	Loss/tok 3.2904 (3.2901)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.135 (0.090)	Data 1.31e-04 (5.40e-04)	Tok/s 261090 (242371)	Loss/tok 3.4583 (3.2891)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.036 (0.090)	Data 1.08e-04 (5.31e-04)	Tok/s 218768 (242240)	Loss/tok 2.6737 (3.2867)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.067 (0.090)	Data 1.16e-04 (5.21e-04)	Tok/s 231710 (242376)	Loss/tok 3.1022 (3.2889)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][460/1291]	Time 0.036 (0.090)	Data 1.06e-04 (5.12e-04)	Tok/s 223460 (242375)	Loss/tok 2.6995 (3.2902)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.099 (0.091)	Data 1.13e-04 (5.04e-04)	Tok/s 250377 (242486)	Loss/tok 3.2565 (3.2933)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.96e-04)	Tok/s 231897 (242357)	Loss/tok 3.1707 (3.2920)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.88e-04)	Tok/s 234820 (242295)	Loss/tok 3.0233 (3.2905)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][500/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.81e-04)	Tok/s 233111 (242321)	Loss/tok 3.1559 (3.2923)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.134 (0.091)	Data 1.13e-04 (4.74e-04)	Tok/s 259681 (242440)	Loss/tok 3.5823 (3.2956)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.067 (0.091)	Data 1.23e-04 (4.68e-04)	Tok/s 231467 (242407)	Loss/tok 3.1024 (3.2973)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.067 (0.091)	Data 1.16e-04 (4.61e-04)	Tok/s 233320 (242367)	Loss/tok 3.1783 (3.2958)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.068 (0.091)	Data 1.15e-04 (4.55e-04)	Tok/s 229445 (242403)	Loss/tok 2.9705 (3.2968)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.100 (0.091)	Data 1.19e-04 (4.49e-04)	Tok/s 250336 (242338)	Loss/tok 3.3225 (3.2959)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.43e-04)	Tok/s 254549 (242303)	Loss/tok 3.2966 (3.2941)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.099 (0.090)	Data 1.12e-04 (4.37e-04)	Tok/s 252173 (242276)	Loss/tok 3.3095 (3.2935)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.135 (0.090)	Data 1.16e-04 (4.31e-04)	Tok/s 257909 (242238)	Loss/tok 3.4389 (3.2923)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.036 (0.090)	Data 1.19e-04 (4.26e-04)	Tok/s 218267 (242113)	Loss/tok 2.6775 (3.2915)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.036 (0.090)	Data 1.17e-04 (4.21e-04)	Tok/s 216121 (242039)	Loss/tok 2.6150 (3.2904)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.067 (0.090)	Data 1.15e-04 (4.16e-04)	Tok/s 233210 (241983)	Loss/tok 3.1172 (3.2897)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.067 (0.090)	Data 1.37e-04 (4.11e-04)	Tok/s 229992 (242043)	Loss/tok 3.0714 (3.2910)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][630/1291]	Time 0.174 (0.090)	Data 1.17e-04 (4.06e-04)	Tok/s 253938 (241994)	Loss/tok 3.6869 (3.2918)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.175 (0.090)	Data 1.15e-04 (4.02e-04)	Tok/s 258682 (242135)	Loss/tok 3.6275 (3.2935)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.138 (0.090)	Data 1.13e-04 (3.98e-04)	Tok/s 254583 (242231)	Loss/tok 3.4564 (3.2955)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.068 (0.090)	Data 1.14e-04 (3.93e-04)	Tok/s 226642 (242224)	Loss/tok 3.0722 (3.2946)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.067 (0.090)	Data 1.36e-04 (3.89e-04)	Tok/s 232259 (242058)	Loss/tok 3.0444 (3.2925)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.099 (0.090)	Data 1.38e-04 (3.86e-04)	Tok/s 255845 (242034)	Loss/tok 3.2664 (3.2929)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.134 (0.090)	Data 1.13e-04 (3.82e-04)	Tok/s 260278 (242117)	Loss/tok 3.4585 (3.2930)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.067 (0.090)	Data 1.33e-04 (3.78e-04)	Tok/s 230345 (242094)	Loss/tok 3.1339 (3.2923)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.099 (0.090)	Data 1.12e-04 (3.74e-04)	Tok/s 254875 (242096)	Loss/tok 3.2558 (3.2913)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.135 (0.090)	Data 1.16e-04 (3.71e-04)	Tok/s 258823 (242073)	Loss/tok 3.4970 (3.2917)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.100 (0.090)	Data 1.09e-04 (3.67e-04)	Tok/s 251626 (242038)	Loss/tok 3.2697 (3.2907)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.100 (0.090)	Data 1.26e-04 (3.64e-04)	Tok/s 249210 (242022)	Loss/tok 3.2602 (3.2899)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.036 (0.090)	Data 1.12e-04 (3.60e-04)	Tok/s 224812 (242010)	Loss/tok 2.6867 (3.2887)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][760/1291]	Time 0.036 (0.089)	Data 1.17e-04 (3.57e-04)	Tok/s 222627 (241946)	Loss/tok 2.5901 (3.2869)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.54e-04)	Tok/s 228694 (241900)	Loss/tok 3.0885 (3.2861)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.135 (0.089)	Data 1.13e-04 (3.51e-04)	Tok/s 261816 (241966)	Loss/tok 3.4022 (3.2861)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.48e-04)	Tok/s 254860 (241897)	Loss/tok 3.3089 (3.2851)	LR 2.875e-03
0: Gradient norm: nan
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][800/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.45e-04)	Tok/s 231161 (241938)	Loss/tok 3.0992 (3.2861)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.42e-04)	Tok/s 232544 (241865)	Loss/tok 3.1296 (3.2853)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.39e-04)	Tok/s 232195 (241753)	Loss/tok 3.1457 (3.2838)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.37e-04)	Tok/s 232559 (241690)	Loss/tok 3.0758 (3.2828)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.34e-04)	Tok/s 231665 (241732)	Loss/tok 3.0294 (3.2842)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.32e-04)	Tok/s 231747 (241786)	Loss/tok 3.0829 (3.2844)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.035 (0.089)	Data 1.11e-04 (3.29e-04)	Tok/s 221156 (241690)	Loss/tok 2.6732 (3.2829)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.136 (0.089)	Data 1.12e-04 (3.27e-04)	Tok/s 260444 (241676)	Loss/tok 3.3898 (3.2826)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.24e-04)	Tok/s 229774 (241698)	Loss/tok 3.0272 (3.2825)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.22e-04)	Tok/s 249475 (241730)	Loss/tok 3.2878 (3.2827)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.20e-04)	Tok/s 232669 (241736)	Loss/tok 3.0672 (3.2824)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.134 (0.089)	Data 1.18e-04 (3.17e-04)	Tok/s 260684 (241715)	Loss/tok 3.5014 (3.2826)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.036 (0.089)	Data 1.12e-04 (3.15e-04)	Tok/s 218179 (241730)	Loss/tok 2.6972 (3.2823)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][930/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.13e-04)	Tok/s 252223 (241732)	Loss/tok 3.2236 (3.2825)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.11e-04)	Tok/s 250576 (241726)	Loss/tok 3.1582 (3.2822)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.09e-04)	Tok/s 232556 (241745)	Loss/tok 3.1061 (3.2822)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.134 (0.089)	Data 1.10e-04 (3.07e-04)	Tok/s 260874 (241763)	Loss/tok 3.4432 (3.2819)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.135 (0.089)	Data 1.14e-04 (3.05e-04)	Tok/s 257718 (241781)	Loss/tok 3.4439 (3.2817)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][980/1291]	Time 0.067 (0.089)	Data 1.17e-04 (3.03e-04)	Tok/s 230696 (241813)	Loss/tok 3.1008 (3.2834)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.01e-04)	Tok/s 229744 (241804)	Loss/tok 3.1447 (3.2833)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.99e-04)	Tok/s 229235 (241709)	Loss/tok 3.0687 (3.2817)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.97e-04)	Tok/s 231826 (241698)	Loss/tok 3.1262 (3.2811)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.95e-04)	Tok/s 255853 (241782)	Loss/tok 3.3023 (3.2817)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.036 (0.089)	Data 1.17e-04 (2.94e-04)	Tok/s 224484 (241783)	Loss/tok 2.7346 (3.2817)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.92e-04)	Tok/s 230707 (241790)	Loss/tok 2.9775 (3.2823)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.134 (0.089)	Data 1.08e-04 (2.90e-04)	Tok/s 258994 (241885)	Loss/tok 3.4098 (3.2828)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.89e-04)	Tok/s 227674 (241811)	Loss/tok 3.1413 (3.2816)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.067 (0.089)	Data 1.23e-04 (2.87e-04)	Tok/s 229071 (241808)	Loss/tok 3.1193 (3.2817)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.85e-04)	Tok/s 231923 (241767)	Loss/tok 3.0728 (3.2806)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.84e-04)	Tok/s 256738 (241771)	Loss/tok 3.2640 (3.2800)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.82e-04)	Tok/s 230706 (241737)	Loss/tok 3.0356 (3.2796)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1110/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.81e-04)	Tok/s 232679 (241693)	Loss/tok 3.0621 (3.2791)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.79e-04)	Tok/s 255543 (241721)	Loss/tok 3.2542 (3.2800)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.78e-04)	Tok/s 253409 (241706)	Loss/tok 3.2786 (3.2802)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.135 (0.089)	Data 1.46e-04 (2.77e-04)	Tok/s 258323 (241717)	Loss/tok 3.4532 (3.2810)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.75e-04)	Tok/s 230524 (241733)	Loss/tok 3.0172 (3.2810)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.134 (0.089)	Data 1.10e-04 (2.74e-04)	Tok/s 262092 (241768)	Loss/tok 3.3874 (3.2819)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.099 (0.089)	Data 1.16e-04 (2.72e-04)	Tok/s 256794 (241806)	Loss/tok 3.2201 (3.2818)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.71e-04)	Tok/s 251994 (241743)	Loss/tok 3.2481 (3.2806)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1190/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.70e-04)	Tok/s 235706 (241773)	Loss/tok 3.0931 (3.2808)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.134 (0.089)	Data 1.10e-04 (2.69e-04)	Tok/s 258337 (241749)	Loss/tok 3.4955 (3.2804)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.099 (0.089)	Data 1.14e-04 (2.67e-04)	Tok/s 254664 (241785)	Loss/tok 3.2657 (3.2804)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.135 (0.089)	Data 1.15e-04 (2.66e-04)	Tok/s 255682 (241855)	Loss/tok 3.5238 (3.2814)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.65e-04)	Tok/s 252895 (241883)	Loss/tok 3.3566 (3.2816)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.066 (0.089)	Data 1.17e-04 (2.64e-04)	Tok/s 234836 (241833)	Loss/tok 3.1078 (3.2809)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.62e-04)	Tok/s 236839 (241787)	Loss/tok 3.0225 (3.2801)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.61e-04)	Tok/s 233953 (241791)	Loss/tok 3.0572 (3.2799)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1270/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.60e-04)	Tok/s 234093 (241771)	Loss/tok 3.1496 (3.2803)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.067 (0.089)	Data 1.22e-04 (2.59e-04)	Tok/s 235114 (241806)	Loss/tok 3.1015 (3.2811)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.066 (0.089)	Data 4.70e-05 (2.61e-04)	Tok/s 233974 (241817)	Loss/tok 3.0120 (3.2815)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593143069987, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593143069987, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.520 (0.520)	Decoder iters 149.0 (149.0)	Tok/s 32593 (32593)
0: Running moses detokenizer
0: BLEU(score=22.212725482497856, counts=[36525, 17741, 9857, 5722], totals=[66840, 63837, 60834, 57836], precisions=[54.64542190305207, 27.79109293983113, 16.203110102902983, 9.893491942734629], bp=1.0, sys_len=66840, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593143072088, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22210000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593143072089, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2806	Test BLEU: 22.21
0: Performance: Epoch: 2	Training: 1935145 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593143072089, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593143072089, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143072089, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1899900022
0: TRAIN [3][0/1291]	Time 0.313 (0.313)	Data 1.94e-01 (1.94e-01)	Tok/s 80941 (80941)	Loss/tok 3.2068 (3.2068)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.099 (0.114)	Data 1.10e-04 (1.77e-02)	Tok/s 255084 (231224)	Loss/tok 3.1853 (3.2008)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.067 (0.098)	Data 1.05e-04 (9.33e-03)	Tok/s 228064 (234798)	Loss/tok 2.9882 (3.1484)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.135 (0.106)	Data 1.08e-04 (6.36e-03)	Tok/s 257230 (240308)	Loss/tok 3.4936 (3.2191)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.135 (0.104)	Data 1.30e-04 (4.84e-03)	Tok/s 256737 (242755)	Loss/tok 3.4381 (3.2131)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.067 (0.100)	Data 1.14e-04 (3.91e-03)	Tok/s 231971 (241772)	Loss/tok 3.0696 (3.2031)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.070 (0.099)	Data 1.09e-04 (3.29e-03)	Tok/s 220307 (242327)	Loss/tok 3.0617 (3.2023)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.067 (0.095)	Data 1.12e-04 (2.84e-03)	Tok/s 229368 (241687)	Loss/tok 3.0783 (3.1884)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.099 (0.094)	Data 1.08e-04 (2.50e-03)	Tok/s 256307 (241863)	Loss/tok 3.1281 (3.1820)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.099 (0.092)	Data 1.09e-04 (2.24e-03)	Tok/s 252744 (241499)	Loss/tok 3.1235 (3.1730)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.036 (0.090)	Data 1.07e-04 (2.03e-03)	Tok/s 225878 (241393)	Loss/tok 2.5771 (3.1681)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][110/1291]	Time 0.099 (0.091)	Data 1.09e-04 (1.86e-03)	Tok/s 255458 (241748)	Loss/tok 3.1836 (3.1787)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.066 (0.093)	Data 1.11e-04 (1.71e-03)	Tok/s 234327 (242266)	Loss/tok 3.0059 (3.1886)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.099 (0.092)	Data 1.09e-04 (1.59e-03)	Tok/s 248842 (241999)	Loss/tok 3.1935 (3.1845)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.174 (0.092)	Data 1.20e-04 (1.49e-03)	Tok/s 256569 (241960)	Loss/tok 3.5855 (3.1878)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.066 (0.091)	Data 1.07e-04 (1.39e-03)	Tok/s 236420 (241620)	Loss/tok 3.0908 (3.1869)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.067 (0.091)	Data 1.07e-04 (1.31e-03)	Tok/s 229004 (241550)	Loss/tok 2.9421 (3.1870)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.036 (0.090)	Data 1.07e-04 (1.24e-03)	Tok/s 220215 (241341)	Loss/tok 2.6473 (3.1856)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.036 (0.089)	Data 1.06e-04 (1.18e-03)	Tok/s 220677 (241139)	Loss/tok 2.6901 (3.1816)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.099 (0.089)	Data 1.28e-04 (1.13e-03)	Tok/s 253548 (241361)	Loss/tok 3.1746 (3.1806)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.067 (0.089)	Data 1.28e-04 (1.07e-03)	Tok/s 234508 (240998)	Loss/tok 2.9393 (3.1783)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.099 (0.089)	Data 1.05e-04 (1.03e-03)	Tok/s 257257 (241079)	Loss/tok 3.1892 (3.1804)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.067 (0.089)	Data 1.04e-04 (9.87e-04)	Tok/s 231424 (241537)	Loss/tok 3.0130 (3.1822)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.067 (0.089)	Data 1.07e-04 (9.49e-04)	Tok/s 232749 (241509)	Loss/tok 2.9817 (3.1802)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][240/1291]	Time 0.067 (0.089)	Data 1.24e-04 (9.15e-04)	Tok/s 234914 (241809)	Loss/tok 2.8539 (3.1812)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.099 (0.090)	Data 1.29e-04 (8.83e-04)	Tok/s 256003 (242138)	Loss/tok 3.1557 (3.1844)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.099 (0.090)	Data 1.08e-04 (8.53e-04)	Tok/s 258823 (242256)	Loss/tok 3.2495 (3.1870)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.136 (0.090)	Data 1.07e-04 (8.26e-04)	Tok/s 254575 (242166)	Loss/tok 3.4153 (3.1857)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.100 (0.089)	Data 1.18e-04 (8.00e-04)	Tok/s 250995 (241990)	Loss/tok 3.0875 (3.1811)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.100 (0.089)	Data 1.06e-04 (7.76e-04)	Tok/s 252121 (241988)	Loss/tok 3.1284 (3.1786)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.067 (0.089)	Data 1.25e-04 (7.54e-04)	Tok/s 233581 (241955)	Loss/tok 2.9299 (3.1784)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.101 (0.089)	Data 1.04e-04 (7.34e-04)	Tok/s 248499 (241772)	Loss/tok 3.1130 (3.1754)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.036 (0.088)	Data 1.06e-04 (7.14e-04)	Tok/s 222221 (241585)	Loss/tok 2.5006 (3.1715)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.067 (0.088)	Data 1.23e-04 (6.96e-04)	Tok/s 233496 (241564)	Loss/tok 3.0484 (3.1685)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.099 (0.088)	Data 1.05e-04 (6.79e-04)	Tok/s 254201 (241691)	Loss/tok 3.1687 (3.1679)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.067 (0.088)	Data 1.09e-04 (6.62e-04)	Tok/s 233836 (241622)	Loss/tok 3.0155 (3.1670)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.067 (0.088)	Data 1.06e-04 (6.47e-04)	Tok/s 227701 (241549)	Loss/tok 3.0435 (3.1648)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][370/1291]	Time 0.134 (0.088)	Data 1.23e-04 (6.33e-04)	Tok/s 265323 (241541)	Loss/tok 3.2265 (3.1635)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.036 (0.088)	Data 1.09e-04 (6.19e-04)	Tok/s 218464 (241658)	Loss/tok 2.5207 (3.1640)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.066 (0.088)	Data 1.21e-04 (6.06e-04)	Tok/s 234662 (241761)	Loss/tok 2.9616 (3.1655)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.136 (0.088)	Data 1.25e-04 (5.94e-04)	Tok/s 257431 (241832)	Loss/tok 3.2869 (3.1650)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.036 (0.089)	Data 1.12e-04 (5.82e-04)	Tok/s 220891 (241974)	Loss/tok 2.6062 (3.1687)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][420/1291]	Time 0.100 (0.089)	Data 1.05e-04 (5.71e-04)	Tok/s 251537 (242075)	Loss/tok 3.1662 (3.1707)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.135 (0.089)	Data 1.08e-04 (5.60e-04)	Tok/s 257333 (242027)	Loss/tok 3.3403 (3.1694)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.100 (0.089)	Data 1.04e-04 (5.50e-04)	Tok/s 253527 (242075)	Loss/tok 3.1580 (3.1681)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.099 (0.089)	Data 1.30e-04 (5.40e-04)	Tok/s 255123 (242216)	Loss/tok 3.0783 (3.1686)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.100 (0.089)	Data 1.07e-04 (5.30e-04)	Tok/s 253274 (242192)	Loss/tok 3.1849 (3.1698)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.090)	Data 1.09e-04 (5.22e-04)	Tok/s 231002 (242262)	Loss/tok 2.8727 (3.1705)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.100 (0.090)	Data 1.07e-04 (5.13e-04)	Tok/s 250325 (242239)	Loss/tok 3.1956 (3.1688)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.036 (0.090)	Data 1.08e-04 (5.05e-04)	Tok/s 217609 (242223)	Loss/tok 2.5825 (3.1694)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.067 (0.090)	Data 1.09e-04 (4.97e-04)	Tok/s 232062 (242360)	Loss/tok 3.0029 (3.1697)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.067 (0.090)	Data 1.09e-04 (4.89e-04)	Tok/s 228622 (242423)	Loss/tok 2.9432 (3.1716)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.099 (0.090)	Data 1.06e-04 (4.82e-04)	Tok/s 252692 (242291)	Loss/tok 3.2791 (3.1697)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.067 (0.090)	Data 1.06e-04 (4.75e-04)	Tok/s 231721 (242323)	Loss/tok 2.9677 (3.1709)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][540/1291]	Time 0.067 (0.090)	Data 1.07e-04 (4.68e-04)	Tok/s 228787 (242275)	Loss/tok 2.9446 (3.1691)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.137 (0.090)	Data 1.05e-04 (4.62e-04)	Tok/s 252315 (242184)	Loss/tok 3.3502 (3.1698)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][560/1291]	Time 0.134 (0.090)	Data 1.07e-04 (4.56e-04)	Tok/s 258469 (242177)	Loss/tok 3.2110 (3.1688)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.134 (0.090)	Data 1.09e-04 (4.49e-04)	Tok/s 260329 (242184)	Loss/tok 3.4285 (3.1698)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.066 (0.090)	Data 1.10e-04 (4.44e-04)	Tok/s 234049 (242162)	Loss/tok 2.9150 (3.1691)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.036 (0.090)	Data 1.30e-04 (4.38e-04)	Tok/s 224274 (242070)	Loss/tok 2.5257 (3.1698)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.174 (0.090)	Data 1.11e-04 (4.33e-04)	Tok/s 257193 (242173)	Loss/tok 3.5013 (3.1712)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.099 (0.090)	Data 1.07e-04 (4.27e-04)	Tok/s 254035 (242254)	Loss/tok 3.1441 (3.1728)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.036 (0.090)	Data 1.07e-04 (4.22e-04)	Tok/s 221477 (242270)	Loss/tok 2.6172 (3.1726)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.067 (0.090)	Data 1.05e-04 (4.17e-04)	Tok/s 228640 (242278)	Loss/tok 2.9954 (3.1717)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.099 (0.090)	Data 1.14e-04 (4.12e-04)	Tok/s 255857 (242331)	Loss/tok 3.1697 (3.1720)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.067 (0.090)	Data 1.05e-04 (4.08e-04)	Tok/s 231028 (242317)	Loss/tok 2.8435 (3.1709)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.174 (0.090)	Data 1.25e-04 (4.03e-04)	Tok/s 259913 (242346)	Loss/tok 3.3984 (3.1714)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.066 (0.090)	Data 1.03e-04 (3.99e-04)	Tok/s 236720 (242188)	Loss/tok 2.8688 (3.1692)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][680/1291]	Time 0.099 (0.089)	Data 1.06e-04 (3.95e-04)	Tok/s 253740 (242075)	Loss/tok 3.1973 (3.1676)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.067 (0.089)	Data 1.05e-04 (3.90e-04)	Tok/s 228498 (242080)	Loss/tok 2.9306 (3.1667)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.067 (0.090)	Data 1.04e-04 (3.86e-04)	Tok/s 228887 (242167)	Loss/tok 2.9319 (3.1678)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.134 (0.090)	Data 1.04e-04 (3.83e-04)	Tok/s 262147 (242162)	Loss/tok 3.3435 (3.1671)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.100 (0.090)	Data 1.07e-04 (3.79e-04)	Tok/s 251607 (242181)	Loss/tok 3.2128 (3.1661)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.173 (0.090)	Data 1.07e-04 (3.75e-04)	Tok/s 260228 (242195)	Loss/tok 3.3683 (3.1670)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.036 (0.090)	Data 1.06e-04 (3.71e-04)	Tok/s 216039 (242232)	Loss/tok 2.5396 (3.1669)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.099 (0.090)	Data 1.08e-04 (3.68e-04)	Tok/s 252287 (242148)	Loss/tok 3.2237 (3.1656)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.174 (0.090)	Data 1.05e-04 (3.65e-04)	Tok/s 255924 (242269)	Loss/tok 3.5184 (3.1666)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.067 (0.090)	Data 1.25e-04 (3.61e-04)	Tok/s 235642 (242165)	Loss/tok 2.8640 (3.1646)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.036 (0.089)	Data 1.22e-04 (3.58e-04)	Tok/s 223154 (242152)	Loss/tok 2.5333 (3.1636)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.067 (0.090)	Data 1.25e-04 (3.55e-04)	Tok/s 228542 (242189)	Loss/tok 2.9472 (3.1639)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.52e-04)	Tok/s 231212 (242092)	Loss/tok 2.9093 (3.1621)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][810/1291]	Time 0.100 (0.089)	Data 1.26e-04 (3.49e-04)	Tok/s 252940 (242071)	Loss/tok 3.1910 (3.1610)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.174 (0.089)	Data 1.18e-04 (3.46e-04)	Tok/s 259011 (242130)	Loss/tok 3.3430 (3.1612)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.44e-04)	Tok/s 229938 (242030)	Loss/tok 2.9436 (3.1597)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.067 (0.089)	Data 1.29e-04 (3.41e-04)	Tok/s 234309 (242054)	Loss/tok 2.9273 (3.1593)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.100 (0.089)	Data 1.38e-04 (3.38e-04)	Tok/s 253225 (242143)	Loss/tok 3.0793 (3.1602)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.36e-04)	Tok/s 254063 (242168)	Loss/tok 3.1821 (3.1593)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.067 (0.089)	Data 1.41e-04 (3.33e-04)	Tok/s 229624 (242128)	Loss/tok 2.9904 (3.1589)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.31e-04)	Tok/s 227646 (242140)	Loss/tok 2.9915 (3.1592)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.29e-04)	Tok/s 229720 (242090)	Loss/tok 2.9881 (3.1585)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.26e-04)	Tok/s 229469 (242119)	Loss/tok 2.8893 (3.1577)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.24e-04)	Tok/s 252870 (242137)	Loss/tok 3.2035 (3.1580)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.22e-04)	Tok/s 231904 (242155)	Loss/tok 2.9452 (3.1583)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.20e-04)	Tok/s 228436 (242221)	Loss/tok 2.9238 (3.1588)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][940/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.17e-04)	Tok/s 229674 (242179)	Loss/tok 2.9030 (3.1579)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.100 (0.090)	Data 1.14e-04 (3.15e-04)	Tok/s 252470 (242206)	Loss/tok 3.1294 (3.1576)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.099 (0.089)	Data 1.25e-04 (3.13e-04)	Tok/s 253883 (242203)	Loss/tok 3.1368 (3.1567)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.11e-04)	Tok/s 229689 (242197)	Loss/tok 2.8889 (3.1562)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.036 (0.089)	Data 1.14e-04 (3.09e-04)	Tok/s 225028 (242123)	Loss/tok 2.7162 (3.1548)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.07e-04)	Tok/s 229969 (242140)	Loss/tok 3.0229 (3.1541)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.05e-04)	Tok/s 254670 (242095)	Loss/tok 3.1809 (3.1537)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.04e-04)	Tok/s 230030 (242079)	Loss/tok 2.8977 (3.1533)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.02e-04)	Tok/s 262209 (242059)	Loss/tok 3.2539 (3.1525)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.099 (0.089)	Data 1.14e-04 (3.00e-04)	Tok/s 253045 (242060)	Loss/tok 3.0918 (3.1518)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.98e-04)	Tok/s 253528 (242094)	Loss/tok 3.1130 (3.1522)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.97e-04)	Tok/s 230838 (242124)	Loss/tok 2.9133 (3.1533)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.067 (0.089)	Data 1.18e-04 (2.95e-04)	Tok/s 230080 (242065)	Loss/tok 2.9697 (3.1521)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1070/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.93e-04)	Tok/s 253313 (242083)	Loss/tok 3.0425 (3.1509)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.92e-04)	Tok/s 254159 (242127)	Loss/tok 3.1509 (3.1505)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.90e-04)	Tok/s 252638 (242185)	Loss/tok 3.1365 (3.1508)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.099 (0.089)	Data 1.20e-04 (2.88e-04)	Tok/s 254154 (242257)	Loss/tok 3.1375 (3.1510)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.136 (0.089)	Data 1.16e-04 (2.87e-04)	Tok/s 259218 (242240)	Loss/tok 3.2707 (3.1505)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.135 (0.089)	Data 1.17e-04 (2.85e-04)	Tok/s 258452 (242218)	Loss/tok 3.3431 (3.1500)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.84e-04)	Tok/s 236782 (242223)	Loss/tok 2.9698 (3.1502)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.82e-04)	Tok/s 253757 (242190)	Loss/tok 3.1593 (3.1494)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.81e-04)	Tok/s 252008 (242183)	Loss/tok 3.0804 (3.1487)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.135 (0.089)	Data 1.15e-04 (2.80e-04)	Tok/s 256548 (242150)	Loss/tok 3.3517 (3.1478)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.78e-04)	Tok/s 231501 (242132)	Loss/tok 2.9022 (3.1472)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.77e-04)	Tok/s 255565 (242111)	Loss/tok 3.1033 (3.1474)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.76e-04)	Tok/s 229349 (242146)	Loss/tok 2.9448 (3.1473)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1200/1291]	Time 0.066 (0.089)	Data 1.17e-04 (2.74e-04)	Tok/s 232172 (242118)	Loss/tok 2.8981 (3.1461)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.135 (0.089)	Data 1.20e-04 (2.73e-04)	Tok/s 257386 (242081)	Loss/tok 3.2806 (3.1453)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.067 (0.089)	Data 1.36e-04 (2.72e-04)	Tok/s 231453 (242093)	Loss/tok 2.9842 (3.1457)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.036 (0.089)	Data 1.26e-04 (2.70e-04)	Tok/s 221200 (242052)	Loss/tok 2.5663 (3.1448)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.69e-04)	Tok/s 249224 (242039)	Loss/tok 3.2087 (3.1450)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.68e-04)	Tok/s 254399 (242019)	Loss/tok 3.1446 (3.1446)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.135 (0.089)	Data 1.13e-04 (2.67e-04)	Tok/s 258925 (242028)	Loss/tok 3.2772 (3.1446)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.66e-04)	Tok/s 231729 (242050)	Loss/tok 3.0077 (3.1454)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.64e-04)	Tok/s 231184 (242037)	Loss/tok 2.9142 (3.1450)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.134 (0.089)	Data 4.34e-05 (2.66e-04)	Tok/s 258450 (242075)	Loss/tok 3.4142 (3.1453)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593143187743, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593143187743, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.398 (0.398)	Decoder iters 100.0 (100.0)	Tok/s 41190 (41190)
0: Running moses detokenizer
0: BLEU(score=24.075429068539922, counts=[37246, 18669, 10597, 6273], totals=[65499, 62496, 59493, 56495], precisions=[56.8649903051955, 29.872311827956988, 17.81217958415276, 11.103637490043367], bp=1.0, sys_len=65499, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593143189701, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2408, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593143189702, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1491	Test BLEU: 24.08
0: Performance: Epoch: 3	Training: 1936175 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593143189702, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593143189702, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 03:46:36 AM
RESULT,RNN_TRANSLATOR,,505,nvidia,2020-06-26 03:38:11 AM
