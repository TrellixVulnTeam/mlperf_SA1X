Beginning trial 5 of 10
:::MLLOG {"namespace": "", "time_ms": 1593143197773, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 18}}
:::MLLOG {"namespace": "", "time_ms": 1593143197821, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Inspur", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 23}}
:::MLLOG {"namespace": "", "time_ms": 1593143197821, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 27}}
:::MLLOG {"namespace": "", "time_ms": 1593143197821, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 31}}
:::MLLOG {"namespace": "", "time_ms": 1593143197821, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNF5488", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 35}}
vm.drop_caches = 3
:::MLLOG {"namespace": "", "time_ms": 1593143200212, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
STARTING TIMING RUN AT 2020-06-26 03:46:40 AM
Using TCMalloc
running benchmark
:::MLLOG {"namespace": "", "time_ms": 1593143202794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143202794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143202794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143202801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143202810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143202812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143202811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593143202837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 535492534
:::MLLOG {"namespace": "", "time_ms": 1593143211399, "event_type": "POINT_IN_TIME", "key": "seed", "value": 535492534, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1866200895
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593143225088, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593143225090, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593143225090, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593143225090, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593143225090, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593143227058, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593143227059, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593143227059, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593143227417, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593143227418, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593143227418, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593143227418, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593143227419, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593143227419, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593143227419, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593143227419, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593143227419, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593143227419, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593143227419, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143227420, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 4240579482
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.451 (0.451)	Data 2.74e-01 (2.74e-01)	Tok/s 99139 (99139)	Loss/tok 10.6849 (10.6849)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.134 (0.127)	Data 1.10e-04 (3.41e-02)	Tok/s 261869 (208252)	Loss/tok 9.6513 (10.0308)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.100 (0.116)	Data 1.18e-04 (2.33e-02)	Tok/s 253303 (210169)	Loss/tok 9.2046 (9.7135)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.067 (0.113)	Data 1.12e-04 (1.58e-02)	Tok/s 231608 (221507)	Loss/tok 8.8002 (9.4850)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.134 (0.107)	Data 1.13e-04 (1.20e-02)	Tok/s 262941 (226171)	Loss/tok 8.7557 (9.3167)	LR 7.390e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][50/1291]	Time 0.133 (0.105)	Data 1.18e-04 (9.68e-03)	Tok/s 259779 (230501)	Loss/tok 8.4557 (9.1496)	LR 9.092e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][60/1291]	Time 0.101 (0.100)	Data 1.33e-04 (8.11e-03)	Tok/s 251141 (230984)	Loss/tok 8.1631 (9.0278)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.037 (0.099)	Data 1.33e-04 (6.99e-03)	Tok/s 214368 (233022)	Loss/tok 7.8844 (8.9041)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.067 (0.098)	Data 1.10e-04 (6.14e-03)	Tok/s 233426 (234343)	Loss/tok 7.8635 (8.8048)	LR 1.773e-04
0: TRAIN [0][90/1291]	Time 0.067 (0.096)	Data 1.11e-04 (5.48e-03)	Tok/s 228423 (234524)	Loss/tok 7.8256 (8.7244)	LR 2.232e-04
0: TRAIN [0][100/1291]	Time 0.068 (0.095)	Data 1.12e-04 (4.95e-03)	Tok/s 227875 (235503)	Loss/tok 7.7400 (8.6479)	LR 2.810e-04
0: TRAIN [0][110/1291]	Time 0.100 (0.095)	Data 1.11e-04 (4.51e-03)	Tok/s 252516 (236184)	Loss/tok 7.9018 (8.5786)	LR 3.537e-04
0: TRAIN [0][120/1291]	Time 0.066 (0.094)	Data 1.10e-04 (4.15e-03)	Tok/s 233657 (236540)	Loss/tok 7.6953 (8.5277)	LR 4.453e-04
0: TRAIN [0][130/1291]	Time 0.099 (0.094)	Data 1.09e-04 (3.84e-03)	Tok/s 258431 (237488)	Loss/tok 7.7715 (8.4735)	LR 5.606e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][140/1291]	Time 0.098 (0.095)	Data 1.15e-04 (3.57e-03)	Tok/s 259751 (238288)	Loss/tok 7.7667 (8.4220)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.067 (0.095)	Data 1.13e-04 (3.35e-03)	Tok/s 231098 (238842)	Loss/tok 7.3409 (8.3700)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.066 (0.095)	Data 1.29e-04 (3.15e-03)	Tok/s 233825 (239259)	Loss/tok 7.3464 (8.3172)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.035 (0.093)	Data 1.09e-04 (2.97e-03)	Tok/s 227074 (238903)	Loss/tok 6.2795 (8.2723)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.066 (0.091)	Data 1.13e-04 (2.81e-03)	Tok/s 232425 (238671)	Loss/tok 6.9408 (8.2258)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.099 (0.091)	Data 1.07e-04 (2.67e-03)	Tok/s 253644 (239165)	Loss/tok 7.0219 (8.1608)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.171 (0.092)	Data 1.09e-04 (2.54e-03)	Tok/s 259520 (239942)	Loss/tok 6.9806 (8.0824)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.066 (0.092)	Data 1.10e-04 (2.43e-03)	Tok/s 235313 (240169)	Loss/tok 6.4776 (8.0222)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.171 (0.092)	Data 1.08e-04 (2.32e-03)	Tok/s 261222 (240456)	Loss/tok 6.7934 (7.9526)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.174 (0.093)	Data 1.10e-04 (2.23e-03)	Tok/s 255063 (240975)	Loss/tok 6.5820 (7.8740)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.066 (0.093)	Data 1.08e-04 (2.14e-03)	Tok/s 238379 (240980)	Loss/tok 6.0482 (7.8127)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.099 (0.093)	Data 1.11e-04 (2.06e-03)	Tok/s 258878 (241320)	Loss/tok 6.0583 (7.7431)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.035 (0.092)	Data 1.11e-04 (1.99e-03)	Tok/s 223634 (241227)	Loss/tok 4.9630 (7.6850)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][270/1291]	Time 0.099 (0.092)	Data 1.06e-04 (1.92e-03)	Tok/s 254436 (241405)	Loss/tok 5.8270 (7.6191)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.066 (0.092)	Data 1.10e-04 (1.85e-03)	Tok/s 233762 (241355)	Loss/tok 5.4726 (7.5580)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.134 (0.092)	Data 1.24e-04 (1.79e-03)	Tok/s 263077 (241604)	Loss/tok 5.9705 (7.4926)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.036 (0.092)	Data 1.07e-04 (1.74e-03)	Tok/s 221083 (241634)	Loss/tok 4.5068 (7.4294)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.099 (0.091)	Data 1.08e-04 (1.68e-03)	Tok/s 255061 (241623)	Loss/tok 5.4116 (7.3721)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.067 (0.091)	Data 1.10e-04 (1.64e-03)	Tok/s 235264 (241776)	Loss/tok 4.9891 (7.3085)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.066 (0.091)	Data 1.08e-04 (1.59e-03)	Tok/s 232750 (241710)	Loss/tok 4.8542 (7.2525)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.099 (0.091)	Data 1.08e-04 (1.55e-03)	Tok/s 257325 (241789)	Loss/tok 5.1028 (7.1925)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.066 (0.090)	Data 1.08e-04 (1.51e-03)	Tok/s 235106 (241550)	Loss/tok 4.7436 (7.1458)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.066 (0.090)	Data 1.07e-04 (1.47e-03)	Tok/s 238393 (241649)	Loss/tok 4.5798 (7.0818)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.067 (0.090)	Data 1.08e-04 (1.43e-03)	Tok/s 231604 (241481)	Loss/tok 4.5508 (7.0307)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.066 (0.089)	Data 1.31e-04 (1.40e-03)	Tok/s 232656 (241388)	Loss/tok 4.4700 (6.9812)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.098 (0.089)	Data 1.11e-04 (1.36e-03)	Tok/s 256256 (241236)	Loss/tok 4.7031 (6.9341)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][400/1291]	Time 0.099 (0.089)	Data 1.11e-04 (1.33e-03)	Tok/s 255144 (241337)	Loss/tok 4.6798 (6.8656)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.066 (0.089)	Data 1.10e-04 (1.30e-03)	Tok/s 232915 (241327)	Loss/tok 4.2762 (6.8102)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.066 (0.089)	Data 1.07e-04 (1.27e-03)	Tok/s 235590 (241463)	Loss/tok 4.1935 (6.7494)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.067 (0.089)	Data 1.08e-04 (1.25e-03)	Tok/s 229647 (241493)	Loss/tok 4.1691 (6.6943)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.135 (0.089)	Data 1.10e-04 (1.22e-03)	Tok/s 261062 (241391)	Loss/tok 4.6362 (6.6497)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][450/1291]	Time 0.174 (0.089)	Data 1.11e-04 (1.20e-03)	Tok/s 256429 (241432)	Loss/tok 4.8219 (6.5937)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.066 (0.089)	Data 1.11e-04 (1.17e-03)	Tok/s 233360 (241359)	Loss/tok 4.0429 (6.5504)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.037 (0.089)	Data 1.30e-04 (1.15e-03)	Tok/s 218430 (241538)	Loss/tok 3.3527 (6.4978)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.173 (0.089)	Data 1.08e-04 (1.13e-03)	Tok/s 260353 (241677)	Loss/tok 4.6274 (6.4477)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.066 (0.089)	Data 1.09e-04 (1.11e-03)	Tok/s 236125 (241685)	Loss/tok 4.0907 (6.4037)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.066 (0.089)	Data 1.11e-04 (1.09e-03)	Tok/s 235037 (241619)	Loss/tok 3.9714 (6.3657)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.067 (0.089)	Data 1.07e-04 (1.07e-03)	Tok/s 232082 (241734)	Loss/tok 3.8801 (6.3191)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.099 (0.089)	Data 1.13e-04 (1.05e-03)	Tok/s 250500 (241778)	Loss/tok 4.1984 (6.2765)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.099 (0.089)	Data 1.13e-04 (1.03e-03)	Tok/s 256201 (241832)	Loss/tok 4.0426 (6.2358)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.099 (0.089)	Data 1.10e-04 (1.02e-03)	Tok/s 254383 (241861)	Loss/tok 4.0667 (6.1973)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.135 (0.089)	Data 1.22e-04 (1.00e-03)	Tok/s 257427 (241953)	Loss/tok 4.3691 (6.1551)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.066 (0.090)	Data 1.11e-04 (9.84e-04)	Tok/s 232401 (242002)	Loss/tok 3.7165 (6.1152)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][570/1291]	Time 0.134 (0.089)	Data 1.07e-04 (9.69e-04)	Tok/s 259822 (241945)	Loss/tok 4.4178 (6.0825)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.135 (0.089)	Data 1.19e-04 (9.54e-04)	Tok/s 259071 (241987)	Loss/tok 4.2525 (6.0471)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.174 (0.089)	Data 1.06e-04 (9.40e-04)	Tok/s 259106 (241888)	Loss/tok 4.3952 (6.0165)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.067 (0.089)	Data 1.07e-04 (9.26e-04)	Tok/s 232669 (241751)	Loss/tok 3.8007 (5.9909)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.036 (0.089)	Data 1.14e-04 (9.13e-04)	Tok/s 223897 (241697)	Loss/tok 3.2328 (5.9624)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.100 (0.088)	Data 1.13e-04 (9.00e-04)	Tok/s 251337 (241663)	Loss/tok 3.9276 (5.9333)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.135 (0.089)	Data 1.07e-04 (8.87e-04)	Tok/s 260184 (241776)	Loss/tok 4.1491 (5.8975)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.036 (0.089)	Data 1.06e-04 (8.75e-04)	Tok/s 226419 (241791)	Loss/tok 3.1703 (5.8669)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.036 (0.088)	Data 1.09e-04 (8.64e-04)	Tok/s 217673 (241797)	Loss/tok 3.1763 (5.8390)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.067 (0.088)	Data 1.17e-04 (8.52e-04)	Tok/s 230940 (241685)	Loss/tok 3.7396 (5.8158)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.099 (0.088)	Data 1.07e-04 (8.41e-04)	Tok/s 254406 (241762)	Loss/tok 3.8674 (5.7860)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.068 (0.088)	Data 1.13e-04 (8.30e-04)	Tok/s 228944 (241750)	Loss/tok 3.6995 (5.7596)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.099 (0.088)	Data 1.09e-04 (8.20e-04)	Tok/s 252019 (241737)	Loss/tok 3.8633 (5.7330)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][700/1291]	Time 0.066 (0.089)	Data 1.35e-04 (8.10e-04)	Tok/s 233577 (241841)	Loss/tok 3.5766 (5.7012)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.067 (0.088)	Data 1.10e-04 (8.00e-04)	Tok/s 233218 (241826)	Loss/tok 3.6452 (5.6761)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.036 (0.088)	Data 1.13e-04 (7.91e-04)	Tok/s 223197 (241771)	Loss/tok 2.9569 (5.6515)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.100 (0.088)	Data 1.08e-04 (7.81e-04)	Tok/s 252422 (241768)	Loss/tok 3.8161 (5.6273)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.134 (0.089)	Data 1.12e-04 (7.72e-04)	Tok/s 262796 (241893)	Loss/tok 4.0591 (5.5987)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.100 (0.089)	Data 1.09e-04 (7.64e-04)	Tok/s 251019 (241858)	Loss/tok 3.7852 (5.5765)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.135 (0.089)	Data 1.14e-04 (7.55e-04)	Tok/s 259708 (241980)	Loss/tok 3.9450 (5.5491)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.066 (0.089)	Data 1.08e-04 (7.47e-04)	Tok/s 229617 (242012)	Loss/tok 3.5482 (5.5247)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.066 (0.089)	Data 1.09e-04 (7.39e-04)	Tok/s 236110 (242031)	Loss/tok 3.5794 (5.5036)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.036 (0.089)	Data 1.30e-04 (7.31e-04)	Tok/s 225195 (242112)	Loss/tok 3.0992 (5.4802)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.066 (0.089)	Data 1.11e-04 (7.23e-04)	Tok/s 235703 (242169)	Loss/tok 3.5723 (5.4571)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.067 (0.089)	Data 1.09e-04 (7.15e-04)	Tok/s 228221 (242162)	Loss/tok 3.6655 (5.4364)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.067 (0.089)	Data 1.09e-04 (7.08e-04)	Tok/s 231733 (242176)	Loss/tok 3.5935 (5.4160)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][830/1291]	Time 0.067 (0.089)	Data 1.09e-04 (7.01e-04)	Tok/s 229022 (242175)	Loss/tok 3.5280 (5.3969)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.099 (0.089)	Data 1.16e-04 (6.94e-04)	Tok/s 251264 (242190)	Loss/tok 3.7749 (5.3763)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.067 (0.089)	Data 1.07e-04 (6.87e-04)	Tok/s 227521 (242028)	Loss/tok 3.5641 (5.3623)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.100 (0.089)	Data 1.13e-04 (6.80e-04)	Tok/s 251164 (242043)	Loss/tok 3.7965 (5.3431)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.173 (0.089)	Data 1.10e-04 (6.74e-04)	Tok/s 260546 (242038)	Loss/tok 4.1712 (5.3253)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.173 (0.089)	Data 1.10e-04 (6.68e-04)	Tok/s 258699 (242076)	Loss/tok 4.2013 (5.3057)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.067 (0.089)	Data 1.10e-04 (6.61e-04)	Tok/s 233613 (242098)	Loss/tok 3.5023 (5.2878)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.036 (0.089)	Data 1.11e-04 (6.55e-04)	Tok/s 220260 (242039)	Loss/tok 2.9288 (5.2729)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.036 (0.089)	Data 1.11e-04 (6.49e-04)	Tok/s 222178 (242008)	Loss/tok 3.0841 (5.2566)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][920/1291]	Time 0.067 (0.089)	Data 1.07e-04 (6.43e-04)	Tok/s 234932 (242054)	Loss/tok 3.5522 (5.2390)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.099 (0.089)	Data 1.06e-04 (6.38e-04)	Tok/s 253872 (242083)	Loss/tok 3.6437 (5.2225)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.067 (0.089)	Data 1.09e-04 (6.32e-04)	Tok/s 235536 (242106)	Loss/tok 3.5672 (5.2061)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.067 (0.089)	Data 1.11e-04 (6.27e-04)	Tok/s 234306 (242090)	Loss/tok 3.3755 (5.1906)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.067 (0.089)	Data 1.07e-04 (6.21e-04)	Tok/s 231762 (242156)	Loss/tok 3.4719 (5.1730)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.035 (0.089)	Data 1.11e-04 (6.16e-04)	Tok/s 225645 (242187)	Loss/tok 3.0437 (5.1572)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.100 (0.089)	Data 1.07e-04 (6.11e-04)	Tok/s 250249 (242148)	Loss/tok 3.7163 (5.1439)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.067 (0.089)	Data 1.13e-04 (6.06e-04)	Tok/s 232107 (242209)	Loss/tok 3.3884 (5.1266)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.067 (0.089)	Data 1.12e-04 (6.01e-04)	Tok/s 230683 (242181)	Loss/tok 3.3929 (5.1131)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.173 (0.089)	Data 1.31e-04 (5.96e-04)	Tok/s 258451 (242234)	Loss/tok 4.0419 (5.0980)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.92e-04)	Tok/s 255738 (242303)	Loss/tok 3.7285 (5.0824)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.87e-04)	Tok/s 236796 (242289)	Loss/tok 3.4347 (5.0688)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.036 (0.089)	Data 1.34e-04 (5.82e-04)	Tok/s 218906 (242262)	Loss/tok 2.8874 (5.0562)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1050/1291]	Time 0.067 (0.089)	Data 1.12e-04 (5.78e-04)	Tok/s 229090 (242202)	Loss/tok 3.5324 (5.0447)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.100 (0.089)	Data 1.06e-04 (5.74e-04)	Tok/s 251859 (242227)	Loss/tok 3.5929 (5.0308)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.067 (0.089)	Data 1.29e-04 (5.69e-04)	Tok/s 235444 (242211)	Loss/tok 3.3751 (5.0179)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.65e-04)	Tok/s 232665 (242240)	Loss/tok 3.3786 (5.0044)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.174 (0.089)	Data 1.07e-04 (5.61e-04)	Tok/s 255975 (242236)	Loss/tok 4.0144 (4.9916)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.067 (0.089)	Data 1.09e-04 (5.57e-04)	Tok/s 237016 (242245)	Loss/tok 3.4087 (4.9795)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.099 (0.089)	Data 1.07e-04 (5.53e-04)	Tok/s 254087 (242273)	Loss/tok 3.7134 (4.9670)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.067 (0.089)	Data 1.13e-04 (5.49e-04)	Tok/s 231253 (242261)	Loss/tok 3.3732 (4.9553)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.099 (0.089)	Data 1.13e-04 (5.45e-04)	Tok/s 250890 (242244)	Loss/tok 3.6332 (4.9436)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.135 (0.089)	Data 1.07e-04 (5.41e-04)	Tok/s 261316 (242243)	Loss/tok 3.8356 (4.9322)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.099 (0.089)	Data 1.10e-04 (5.37e-04)	Tok/s 251554 (242321)	Loss/tok 3.5836 (4.9178)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.136 (0.089)	Data 1.10e-04 (5.34e-04)	Tok/s 257811 (242293)	Loss/tok 3.8185 (4.9075)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.135 (0.089)	Data 1.11e-04 (5.30e-04)	Tok/s 256976 (242265)	Loss/tok 3.8754 (4.8973)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1180/1291]	Time 0.100 (0.089)	Data 1.07e-04 (5.27e-04)	Tok/s 254243 (242324)	Loss/tok 3.5881 (4.8848)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.134 (0.089)	Data 1.27e-04 (5.23e-04)	Tok/s 262149 (242311)	Loss/tok 3.7247 (4.8744)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.067 (0.089)	Data 1.09e-04 (5.20e-04)	Tok/s 233690 (242241)	Loss/tok 3.4301 (4.8654)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.16e-04)	Tok/s 231504 (242285)	Loss/tok 3.3047 (4.8539)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.100 (0.089)	Data 1.10e-04 (5.13e-04)	Tok/s 251916 (242333)	Loss/tok 3.6700 (4.8423)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.135 (0.089)	Data 1.11e-04 (5.10e-04)	Tok/s 259714 (242316)	Loss/tok 3.8072 (4.8328)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.100 (0.089)	Data 1.09e-04 (5.07e-04)	Tok/s 251251 (242304)	Loss/tok 3.6120 (4.8231)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.067 (0.089)	Data 1.12e-04 (5.03e-04)	Tok/s 226261 (242298)	Loss/tok 3.4072 (4.8135)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.136 (0.089)	Data 1.10e-04 (5.00e-04)	Tok/s 259541 (242324)	Loss/tok 3.8039 (4.8036)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.099 (0.089)	Data 1.07e-04 (4.97e-04)	Tok/s 254106 (242303)	Loss/tok 3.6527 (4.7944)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1280/1291]	Time 0.068 (0.089)	Data 1.09e-04 (4.94e-04)	Tok/s 225515 (242301)	Loss/tok 3.3542 (4.7845)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.100 (0.089)	Data 4.46e-05 (4.94e-04)	Tok/s 254156 (242305)	Loss/tok 3.5229 (4.7746)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593143343025, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143343025, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.501 (0.501)	Decoder iters 149.0 (149.0)	Tok/s 34547 (34547)
0: Running moses detokenizer
0: BLEU(score=17.71120415554195, counts=[35118, 15833, 8320, 4543], totals=[72568, 69565, 66562, 63563], precisions=[48.393231176276046, 22.760008625026952, 12.49962441032421, 7.147239746393342], bp=1.0, sys_len=72568, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593143345384, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1771, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143345384, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7735	Test BLEU: 17.71
0: Performance: Epoch: 0	Training: 1938186 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593143345384, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143345384, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143345384, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1488332772
0: TRAIN [1][0/1291]	Time 0.329 (0.329)	Data 1.85e-01 (1.85e-01)	Tok/s 106344 (106344)	Loss/tok 3.7322 (3.7322)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.099 (0.104)	Data 1.15e-04 (1.69e-02)	Tok/s 256154 (227684)	Loss/tok 3.5128 (3.5228)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.100 (0.099)	Data 1.14e-04 (8.91e-03)	Tok/s 257416 (236376)	Loss/tok 3.5152 (3.5134)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.066 (0.088)	Data 1.29e-04 (6.07e-03)	Tok/s 238987 (235591)	Loss/tok 3.1981 (3.4412)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.036 (0.084)	Data 1.09e-04 (4.62e-03)	Tok/s 217198 (236597)	Loss/tok 2.7949 (3.4247)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.100 (0.085)	Data 1.12e-04 (3.76e-03)	Tok/s 251550 (238268)	Loss/tok 3.4843 (3.4296)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.100 (0.085)	Data 1.12e-04 (3.16e-03)	Tok/s 252954 (238802)	Loss/tok 3.4782 (3.4290)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.173 (0.087)	Data 1.13e-04 (2.73e-03)	Tok/s 256834 (239681)	Loss/tok 3.9472 (3.4554)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.135 (0.088)	Data 1.09e-04 (2.41e-03)	Tok/s 260016 (240045)	Loss/tok 3.6372 (3.4662)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.067 (0.086)	Data 1.14e-04 (2.16e-03)	Tok/s 235421 (239578)	Loss/tok 3.2797 (3.4502)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.134 (0.086)	Data 1.11e-04 (1.95e-03)	Tok/s 260843 (240038)	Loss/tok 3.7068 (3.4532)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.099 (0.087)	Data 1.24e-04 (1.79e-03)	Tok/s 252858 (240269)	Loss/tok 3.4926 (3.4593)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][120/1291]	Time 0.067 (0.087)	Data 1.13e-04 (1.65e-03)	Tok/s 233840 (240737)	Loss/tok 3.2659 (3.4574)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.067 (0.087)	Data 1.10e-04 (1.53e-03)	Tok/s 233062 (241201)	Loss/tok 3.1660 (3.4546)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.067 (0.088)	Data 1.33e-04 (1.43e-03)	Tok/s 232275 (241377)	Loss/tok 3.2700 (3.4590)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][150/1291]	Time 0.100 (0.089)	Data 1.33e-04 (1.35e-03)	Tok/s 250302 (241873)	Loss/tok 3.5863 (3.4708)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.100 (0.089)	Data 1.12e-04 (1.27e-03)	Tok/s 252061 (241918)	Loss/tok 3.5170 (3.4684)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.135 (0.089)	Data 1.10e-04 (1.20e-03)	Tok/s 263463 (241919)	Loss/tok 3.6165 (3.4681)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.135 (0.088)	Data 1.09e-04 (1.14e-03)	Tok/s 261160 (241692)	Loss/tok 3.5989 (3.4636)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.100 (0.088)	Data 1.51e-04 (1.09e-03)	Tok/s 254023 (241746)	Loss/tok 3.4030 (3.4633)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.066 (0.089)	Data 1.09e-04 (1.04e-03)	Tok/s 237662 (242142)	Loss/tok 3.2583 (3.4664)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.067 (0.089)	Data 1.10e-04 (9.95e-04)	Tok/s 232617 (242294)	Loss/tok 3.1996 (3.4683)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.099 (0.088)	Data 1.10e-04 (9.55e-04)	Tok/s 255049 (242245)	Loss/tok 3.5862 (3.4647)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.036 (0.089)	Data 1.08e-04 (9.18e-04)	Tok/s 223388 (242410)	Loss/tok 2.8367 (3.4690)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.135 (0.089)	Data 1.11e-04 (8.85e-04)	Tok/s 259403 (242344)	Loss/tok 3.5446 (3.4759)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.036 (0.089)	Data 1.19e-04 (8.54e-04)	Tok/s 214840 (242272)	Loss/tok 2.7497 (3.4750)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.067 (0.089)	Data 1.11e-04 (8.26e-04)	Tok/s 230751 (242360)	Loss/tok 3.3025 (3.4725)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.067 (0.089)	Data 1.10e-04 (7.99e-04)	Tok/s 230566 (242362)	Loss/tok 3.2801 (3.4732)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][280/1291]	Time 0.173 (0.090)	Data 1.12e-04 (7.75e-04)	Tok/s 257759 (242518)	Loss/tok 3.8226 (3.4781)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][290/1291]	Time 0.066 (0.090)	Data 1.15e-04 (7.52e-04)	Tok/s 237695 (242421)	Loss/tok 3.3092 (3.4790)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.066 (0.090)	Data 1.09e-04 (7.31e-04)	Tok/s 234687 (242499)	Loss/tok 3.2452 (3.4809)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.135 (0.090)	Data 1.09e-04 (7.11e-04)	Tok/s 259699 (242526)	Loss/tok 3.6221 (3.4787)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.100 (0.090)	Data 1.08e-04 (6.92e-04)	Tok/s 252310 (242637)	Loss/tok 3.4331 (3.4829)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.135 (0.090)	Data 1.11e-04 (6.75e-04)	Tok/s 257806 (242765)	Loss/tok 3.6625 (3.4844)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.136 (0.090)	Data 1.12e-04 (6.58e-04)	Tok/s 259457 (242500)	Loss/tok 3.6157 (3.4806)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.099 (0.090)	Data 1.13e-04 (6.43e-04)	Tok/s 252360 (242556)	Loss/tok 3.4954 (3.4802)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.067 (0.090)	Data 1.33e-04 (6.28e-04)	Tok/s 234443 (242424)	Loss/tok 3.2108 (3.4760)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.067 (0.089)	Data 1.13e-04 (6.15e-04)	Tok/s 233389 (242373)	Loss/tok 3.2057 (3.4737)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.134 (0.089)	Data 1.14e-04 (6.01e-04)	Tok/s 261772 (242371)	Loss/tok 3.5320 (3.4708)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.067 (0.089)	Data 1.15e-04 (5.89e-04)	Tok/s 230372 (242193)	Loss/tok 3.2582 (3.4669)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.099 (0.089)	Data 1.30e-04 (5.77e-04)	Tok/s 251545 (242290)	Loss/tok 3.4743 (3.4677)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][410/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.66e-04)	Tok/s 252214 (242272)	Loss/tok 3.4914 (3.4684)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.088)	Data 1.11e-04 (5.55e-04)	Tok/s 231400 (242127)	Loss/tok 3.1789 (3.4655)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.067 (0.089)	Data 1.13e-04 (5.45e-04)	Tok/s 234425 (242123)	Loss/tok 3.3271 (3.4652)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.099 (0.088)	Data 1.31e-04 (5.35e-04)	Tok/s 250664 (242079)	Loss/tok 3.5994 (3.4626)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.100 (0.088)	Data 1.11e-04 (5.26e-04)	Tok/s 250703 (242023)	Loss/tok 3.4585 (3.4599)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.067 (0.088)	Data 1.13e-04 (5.17e-04)	Tok/s 235829 (241915)	Loss/tok 3.1860 (3.4564)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.067 (0.088)	Data 1.13e-04 (5.09e-04)	Tok/s 228099 (241918)	Loss/tok 3.3031 (3.4573)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.135 (0.088)	Data 1.13e-04 (5.00e-04)	Tok/s 257509 (242156)	Loss/tok 3.5616 (3.4570)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.067 (0.088)	Data 1.10e-04 (4.92e-04)	Tok/s 233475 (242122)	Loss/tok 3.1628 (3.4560)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.067 (0.088)	Data 1.32e-04 (4.85e-04)	Tok/s 238047 (241959)	Loss/tok 3.2587 (3.4531)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][510/1291]	Time 0.098 (0.088)	Data 1.12e-04 (4.78e-04)	Tok/s 251300 (241921)	Loss/tok 3.5029 (3.4548)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.067 (0.088)	Data 1.24e-04 (4.71e-04)	Tok/s 230235 (241926)	Loss/tok 3.2864 (3.4558)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.099 (0.088)	Data 1.12e-04 (4.64e-04)	Tok/s 259922 (242017)	Loss/tok 3.2252 (3.4549)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.067 (0.088)	Data 1.14e-04 (4.58e-04)	Tok/s 235455 (242065)	Loss/tok 3.1664 (3.4559)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.134 (0.088)	Data 1.18e-04 (4.51e-04)	Tok/s 260119 (242057)	Loss/tok 3.6951 (3.4548)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.067 (0.088)	Data 1.11e-04 (4.45e-04)	Tok/s 232260 (242058)	Loss/tok 3.2169 (3.4560)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.099 (0.088)	Data 1.12e-04 (4.39e-04)	Tok/s 253274 (242099)	Loss/tok 3.4318 (3.4580)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.100 (0.088)	Data 1.10e-04 (4.34e-04)	Tok/s 248284 (242116)	Loss/tok 3.4347 (3.4584)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.135 (0.088)	Data 1.35e-04 (4.29e-04)	Tok/s 261715 (242081)	Loss/tok 3.6854 (3.4586)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.174 (0.088)	Data 1.08e-04 (4.23e-04)	Tok/s 254387 (242057)	Loss/tok 3.8023 (3.4581)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.067 (0.088)	Data 1.14e-04 (4.18e-04)	Tok/s 234976 (242044)	Loss/tok 3.1952 (3.4565)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.066 (0.088)	Data 1.15e-04 (4.13e-04)	Tok/s 232532 (242020)	Loss/tok 3.1429 (3.4560)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.066 (0.088)	Data 1.13e-04 (4.09e-04)	Tok/s 232427 (242081)	Loss/tok 3.2241 (3.4553)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][640/1291]	Time 0.100 (0.088)	Data 1.11e-04 (4.04e-04)	Tok/s 252070 (242021)	Loss/tok 3.3783 (3.4551)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.174 (0.089)	Data 1.12e-04 (3.99e-04)	Tok/s 256033 (242104)	Loss/tok 3.7158 (3.4571)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.135 (0.089)	Data 1.10e-04 (3.95e-04)	Tok/s 260613 (242205)	Loss/tok 3.5784 (3.4590)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.91e-04)	Tok/s 254532 (242255)	Loss/tok 3.3846 (3.4593)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.036 (0.089)	Data 1.10e-04 (3.87e-04)	Tok/s 227364 (242246)	Loss/tok 2.8448 (3.4589)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.83e-04)	Tok/s 232582 (242125)	Loss/tok 3.2329 (3.4569)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.099 (0.089)	Data 1.27e-04 (3.79e-04)	Tok/s 253949 (242053)	Loss/tok 3.3994 (3.4547)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.134 (0.089)	Data 1.12e-04 (3.75e-04)	Tok/s 257259 (242084)	Loss/tok 3.7165 (3.4551)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][720/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.72e-04)	Tok/s 255152 (242172)	Loss/tok 3.3968 (3.4564)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.099 (0.089)	Data 1.14e-04 (3.68e-04)	Tok/s 252325 (242123)	Loss/tok 3.3650 (3.4549)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.65e-04)	Tok/s 253845 (242173)	Loss/tok 3.4166 (3.4545)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.61e-04)	Tok/s 231359 (242151)	Loss/tok 3.2325 (3.4543)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.100 (0.089)	Data 1.16e-04 (3.58e-04)	Tok/s 247237 (242186)	Loss/tok 3.4631 (3.4545)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.55e-04)	Tok/s 231723 (242185)	Loss/tok 3.1470 (3.4540)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.100 (0.089)	Data 1.17e-04 (3.52e-04)	Tok/s 252251 (242229)	Loss/tok 3.4511 (3.4535)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.49e-04)	Tok/s 219346 (242259)	Loss/tok 2.7131 (3.4531)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.100 (0.089)	Data 1.08e-04 (3.46e-04)	Tok/s 251334 (242290)	Loss/tok 3.4263 (3.4522)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.134 (0.089)	Data 1.18e-04 (3.43e-04)	Tok/s 259657 (242317)	Loss/tok 3.6129 (3.4521)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.100 (0.089)	Data 1.17e-04 (3.40e-04)	Tok/s 255349 (242226)	Loss/tok 3.4432 (3.4502)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.37e-04)	Tok/s 253546 (242239)	Loss/tok 3.4555 (3.4488)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.134 (0.089)	Data 1.21e-04 (3.35e-04)	Tok/s 260639 (242252)	Loss/tok 3.5645 (3.4491)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][850/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.32e-04)	Tok/s 234908 (242232)	Loss/tok 3.0775 (3.4478)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.067 (0.089)	Data 1.07e-04 (3.30e-04)	Tok/s 233541 (242219)	Loss/tok 3.1413 (3.4466)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.27e-04)	Tok/s 251644 (242321)	Loss/tok 3.3835 (3.4466)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.25e-04)	Tok/s 254498 (242367)	Loss/tok 3.4109 (3.4458)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.22e-04)	Tok/s 232253 (242445)	Loss/tok 3.2285 (3.4466)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.20e-04)	Tok/s 253748 (242434)	Loss/tok 3.4423 (3.4461)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.18e-04)	Tok/s 254105 (242436)	Loss/tok 3.5116 (3.4463)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.16e-04)	Tok/s 230983 (242460)	Loss/tok 3.1566 (3.4459)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.13e-04)	Tok/s 231396 (242480)	Loss/tok 3.1977 (3.4455)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.11e-04)	Tok/s 252911 (242513)	Loss/tok 3.3471 (3.4445)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.09e-04)	Tok/s 231598 (242493)	Loss/tok 3.2316 (3.4442)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.067 (0.089)	Data 1.31e-04 (3.07e-04)	Tok/s 229771 (242504)	Loss/tok 3.2149 (3.4448)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.099 (0.089)	Data 1.24e-04 (3.05e-04)	Tok/s 252449 (242499)	Loss/tok 3.4068 (3.4439)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][980/1291]	Time 0.069 (0.089)	Data 1.10e-04 (3.03e-04)	Tok/s 224301 (242481)	Loss/tok 3.1808 (3.4435)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.01e-04)	Tok/s 231088 (242429)	Loss/tok 3.0847 (3.4423)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1000/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.00e-04)	Tok/s 233727 (242398)	Loss/tok 3.2017 (3.4411)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.98e-04)	Tok/s 233751 (242386)	Loss/tok 3.1866 (3.4397)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.174 (0.089)	Data 1.12e-04 (2.96e-04)	Tok/s 257703 (242399)	Loss/tok 3.6487 (3.4389)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.94e-04)	Tok/s 258600 (242402)	Loss/tok 3.6264 (3.4380)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.92e-04)	Tok/s 233572 (242403)	Loss/tok 3.1378 (3.4373)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.91e-04)	Tok/s 224739 (242437)	Loss/tok 2.8146 (3.4366)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.89e-04)	Tok/s 233568 (242486)	Loss/tok 3.1928 (3.4373)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.87e-04)	Tok/s 238024 (242525)	Loss/tok 3.2463 (3.4372)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.86e-04)	Tok/s 234202 (242487)	Loss/tok 3.1450 (3.4359)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.84e-04)	Tok/s 237172 (242511)	Loss/tok 3.1214 (3.4362)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.134 (0.089)	Data 1.12e-04 (2.83e-04)	Tok/s 262050 (242501)	Loss/tok 3.5791 (3.4358)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.067 (0.089)	Data 1.19e-04 (2.81e-04)	Tok/s 233205 (242532)	Loss/tok 3.0614 (3.4359)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.067 (0.090)	Data 1.32e-04 (2.80e-04)	Tok/s 230136 (242586)	Loss/tok 3.1639 (3.4364)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1130/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.78e-04)	Tok/s 252876 (242591)	Loss/tok 3.4280 (3.4358)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.77e-04)	Tok/s 230373 (242584)	Loss/tok 3.1953 (3.4348)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.135 (0.089)	Data 1.09e-04 (2.75e-04)	Tok/s 258898 (242600)	Loss/tok 3.5117 (3.4344)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1160/1291]	Time 0.067 (0.090)	Data 1.10e-04 (2.74e-04)	Tok/s 233922 (242634)	Loss/tok 3.0930 (3.4347)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.067 (0.090)	Data 1.09e-04 (2.72e-04)	Tok/s 227577 (242616)	Loss/tok 3.2181 (3.4337)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.71e-04)	Tok/s 254575 (242567)	Loss/tok 3.2926 (3.4324)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.70e-04)	Tok/s 231141 (242570)	Loss/tok 3.2119 (3.4317)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.099 (0.089)	Data 1.35e-04 (2.68e-04)	Tok/s 256056 (242595)	Loss/tok 3.3661 (3.4308)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1210/1291]	Time 0.135 (0.089)	Data 1.35e-04 (2.67e-04)	Tok/s 258303 (242588)	Loss/tok 3.4860 (3.4303)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.66e-04)	Tok/s 233443 (242575)	Loss/tok 3.1056 (3.4295)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.65e-04)	Tok/s 253040 (242607)	Loss/tok 3.3817 (3.4291)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.63e-04)	Tok/s 255778 (242527)	Loss/tok 3.3330 (3.4276)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.174 (0.089)	Data 1.10e-04 (2.62e-04)	Tok/s 253173 (242502)	Loss/tok 3.8300 (3.4272)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.067 (0.089)	Data 1.27e-04 (2.61e-04)	Tok/s 229791 (242470)	Loss/tok 3.1884 (3.4260)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.60e-04)	Tok/s 223385 (242447)	Loss/tok 2.6749 (3.4248)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.59e-04)	Tok/s 223017 (242463)	Loss/tok 2.7172 (3.4239)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.066 (0.089)	Data 4.20e-05 (2.61e-04)	Tok/s 229894 (242414)	Loss/tok 3.0538 (3.4231)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593143460928, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593143460928, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.498 (0.498)	Decoder iters 149.0 (149.0)	Tok/s 33124 (33124)
0: Running moses detokenizer
0: BLEU(score=21.91201875816083, counts=[35656, 17042, 9376, 5362], totals=[64931, 61928, 58925, 55928], precisions=[54.913677596217525, 27.519054385738276, 15.911752227407721, 9.587326562723502], bp=1.0, sys_len=64931, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593143462951, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2191, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593143462951, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4244	Test BLEU: 21.91
0: Performance: Epoch: 1	Training: 1938325 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593143462952, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593143462952, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143462952, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2452250704
0: TRAIN [2][0/1291]	Time 0.333 (0.333)	Data 1.76e-01 (1.76e-01)	Tok/s 104503 (104503)	Loss/tok 3.4193 (3.4193)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.099 (0.106)	Data 1.09e-04 (1.61e-02)	Tok/s 254716 (232641)	Loss/tok 3.3091 (3.2714)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.067 (0.092)	Data 1.03e-04 (8.50e-03)	Tok/s 232690 (235859)	Loss/tok 3.0724 (3.2229)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.067 (0.094)	Data 1.42e-04 (5.80e-03)	Tok/s 233698 (238544)	Loss/tok 3.0137 (3.2754)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][40/1291]	Time 0.134 (0.094)	Data 1.28e-04 (4.41e-03)	Tok/s 260183 (239831)	Loss/tok 3.4802 (3.2889)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.134 (0.097)	Data 1.07e-04 (3.57e-03)	Tok/s 259346 (241308)	Loss/tok 3.4034 (3.3176)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.102 (0.095)	Data 1.24e-04 (3.00e-03)	Tok/s 246030 (240980)	Loss/tok 3.3601 (3.3157)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.067 (0.093)	Data 1.11e-04 (2.60e-03)	Tok/s 234420 (240284)	Loss/tok 3.0305 (3.3046)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.099 (0.092)	Data 1.08e-04 (2.29e-03)	Tok/s 254403 (240191)	Loss/tok 3.3182 (3.2980)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.036 (0.092)	Data 1.25e-04 (2.05e-03)	Tok/s 218556 (240680)	Loss/tok 2.6220 (3.3088)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.066 (0.092)	Data 1.09e-04 (1.86e-03)	Tok/s 236078 (240622)	Loss/tok 2.9953 (3.3162)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.099 (0.095)	Data 1.07e-04 (1.70e-03)	Tok/s 254523 (241635)	Loss/tok 3.2720 (3.3316)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.066 (0.093)	Data 1.05e-04 (1.57e-03)	Tok/s 234215 (241545)	Loss/tok 3.0846 (3.3226)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.099 (0.093)	Data 1.05e-04 (1.46e-03)	Tok/s 256547 (241506)	Loss/tok 3.3873 (3.3221)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.067 (0.091)	Data 1.05e-04 (1.36e-03)	Tok/s 234539 (240958)	Loss/tok 3.1224 (3.3098)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.134 (0.090)	Data 1.08e-04 (1.28e-03)	Tok/s 263644 (241074)	Loss/tok 3.3751 (3.3066)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.066 (0.090)	Data 1.05e-04 (1.21e-03)	Tok/s 234540 (241107)	Loss/tok 3.0050 (3.3039)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][170/1291]	Time 0.036 (0.090)	Data 1.06e-04 (1.14e-03)	Tok/s 223863 (241044)	Loss/tok 2.6834 (3.3077)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.099 (0.089)	Data 1.04e-04 (1.08e-03)	Tok/s 256175 (240881)	Loss/tok 3.3365 (3.3051)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.099 (0.089)	Data 1.05e-04 (1.03e-03)	Tok/s 255158 (240846)	Loss/tok 3.2660 (3.3000)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.066 (0.090)	Data 1.07e-04 (9.87e-04)	Tok/s 232043 (241149)	Loss/tok 3.0702 (3.3022)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.099 (0.089)	Data 1.06e-04 (9.46e-04)	Tok/s 255118 (241105)	Loss/tok 3.3405 (3.3014)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.134 (0.089)	Data 1.05e-04 (9.08e-04)	Tok/s 259201 (241173)	Loss/tok 3.3857 (3.2996)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.099 (0.089)	Data 1.04e-04 (8.73e-04)	Tok/s 252550 (241183)	Loss/tok 3.2918 (3.2968)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.067 (0.089)	Data 1.03e-04 (8.41e-04)	Tok/s 233116 (241395)	Loss/tok 3.1504 (3.2997)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][250/1291]	Time 0.098 (0.090)	Data 1.03e-04 (8.12e-04)	Tok/s 257114 (241563)	Loss/tok 3.2058 (3.3018)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.066 (0.089)	Data 1.28e-04 (7.85e-04)	Tok/s 234224 (241563)	Loss/tok 3.1571 (3.2996)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.134 (0.089)	Data 1.05e-04 (7.60e-04)	Tok/s 260615 (241415)	Loss/tok 3.4319 (3.2961)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.134 (0.089)	Data 1.07e-04 (7.37e-04)	Tok/s 259424 (241386)	Loss/tok 3.5107 (3.2951)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.099 (0.089)	Data 1.03e-04 (7.16e-04)	Tok/s 256507 (241489)	Loss/tok 3.2180 (3.2930)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.135 (0.089)	Data 1.06e-04 (6.95e-04)	Tok/s 260973 (241718)	Loss/tok 3.4320 (3.2959)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.099 (0.089)	Data 1.03e-04 (6.77e-04)	Tok/s 252829 (241742)	Loss/tok 3.3575 (3.2949)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.067 (0.089)	Data 1.09e-04 (6.60e-04)	Tok/s 232207 (241775)	Loss/tok 3.0609 (3.2934)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.173 (0.090)	Data 1.08e-04 (6.44e-04)	Tok/s 259762 (242017)	Loss/tok 3.5771 (3.2960)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.067 (0.089)	Data 1.06e-04 (6.28e-04)	Tok/s 231777 (241872)	Loss/tok 3.0184 (3.2930)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.066 (0.089)	Data 1.04e-04 (6.13e-04)	Tok/s 231159 (241912)	Loss/tok 3.0585 (3.2918)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.035 (0.089)	Data 1.29e-04 (5.99e-04)	Tok/s 219505 (241996)	Loss/tok 2.6639 (3.2911)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][370/1291]	Time 0.036 (0.089)	Data 1.04e-04 (5.86e-04)	Tok/s 218711 (241777)	Loss/tok 2.6191 (3.2881)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][380/1291]	Time 0.135 (0.089)	Data 1.04e-04 (5.73e-04)	Tok/s 259612 (241931)	Loss/tok 3.3919 (3.2889)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][390/1291]	Time 0.099 (0.089)	Data 1.12e-04 (5.62e-04)	Tok/s 251775 (241977)	Loss/tok 3.2540 (3.2910)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.134 (0.089)	Data 1.03e-04 (5.50e-04)	Tok/s 260353 (242005)	Loss/tok 3.4639 (3.2901)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.068 (0.090)	Data 1.04e-04 (5.39e-04)	Tok/s 229763 (242274)	Loss/tok 3.0955 (3.2948)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.100 (0.090)	Data 1.03e-04 (5.29e-04)	Tok/s 255611 (242349)	Loss/tok 3.2710 (3.2948)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.099 (0.090)	Data 1.12e-04 (5.19e-04)	Tok/s 253493 (242348)	Loss/tok 3.3182 (3.2974)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.099 (0.090)	Data 1.05e-04 (5.10e-04)	Tok/s 252801 (242403)	Loss/tok 3.2914 (3.2969)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.099 (0.090)	Data 1.04e-04 (5.01e-04)	Tok/s 254258 (242354)	Loss/tok 3.3412 (3.2951)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.067 (0.090)	Data 1.06e-04 (4.93e-04)	Tok/s 232665 (242387)	Loss/tok 3.0960 (3.2959)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.100 (0.090)	Data 1.04e-04 (4.85e-04)	Tok/s 253941 (242372)	Loss/tok 3.3399 (3.2958)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.100 (0.090)	Data 1.06e-04 (4.77e-04)	Tok/s 249393 (242465)	Loss/tok 3.2575 (3.2998)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.099 (0.090)	Data 1.07e-04 (4.69e-04)	Tok/s 256517 (242529)	Loss/tok 3.2562 (3.2996)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.067 (0.090)	Data 1.03e-04 (4.62e-04)	Tok/s 235578 (242724)	Loss/tok 3.1001 (3.3008)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.066 (0.090)	Data 1.07e-04 (4.55e-04)	Tok/s 228520 (242785)	Loss/tok 3.0817 (3.3000)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][520/1291]	Time 0.035 (0.090)	Data 1.06e-04 (4.49e-04)	Tok/s 223494 (242690)	Loss/tok 2.6919 (3.2978)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.099 (0.090)	Data 1.05e-04 (4.42e-04)	Tok/s 255497 (242643)	Loss/tok 3.2321 (3.2982)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.067 (0.090)	Data 1.05e-04 (4.36e-04)	Tok/s 233157 (242561)	Loss/tok 2.9734 (3.2986)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.174 (0.090)	Data 1.07e-04 (4.30e-04)	Tok/s 256764 (242544)	Loss/tok 3.7509 (3.3002)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.135 (0.090)	Data 1.27e-04 (4.24e-04)	Tok/s 258154 (242608)	Loss/tok 3.5286 (3.2992)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.066 (0.090)	Data 1.14e-04 (4.19e-04)	Tok/s 231419 (242624)	Loss/tok 3.1626 (3.2989)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.099 (0.090)	Data 1.03e-04 (4.13e-04)	Tok/s 254851 (242630)	Loss/tok 3.2296 (3.2969)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.134 (0.090)	Data 1.04e-04 (4.09e-04)	Tok/s 261838 (242657)	Loss/tok 3.3953 (3.2974)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.067 (0.090)	Data 1.06e-04 (4.04e-04)	Tok/s 233305 (242641)	Loss/tok 3.1165 (3.2975)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.067 (0.090)	Data 1.27e-04 (3.99e-04)	Tok/s 230238 (242627)	Loss/tok 3.0830 (3.2963)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.067 (0.089)	Data 1.27e-04 (3.94e-04)	Tok/s 232482 (242480)	Loss/tok 3.0869 (3.2942)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.066 (0.090)	Data 1.04e-04 (3.90e-04)	Tok/s 239358 (242518)	Loss/tok 3.0472 (3.2955)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.099 (0.089)	Data 1.04e-04 (3.85e-04)	Tok/s 257424 (242474)	Loss/tok 3.1857 (3.2942)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][650/1291]	Time 0.067 (0.089)	Data 1.05e-04 (3.81e-04)	Tok/s 233024 (242496)	Loss/tok 3.0252 (3.2935)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.77e-04)	Tok/s 232009 (242535)	Loss/tok 3.0480 (3.2929)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.035 (0.089)	Data 1.05e-04 (3.73e-04)	Tok/s 218378 (242555)	Loss/tok 2.6355 (3.2920)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.69e-04)	Tok/s 233947 (242507)	Loss/tok 3.0577 (3.2922)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][690/1291]	Time 0.099 (0.089)	Data 1.06e-04 (3.65e-04)	Tok/s 253185 (242503)	Loss/tok 3.2936 (3.2913)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.067 (0.089)	Data 1.05e-04 (3.62e-04)	Tok/s 227619 (242518)	Loss/tok 3.1500 (3.2905)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.58e-04)	Tok/s 253433 (242596)	Loss/tok 3.2367 (3.2915)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.066 (0.089)	Data 1.06e-04 (3.55e-04)	Tok/s 235738 (242625)	Loss/tok 3.0790 (3.2917)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.174 (0.090)	Data 1.12e-04 (3.51e-04)	Tok/s 257024 (242677)	Loss/tok 3.6293 (3.2922)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.099 (0.090)	Data 1.07e-04 (3.48e-04)	Tok/s 254665 (242773)	Loss/tok 3.2818 (3.2924)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.066 (0.089)	Data 1.06e-04 (3.45e-04)	Tok/s 235559 (242659)	Loss/tok 3.0565 (3.2911)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.067 (0.090)	Data 1.05e-04 (3.42e-04)	Tok/s 225176 (242738)	Loss/tok 3.0924 (3.2940)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.066 (0.090)	Data 1.03e-04 (3.39e-04)	Tok/s 236796 (242674)	Loss/tok 3.0855 (3.2929)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.099 (0.089)	Data 1.05e-04 (3.36e-04)	Tok/s 254034 (242637)	Loss/tok 3.3223 (3.2921)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.134 (0.090)	Data 1.08e-04 (3.33e-04)	Tok/s 258551 (242678)	Loss/tok 3.5408 (3.2926)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.100 (0.090)	Data 1.04e-04 (3.30e-04)	Tok/s 252826 (242715)	Loss/tok 3.2238 (3.2922)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][810/1291]	Time 0.067 (0.090)	Data 1.09e-04 (3.27e-04)	Tok/s 228485 (242798)	Loss/tok 3.1294 (3.2929)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.066 (0.090)	Data 1.01e-04 (3.25e-04)	Tok/s 233906 (242775)	Loss/tok 3.0710 (3.2925)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.066 (0.090)	Data 1.04e-04 (3.22e-04)	Tok/s 235366 (242802)	Loss/tok 3.0706 (3.2916)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.134 (0.090)	Data 1.05e-04 (3.20e-04)	Tok/s 260444 (242794)	Loss/tok 3.3929 (3.2909)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][850/1291]	Time 0.067 (0.090)	Data 1.06e-04 (3.17e-04)	Tok/s 234217 (242850)	Loss/tok 3.0943 (3.2914)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.067 (0.090)	Data 1.05e-04 (3.15e-04)	Tok/s 225762 (242852)	Loss/tok 3.0870 (3.2907)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.067 (0.090)	Data 1.07e-04 (3.12e-04)	Tok/s 232260 (242791)	Loss/tok 3.0450 (3.2895)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.100 (0.090)	Data 1.03e-04 (3.10e-04)	Tok/s 251366 (242775)	Loss/tok 3.3579 (3.2893)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.100 (0.090)	Data 1.13e-04 (3.08e-04)	Tok/s 251469 (242819)	Loss/tok 3.3155 (3.2899)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.100 (0.090)	Data 1.07e-04 (3.06e-04)	Tok/s 255622 (242878)	Loss/tok 3.2015 (3.2908)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.090)	Data 1.08e-04 (3.03e-04)	Tok/s 233289 (242778)	Loss/tok 2.9472 (3.2901)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.01e-04)	Tok/s 232425 (242742)	Loss/tok 3.0215 (3.2895)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.135 (0.090)	Data 1.28e-04 (2.99e-04)	Tok/s 260506 (242804)	Loss/tok 3.4756 (3.2897)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.099 (0.089)	Data 1.24e-04 (2.97e-04)	Tok/s 252921 (242721)	Loss/tok 3.3865 (3.2888)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.100 (0.089)	Data 1.06e-04 (2.95e-04)	Tok/s 249427 (242695)	Loss/tok 3.3113 (3.2883)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.036 (0.089)	Data 1.04e-04 (2.93e-04)	Tok/s 224571 (242660)	Loss/tok 2.6100 (3.2880)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.099 (0.089)	Data 1.05e-04 (2.91e-04)	Tok/s 253489 (242678)	Loss/tok 3.2902 (3.2877)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][980/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.90e-04)	Tok/s 230484 (242679)	Loss/tok 3.0005 (3.2878)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.135 (0.089)	Data 1.03e-04 (2.88e-04)	Tok/s 255255 (242657)	Loss/tok 3.4824 (3.2883)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.134 (0.089)	Data 1.05e-04 (2.86e-04)	Tok/s 259636 (242699)	Loss/tok 3.4172 (3.2879)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.066 (0.089)	Data 1.04e-04 (2.84e-04)	Tok/s 233095 (242707)	Loss/tok 3.1439 (3.2871)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.066 (0.089)	Data 1.04e-04 (2.82e-04)	Tok/s 237497 (242746)	Loss/tok 3.0799 (3.2872)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.067 (0.089)	Data 1.01e-04 (2.81e-04)	Tok/s 234263 (242768)	Loss/tok 3.0495 (3.2872)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.066 (0.089)	Data 1.05e-04 (2.79e-04)	Tok/s 235591 (242750)	Loss/tok 3.1369 (3.2865)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.78e-04)	Tok/s 229144 (242774)	Loss/tok 3.0908 (3.2867)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.066 (0.089)	Data 1.05e-04 (2.76e-04)	Tok/s 235402 (242763)	Loss/tok 3.0271 (3.2855)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.74e-04)	Tok/s 234092 (242752)	Loss/tok 2.9685 (3.2848)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.173 (0.089)	Data 1.05e-04 (2.73e-04)	Tok/s 260091 (242773)	Loss/tok 3.5309 (3.2848)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.099 (0.089)	Data 1.06e-04 (2.72e-04)	Tok/s 254751 (242808)	Loss/tok 3.1976 (3.2852)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1100/1291]	Time 0.067 (0.089)	Data 1.04e-04 (2.70e-04)	Tok/s 230299 (242804)	Loss/tok 2.9678 (3.2847)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.036 (0.089)	Data 1.16e-04 (2.69e-04)	Tok/s 215122 (242762)	Loss/tok 2.6243 (3.2840)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1120/1291]	Time 0.134 (0.089)	Data 1.07e-04 (2.67e-04)	Tok/s 258912 (242834)	Loss/tok 3.5256 (3.2856)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.067 (0.089)	Data 1.06e-04 (2.66e-04)	Tok/s 238032 (242824)	Loss/tok 3.0674 (3.2848)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.099 (0.089)	Data 1.03e-04 (2.64e-04)	Tok/s 253309 (242847)	Loss/tok 3.2807 (3.2853)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.099 (0.089)	Data 1.05e-04 (2.63e-04)	Tok/s 251799 (242820)	Loss/tok 3.2493 (3.2848)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.62e-04)	Tok/s 234742 (242786)	Loss/tok 3.0213 (3.2846)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.098 (0.089)	Data 1.08e-04 (2.61e-04)	Tok/s 254448 (242762)	Loss/tok 3.2895 (3.2837)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.59e-04)	Tok/s 253674 (242760)	Loss/tok 3.2427 (3.2837)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.099 (0.089)	Data 1.07e-04 (2.58e-04)	Tok/s 254893 (242786)	Loss/tok 3.3132 (3.2836)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.57e-04)	Tok/s 231655 (242735)	Loss/tok 3.0784 (3.2826)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.099 (0.089)	Data 1.06e-04 (2.55e-04)	Tok/s 253643 (242721)	Loss/tok 3.2251 (3.2820)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.100 (0.089)	Data 1.18e-04 (2.54e-04)	Tok/s 251462 (242737)	Loss/tok 3.2306 (3.2819)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.53e-04)	Tok/s 233551 (242678)	Loss/tok 3.0444 (3.2809)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.100 (0.089)	Data 1.33e-04 (2.52e-04)	Tok/s 251509 (242686)	Loss/tok 3.3059 (3.2805)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1250/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.51e-04)	Tok/s 248793 (242694)	Loss/tok 3.2757 (3.2808)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.067 (0.089)	Data 1.06e-04 (2.50e-04)	Tok/s 234875 (242653)	Loss/tok 3.0769 (3.2798)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.066 (0.089)	Data 1.29e-04 (2.49e-04)	Tok/s 231844 (242616)	Loss/tok 3.0603 (3.2792)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.174 (0.089)	Data 1.05e-04 (2.48e-04)	Tok/s 257586 (242681)	Loss/tok 3.5918 (3.2798)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.067 (0.089)	Data 4.41e-05 (2.49e-04)	Tok/s 235977 (242658)	Loss/tok 2.9823 (3.2796)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593143578248, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593143578248, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.462 (0.462)	Decoder iters 128.0 (128.0)	Tok/s 35539 (35539)
0: Running moses detokenizer
0: BLEU(score=22.717354404812056, counts=[35738, 17483, 9758, 5664], totals=[63583, 60580, 57577, 54578], precisions=[56.20684774232106, 28.859359524595575, 16.94773954877816, 10.377807907948258], bp=0.9829567778483522, sys_len=63583, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593143580210, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22719999999999999, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593143580210, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2793	Test BLEU: 22.72
0: Performance: Epoch: 2	Training: 1941922 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593143580210, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593143580211, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593143580211, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3153188411
0: TRAIN [3][0/1291]	Time 0.261 (0.261)	Data 1.92e-01 (1.92e-01)	Tok/s 60464 (60464)	Loss/tok 2.9173 (2.9173)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.066 (0.098)	Data 1.25e-04 (1.75e-02)	Tok/s 230928 (218695)	Loss/tok 2.9672 (3.1087)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.067 (0.091)	Data 1.20e-04 (9.25e-03)	Tok/s 228301 (229078)	Loss/tok 2.9931 (3.1163)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.134 (0.091)	Data 1.26e-04 (6.31e-03)	Tok/s 259848 (233608)	Loss/tok 3.4697 (3.1728)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.066 (0.094)	Data 1.31e-04 (4.80e-03)	Tok/s 232676 (237519)	Loss/tok 3.0056 (3.2063)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.069 (0.096)	Data 1.22e-04 (3.88e-03)	Tok/s 224573 (239193)	Loss/tok 3.0400 (3.2208)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.102 (0.094)	Data 1.39e-04 (3.27e-03)	Tok/s 246859 (239480)	Loss/tok 3.2125 (3.2112)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.099 (0.094)	Data 1.20e-04 (2.82e-03)	Tok/s 251242 (240399)	Loss/tok 3.2477 (3.2134)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][80/1291]	Time 0.067 (0.093)	Data 1.14e-04 (2.49e-03)	Tok/s 233421 (240228)	Loss/tok 2.9323 (3.2082)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.099 (0.092)	Data 1.18e-04 (2.23e-03)	Tok/s 255486 (240269)	Loss/tok 3.1518 (3.1971)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.036 (0.091)	Data 1.22e-04 (2.02e-03)	Tok/s 222642 (240638)	Loss/tok 2.5891 (3.1939)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.173 (0.093)	Data 1.16e-04 (1.85e-03)	Tok/s 257088 (241635)	Loss/tok 3.6084 (3.2080)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.067 (0.092)	Data 1.25e-04 (1.71e-03)	Tok/s 231542 (241371)	Loss/tok 3.0114 (3.2009)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.067 (0.091)	Data 1.39e-04 (1.59e-03)	Tok/s 232169 (241395)	Loss/tok 3.0121 (3.1961)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.035 (0.090)	Data 1.18e-04 (1.48e-03)	Tok/s 223656 (241444)	Loss/tok 2.5781 (3.1899)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][150/1291]	Time 0.135 (0.091)	Data 1.23e-04 (1.39e-03)	Tok/s 257754 (241615)	Loss/tok 3.4689 (3.1996)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.067 (0.091)	Data 1.18e-04 (1.31e-03)	Tok/s 228917 (241565)	Loss/tok 3.0718 (3.1945)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.067 (0.090)	Data 1.20e-04 (1.24e-03)	Tok/s 236712 (241406)	Loss/tok 2.9872 (3.1925)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.175 (0.091)	Data 1.19e-04 (1.18e-03)	Tok/s 255720 (241840)	Loss/tok 3.5267 (3.1970)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.135 (0.091)	Data 1.27e-04 (1.12e-03)	Tok/s 260407 (241830)	Loss/tok 3.3543 (3.1944)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][200/1291]	Time 0.134 (0.091)	Data 1.19e-04 (1.07e-03)	Tok/s 259216 (241833)	Loss/tok 3.3353 (3.1969)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.099 (0.090)	Data 1.18e-04 (1.03e-03)	Tok/s 255772 (241590)	Loss/tok 3.2383 (3.1929)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.067 (0.090)	Data 1.25e-04 (9.89e-04)	Tok/s 231362 (241755)	Loss/tok 2.9417 (3.1928)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.174 (0.090)	Data 1.18e-04 (9.51e-04)	Tok/s 259225 (241883)	Loss/tok 3.4202 (3.1935)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.067 (0.090)	Data 1.19e-04 (9.17e-04)	Tok/s 239867 (241860)	Loss/tok 2.9538 (3.1932)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.134 (0.090)	Data 1.15e-04 (8.85e-04)	Tok/s 259600 (242170)	Loss/tok 3.4793 (3.1941)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.036 (0.089)	Data 1.17e-04 (8.56e-04)	Tok/s 226052 (242018)	Loss/tok 2.5492 (3.1897)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.099 (0.089)	Data 1.17e-04 (8.29e-04)	Tok/s 255778 (242073)	Loss/tok 3.1977 (3.1864)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.067 (0.089)	Data 1.40e-04 (8.04e-04)	Tok/s 230744 (242063)	Loss/tok 2.9502 (3.1885)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.099 (0.089)	Data 1.22e-04 (7.81e-04)	Tok/s 253763 (242135)	Loss/tok 3.2061 (3.1864)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.036 (0.089)	Data 1.34e-04 (7.59e-04)	Tok/s 220183 (242190)	Loss/tok 2.6074 (3.1859)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.175 (0.090)	Data 1.33e-04 (7.38e-04)	Tok/s 254879 (242304)	Loss/tok 3.4637 (3.1862)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.067 (0.090)	Data 1.19e-04 (7.19e-04)	Tok/s 232034 (242415)	Loss/tok 3.0321 (3.1858)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][330/1291]	Time 0.067 (0.090)	Data 1.31e-04 (7.01e-04)	Tok/s 231408 (242417)	Loss/tok 2.9549 (3.1829)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.067 (0.089)	Data 1.18e-04 (6.84e-04)	Tok/s 226595 (242360)	Loss/tok 3.0350 (3.1803)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.099 (0.089)	Data 1.29e-04 (6.68e-04)	Tok/s 256087 (242502)	Loss/tok 3.1220 (3.1797)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.100 (0.090)	Data 1.20e-04 (6.53e-04)	Tok/s 249147 (242586)	Loss/tok 3.1257 (3.1819)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.134 (0.090)	Data 1.34e-04 (6.39e-04)	Tok/s 259114 (242727)	Loss/tok 3.3997 (3.1830)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.067 (0.090)	Data 1.27e-04 (6.25e-04)	Tok/s 230771 (242651)	Loss/tok 2.9771 (3.1807)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.099 (0.090)	Data 1.29e-04 (6.12e-04)	Tok/s 256645 (242680)	Loss/tok 3.1319 (3.1787)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.099 (0.089)	Data 1.39e-04 (6.00e-04)	Tok/s 254671 (242534)	Loss/tok 3.1645 (3.1760)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.067 (0.089)	Data 1.37e-04 (5.88e-04)	Tok/s 233691 (242465)	Loss/tok 3.0238 (3.1734)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.067 (0.089)	Data 1.36e-04 (5.77e-04)	Tok/s 234931 (242597)	Loss/tok 2.9555 (3.1754)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.134 (0.089)	Data 1.35e-04 (5.67e-04)	Tok/s 260716 (242633)	Loss/tok 3.2363 (3.1735)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.135 (0.089)	Data 1.20e-04 (5.57e-04)	Tok/s 257294 (242794)	Loss/tok 3.3460 (3.1744)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][450/1291]	Time 0.036 (0.089)	Data 1.26e-04 (5.47e-04)	Tok/s 220104 (242765)	Loss/tok 2.5740 (3.1752)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.099 (0.089)	Data 1.23e-04 (5.38e-04)	Tok/s 254200 (242851)	Loss/tok 3.1923 (3.1752)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.099 (0.090)	Data 1.40e-04 (5.29e-04)	Tok/s 253587 (242900)	Loss/tok 3.1384 (3.1748)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.067 (0.089)	Data 1.30e-04 (5.21e-04)	Tok/s 233572 (242824)	Loss/tok 2.9342 (3.1736)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.134 (0.090)	Data 1.15e-04 (5.13e-04)	Tok/s 259916 (242894)	Loss/tok 3.3719 (3.1756)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.099 (0.090)	Data 1.16e-04 (5.05e-04)	Tok/s 256361 (242964)	Loss/tok 3.1297 (3.1745)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.067 (0.089)	Data 1.18e-04 (4.98e-04)	Tok/s 231504 (242890)	Loss/tok 2.9101 (3.1728)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.100 (0.089)	Data 1.16e-04 (4.90e-04)	Tok/s 248718 (242818)	Loss/tok 3.1809 (3.1723)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.134 (0.089)	Data 1.18e-04 (4.83e-04)	Tok/s 261115 (242785)	Loss/tok 3.3305 (3.1713)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.099 (0.089)	Data 1.17e-04 (4.77e-04)	Tok/s 255721 (242886)	Loss/tok 3.0531 (3.1725)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.099 (0.089)	Data 1.17e-04 (4.70e-04)	Tok/s 253466 (242812)	Loss/tok 3.2178 (3.1703)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.134 (0.089)	Data 1.17e-04 (4.64e-04)	Tok/s 259025 (242868)	Loss/tok 3.2878 (3.1703)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.066 (0.089)	Data 1.18e-04 (4.58e-04)	Tok/s 231727 (242835)	Loss/tok 2.9195 (3.1695)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][580/1291]	Time 0.100 (0.089)	Data 1.16e-04 (4.52e-04)	Tok/s 251160 (242872)	Loss/tok 3.1577 (3.1698)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.173 (0.089)	Data 1.17e-04 (4.47e-04)	Tok/s 258714 (242781)	Loss/tok 3.5061 (3.1688)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.067 (0.089)	Data 1.18e-04 (4.41e-04)	Tok/s 232124 (242749)	Loss/tok 3.0062 (3.1676)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.135 (0.089)	Data 1.16e-04 (4.36e-04)	Tok/s 261717 (242818)	Loss/tok 3.2738 (3.1674)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.067 (0.089)	Data 1.16e-04 (4.31e-04)	Tok/s 230859 (242800)	Loss/tok 2.9803 (3.1664)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.036 (0.089)	Data 1.20e-04 (4.26e-04)	Tok/s 218104 (242747)	Loss/tok 2.6586 (3.1656)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.067 (0.089)	Data 1.17e-04 (4.21e-04)	Tok/s 230787 (242728)	Loss/tok 2.9606 (3.1658)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.066 (0.089)	Data 1.20e-04 (4.17e-04)	Tok/s 230853 (242667)	Loss/tok 3.0377 (3.1644)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.101 (0.088)	Data 1.16e-04 (4.12e-04)	Tok/s 248604 (242619)	Loss/tok 3.1173 (3.1631)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.066 (0.089)	Data 1.17e-04 (4.08e-04)	Tok/s 233463 (242704)	Loss/tok 2.8369 (3.1645)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.036 (0.089)	Data 1.17e-04 (4.04e-04)	Tok/s 219901 (242710)	Loss/tok 2.5630 (3.1649)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.099 (0.089)	Data 1.16e-04 (4.00e-04)	Tok/s 252793 (242689)	Loss/tok 3.2216 (3.1646)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.099 (0.089)	Data 1.29e-04 (3.96e-04)	Tok/s 255319 (242692)	Loss/tok 3.1885 (3.1663)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][710/1291]	Time 0.067 (0.089)	Data 1.39e-04 (3.92e-04)	Tok/s 232200 (242815)	Loss/tok 2.9503 (3.1672)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.067 (0.089)	Data 1.41e-04 (3.88e-04)	Tok/s 229986 (242730)	Loss/tok 2.9684 (3.1655)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.067 (0.089)	Data 1.24e-04 (3.84e-04)	Tok/s 234976 (242748)	Loss/tok 3.0166 (3.1659)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.067 (0.089)	Data 1.20e-04 (3.81e-04)	Tok/s 234475 (242724)	Loss/tok 2.9270 (3.1656)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.066 (0.089)	Data 1.19e-04 (3.77e-04)	Tok/s 229968 (242768)	Loss/tok 2.9963 (3.1653)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.100 (0.089)	Data 1.24e-04 (3.74e-04)	Tok/s 255503 (242817)	Loss/tok 3.1816 (3.1657)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.036 (0.089)	Data 1.22e-04 (3.71e-04)	Tok/s 220336 (242797)	Loss/tok 2.5424 (3.1651)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.135 (0.089)	Data 1.17e-04 (3.67e-04)	Tok/s 258208 (242766)	Loss/tok 3.1911 (3.1642)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.067 (0.089)	Data 1.21e-04 (3.64e-04)	Tok/s 230741 (242769)	Loss/tok 2.8963 (3.1649)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.100 (0.089)	Data 1.25e-04 (3.61e-04)	Tok/s 251921 (242850)	Loss/tok 3.1484 (3.1650)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.173 (0.089)	Data 1.18e-04 (3.58e-04)	Tok/s 260406 (242834)	Loss/tok 3.4329 (3.1648)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.135 (0.089)	Data 1.18e-04 (3.55e-04)	Tok/s 260917 (242864)	Loss/tok 3.2111 (3.1641)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.53e-04)	Tok/s 256678 (242926)	Loss/tok 3.1223 (3.1640)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][840/1291]	Time 0.067 (0.089)	Data 1.22e-04 (3.50e-04)	Tok/s 227281 (242847)	Loss/tok 2.9354 (3.1626)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.47e-04)	Tok/s 233657 (242913)	Loss/tok 2.9151 (3.1632)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.45e-04)	Tok/s 230899 (242884)	Loss/tok 2.9942 (3.1629)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.135 (0.089)	Data 1.41e-04 (3.42e-04)	Tok/s 261905 (242829)	Loss/tok 3.2445 (3.1616)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.39e-04)	Tok/s 255981 (242922)	Loss/tok 3.0995 (3.1621)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.37e-04)	Tok/s 255590 (242892)	Loss/tok 3.1935 (3.1609)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.35e-04)	Tok/s 230920 (242840)	Loss/tok 2.9937 (3.1596)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.067 (0.089)	Data 1.20e-04 (3.32e-04)	Tok/s 229138 (242791)	Loss/tok 3.0427 (3.1586)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.30e-04)	Tok/s 232471 (242830)	Loss/tok 2.9295 (3.1580)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.036 (0.089)	Data 1.23e-04 (3.28e-04)	Tok/s 221690 (242809)	Loss/tok 2.6569 (3.1569)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.174 (0.089)	Data 1.28e-04 (3.26e-04)	Tok/s 256574 (242794)	Loss/tok 3.4026 (3.1562)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.174 (0.089)	Data 1.18e-04 (3.24e-04)	Tok/s 257708 (242820)	Loss/tok 3.4862 (3.1574)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.067 (0.089)	Data 1.26e-04 (3.22e-04)	Tok/s 232118 (242811)	Loss/tok 2.8622 (3.1568)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][970/1291]	Time 0.036 (0.089)	Data 1.49e-04 (3.20e-04)	Tok/s 222493 (242772)	Loss/tok 2.5277 (3.1562)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.18e-04)	Tok/s 233166 (242836)	Loss/tok 2.9060 (3.1563)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.134 (0.089)	Data 1.33e-04 (3.16e-04)	Tok/s 260018 (242890)	Loss/tok 3.4761 (3.1563)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.099 (0.089)	Data 1.21e-04 (3.14e-04)	Tok/s 253362 (242888)	Loss/tok 3.0821 (3.1553)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.066 (0.089)	Data 1.42e-04 (3.12e-04)	Tok/s 235390 (242887)	Loss/tok 2.9125 (3.1552)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.067 (0.089)	Data 1.20e-04 (3.10e-04)	Tok/s 234166 (242900)	Loss/tok 3.0073 (3.1562)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.036 (0.089)	Data 1.20e-04 (3.08e-04)	Tok/s 222516 (242884)	Loss/tok 2.4932 (3.1557)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.067 (0.089)	Data 1.20e-04 (3.06e-04)	Tok/s 229059 (242887)	Loss/tok 2.9131 (3.1549)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.05e-04)	Tok/s 233405 (242892)	Loss/tok 2.9502 (3.1539)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.036 (0.089)	Data 1.17e-04 (3.03e-04)	Tok/s 220535 (242952)	Loss/tok 2.4988 (3.1538)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.100 (0.089)	Data 1.19e-04 (3.01e-04)	Tok/s 250414 (242980)	Loss/tok 3.1017 (3.1529)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.99e-04)	Tok/s 232079 (242927)	Loss/tok 2.9015 (3.1517)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1090/1291]	Time 0.135 (0.089)	Data 1.25e-04 (2.98e-04)	Tok/s 258772 (242960)	Loss/tok 3.2718 (3.1520)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.96e-04)	Tok/s 253090 (243025)	Loss/tok 3.1098 (3.1520)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.067 (0.089)	Data 1.18e-04 (2.95e-04)	Tok/s 230103 (243004)	Loss/tok 2.9349 (3.1519)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.100 (0.089)	Data 1.32e-04 (2.93e-04)	Tok/s 253152 (242962)	Loss/tok 3.0613 (3.1513)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.066 (0.089)	Data 1.17e-04 (2.92e-04)	Tok/s 233540 (242989)	Loss/tok 2.9175 (3.1515)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.90e-04)	Tok/s 228127 (242979)	Loss/tok 2.5679 (3.1511)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.89e-04)	Tok/s 230467 (242926)	Loss/tok 2.9131 (3.1501)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.100 (0.089)	Data 1.18e-04 (2.87e-04)	Tok/s 254531 (242888)	Loss/tok 3.1264 (3.1497)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.067 (0.089)	Data 1.37e-04 (2.86e-04)	Tok/s 233132 (242886)	Loss/tok 2.9420 (3.1496)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.036 (0.089)	Data 1.15e-04 (2.84e-04)	Tok/s 220984 (242903)	Loss/tok 2.5496 (3.1491)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.067 (0.089)	Data 1.18e-04 (2.83e-04)	Tok/s 229638 (242940)	Loss/tok 2.9599 (3.1495)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.099 (0.089)	Data 1.40e-04 (2.82e-04)	Tok/s 253052 (242912)	Loss/tok 3.1568 (3.1487)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.099 (0.089)	Data 1.19e-04 (2.80e-04)	Tok/s 255064 (242893)	Loss/tok 3.0821 (3.1482)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1220/1291]	Time 0.100 (0.089)	Data 1.18e-04 (2.79e-04)	Tok/s 254065 (242920)	Loss/tok 3.0698 (3.1479)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.067 (0.089)	Data 1.21e-04 (2.78e-04)	Tok/s 233630 (242903)	Loss/tok 2.9454 (3.1471)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.100 (0.089)	Data 1.18e-04 (2.76e-04)	Tok/s 252904 (242960)	Loss/tok 3.1171 (3.1477)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.134 (0.089)	Data 1.18e-04 (2.75e-04)	Tok/s 260914 (242919)	Loss/tok 3.1758 (3.1473)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.135 (0.089)	Data 1.16e-04 (2.74e-04)	Tok/s 260949 (242926)	Loss/tok 3.1961 (3.1468)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.134 (0.089)	Data 1.19e-04 (2.73e-04)	Tok/s 261161 (242918)	Loss/tok 3.1702 (3.1470)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.72e-04)	Tok/s 230837 (242883)	Loss/tok 2.8544 (3.1464)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.098 (0.089)	Data 4.29e-05 (2.73e-04)	Tok/s 257522 (242844)	Loss/tok 3.1413 (3.1456)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593143695471, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593143695472, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.403 (0.403)	Decoder iters 110.0 (110.0)	Tok/s 40679 (40679)
0: Running moses detokenizer
0: BLEU(score=24.01744956730964, counts=[36976, 18430, 10449, 6212], totals=[64979, 61976, 58973, 55975], precisions=[56.90453838932578, 29.737317671356653, 17.718277855967987, 11.09781152300134], bp=1.0, sys_len=64979, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593143697464, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2402, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593143697464, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1479	Test BLEU: 24.02
0: Performance: Epoch: 3	Training: 1942684 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593143697465, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593143697465, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
ENDING TIMING RUN AT 2020-06-26 03:55:04 AM
RESULT,RNN_TRANSLATOR,,504,nvidia,2020-06-26 03:46:40 AM
